2023-04-20 10:51:24.397121 (MainThread): Running with dbt=1.4.6
2023-04-20 10:51:24.829998 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt_rpc.task.server.RPCServerTask'>, debug=None, defer=None, exclude=None, fail_fast=None, host='0.0.0.0', log_cache_events=False, log_format=None, models=None, partial_parse=True, port=8580, printer_width=None, profile='user', profiles_dir='/usr/src/develop/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, send_anonymous_usage_stats=None, single_threaded=False, state=None, static_parser=None, target=None, threads=None, use_colors=None, use_experimental_parser=None, vars='{}', version_check=None, warn_error=None, which='rpc', write_json=None)
2023-04-20 10:51:24.842670 (MainThread): Tracking: tracking
2023-04-20 10:51:24.844251 (MainThread): 10:51:24  Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb57fecb820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5699efa60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5699ef5e0>]}
2023-04-20 10:51:24.844621 (MainThread): Serving RPC server at 0.0.0.0:8580, pid=39
2023-04-20 10:51:24.844937 (MainThread): Supported methods: ['build', 'cli_args', 'compile', 'compile_sql', 'deps', 'docs.generate', 'gc', 'get-manifest', 'kill', 'list', 'poll', 'ps', 'run', 'run-operation', 'run_sql', 'seed', 'snapshot', 'snapshot-freshness', 'source-freshness', 'status', 'test']
2023-04-20 10:51:24.845108 (MainThread): Send requests to http://localhost:8580/jsonrpc
2023-04-20 10:51:28.054058 (Thread-13): handling status request
2023-04-20 10:51:28.054550 (Thread-13): 10:51:28  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5699f9550>]}
2023-04-20 10:51:28.055147 (Thread-13): sending response (<Response 339 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:28.845769 (Thread-14): handling ps request
2023-04-20 10:51:28.846285 (Thread-14): 10:51:28  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5699f9700>]}
2023-04-20 10:51:28.846743 (Thread-14): sending response (<Response 100 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:29.286980 (Thread-15): handling status request
2023-04-20 10:51:29.287483 (Thread-15): 10:51:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5699f98b0>]}
2023-04-20 10:51:29.287959 (Thread-15): sending response (<Response 339 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:39.891489 (Thread-16): handling deps request
2023-04-20 10:51:39.893479 (Thread-16): 10:51:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5699f9940>]}
2023-04-20 10:51:40.020563 (Thread-16): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:40.581551 (Thread-17): handling status request
2023-04-20 10:51:40.582222 (Thread-17): 10:51:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a21d60>]}
2023-04-20 10:51:40.582891 (Thread-17): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:40.636084 (Thread-18): handling ps request
2023-04-20 10:51:40.636563 (Thread-18): 10:51:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a21280>]}
2023-04-20 10:51:40.637138 (Thread-18): sending response (<Response 465 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:40.760029 (Thread-19): handling poll request
2023-04-20 10:51:40.760515 (Thread-19): 10:51:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a21940>]}
2023-04-20 10:51:40.761049 (Thread-19): sending response (<Response 279 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:41.095434 (Thread-20): handling status request
2023-04-20 10:51:41.095949 (Thread-20): 10:51:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a21580>]}
2023-04-20 10:51:41.096363 (Thread-20): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:42.056507 (Thread-21): handling status request
2023-04-20 10:51:42.057004 (Thread-21): 10:51:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a215b0>]}
2023-04-20 10:51:42.057423 (Thread-21): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:42.408016 (Thread-22): handling poll request
2023-04-20 10:51:42.408811 (Thread-22): 10:51:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a219a0>]}
2023-04-20 10:51:42.409573 (Thread-22): sending response (<Response 279 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:42.805425 (MainThread): 10:51:42  Set downloads directory='/tmp/dbt-downloads-px3mtwmk'
2023-04-20 10:51:42.836156 (MainThread): 10:51:42  Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
2023-04-20 10:51:42.859107 (MainThread): 10:51:42  Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
2023-04-20 10:51:42.859464 (MainThread): 10:51:42  Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
2023-04-20 10:51:42.873893 (MainThread): 10:51:42  Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
2023-04-20 10:51:42.880049 (MainThread): 10:51:42  Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/spark_utils.json
2023-04-20 10:51:42.894075 (MainThread): 10:51:42  Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/spark_utils.json 200
2023-04-20 10:51:42.900879 (MainThread): 10:51:42  Installing dbt-labs/dbt_utils
2023-04-20 10:51:43.007994 (Thread-23): handling status request
2023-04-20 10:51:43.008483 (Thread-23): 10:51:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a21fd0>]}
2023-04-20 10:51:43.008901 (Thread-23): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:43.333801 (Thread-24): handling ps request
2023-04-20 10:51:43.334297 (Thread-24): 10:51:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56998aa90>]}
2023-04-20 10:51:43.334788 (Thread-24): sending response (<Response 465 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:44.324803 (Thread-25): handling status request
2023-04-20 10:51:44.325329 (Thread-25): 10:51:44  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56998a700>]}
2023-04-20 10:51:44.325777 (Thread-25): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:44.443736 (Thread-26): handling poll request
2023-04-20 10:51:44.444236 (Thread-26): 10:51:44  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56998adf0>]}
2023-04-20 10:51:44.444784 (Thread-26): sending response (<Response 2825 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:45.265859 (Thread-27): handling status request
2023-04-20 10:51:45.266349 (Thread-27): 10:51:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56998ad00>]}
2023-04-20 10:51:45.266763 (Thread-27): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:46.067842 (Thread-28): handling poll request
2023-04-20 10:51:46.068344 (Thread-28): 10:51:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56998aee0>]}
2023-04-20 10:51:46.068823 (Thread-28): sending response (<Response 279 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:46.218422 (Thread-29): handling status request
2023-04-20 10:51:46.218867 (Thread-29): 10:51:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56998a280>]}
2023-04-20 10:51:46.219265 (Thread-29): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:46.475490 (Thread-30): handling ps request
2023-04-20 10:51:46.476061 (Thread-30): 10:51:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a21220>]}
2023-04-20 10:51:46.476564 (Thread-30): sending response (<Response 465 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:46.928489 (Thread-31): handling status request
2023-04-20 10:51:46.928995 (Thread-31): 10:51:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56998a850>]}
2023-04-20 10:51:46.929425 (Thread-31): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:47.482463 (MainThread): 10:51:47    Installed from version 0.8.0
2023-04-20 10:51:47.482817 (MainThread): 10:51:47    Updated version available: 1.0.0
2023-04-20 10:51:47.483179 (MainThread): 10:51:47  Sending event: {'category': 'dbt', 'action': 'package', 'label': '25bbcb94-0a47-46e4-babe-6ef82036ec83', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fde11daf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fde13d1f0>]}
2023-04-20 10:51:47.483676 (MainThread): 10:51:47  Installing dbt-labs/spark_utils
2023-04-20 10:51:47.737119 (Thread-32): handling poll request
2023-04-20 10:51:47.737617 (Thread-32): 10:51:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56998aca0>]}
2023-04-20 10:51:47.738152 (Thread-32): sending response (<Response 1656 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:47.851289 (Thread-33): handling status request
2023-04-20 10:51:47.852195 (Thread-33): 10:51:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56998a190>]}
2023-04-20 10:51:47.852589 (Thread-33): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:48.264640 (MainThread): 10:51:48    Installed from version 0.3.0
2023-04-20 10:51:48.264846 (MainThread): 10:51:48    Up to date!
2023-04-20 10:51:48.265049 (MainThread): 10:51:48  Sending event: {'category': 'dbt', 'action': 'package', 'label': '25bbcb94-0a47-46e4-babe-6ef82036ec83', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fde13dd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fde13dca0>]}
2023-04-20 10:51:48.265304 (MainThread): 10:51:48  
2023-04-20 10:51:48.265439 (MainThread): 10:51:48  Updates available for packages: ['dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
2023-04-20 10:51:48.518302 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:48  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 10:51:48.526374 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:48  Unable to do partial parsing because saved manifest not found. Starting full parse.
2023-04-20 10:51:48.526723 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:48  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a1b0a0>]}
2023-04-20 10:51:48.820412 (Thread-34): handling status request
2023-04-20 10:51:48.820947 (Thread-34): 10:51:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a21dc0>]}
2023-04-20 10:51:48.821368 (Thread-34): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:49.110348 (Thread-35): handling ps request
2023-04-20 10:51:49.131257 (Thread-35): 10:51:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5698fddf0>]}
2023-04-20 10:51:49.136930 (Thread-35): sending response (<Response 465 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:49.493967 (Thread-36): handling poll request
2023-04-20 10:51:49.514853 (Thread-36): 10:51:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56990b4f0>]}
2023-04-20 10:51:49.520540 (Thread-36): sending response (<Response 1975 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:49.608580 (Thread-37): handling status request
2023-04-20 10:51:49.619283 (Thread-37): 10:51:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56991f3a0>]}
2023-04-20 10:51:49.630050 (Thread-37): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:50.487385 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 10:51:50.501353 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 10:51:50.504705 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 10:51:50.507644 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 10:51:50.510545 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 10:51:50.513540 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 10:51:50.516539 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 10:51:50.519942 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 10:51:50.522970 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 10:51:50.525858 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 10:51:50.528820 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 10:51:50.532115 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 10:51:50.534797 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 10:51:50.537709 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 10:51:50.540675 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 10:51:50.565926 (Thread-38): handling status request
2023-04-20 10:51:50.566352 (Thread-38): 10:51:50  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5622ab490>]}
2023-04-20 10:51:50.566765 (Thread-38): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:50.714672 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56229f9a0>]}
2023-04-20 10:51:51.089000 (Thread-39): handling poll request
2023-04-20 10:51:51.089545 (Thread-39): 10:51:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569997fa0>]}
2023-04-20 10:51:51.111633 (Thread-39): sending response (<Response 352 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:51.473764 (Thread-40): handling status request
2023-04-20 10:51:51.474270 (Thread-40): 10:51:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56229ffd0>]}
2023-04-20 10:51:51.474855 (Thread-40): sending response (<Response 5782 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:51.776548 (Thread-41): handling ps request
2023-04-20 10:51:51.777066 (Thread-41): 10:51:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56229fa90>]}
2023-04-20 10:51:51.777585 (Thread-41): sending response (<Response 491 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:52.203659 (Thread-42): handling status request
2023-04-20 10:51:52.204151 (Thread-42): 10:51:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56229f880>]}
2023-04-20 10:51:52.204737 (Thread-42): sending response (<Response 5782 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:57.366942 (Thread-43): 10:51:57  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 10:51:57.724306 (Thread-44): handling status request
2023-04-20 10:51:57.724928 (Thread-44): 10:51:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa11ca0>]}
2023-04-20 10:51:57.725370 (Thread-44): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:57.782516 (Thread-43): 10:51:57  Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
2023-04-20 10:51:57.782788 (Thread-43): 10:51:57  Partial parsing enabled, no changes found, skipping parsing
2023-04-20 10:51:57.791062 (Thread-43): 10:51:57  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f979f40>]}
2023-04-20 10:51:58.969279 (Thread-45): handling status request
2023-04-20 10:51:58.969776 (Thread-45): 10:51:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa11d00>]}
2023-04-20 10:51:58.970260 (Thread-45): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:04.030870 (Thread-46): handling ps request
2023-04-20 10:52:04.031366 (Thread-46): 10:52:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa11af0>]}
2023-04-20 10:52:04.031897 (Thread-46): sending response (<Response 491 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:04.042442 (Thread-47): handling ps request
2023-04-20 10:52:04.042775 (Thread-47): 10:52:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa11e80>]}
2023-04-20 10:52:04.043161 (Thread-47): sending response (<Response 491 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:04.232933 (Thread-48): handling status request
2023-04-20 10:52:04.233423 (Thread-48): 10:52:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa119d0>]}
2023-04-20 10:52:04.233906 (Thread-48): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:04.375629 (Thread-49): handling status request
2023-04-20 10:52:04.376115 (Thread-49): 10:52:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa11a00>]}
2023-04-20 10:52:04.376573 (Thread-49): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:04.381145 (Thread-50): handling list request
2023-04-20 10:52:04.381464 (Thread-50): 10:52:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa11730>]}
2023-04-20 10:52:04.410242 (Thread-50): 10:52:04  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa11100>]}
2023-04-20 10:52:04.410734 (Thread-50): 10:52:04  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 10:52:04.411071 (Thread-50): 10:52:04  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb57b790be0>]}
2023-04-20 10:52:04.413486 (Thread-50): sending response (<Response 5675 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:13.699783 (Thread-51): handling status request
2023-04-20 10:52:13.700295 (Thread-51): 10:52:13  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa11f40>]}
2023-04-20 10:52:13.700792 (Thread-51): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:17.159658 (Thread-52): handling status request
2023-04-20 10:52:17.160165 (Thread-52): 10:52:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa11c10>]}
2023-04-20 10:52:17.160635 (Thread-52): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:17.494576 (Thread-53): handling cli_args request
2023-04-20 10:52:17.495070 (Thread-53): 10:52:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5699efeb0>]}
2023-04-20 10:52:20.242811 (Thread-53): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:20.304440 (MainThread): 10:52:20  checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
2023-04-20 10:52:20.326492 (MainThread): 10:52:20  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 10:52:20.326671 (MainThread): 10:52:20  previous checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, current checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd
2023-04-20 10:52:20.326827 (MainThread): 10:52:20  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4fe7467ca0>]}
2023-04-20 10:52:20.908293 (Thread-54): handling ps request
2023-04-20 10:52:20.908951 (Thread-54): 10:52:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa1ad00>]}
2023-04-20 10:52:20.909843 (Thread-54): sending response (<Response 1164 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:20.965186 (Thread-55): handling poll request
2023-04-20 10:52:20.965647 (Thread-55): 10:52:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa1aa00>]}
2023-04-20 10:52:20.966222 (Thread-55): sending response (<Response 1859 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:21.210435 (Thread-56): handling status request
2023-04-20 10:52:21.210913 (Thread-56): 10:52:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa1ab50>]}
2023-04-20 10:52:21.211414 (Thread-56): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:21.299685 (Thread-57): handling ps request
2023-04-20 10:52:21.300152 (Thread-57): 10:52:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa1ac10>]}
2023-04-20 10:52:21.300698 (Thread-57): sending response (<Response 1165 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:22.033183 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 10:52:22.052164 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 10:52:22.056773 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 10:52:22.061244 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 10:52:22.065803 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 10:52:22.070276 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 10:52:22.076233 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 10:52:22.081096 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 10:52:22.085846 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 10:52:22.090362 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 10:52:22.095578 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 10:52:22.100250 (MainThread): 10:52:22  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 10:52:22.104336 (MainThread): 10:52:22  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 10:52:22.107061 (MainThread): 10:52:22  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 10:52:22.110280 (MainThread): 10:52:22  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 10:52:22.274660 (MainThread): 10:52:22  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f9a56fb20>]}
2023-04-20 10:52:22.303932 (MainThread): 10:52:22  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4fe70a2430>]}
2023-04-20 10:52:22.304302 (MainThread): 10:52:22  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 10:52:22.304457 (MainThread): 10:52:22  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4fe73059a0>]}
2023-04-20 10:52:22.306391 (MainThread): 10:52:22  
2023-04-20 10:52:22.307692 (MainThread): 10:52:22  Acquiring new databricks connection 'master'
2023-04-20 10:52:22.309445 (ThreadPoolExecutor-0_0): 10:52:22  Acquiring new databricks connection 'list_schemas'
2023-04-20 10:52:22.320669 (ThreadPoolExecutor-0_0): 10:52:22  Using databricks connection "list_schemas"
2023-04-20 10:52:22.320993 (ThreadPoolExecutor-0_0): 10:52:22  On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_schemas"} */

    show databases
  
2023-04-20 10:52:22.321156 (ThreadPoolExecutor-0_0): 10:52:22  Opening a new connection, currently in state init
2023-04-20 10:52:22.687695 (Thread-58): handling poll request
2023-04-20 10:52:22.688194 (Thread-58): 10:52:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f97b940>]}
2023-04-20 10:52:22.688962 (Thread-58): sending response (<Response 8664 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:23.391639 (ThreadPoolExecutor-0_0): 10:52:23  SQL status: OK in 1.07 seconds
2023-04-20 10:52:23.516664 (ThreadPoolExecutor-0_0): 10:52:23  On list_schemas: Close
2023-04-20 10:52:23.679581 (Thread-59): handling ps request
2023-04-20 10:52:23.680107 (Thread-59): 10:52:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa2d4c0>]}
2023-04-20 10:52:23.680676 (Thread-59): sending response (<Response 1165 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:23.872199 (ThreadPoolExecutor-1_0): 10:52:23  Acquiring new databricks connection 'list_None_dbt_shabbirkdb'
2023-04-20 10:52:23.882664 (ThreadPoolExecutor-1_0): 10:52:23  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:23.882857 (ThreadPoolExecutor-1_0): 10:52:23  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 10:52:23.883015 (ThreadPoolExecutor-1_0): 10:52:23  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show tables in `dbt_shabbirkdb`
  
2023-04-20 10:52:23.883166 (ThreadPoolExecutor-1_0): 10:52:23  Opening a new connection, currently in state closed
2023-04-20 10:52:24.416515 (Thread-60): handling status request
2023-04-20 10:52:24.417011 (Thread-60): 10:52:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f97dee0>]}
2023-04-20 10:52:24.417493 (Thread-60): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:24.608985 (ThreadPoolExecutor-1_0): 10:52:24  SQL status: OK in 0.73 seconds
2023-04-20 10:52:24.618169 (ThreadPoolExecutor-1_0): 10:52:24  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 10:52:24.618393 (ThreadPoolExecutor-1_0): 10:52:24  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show views in `dbt_shabbirkdb`
  
2023-04-20 10:52:24.682714 (Thread-61): handling poll request
2023-04-20 10:52:24.683207 (Thread-61): 10:52:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f986490>]}
2023-04-20 10:52:24.683838 (Thread-61): sending response (<Response 3830 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:25.034823 (ThreadPoolExecutor-1_0): 10:52:25  SQL status: OK in 0.42 seconds
2023-04-20 10:52:25.038347 (ThreadPoolExecutor-1_0): 10:52:25  On list_None_dbt_shabbirkdb: ROLLBACK
2023-04-20 10:52:25.038560 (ThreadPoolExecutor-1_0): 10:52:25  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:25.038709 (ThreadPoolExecutor-1_0): 10:52:25  On list_None_dbt_shabbirkdb: Close
2023-04-20 10:52:25.321001 (MainThread): 10:52:25  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f9a56fd60>]}
2023-04-20 10:52:25.321416 (MainThread): 10:52:25  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:25.321577 (MainThread): 10:52:25  Spark adapter: NotImplemented: commit
2023-04-20 10:52:25.322094 (MainThread): 10:52:25  Concurrency: 4 threads (target='default')
2023-04-20 10:52:25.322245 (MainThread): 10:52:25  
2023-04-20 10:52:25.325079 (Thread-1): 10:52:25  Began running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 10:52:25.325455 (Thread-1): 10:52:25  1 of 15 START sql table model dbt_shabbirkdb.DimBroker ......................... [RUN]
2023-04-20 10:52:25.325985 (Thread-1): 10:52:25  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
2023-04-20 10:52:25.326220 (Thread-1): 10:52:25  Began compiling node model.dbsql_dbt_tpch.DimBroker
2023-04-20 10:52:25.330150 (Thread-2): 10:52:25  Began running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 10:52:25.330466 (Thread-2): 10:52:25  2 of 15 START sql table model dbt_shabbirkdb.DimCompany ........................ [RUN]
2023-04-20 10:52:25.330973 (Thread-2): 10:52:25  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCompany'
2023-04-20 10:52:25.331283 (Thread-2): 10:52:25  Began compiling node model.dbsql_dbt_tpch.DimCompany
2023-04-20 10:52:25.335211 (Thread-3): 10:52:25  Began running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 10:52:25.335506 (Thread-3): 10:52:25  3 of 15 START sql table model dbt_shabbirkdb.DimCustomerStg .................... [RUN]
2023-04-20 10:52:25.336044 (Thread-3): 10:52:25  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomerStg'
2023-04-20 10:52:25.336218 (Thread-3): 10:52:25  Began compiling node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 10:52:25.340278 (Thread-4): 10:52:25  Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 10:52:25.340567 (Thread-4): 10:52:25  4 of 15 START sql table model dbt_shabbirkdb.tempDailyMarketHistorical ......... [RUN]
2023-04-20 10:52:25.341070 (Thread-4): 10:52:25  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempDailyMarketHistorical'
2023-04-20 10:52:25.341247 (Thread-4): 10:52:25  Began compiling node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 10:52:25.344962 (Thread-4): 10:52:25  Writing injected SQL for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 10:52:25.346244 (Thread-1): 10:52:25  Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 10:52:25.346742 (Thread-2): 10:52:25  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 10:52:25.347304 (Thread-3): 10:52:25  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 10:52:25.381598 (Thread-4): 10:52:25  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (compile): 2023-04-20 10:52:25.341290 => 2023-04-20 10:52:25.381368
2023-04-20 10:52:25.381880 (Thread-4): 10:52:25  Began executing node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 10:52:25.387089 (Thread-1): 10:52:25  Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 2023-04-20 10:52:25.326274 => 2023-04-20 10:52:25.386909
2023-04-20 10:52:25.387316 (Thread-1): 10:52:25  Began executing node model.dbsql_dbt_tpch.DimBroker
2023-04-20 10:52:25.397390 (Thread-3): 10:52:25  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (compile): 2023-04-20 10:52:25.336262 => 2023-04-20 10:52:25.397214
2023-04-20 10:52:25.397609 (Thread-3): 10:52:25  Began executing node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 10:52:25.415304 (Thread-4): 10:52:25  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:25.415491 (Thread-4): 10:52:25  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 10:52:25.415680 (Thread-4): 10:52:25  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

      describe extended `dbt_shabbirkdb`.`tempdailymarkethistorical`
  
2023-04-20 10:52:25.415818 (Thread-4): 10:52:25  Opening a new connection, currently in state init
2023-04-20 10:52:25.421315 (Thread-1): 10:52:25  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:25.421504 (Thread-1): 10:52:25  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 10:52:25.421665 (Thread-1): 10:52:25  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

      describe extended `dbt_shabbirkdb`.`dimbroker`
  
2023-04-20 10:52:25.421794 (Thread-1): 10:52:25  Opening a new connection, currently in state closed
2023-04-20 10:52:25.434011 (Thread-2): 10:52:25  Timing info for model.dbsql_dbt_tpch.DimCompany (compile): 2023-04-20 10:52:25.331340 => 2023-04-20 10:52:25.433839
2023-04-20 10:52:25.434232 (Thread-2): 10:52:25  Began executing node model.dbsql_dbt_tpch.DimCompany
2023-04-20 10:52:25.479700 (Thread-2): 10:52:25  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 10:52:25.482305 (Thread-3): 10:52:25  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 10:52:25.517408 (Thread-2): 10:52:25  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:25.517624 (Thread-2): 10:52:25  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 10:52:25.517862 (Thread-2): 10:52:25  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCompany`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
  
2023-04-20 10:52:25.518009 (Thread-2): 10:52:25  Opening a new connection, currently in state init
2023-04-20 10:52:25.533638 (Thread-3): 10:52:25  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:25.533966 (Thread-3): 10:52:25  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 10:52:25.534495 (Thread-3): 10:52:25  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomerStg`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT * FROM (
  SELECT
    customerid,
    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) taxid,
    status,
    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) lastname,
    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) firstname,
    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) middleinitial,
    coalesce(gender, last_value(gender) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) gender,
    coalesce(tier, last_value(tier) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) tier,
    coalesce(dob, last_value(dob) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) dob,
    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline1,
    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline2,
    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) postalcode,
    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) CITY,
    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) stateprov,
    coalesce(country, last_value(country) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) country,
    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone1,
    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone2,
    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone3,
    coalesce(email1, last_value(email1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email1,
    coalesce(email2, last_value(email2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email2,
    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) LCL_TX_ID,
    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) NAT_TX_ID,
    batchid,
    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,
    date(update_ts) effectivedate,
    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate
  FROM (
    SELECT
      customerid,
      taxid,
      status,
      lastname,
      firstname,
      middleinitial,
      gender,
      tier,
      dob,
      addressline1,
      addressline2,
      postalcode,
      city,
      stateprov,
      country,
      phone1,
      phone2,
      phone3,
      email1,
      email2,
      lcl_tx_id,
      nat_tx_id,
      1 batchid,
      update_ts
    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')
    UNION ALL
    SELECT
      c.customerid,
      nullif(c.taxid, '') taxid,
      nullif(s.st_name, '') as status,
      nullif(c.lastname, '') lastname,
      nullif(c.firstname, '') firstname,
      nullif(c.middleinitial, '') middleinitial,
      gender,
      c.tier,
      c.dob,
      nullif(c.addressline1, '') addressline1,
      nullif(c.addressline2, '') addressline2,
      nullif(c.postalcode, '') postalcode,
      nullif(c.city, '') city,
      nullif(c.stateprov, '') stateprov,
      nullif(c.country, '') country,
      CASE
        WHEN isnull(c_local_1) then c_local_1
        ELSE concat(
          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),
          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),
          c_local_1,
          nvl(c_ext_1, '')) END as phone1,
      CASE
        WHEN isnull(c_local_2) then c_local_2
        ELSE concat(
          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),
          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),
          c_local_2,
          nvl(c_ext_2, '')) END as phone2,
      CASE
        WHEN isnull(c_local_3) then c_local_3
        ELSE concat(
          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),
          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),
          c_local_3,
          nvl(c_ext_3, '')) END as phone3,
      nullif(c.email1, '') email1,
      nullif(c.email2, '') email2,
      c.LCL_TX_ID, 
      c.NAT_TX_ID,
      c.batchid,
      timestamp(bd.batchdate) update_ts
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CustomerIncremental` c
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
      ON c.batchid = bd.batchid
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` s 
      ON c.status = s.st_id
  ) c
  )
  
2023-04-20 10:52:25.534652 (Thread-3): 10:52:25  Opening a new connection, currently in state init
2023-04-20 10:52:26.179434 (Thread-4): 10:52:26  SQL status: OK in 0.76 seconds
2023-04-20 10:52:26.185390 (Thread-4): 10:52:26  Writing runtime sql for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 10:52:26.197423 (Thread-4): 10:52:26  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 10:52:26.197657 (Thread-4): 10:52:26  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempDailyMarketHistorical`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  dmh.*,
  sk_dateid,
  min(dm_low) OVER (
    PARTITION BY dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweeklow,
  max(dm_high) OVER (
    PARTITION by dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweekhigh
FROM (
  SELECT * FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketHistorical`
  UNION ALL
  SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketIncremental`) dmh
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
  ON d.datevalue = dm_date;
  
2023-04-20 10:52:26.251879 (Thread-1): 10:52:26  SQL status: OK in 0.83 seconds
2023-04-20 10:52:26.257394 (Thread-1): 10:52:26  Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 10:52:26.269725 (Thread-1): 10:52:26  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 10:52:26.269962 (Thread-1): 10:52:26  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT
  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate`) effectivedate,
  date('9999-12-31') enddate
FROM  `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`HR`
WHERE employeejobcode = 314
  
2023-04-20 10:52:26.316404 (Thread-62): handling poll request
2023-04-20 10:52:26.316875 (Thread-62): 10:52:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f90b190>]}
2023-04-20 10:52:26.318109 (Thread-62): sending response (<Response 35562 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:26.539986 (Thread-63): handling ps request
2023-04-20 10:52:26.540531 (Thread-63): 10:52:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f90b280>]}
2023-04-20 10:52:26.541111 (Thread-63): sending response (<Response 1165 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:26.972830 (Thread-64): handling status request
2023-04-20 10:52:26.973321 (Thread-64): 10:52:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f90b520>]}
2023-04-20 10:52:26.973789 (Thread-64): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:27.085595 (Thread-2): 10:52:27  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCompany`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
  
2023-04-20 10:52:27.085834 (Thread-2): 10:52:27  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 10:52:27.085975 (Thread-2): 10:52:27  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3545.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3545.0 (TID 28490) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3545.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3545.0 (TID 28490) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
Caused by: org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

2023-04-20 10:52:27.086095 (Thread-2): 10:52:27  Databricks adapter: operation-id: b'\x01\xed\xdfio\xe6\x1aT\xb2\xbefA\xbf\x9f\xa9\xba'
2023-04-20 10:52:27.086342 (Thread-2): 10:52:27  Timing info for model.dbsql_dbt_tpch.DimCompany (execute): 2023-04-20 10:52:25.434280 => 2023-04-20 10:52:27.086210
2023-04-20 10:52:27.086529 (Thread-2): 10:52:27  On model.dbsql_dbt_tpch.DimCompany: ROLLBACK
2023-04-20 10:52:27.086656 (Thread-2): 10:52:27  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:27.086779 (Thread-2): 10:52:27  On model.dbsql_dbt_tpch.DimCompany: Close
2023-04-20 10:52:27.370372 (Thread-2): 10:52:27  Runtime Error in model DimCompany (models/silver/DimCompany.sql)
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 10:52:27.370801 (Thread-2): 10:52:27  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4fe6877280>]}
2023-04-20 10:52:27.371251 (Thread-2): 10:52:27  2 of 15 ERROR creating sql table model dbt_shabbirkdb.DimCompany ............... [ERROR in 2.04s]
2023-04-20 10:52:27.372425 (Thread-2): 10:52:27  Finished running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 10:52:27.373345 (Thread-2): 10:52:27  Began running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 10:52:27.373597 (Thread-2): 10:52:27  5 of 15 SKIP relation dbt_shabbirkdb.DimSecurity ............................... [SKIP]
2023-04-20 10:52:27.373786 (Thread-2): 10:52:27  Finished running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 10:52:27.374034 (Thread-2): 10:52:27  Began running node model.dbsql_dbt_tpch.Financial
2023-04-20 10:52:27.374228 (Thread-2): 10:52:27  6 of 15 SKIP relation dbt_shabbirkdb.Financial ................................. [SKIP]
2023-04-20 10:52:27.374400 (Thread-2): 10:52:27  Finished running node model.dbsql_dbt_tpch.Financial
2023-04-20 10:52:27.375061 (Thread-2): 10:52:27  Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 10:52:27.375275 (Thread-2): 10:52:27  7 of 15 SKIP relation dbt_shabbirkdb.tempSumpFiBasicEps ........................ [SKIP]
2023-04-20 10:52:27.375450 (Thread-2): 10:52:27  Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 10:52:27.900641 (Thread-65): handling poll request
2023-04-20 10:52:27.901135 (Thread-65): 10:52:27  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f90bd30>]}
2023-04-20 10:52:27.901944 (Thread-65): sending response (<Response 34219 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:29.126298 (Thread-66): handling ps request
2023-04-20 10:52:29.126791 (Thread-66): 10:52:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f90be20>]}
2023-04-20 10:52:29.127346 (Thread-66): sending response (<Response 1166 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:29.187887 (Thread-1): 10:52:29  SQL status: OK in 2.92 seconds
2023-04-20 10:52:29.216028 (Thread-1): 10:52:29  Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 2023-04-20 10:52:25.387364 => 2023-04-20 10:52:29.215861
2023-04-20 10:52:29.216272 (Thread-1): 10:52:29  On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
2023-04-20 10:52:29.216422 (Thread-1): 10:52:29  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:29.216545 (Thread-1): 10:52:29  On model.dbsql_dbt_tpch.DimBroker: Close
2023-04-20 10:52:29.518348 (Thread-1): 10:52:29  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4fe6867490>]}
2023-04-20 10:52:29.518877 (Thread-1): 10:52:29  1 of 15 OK created sql table model dbt_shabbirkdb.DimBroker .................... [OK in 4.19s]
2023-04-20 10:52:29.519113 (Thread-1): 10:52:29  Finished running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 10:52:29.523471 (Thread-67): handling poll request
2023-04-20 10:52:29.523887 (Thread-67): 10:52:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f90e2b0>]}
2023-04-20 10:52:29.546471 (Thread-67): sending response (<Response 3405 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:29.583967 (Thread-68): handling status request
2023-04-20 10:52:29.584314 (Thread-68): 10:52:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f90ef40>]}
2023-04-20 10:52:29.584704 (Thread-68): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:29.917313 (Thread-3): 10:52:29  SQL status: OK in 4.38 seconds
2023-04-20 10:52:29.919620 (Thread-3): 10:52:29  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (execute): 2023-04-20 10:52:25.397658 => 2023-04-20 10:52:29.919422
2023-04-20 10:52:29.919839 (Thread-3): 10:52:29  On model.dbsql_dbt_tpch.DimCustomerStg: ROLLBACK
2023-04-20 10:52:29.919983 (Thread-3): 10:52:29  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:29.920108 (Thread-3): 10:52:29  On model.dbsql_dbt_tpch.DimCustomerStg: Close
2023-04-20 10:52:30.195809 (Thread-3): 10:52:30  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f9a54f7f0>]}
2023-04-20 10:52:30.196342 (Thread-3): 10:52:30  3 of 15 OK created sql table model dbt_shabbirkdb.DimCustomerStg ............... [OK in 4.86s]
2023-04-20 10:52:30.196579 (Thread-3): 10:52:30  Finished running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 10:52:30.197516 (Thread-2): 10:52:30  Began running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 10:52:30.197880 (Thread-2): 10:52:30  8 of 15 START sql table model dbt_shabbirkdb.DimAccount ........................ [RUN]
2023-04-20 10:52:30.198390 (Thread-2): 10:52:30  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimAccount'
2023-04-20 10:52:30.198580 (Thread-2): 10:52:30  Began compiling node model.dbsql_dbt_tpch.DimAccount
2023-04-20 10:52:30.202577 (Thread-1): 10:52:30  Began running node model.dbsql_dbt_tpch.Prospect
2023-04-20 10:52:30.202869 (Thread-1): 10:52:30  9 of 15 START sql table model dbt_shabbirkdb.Prospect .......................... [RUN]
2023-04-20 10:52:30.203343 (Thread-1): 10:52:30  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Prospect'
2023-04-20 10:52:30.203559 (Thread-1): 10:52:30  Began compiling node model.dbsql_dbt_tpch.Prospect
2023-04-20 10:52:30.208885 (Thread-2): 10:52:30  Writing injected SQL for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 10:52:30.210347 (Thread-1): 10:52:30  Writing injected SQL for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 10:52:30.220022 (Thread-2): 10:52:30  Timing info for model.dbsql_dbt_tpch.DimAccount (compile): 2023-04-20 10:52:30.198626 => 2023-04-20 10:52:30.219860
2023-04-20 10:52:30.220250 (Thread-2): 10:52:30  Began executing node model.dbsql_dbt_tpch.DimAccount
2023-04-20 10:52:30.224231 (Thread-2): 10:52:30  Writing runtime sql for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 10:52:30.224607 (Thread-1): 10:52:30  Timing info for model.dbsql_dbt_tpch.Prospect (compile): 2023-04-20 10:52:30.203627 => 2023-04-20 10:52:30.224440
2023-04-20 10:52:30.224819 (Thread-1): 10:52:30  Began executing node model.dbsql_dbt_tpch.Prospect
2023-04-20 10:52:30.230096 (Thread-1): 10:52:30  Writing runtime sql for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 10:52:30.238748 (Thread-2): 10:52:30  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:30.238932 (Thread-2): 10:52:30  Using databricks connection "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 10:52:30.239164 (Thread-2): 10:52:30  On model.dbsql_dbt_tpch.DimAccount: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 10:52:30.239299 (Thread-2): 10:52:30  Opening a new connection, currently in state closed
2023-04-20 10:52:30.242528 (Thread-1): 10:52:30  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:30.242714 (Thread-1): 10:52:30  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 10:52:30.242973 (Thread-1): 10:52:30  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Prospect`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  agencyid,
  recdate.sk_dateid sk_recorddateid,
  origdate.sk_dateid sk_updatedateid,
  p.batchid,
  nvl2(c.customerid, True, False) iscustomer, 
  p.lastname,
  p.firstname,
  p.middleinitial,
  p.gender,
  p.addressline1,
  p.addressline2,
  p.postalcode,
  city,
  state,
  country,
  phone,
  income,
  numbercars,
  numberchildren,
  maritalstatus,
  age,
  creditrating,
  ownorrentflag,
  employer,
  numbercreditcards,
  networth,
  if(
    isnotnull(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+","")),
    left(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+",""),
      length(
        if(networth > 1000000 or income > 200000,"HighValue+","") || 
        if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
        if(age > 45, "Boomer+", "") ||
        if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
        if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
        if(age < 25 and networth > 1000000, "Inherited+",""))
      -1),
    NULL) marketingnameplate
FROM (
  SELECT 
    * FROM (
    SELECT
      agencyid,
      max(batchid) recordbatchid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth,
      min(batchid) batchid
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`ProspectRaw` p
    GROUP BY
      agencyid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth)
  QUALIFY ROW_NUMBER() OVER (PARTITION BY agencyid ORDER BY batchid DESC) = 1) p
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) recdate
  ON p.recordbatchid = recdate.batchid
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) origdate
  ON p.batchid = origdate.batchid
LEFT JOIN (
  SELECT 
    customerid,
    lastname,
    firstname,
    addressline1,
    addressline2,
    postalcode
  FROM `dbt_shabbirkdb`.`DimCustomerStg`
  WHERE iscurrent) c
  ON 
    upper(p.LastName) = upper(c.lastname)
    and upper(p.FirstName) = upper(c.firstname)
    and upper(p.AddressLine1) = upper(c.addressline1)
    and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
    and upper(p.PostalCode) = upper(c.postalcode)
  
2023-04-20 10:52:30.243102 (Thread-1): 10:52:30  Opening a new connection, currently in state closed
2023-04-20 10:52:31.056354 (Thread-2): 10:52:31  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 10:52:31.056593 (Thread-2): 10:52:31  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 10:52:31.056755 (Thread-2): 10:52:31  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 10:52:31.056895 (Thread-2): 10:52:31  Databricks adapter: operation-id: b'\x01\xed\xdfir\xb6\x19\x98\xbb\xc9\x94\xf7\xd0\x82\xe2\xbb'
2023-04-20 10:52:31.057142 (Thread-2): 10:52:31  Timing info for model.dbsql_dbt_tpch.DimAccount (execute): 2023-04-20 10:52:30.220301 => 2023-04-20 10:52:31.057008
2023-04-20 10:52:31.057328 (Thread-2): 10:52:31  On model.dbsql_dbt_tpch.DimAccount: ROLLBACK
2023-04-20 10:52:31.057455 (Thread-2): 10:52:31  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:31.057577 (Thread-2): 10:52:31  On model.dbsql_dbt_tpch.DimAccount: Close
2023-04-20 10:52:31.179564 (Thread-69): handling poll request
2023-04-20 10:52:31.180068 (Thread-69): 10:52:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa1a2b0>]}
2023-04-20 10:52:31.181003 (Thread-69): sending response (<Response 29512 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:31.343123 (Thread-2): 10:52:31  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 10:52:31.343713 (Thread-2): 10:52:31  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f9930d040>]}
2023-04-20 10:52:31.344177 (Thread-2): 10:52:31  8 of 15 ERROR creating sql table model dbt_shabbirkdb.DimAccount ............... [ERROR in 1.15s]
2023-04-20 10:52:31.344392 (Thread-2): 10:52:31  Finished running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 10:52:31.345258 (Thread-3): 10:52:31  Began running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 10:52:31.345534 (Thread-3): 10:52:31  10 of 15 SKIP relation dbt_shabbirkdb.DimTrade ................................. [SKIP]
2023-04-20 10:52:31.345728 (Thread-3): 10:52:31  Finished running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 10:52:31.345987 (Thread-3): 10:52:31  Began running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 10:52:31.346188 (Thread-3): 10:52:31  11 of 15 SKIP relation dbt_shabbirkdb.FactCashBalances ......................... [SKIP]
2023-04-20 10:52:31.346366 (Thread-3): 10:52:31  Finished running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 10:52:31.347056 (Thread-3): 10:52:31  Began running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 10:52:31.347284 (Thread-3): 10:52:31  12 of 15 SKIP relation dbt_shabbirkdb.FactHoldings ............................. [SKIP]
2023-04-20 10:52:31.347469 (Thread-3): 10:52:31  Finished running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 10:52:31.757851 (Thread-70): handling ps request
2023-04-20 10:52:31.758345 (Thread-70): 10:52:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f919490>]}
2023-04-20 10:52:31.758940 (Thread-70): sending response (<Response 1166 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:32.321532 (Thread-71): handling status request
2023-04-20 10:52:32.322028 (Thread-71): 10:52:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f919940>]}
2023-04-20 10:52:32.322494 (Thread-71): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:32.751834 (Thread-72): handling poll request
2023-04-20 10:52:32.752324 (Thread-72): 10:52:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f9195b0>]}
2023-04-20 10:52:32.752960 (Thread-72): sending response (<Response 7112 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:33.218266 (Thread-4): 10:52:33  SQL status: OK in 7.02 seconds
2023-04-20 10:52:33.525068 (Thread-4): 10:52:33  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (execute): 2023-04-20 10:52:25.381935 => 2023-04-20 10:52:33.524880
2023-04-20 10:52:33.525342 (Thread-4): 10:52:33  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: ROLLBACK
2023-04-20 10:52:33.525498 (Thread-4): 10:52:33  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:33.525624 (Thread-4): 10:52:33  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: Close
2023-04-20 10:52:33.813337 (Thread-4): 10:52:33  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4fe40561f0>]}
2023-04-20 10:52:33.814130 (Thread-4): 10:52:33  4 of 15 OK created sql table model dbt_shabbirkdb.tempDailyMarketHistorical .... [OK in 8.47s]
2023-04-20 10:52:33.814530 (Thread-4): 10:52:33  Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 10:52:33.815464 (Thread-3): 10:52:33  Began running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 10:52:33.815769 (Thread-3): 10:52:33  13 of 15 SKIP relation dbt_shabbirkdb.FactMarketHistory ........................ [SKIP]
2023-04-20 10:52:33.815965 (Thread-3): 10:52:33  Finished running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 10:52:34.319174 (Thread-73): handling poll request
2023-04-20 10:52:34.319695 (Thread-73): 10:52:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f919dc0>]}
2023-04-20 10:52:34.320331 (Thread-73): sending response (<Response 5279 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:34.352131 (Thread-74): handling ps request
2023-04-20 10:52:34.352514 (Thread-74): 10:52:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f919ee0>]}
2023-04-20 10:52:34.353009 (Thread-74): sending response (<Response 1166 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:34.802459 (Thread-75): handling status request
2023-04-20 10:52:34.802944 (Thread-75): 10:52:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f91d040>]}
2023-04-20 10:52:34.803404 (Thread-75): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:35.165012 (Thread-1): 10:52:35  SQL status: OK in 4.92 seconds
2023-04-20 10:52:35.167178 (Thread-1): 10:52:35  Timing info for model.dbsql_dbt_tpch.Prospect (execute): 2023-04-20 10:52:30.224868 => 2023-04-20 10:52:35.167026
2023-04-20 10:52:35.167385 (Thread-1): 10:52:35  On model.dbsql_dbt_tpch.Prospect: ROLLBACK
2023-04-20 10:52:35.167549 (Thread-1): 10:52:35  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:35.167680 (Thread-1): 10:52:35  On model.dbsql_dbt_tpch.Prospect: Close
2023-04-20 10:52:35.440429 (Thread-1): 10:52:35  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4fe400cd30>]}
2023-04-20 10:52:35.440963 (Thread-1): 10:52:35  9 of 15 OK created sql table model dbt_shabbirkdb.Prospect ..................... [OK in 5.24s]
2023-04-20 10:52:35.441207 (Thread-1): 10:52:35  Finished running node model.dbsql_dbt_tpch.Prospect
2023-04-20 10:52:35.442311 (Thread-4): 10:52:35  Began running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 10:52:35.442640 (Thread-4): 10:52:35  14 of 15 START sql table model dbt_shabbirkdb.DimCustomer ...................... [RUN]
2023-04-20 10:52:35.443175 (Thread-4): 10:52:35  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomer'
2023-04-20 10:52:35.443363 (Thread-4): 10:52:35  Began compiling node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 10:52:35.448357 (Thread-4): 10:52:35  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 10:52:35.459577 (Thread-4): 10:52:35  Timing info for model.dbsql_dbt_tpch.DimCustomer (compile): 2023-04-20 10:52:35.443410 => 2023-04-20 10:52:35.459378
2023-04-20 10:52:35.459811 (Thread-4): 10:52:35  Began executing node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 10:52:35.463887 (Thread-4): 10:52:35  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 10:52:35.474833 (Thread-4): 10:52:35  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:35.475019 (Thread-4): 10:52:35  Using databricks connection "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 10:52:35.475240 (Thread-4): 10:52:35  On model.dbsql_dbt_tpch.DimCustomer: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 10:52:35.475379 (Thread-4): 10:52:35  Opening a new connection, currently in state closed
2023-04-20 10:52:35.908833 (Thread-76): handling poll request
2023-04-20 10:52:35.909308 (Thread-76): 10:52:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f91d910>]}
2023-04-20 10:52:35.910033 (Thread-76): sending response (<Response 9778 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:36.282032 (Thread-4): 10:52:36  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 10:52:36.282261 (Thread-4): 10:52:36  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 10:52:36.282389 (Thread-4): 10:52:36  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 10:52:36.282505 (Thread-4): 10:52:36  Databricks adapter: operation-id: b'\x01\xed\xdfiu\xd4\x1c~\xb3X\xa3\x0e\xe1.\x82h'
2023-04-20 10:52:36.283080 (Thread-4): 10:52:36  Timing info for model.dbsql_dbt_tpch.DimCustomer (execute): 2023-04-20 10:52:35.459863 => 2023-04-20 10:52:36.282615
2023-04-20 10:52:36.283322 (Thread-4): 10:52:36  On model.dbsql_dbt_tpch.DimCustomer: ROLLBACK
2023-04-20 10:52:36.283459 (Thread-4): 10:52:36  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:36.283625 (Thread-4): 10:52:36  On model.dbsql_dbt_tpch.DimCustomer: Close
2023-04-20 10:52:36.555306 (Thread-4): 10:52:36  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 10:52:36.555749 (Thread-4): 10:52:36  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4fe4006910>]}
2023-04-20 10:52:36.556196 (Thread-4): 10:52:36  14 of 15 ERROR creating sql table model dbt_shabbirkdb.DimCustomer ............. [ERROR in 1.11s]
2023-04-20 10:52:36.556411 (Thread-4): 10:52:36  Finished running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 10:52:36.557563 (Thread-2): 10:52:36  Began running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 10:52:36.557858 (Thread-2): 10:52:36  15 of 15 SKIP relation dbt_shabbirkdb.FactWatches .............................. [SKIP]
2023-04-20 10:52:36.558058 (Thread-2): 10:52:36  Finished running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 10:52:36.561273 (MainThread): 10:52:36  Acquiring new databricks connection 'master'
2023-04-20 10:52:36.561664 (MainThread): 10:52:36  On master: ROLLBACK
2023-04-20 10:52:36.561956 (MainThread): 10:52:36  Opening a new connection, currently in state init
2023-04-20 10:52:36.850095 (MainThread): 10:52:36  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:36.850345 (MainThread): 10:52:36  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:36.850487 (MainThread): 10:52:36  Spark adapter: NotImplemented: commit
2023-04-20 10:52:36.850654 (MainThread): 10:52:36  On master: ROLLBACK
2023-04-20 10:52:36.850789 (MainThread): 10:52:36  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:36.850927 (MainThread): 10:52:36  On master: Close
2023-04-20 10:52:36.918271 (Thread-77): handling ps request
2023-04-20 10:52:36.918752 (Thread-77): 10:52:36  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f925370>]}
2023-04-20 10:52:36.919303 (Thread-77): sending response (<Response 1166 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:37.135251 (MainThread): 10:52:37  Connection 'master' was properly closed.
2023-04-20 10:52:37.135449 (MainThread): 10:52:37  Connection 'model.dbsql_dbt_tpch.Prospect' was properly closed.
2023-04-20 10:52:37.135587 (MainThread): 10:52:37  Connection 'model.dbsql_dbt_tpch.DimAccount' was properly closed.
2023-04-20 10:52:37.135696 (MainThread): 10:52:37  Connection 'model.dbsql_dbt_tpch.DimCustomerStg' was properly closed.
2023-04-20 10:52:37.135799 (MainThread): 10:52:37  Connection 'model.dbsql_dbt_tpch.DimCustomer' was properly closed.
2023-04-20 10:52:37.136096 (MainThread): 10:52:37  
2023-04-20 10:52:37.136243 (MainThread): 10:52:37  Finished running 15 table models in 0 hours 0 minutes and 14.83 seconds (14.83s).
2023-04-20 10:52:37.227948 (MainThread): 10:52:37  
2023-04-20 10:52:37.228232 (MainThread): 10:52:37  Completed with 3 errors and 0 warnings:
2023-04-20 10:52:37.228370 (MainThread): 10:52:37  
2023-04-20 10:52:37.228515 (MainThread): 10:52:37  Runtime Error in model DimCompany (models/silver/DimCompany.sql)
2023-04-20 10:52:37.228643 (MainThread): 10:52:37    Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 10:52:37.228756 (MainThread): 10:52:37  
2023-04-20 10:52:37.228875 (MainThread): 10:52:37  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
2023-04-20 10:52:37.228988 (MainThread): 10:52:37    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 10:52:37.229098 (MainThread): 10:52:37  
2023-04-20 10:52:37.229216 (MainThread): 10:52:37  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
2023-04-20 10:52:37.229326 (MainThread): 10:52:37    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 10:52:37.229452 (MainThread): 10:52:37  
2023-04-20 10:52:37.229581 (MainThread): 10:52:37  Done. PASS=4 WARN=0 ERROR=3 SKIP=8 TOTAL=15
2023-04-20 10:52:37.651984 (Thread-78): handling status request
2023-04-20 10:52:37.652495 (Thread-78): 10:52:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f92c190>]}
2023-04-20 10:52:37.652971 (Thread-78): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:37.668831 (Thread-79): handling poll request
2023-04-20 10:52:37.669162 (Thread-79): 10:52:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f933640>]}
2023-04-20 10:52:37.671573 (Thread-79): sending response (<Response 100133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:39.506112 (Thread-80): handling ps request
2023-04-20 10:52:39.506585 (Thread-80): 10:52:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f933850>]}
2023-04-20 10:52:39.507125 (Thread-80): sending response (<Response 1190 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:39.946771 (Thread-81): handling status request
2023-04-20 10:52:39.947284 (Thread-81): 10:52:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f933a30>]}
2023-04-20 10:52:39.947804 (Thread-81): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:51.186362 (Thread-82): handling status request
2023-04-20 10:53:51.188362 (Thread-82): 10:53:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f933910>]}
2023-04-20 10:53:51.188852 (Thread-82): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:51.297688 (Thread-83): handling ps request
2023-04-20 10:53:51.298156 (Thread-83): 10:53:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f933af0>]}
2023-04-20 10:53:51.298694 (Thread-83): sending response (<Response 1190 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:51.972444 (Thread-84): handling ps request
2023-04-20 10:53:51.972976 (Thread-84): 10:53:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f933cd0>]}
2023-04-20 10:53:51.973518 (Thread-84): sending response (<Response 1190 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:51.988653 (Thread-85): handling run_sql request
2023-04-20 10:53:51.988989 (Thread-85): 10:53:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f933df0>]}
2023-04-20 10:53:54.762668 (Thread-85): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:54.802841 (MainThread): 10:53:54  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9ba7cc92-65e2-46bc-80f2-bde4d32276c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a5048fca0>]}
2023-04-20 10:53:54.803447 (MainThread): 10:53:54  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 10:53:54.804864 (Thread-1): 10:53:54  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 10:53:54.805099 (Thread-1): 10:53:54  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 10:53:54.809856 (Thread-1): 10:53:54  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 10:53:54.805147 => 2023-04-20 10:53:54.809677
2023-04-20 10:53:54.810082 (Thread-1): 10:53:54  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 10:53:54.810480 (Thread-1): 10:53:54  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 10:53:54.810973 (Thread-1): 10:53:54  On rpc.dbsql_dbt_tpch.request: 
SELECT * FROM (
  SELECT
    customerid,
    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) taxid,
    status,
    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) lastname,
    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) firstname,
    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) middleinitial,
    coalesce(gender, last_value(gender) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) gender,
    coalesce(tier, last_value(tier) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) tier,
    coalesce(dob, last_value(dob) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) dob,
    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline1,
    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline2,
    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) postalcode,
    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) CITY,
    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) stateprov,
    coalesce(country, last_value(country) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) country,
    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone1,
    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone2,
    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone3,
    coalesce(email1, last_value(email1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email1,
    coalesce(email2, last_value(email2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email2,
    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) LCL_TX_ID,
    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) NAT_TX_ID,
    batchid,
    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,
    date(update_ts) effectivedate,
    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate
  FROM (
    SELECT
      customerid,
      taxid,
      status,
      lastname,
      firstname,
      middleinitial,
      gender,
      tier,
      dob,
      addressline1,
      addressline2,
      postalcode,
      city,
      stateprov,
      country,
      phone1,
      phone2,
      phone3,
      email1,
      email2,
      lcl_tx_id,
      nat_tx_id,
      1 batchid,
      update_ts
    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')
    UNION ALL
    SELECT
      c.customerid,
      nullif(c.taxid, '') taxid,
      nullif(s.st_name, '') as status,
      nullif(c.lastname, '') lastname,
      nullif(c.firstname, '') firstname,
      nullif(c.middleinitial, '') middleinitial,
      gender,
      c.tier,
      c.dob,
      nullif(c.addressline1, '') addressline1,
      nullif(c.addressline2, '') addressline2,
      nullif(c.postalcode, '') postalcode,
      nullif(c.city, '') city,
      nullif(c.stateprov, '') stateprov,
      nullif(c.country, '') country,
      CASE
        WHEN isnull(c_local_1) then c_local_1
        ELSE concat(
          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),
          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),
          c_local_1,
          nvl(c_ext_1, '')) END as phone1,
      CASE
        WHEN isnull(c_local_2) then c_local_2
        ELSE concat(
          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),
          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),
          c_local_2,
          nvl(c_ext_2, '')) END as phone2,
      CASE
        WHEN isnull(c_local_3) then c_local_3
        ELSE concat(
          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),
          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),
          c_local_3,
          nvl(c_ext_3, '')) END as phone3,
      nullif(c.email1, '') email1,
      nullif(c.email2, '') email2,
      c.LCL_TX_ID, 
      c.NAT_TX_ID,
      c.batchid,
      timestamp(bd.batchdate) update_ts
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CustomerIncremental` c
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
      ON c.batchid = bd.batchid
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` s 
      ON c.status = s.st_id
  ) c
  )
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 10:53:54.811132 (Thread-1): 10:53:54  Opening a new connection, currently in state init
2023-04-20 10:53:55.364574 (Thread-86): handling ps request
2023-04-20 10:53:55.365221 (Thread-86): 10:53:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56997aee0>]}
2023-04-20 10:53:55.366141 (Thread-86): sending response (<Response 1716 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:55.374075 (Thread-87): handling ps request
2023-04-20 10:53:55.374406 (Thread-87): 10:53:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93e040>]}
2023-04-20 10:53:55.374864 (Thread-87): sending response (<Response 1716 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:55.840424 (Thread-88): handling status request
2023-04-20 10:53:55.840903 (Thread-88): 10:53:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93e2b0>]}
2023-04-20 10:53:55.841425 (Thread-88): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:56.034514 (Thread-89): handling poll request
2023-04-20 10:53:56.035002 (Thread-89): 10:53:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93e250>]}
2023-04-20 10:53:56.035664 (Thread-89): sending response (<Response 8910 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:56.116440 (Thread-90): handling poll request
2023-04-20 10:53:56.116899 (Thread-90): 10:53:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93e400>]}
2023-04-20 10:53:56.117471 (Thread-90): sending response (<Response 8910 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:56.580943 (Thread-91): handling status request
2023-04-20 10:53:56.581411 (Thread-91): 10:53:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93e370>]}
2023-04-20 10:53:56.581874 (Thread-91): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:56.790517 (Thread-1): 10:53:56  SQL status: OK in 1.98 seconds
2023-04-20 10:53:56.828426 (Thread-1): 10:53:56  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 10:53:54.810135 => 2023-04-20 10:53:56.828190
2023-04-20 10:53:56.828684 (Thread-1): 10:53:56  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 10:53:57.631833 (Thread-92): handling poll request
2023-04-20 10:53:57.632328 (Thread-92): 10:53:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564928880>]}
2023-04-20 10:53:57.638749 (Thread-92): sending response (<Response 183382 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:58.010215 (Thread-93): handling ps request
2023-04-20 10:53:58.010698 (Thread-93): 10:53:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564928d30>]}
2023-04-20 10:53:58.011295 (Thread-93): sending response (<Response 1741 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:58.173415 (Thread-94): handling poll request
2023-04-20 10:53:58.173904 (Thread-94): 10:53:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5621ea1c0>]}
2023-04-20 10:53:58.179790 (Thread-94): sending response (<Response 191859 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:58.551291 (Thread-95): handling status request
2023-04-20 10:53:58.551804 (Thread-95): 10:53:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5621ea220>]}
2023-04-20 10:53:58.552291 (Thread-95): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:59.778027 (Thread-96): handling ps request
2023-04-20 10:53:59.778534 (Thread-96): 10:53:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5621ea340>]}
2023-04-20 10:53:59.779129 (Thread-96): sending response (<Response 1741 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:59.816699 (Thread-97): handling run_sql request
2023-04-20 10:53:59.817057 (Thread-97): 10:53:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5621ea4f0>]}
2023-04-20 10:54:02.578526 (Thread-97): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:02.614472 (MainThread): 10:54:02  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2348f96b-a6cd-433b-b38f-71a5b6449a42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fce88e72d30>]}
2023-04-20 10:54:02.615060 (MainThread): 10:54:02  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 10:54:02.616413 (Thread-1): 10:54:02  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 10:54:02.616648 (Thread-1): 10:54:02  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 10:54:02.620905 (Thread-1): 10:54:02  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 10:54:02.616697 => 2023-04-20 10:54:02.620731
2023-04-20 10:54:02.621122 (Thread-1): 10:54:02  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 10:54:02.621468 (Thread-1): 10:54:02  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 10:54:02.621923 (Thread-1): 10:54:02  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 10:54:02.622072 (Thread-1): 10:54:02  Opening a new connection, currently in state init
2023-04-20 10:54:03.235823 (Thread-98): handling ps request
2023-04-20 10:54:03.236509 (Thread-98): 10:54:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2a51f0>]}
2023-04-20 10:54:03.237465 (Thread-98): sending response (<Response 2249 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:03.382510 (Thread-99): handling ps request
2023-04-20 10:54:03.383016 (Thread-99): 10:54:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2a5c10>]}
2023-04-20 10:54:03.383701 (Thread-99): sending response (<Response 2249 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:03.850021 (Thread-1): 10:54:03  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 10:54:03.850265 (Thread-1): 10:54:03  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 10:54:03.850398 (Thread-1): 10:54:03  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3612.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3612.0 (TID 28609) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3612.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3612.0 (TID 28609) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
Caused by: org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

2023-04-20 10:54:03.850512 (Thread-1): 10:54:03  Databricks adapter: operation-id: b'\x01\xed\xdfi\xa9\xcc\x12o\xbcL\xff\x1fP.n\t'
2023-04-20 10:54:03.850715 (Thread-1): 10:54:03  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 10:54:02.621167 => 2023-04-20 10:54:03.850584
2023-04-20 10:54:03.850901 (Thread-1): 10:54:03  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 10:54:03.972596 (Thread-100): handling status request
2023-04-20 10:54:03.973085 (Thread-100): 10:54:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2ab3d0>]}
2023-04-20 10:54:03.973645 (Thread-100): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:04.065797 (Thread-101): handling poll request
2023-04-20 10:54:04.066296 (Thread-101): 10:54:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2ab520>]}
2023-04-20 10:54:04.067586 (Thread-101): sending response (<Response 31903 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:04.145210 (Thread-102): handling poll request
2023-04-20 10:54:04.145682 (Thread-102): 10:54:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564fee700>]}
2023-04-20 10:54:04.146486 (Thread-102): sending response (<Response 31903 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:04.144922 (Thread-1): Got an exception: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 10:54:04.146741 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 10:54:04.509519 (Thread-103): handling status request
2023-04-20 10:54:04.510028 (Thread-103): 10:54:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564fee8e0>]}
2023-04-20 10:54:04.510516 (Thread-103): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:05.778844 (Thread-104): handling poll request
2023-04-20 10:54:05.779337 (Thread-104): 10:54:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564feea00>]}
2023-04-20 10:54:05.780212 (Thread-104): sending response (<Response 50997 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:05.817173 (Thread-105): handling ps request
2023-04-20 10:54:05.817559 (Thread-105): 10:54:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56499ebe0>]}
2023-04-20 10:54:05.818141 (Thread-105): sending response (<Response 2272 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:06.357130 (Thread-106): handling status request
2023-04-20 10:54:06.357620 (Thread-106): 10:54:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5621ea1c0>]}
2023-04-20 10:54:06.358108 (Thread-106): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:06.663226 (Thread-107): handling poll request
2023-04-20 10:54:06.663751 (Thread-107): 10:54:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564fee400>]}
2023-04-20 10:54:06.664511 (Thread-107): sending response (<Response 50997 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:07.418358 (Thread-108): handling status request
2023-04-20 10:54:07.418852 (Thread-108): 10:54:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564928880>]}
2023-04-20 10:54:07.419359 (Thread-108): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:34.103170 (Thread-109): handling ps request
2023-04-20 10:54:34.105119 (Thread-109): 10:54:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5649288b0>]}
2023-04-20 10:54:34.105741 (Thread-109): sending response (<Response 2272 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:34.126011 (Thread-110): handling run_sql request
2023-04-20 10:54:34.126373 (Thread-110): 10:54:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564928af0>]}
2023-04-20 10:54:36.895963 (Thread-110): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:36.931843 (MainThread): 10:54:36  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4305e098-6241-41ca-8cc8-ee8b6282db05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f5bf62c70>]}
2023-04-20 10:54:36.932425 (MainThread): 10:54:36  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 10:54:36.933777 (Thread-1): 10:54:36  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 10:54:36.934019 (Thread-1): 10:54:36  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 10:54:36.938185 (Thread-1): 10:54:36  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 10:54:36.934070 => 2023-04-20 10:54:36.938017
2023-04-20 10:54:36.938402 (Thread-1): 10:54:36  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 10:54:36.938759 (Thread-1): 10:54:36  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 10:54:36.939206 (Thread-1): 10:54:36  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
     -- to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 10:54:36.939358 (Thread-1): 10:54:36  Opening a new connection, currently in state init
2023-04-20 10:54:37.470143 (Thread-111): handling ps request
2023-04-20 10:54:37.470820 (Thread-111): 10:54:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93edc0>]}
2023-04-20 10:54:37.471846 (Thread-111): sending response (<Response 2780 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:37.569758 (Thread-112): handling ps request
2023-04-20 10:54:37.570291 (Thread-112): 10:54:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564928910>]}
2023-04-20 10:54:37.570928 (Thread-112): sending response (<Response 2780 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:37.692523 (Thread-1): 10:54:37  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
     -- to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 10:54:37.692754 (Thread-1): 10:54:37  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pts` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `st`.`st_id`, `cmp`.`Status`, `st`.`st_name`].; line 27 pos 14
2023-04-20 10:54:37.692881 (Thread-1): 10:54:37  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pts` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `st`.`st_id`, `cmp`.`Status`, `st`.`st_name`].; line 27 pos 14
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pts` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `st`.`st_id`, `cmp`.`Status`, `st`.`st_name`].; line 27 pos 14
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 10:54:37.692999 (Thread-1): 10:54:37  Databricks adapter: operation-id: b'\x01\xed\xdfi\xbe@\x1b@\xa8\x19\xfaI,\x13+\x10'
2023-04-20 10:54:37.693200 (Thread-1): 10:54:37  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 10:54:36.938449 => 2023-04-20 10:54:37.693070
2023-04-20 10:54:37.693392 (Thread-1): 10:54:37  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 10:54:37.978008 (Thread-1): Got an exception: Runtime Error
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pts` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `st`.`st_id`, `cmp`.`Status`, `st`.`st_name`].; line 27 pos 14
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pts` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `st`.`st_id`, `cmp`.`Status`, `st`.`st_name`].; line 27 pos 14

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pts` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `st`.`st_id`, `cmp`.`Status`, `st`.`st_name`].; line 27 pos 14
2023-04-20 10:54:37.979971 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pts` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `st`.`st_id`, `cmp`.`Status`, `st`.`st_name`].; line 27 pos 14', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n     -- to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n     -- to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pts` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `st`.`st_id`, `cmp`.`Status`, `st`.`st_name`].; line 27 pos 14', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n     -- to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n     -- to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 10:54:38.040222 (Thread-113): handling status request
2023-04-20 10:54:38.040710 (Thread-113): 10:54:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56997adc0>]}
2023-04-20 10:54:38.041227 (Thread-113): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:38.138902 (Thread-114): handling poll request
2023-04-20 10:54:38.139394 (Thread-114): 10:54:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56997a8e0>]}
2023-04-20 10:54:38.140210 (Thread-114): sending response (<Response 28275 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:38.241554 (Thread-115): handling poll request
2023-04-20 10:54:38.242057 (Thread-115): 10:54:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56997afa0>]}
2023-04-20 10:54:38.242792 (Thread-115): sending response (<Response 32783 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:38.591451 (Thread-116): handling status request
2023-04-20 10:54:38.591961 (Thread-116): 10:54:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2b9ee0>]}
2023-04-20 10:54:38.592423 (Thread-116): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:40.138442 (Thread-117): handling ps request
2023-04-20 10:54:40.138938 (Thread-117): 10:54:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2bf2b0>]}
2023-04-20 10:54:40.139615 (Thread-117): sending response (<Response 2803 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:40.607792 (Thread-118): handling status request
2023-04-20 10:54:40.608274 (Thread-118): 10:54:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2bfb20>]}
2023-04-20 10:54:40.608740 (Thread-118): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:40.802036 (Thread-119): handling poll request
2023-04-20 10:54:40.802513 (Thread-119): 10:54:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2bf070>]}
2023-04-20 10:54:40.803191 (Thread-119): sending response (<Response 32783 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:41.522106 (Thread-120): handling status request
2023-04-20 10:54:41.522591 (Thread-120): 10:54:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2c6970>]}
2023-04-20 10:54:41.523052 (Thread-120): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:56:03.436583 (Thread-121): handling status request
2023-04-20 10:56:03.438367 (Thread-121): 10:56:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56997a970>]}
2023-04-20 10:56:03.438849 (Thread-121): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:56:43.940853 (Thread-122): handling status request
2023-04-20 10:56:43.941376 (Thread-122): 10:56:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93e400>]}
2023-04-20 10:56:43.941884 (Thread-122): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:56:59.627640 (Thread-123): handling status request
2023-04-20 10:56:59.628136 (Thread-123): 10:56:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93ed60>]}
2023-04-20 10:56:59.628606 (Thread-123): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:14.495193 (Thread-124): handling ps request
2023-04-20 10:57:14.495721 (Thread-124): 10:57:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93e6d0>]}
2023-04-20 10:57:14.496384 (Thread-124): sending response (<Response 2803 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:14.534619 (Thread-125): handling run_sql request
2023-04-20 10:57:14.535030 (Thread-125): 10:57:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93e0d0>]}
2023-04-20 10:57:17.371184 (Thread-125): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:17.411068 (MainThread): 10:57:17  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '99f1d357-8114-41fe-9af2-a95895f245f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ec2feaca0>]}
2023-04-20 10:57:17.411751 (MainThread): 10:57:17  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 10:57:17.413170 (Thread-1): 10:57:17  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 10:57:17.413403 (Thread-1): 10:57:17  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 10:57:17.417678 (Thread-1): 10:57:17  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 10:57:17.413451 => 2023-04-20 10:57:17.417511
2023-04-20 10:57:17.417908 (Thread-1): 10:57:17  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 10:57:17.418259 (Thread-1): 10:57:17  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 10:57:17.418706 (Thread-1): 10:57:17  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 0, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 10:57:17.418853 (Thread-1): 10:57:17  Opening a new connection, currently in state init
2023-04-20 10:57:17.986277 (Thread-126): handling ps request
2023-04-20 10:57:17.986944 (Thread-126): 10:57:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d254400>]}
2023-04-20 10:57:17.988008 (Thread-126): sending response (<Response 3311 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:17.999258 (Thread-127): handling ps request
2023-04-20 10:57:17.999634 (Thread-127): 10:57:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d254d60>]}
2023-04-20 10:57:18.025433 (Thread-127): sending response (<Response 3311 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:18.470753 (Thread-128): handling status request
2023-04-20 10:57:18.471245 (Thread-128): 10:57:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2547f0>]}
2023-04-20 10:57:18.471807 (Thread-128): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:18.575122 (Thread-129): handling poll request
2023-04-20 10:57:18.575663 (Thread-129): 10:57:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d25b7f0>]}
2023-04-20 10:57:18.576312 (Thread-129): sending response (<Response 5691 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:18.700004 (Thread-1): 10:57:18  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 0, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 10:57:18.700232 (Thread-1): 10:57:18  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 10:57:18.700362 (Thread-1): 10:57:18  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3629.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3629.0 (TID 28813) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3629.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3629.0 (TID 28813) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
Caused by: org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

2023-04-20 10:57:18.700477 (Thread-1): 10:57:18  Databricks adapter: operation-id: b'\x01\xed\xdfj\x1d\xea\x1e\xe0\x8e2S\xe4\xcf\xe8\xbct'
2023-04-20 10:57:18.700677 (Thread-1): 10:57:18  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 10:57:17.417955 => 2023-04-20 10:57:18.700549
2023-04-20 10:57:18.700864 (Thread-1): 10:57:18  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 10:57:18.744143 (Thread-130): handling poll request
2023-04-20 10:57:18.744626 (Thread-130): 10:57:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d25beb0>]}
2023-04-20 10:57:18.745342 (Thread-130): sending response (<Response 31912 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:18.983764 (Thread-131): handling status request
2023-04-20 10:57:18.984262 (Thread-131): 10:57:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d26a5b0>]}
2023-04-20 10:57:18.984734 (Thread-131): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:19.050629 (Thread-1): Got an exception: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 10:57:19.052464 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 0, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 0, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 0, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 0, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 10:57:20.275972 (Thread-132): handling poll request
2023-04-20 10:57:20.276470 (Thread-132): 10:57:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d26a5b0>]}
2023-04-20 10:57:20.277246 (Thread-132): sending response (<Response 51007 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:20.589984 (Thread-133): handling ps request
2023-04-20 10:57:20.590481 (Thread-133): 10:57:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d262ee0>]}
2023-04-20 10:57:20.591144 (Thread-133): sending response (<Response 3334 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:21.025791 (Thread-134): handling status request
2023-04-20 10:57:21.026294 (Thread-134): 10:57:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f90ed90>]}
2023-04-20 10:57:21.026754 (Thread-134): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:21.138082 (Thread-135): handling poll request
2023-04-20 10:57:21.138498 (Thread-135): 10:57:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d262f70>]}
2023-04-20 10:57:21.139220 (Thread-135): sending response (<Response 51007 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:21.896514 (Thread-136): handling status request
2023-04-20 10:57:21.897009 (Thread-136): 10:57:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa7b730>]}
2023-04-20 10:57:21.897480 (Thread-136): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:32.074361 (Thread-137): handling run_sql request
2023-04-20 10:57:32.074867 (Thread-137): 10:57:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f987220>]}
2023-04-20 10:57:32.112660 (Thread-138): handling ps request
2023-04-20 10:57:32.113589 (Thread-138): 10:57:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d26acd0>]}
2023-04-20 10:57:32.114739 (Thread-138): sending response (<Response 3847 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:32.664912 (Thread-139): handling poll request
2023-04-20 10:57:32.665421 (Thread-139): 10:57:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d25beb0>]}
2023-04-20 10:57:32.665964 (Thread-139): sending response (<Response 422 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:33.099371 (Thread-140): handling status request
2023-04-20 10:57:33.099888 (Thread-140): 10:57:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d254d60>]}
2023-04-20 10:57:33.100405 (Thread-140): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:33.394273 (Thread-141): handling ps request
2023-04-20 10:57:33.394762 (Thread-141): 10:57:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d254550>]}
2023-04-20 10:57:33.395449 (Thread-141): sending response (<Response 3847 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:34.886762 (Thread-137): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:34.924022 (MainThread): 10:57:34  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dfc9645e-9c14-4145-a1ad-38eb2cb92e13', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb723ca2c70>]}
2023-04-20 10:57:34.924664 (MainThread): 10:57:34  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 10:57:34.926063 (Thread-1): 10:57:34  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 10:57:34.926299 (Thread-1): 10:57:34  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 10:57:34.930589 (Thread-1): 10:57:34  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 10:57:34.926348 => 2023-04-20 10:57:34.930417
2023-04-20 10:57:34.930810 (Thread-1): 10:57:34  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 10:57:34.931161 (Thread-1): 10:57:34  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 10:57:34.931640 (Thread-1): 10:57:34  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 10:57:34.931790 (Thread-1): 10:57:34  Opening a new connection, currently in state init
2023-04-20 10:57:35.492467 (Thread-142): handling poll request
2023-04-20 10:57:35.492990 (Thread-142): 10:57:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d24d4c0>]}
2023-04-20 10:57:35.493613 (Thread-142): sending response (<Response 5691 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:35.552163 (Thread-143): handling ps request
2023-04-20 10:57:35.552658 (Thread-143): 10:57:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d254c70>]}
2023-04-20 10:57:35.553361 (Thread-143): sending response (<Response 3842 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:35.668466 (Thread-144): handling ps request
2023-04-20 10:57:35.668953 (Thread-144): 10:57:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2546a0>]}
2023-04-20 10:57:35.669630 (Thread-144): sending response (<Response 3842 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:35.971226 (Thread-145): handling status request
2023-04-20 10:57:35.971745 (Thread-145): 10:57:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56997a970>]}
2023-04-20 10:57:35.972244 (Thread-145): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:36.114381 (Thread-1): 10:57:36  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 10:57:36.114616 (Thread-1): 10:57:36  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 10:57:36.114751 (Thread-1): 10:57:36  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3634.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3634.0 (TID 28819) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3634.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3634.0 (TID 28819) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
Caused by: org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

2023-04-20 10:57:36.114867 (Thread-1): 10:57:36  Databricks adapter: operation-id: b'\x01\xed\xdfj(S\x1b\xa4\xa2\x11V\xf2\x80\x8f\xd3\xaf'
2023-04-20 10:57:36.115064 (Thread-1): 10:57:36  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 10:57:34.930856 => 2023-04-20 10:57:36.114938
2023-04-20 10:57:36.115249 (Thread-1): 10:57:36  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 10:57:36.258886 (Thread-146): handling poll request
2023-04-20 10:57:36.259365 (Thread-146): 10:57:36  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2b9b50>]}
2023-04-20 10:57:36.260126 (Thread-146): sending response (<Response 31913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:36.389838 (Thread-1): Got an exception: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 10:57:36.391807 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 10:57:38.575790 (Thread-147): handling ps request
2023-04-20 10:57:38.576277 (Thread-147): 10:57:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2548e0>]}
2023-04-20 10:57:38.576958 (Thread-147): sending response (<Response 3865 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:38.602314 (Thread-148): handling poll request
2023-04-20 10:57:38.602702 (Thread-148): 10:57:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f933640>]}
2023-04-20 10:57:38.603434 (Thread-148): sending response (<Response 51007 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:38.620193 (Thread-149): handling poll request
2023-04-20 10:57:38.620564 (Thread-149): 10:57:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f986460>]}
2023-04-20 10:57:38.621230 (Thread-149): sending response (<Response 51007 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:39.033328 (Thread-150): handling status request
2023-04-20 10:57:39.033830 (Thread-150): 10:57:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f97b670>]}
2023-04-20 10:57:39.034314 (Thread-150): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:56.324528 (Thread-151): handling status request
2023-04-20 10:57:56.325007 (Thread-151): 10:57:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f97b760>]}
2023-04-20 10:57:56.325460 (Thread-151): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:58:08.090005 (Thread-152): handling status request
2023-04-20 10:58:08.090567 (Thread-152): 10:58:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56997a1c0>]}
2023-04-20 10:58:08.091045 (Thread-152): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:30.321365 (Thread-153): handling status request
2023-04-20 11:02:30.323431 (Thread-153): 11:02:30  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5622a7c10>]}
2023-04-20 11:02:30.323941 (Thread-153): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:42.496002 (Thread-154): handling ps request
2023-04-20 11:02:42.496546 (Thread-154): 11:02:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5622a7160>]}
2023-04-20 11:02:42.497530 (Thread-154): sending response (<Response 3865 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:42.498077 (Thread-155): handling run_sql request
2023-04-20 11:02:42.498847 (Thread-155): 11:02:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5622a73d0>]}
2023-04-20 11:02:45.327769 (Thread-155): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:45.367685 (MainThread): 11:02:45  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '22b05fdf-8aac-4115-aa37-8709f49ebf0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe9d5c8fbe0>]}
2023-04-20 11:02:45.368322 (MainThread): 11:02:45  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:02:45.369740 (Thread-1): 11:02:45  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:02:45.369996 (Thread-1): 11:02:45  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:02:45.374572 (Thread-1): 11:02:45  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:02:45.370047 => 2023-04-20 11:02:45.374392
2023-04-20 11:02:45.374796 (Thread-1): 11:02:45  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:02:45.375166 (Thread-1): 11:02:45  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:02:45.375665 (Thread-1): 11:02:45  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:02:45.375824 (Thread-1): 11:02:45  Opening a new connection, currently in state init
2023-04-20 11:02:45.946768 (Thread-156): handling ps request
2023-04-20 11:02:45.947502 (Thread-156): 11:02:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f986070>]}
2023-04-20 11:02:45.948629 (Thread-156): sending response (<Response 4373 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:45.982513 (Thread-157): handling ps request
2023-04-20 11:02:45.983009 (Thread-157): 11:02:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d20df70>]}
2023-04-20 11:02:46.009195 (Thread-157): sending response (<Response 4373 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:46.420671 (Thread-158): handling status request
2023-04-20 11:02:46.421170 (Thread-158): 11:02:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d20d850>]}
2023-04-20 11:02:46.421721 (Thread-158): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:46.615877 (Thread-159): handling poll request
2023-04-20 11:02:46.616373 (Thread-159): 11:02:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2077f0>]}
2023-04-20 11:02:46.616987 (Thread-159): sending response (<Response 5695 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:46.715271 (Thread-1): 11:02:46  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:02:46.715509 (Thread-1): 11:02:46  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:02:46.715681 (Thread-1): 11:02:46  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3671.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3671.0 (TID 29152) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3671.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3671.0 (TID 29152) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
Caused by: org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

2023-04-20 11:02:46.715799 (Thread-1): 11:02:46  Databricks adapter: operation-id: b'\x01\xed\xdfj\xe1l\x16J\x86\xd0t\\\xa1\xe6\xfa\x83'
2023-04-20 11:02:46.716000 (Thread-1): 11:02:46  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:02:45.374843 => 2023-04-20 11:02:46.715872
2023-04-20 11:02:46.716189 (Thread-1): 11:02:46  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:02:46.719540 (Thread-160): handling poll request
2023-04-20 11:02:46.719977 (Thread-160): 11:02:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d24d8e0>]}
2023-04-20 11:02:46.720702 (Thread-160): sending response (<Response 31920 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:47.002848 (Thread-1): Got an exception: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:02:47.004776 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 11:02:47.185684 (Thread-161): handling status request
2023-04-20 11:02:47.186200 (Thread-161): 11:02:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d21b880>]}
2023-04-20 11:02:47.186697 (Thread-161): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:48.271249 (Thread-162): handling poll request
2023-04-20 11:02:48.271789 (Thread-162): 11:02:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d24dc70>]}
2023-04-20 11:02:48.272602 (Thread-162): sending response (<Response 51038 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:48.561988 (Thread-163): handling ps request
2023-04-20 11:02:48.562508 (Thread-163): 11:02:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d20d6d0>]}
2023-04-20 11:02:48.563266 (Thread-163): sending response (<Response 4396 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:49.080529 (Thread-164): handling status request
2023-04-20 11:02:49.081027 (Thread-164): 11:02:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2549a0>]}
2023-04-20 11:02:49.081509 (Thread-164): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:49.342424 (Thread-165): handling poll request
2023-04-20 11:02:49.342919 (Thread-165): 11:02:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d254c70>]}
2023-04-20 11:02:49.343713 (Thread-165): sending response (<Response 51038 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:50.101257 (Thread-166): handling status request
2023-04-20 11:02:50.101764 (Thread-166): 11:02:50  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d26ae50>]}
2023-04-20 11:02:50.102265 (Thread-166): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:08.110803 (Thread-167): handling status request
2023-04-20 11:04:08.112737 (Thread-167): 11:04:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2541f0>]}
2023-04-20 11:04:08.113217 (Thread-167): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:27.206917 (Thread-168): handling ps request
2023-04-20 11:04:27.207432 (Thread-168): 11:04:27  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d207730>]}
2023-04-20 11:04:27.208161 (Thread-168): sending response (<Response 4396 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:27.242644 (Thread-169): handling run_sql request
2023-04-20 11:04:27.242996 (Thread-169): 11:04:27  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d222ca0>]}
2023-04-20 11:04:29.960802 (Thread-169): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:29.999145 (MainThread): 11:04:29  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bfac63d5-cac9-46a9-882e-2fd273172687', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd226742dc0>]}
2023-04-20 11:04:29.999754 (MainThread): 11:04:29  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:04:30.001093 (Thread-1): 11:04:30  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:04:30.001328 (Thread-1): 11:04:30  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:04:30.005696 (Thread-1): 11:04:30  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:04:30.001383 => 2023-04-20 11:04:30.005522
2023-04-20 11:04:30.005927 (Thread-1): 11:04:30  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:04:30.006279 (Thread-1): 11:04:30  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:04:30.006766 (Thread-1): 11:04:30  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      substring(value, 1, 15) as PTS,
      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:04:30.006996 (Thread-1): 11:04:30  Opening a new connection, currently in state init
2023-04-20 11:04:30.525459 (Thread-170): handling ps request
2023-04-20 11:04:30.526131 (Thread-170): 11:04:30  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d237fd0>]}
2023-04-20 11:04:30.527293 (Thread-170): sending response (<Response 4904 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:30.528504 (Thread-171): handling ps request
2023-04-20 11:04:30.528912 (Thread-171): 11:04:30  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d23e220>]}
2023-04-20 11:04:30.529525 (Thread-171): sending response (<Response 4904 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:31.020802 (Thread-172): handling status request
2023-04-20 11:04:31.021284 (Thread-172): 11:04:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d222ca0>]}
2023-04-20 11:04:31.021797 (Thread-172): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:31.133763 (Thread-173): handling poll request
2023-04-20 11:04:31.134264 (Thread-173): 11:04:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d231ee0>]}
2023-04-20 11:04:31.134893 (Thread-173): sending response (<Response 5745 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:31.186553 (Thread-1): 11:04:31  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      substring(value, 1, 15) as PTS,
      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:04:31.186788 (Thread-1): 11:04:31  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:04:31.186918 (Thread-1): 11:04:31  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3682.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3682.0 (TID 29257) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3682.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3682.0 (TID 29257) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
Caused by: org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

2023-04-20 11:04:31.187032 (Thread-1): 11:04:31  Databricks adapter: operation-id: b"\x01\xed\xdfk\x1f\xbe\x13\xd7\x98\xd1\x11\xd8I\x7f'\x14"
2023-04-20 11:04:31.187230 (Thread-1): 11:04:31  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:04:30.005975 => 2023-04-20 11:04:31.187105
2023-04-20 11:04:31.187418 (Thread-1): 11:04:31  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:04:31.330612 (Thread-174): handling poll request
2023-04-20 11:04:31.331061 (Thread-174): 11:04:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d246e80>]}
2023-04-20 11:04:31.331832 (Thread-174): sending response (<Response 32024 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:31.470601 (Thread-1): Got an exception: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:04:31.472444 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n        ,ansi_mode = 'false'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      substring(value, 1, 15) as PTS,\n      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      substring(value, 1, 15) as PTS,\n      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n        ,ansi_mode = 'false'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      substring(value, 1, 15) as PTS,\n      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      substring(value, 1, 15) as PTS,\n      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 11:04:31.624495 (Thread-175): handling status request
2023-04-20 11:04:31.624994 (Thread-175): 11:04:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1cf880>]}
2023-04-20 11:04:31.625457 (Thread-175): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:32.922325 (Thread-176): handling poll request
2023-04-20 11:04:32.922807 (Thread-176): 11:04:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1cf100>]}
2023-04-20 11:04:32.923620 (Thread-176): sending response (<Response 51486 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:33.179183 (Thread-177): handling ps request
2023-04-20 11:04:33.179722 (Thread-177): 11:04:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8850>]}
2023-04-20 11:04:33.180462 (Thread-177): sending response (<Response 4927 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:33.611324 (Thread-178): handling status request
2023-04-20 11:04:33.611842 (Thread-178): 11:04:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8e50>]}
2023-04-20 11:04:33.612898 (Thread-178): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:33.782323 (Thread-179): handling poll request
2023-04-20 11:04:33.782816 (Thread-179): 11:04:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d283e20>]}
2023-04-20 11:04:33.783605 (Thread-179): sending response (<Response 51486 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:34.627737 (Thread-180): handling status request
2023-04-20 11:04:34.628235 (Thread-180): 11:04:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d237970>]}
2023-04-20 11:04:34.628744 (Thread-180): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:07.509688 (Thread-181): handling ps request
2023-04-20 11:05:07.510445 (Thread-181): 11:05:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d222d90>]}
2023-04-20 11:05:07.510960 (Thread-182): handling run_sql request
2023-04-20 11:05:07.511933 (Thread-181): sending response (<Response 4927 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:07.512419 (Thread-182): 11:05:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f986be0>]}
2023-04-20 11:05:09.043073 (Thread-183): handling ps request
2023-04-20 11:05:09.044123 (Thread-183): 11:05:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d237fd0>]}
2023-04-20 11:05:09.045424 (Thread-183): sending response (<Response 5440 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:09.630683 (Thread-184): handling poll request
2023-04-20 11:05:09.631160 (Thread-184): 11:05:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df700>]}
2023-04-20 11:05:09.631686 (Thread-184): sending response (<Response 422 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:10.114692 (Thread-185): handling status request
2023-04-20 11:05:10.115188 (Thread-185): 11:05:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df6a0>]}
2023-04-20 11:05:10.115735 (Thread-185): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:10.377888 (Thread-182): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:10.414789 (MainThread): 11:05:10  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ac2bdbbc-0707-4c45-b7db-317abd51750f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9bbea2cd0>]}
2023-04-20 11:05:10.415408 (MainThread): 11:05:10  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:05:10.416796 (Thread-1): 11:05:10  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:05:10.417027 (Thread-1): 11:05:10  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:05:10.421380 (Thread-1): 11:05:10  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:05:10.417076 => 2023-04-20 11:05:10.421207
2023-04-20 11:05:10.421601 (Thread-1): 11:05:10  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:05:10.421964 (Thread-1): 11:05:10  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:05:10.422414 (Thread-1): 11:05:10  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    --nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      substring(value, 1, 15) as PTS,
      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:05:10.422561 (Thread-1): 11:05:10  Opening a new connection, currently in state init
2023-04-20 11:05:10.926790 (Thread-186): handling ps request
2023-04-20 11:05:10.927278 (Thread-186): 11:05:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df820>]}
2023-04-20 11:05:10.928080 (Thread-186): sending response (<Response 5435 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:10.951301 (Thread-187): handling ps request
2023-04-20 11:05:10.951695 (Thread-187): 11:05:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df910>]}
2023-04-20 11:05:10.978625 (Thread-187): sending response (<Response 5435 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:11.379889 (Thread-188): handling status request
2023-04-20 11:05:11.380383 (Thread-188): 11:05:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df460>]}
2023-04-20 11:05:11.380859 (Thread-188): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:11.666026 (Thread-1): 11:05:11  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    --nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      substring(value, 1, 15) as PTS,
      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:05:11.666258 (Thread-1): 11:05:11  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:05:11.666386 (Thread-1): 11:05:11  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3687.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3687.0 (TID 29263) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3687.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3687.0 (TID 29263) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
Caused by: org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

2023-04-20 11:05:11.666500 (Thread-1): 11:05:11  Databricks adapter: operation-id: b'\x01\xed\xdfk7\xd8\x14s\xb6\xb2\xc2\xbb\xaa\x81L:'
2023-04-20 11:05:11.666695 (Thread-1): 11:05:11  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:05:10.421648 => 2023-04-20 11:05:11.666571
2023-04-20 11:05:11.666889 (Thread-1): 11:05:11  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:05:11.699925 (Thread-189): handling poll request
2023-04-20 11:05:11.700473 (Thread-189): 11:05:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f97b1c0>]}
2023-04-20 11:05:11.701208 (Thread-189): sending response (<Response 32018 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:11.972183 (Thread-1): Got an exception: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:05:11.974064 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    --nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      substring(value, 1, 15) as PTS,\n      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    --nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      substring(value, 1, 15) as PTS,\n      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    --nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      substring(value, 1, 15) as PTS,\n      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    --nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      substring(value, 1, 15) as PTS,\n      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 11:05:12.180961 (Thread-190): handling poll request
2023-04-20 11:05:12.181458 (Thread-190): 11:05:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1dfc40>]}
2023-04-20 11:05:12.182275 (Thread-190): sending response (<Response 46858 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:12.591497 (Thread-191): handling status request
2023-04-20 11:05:12.591997 (Thread-191): 11:05:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df9a0>]}
2023-04-20 11:05:12.592460 (Thread-191): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:13.278674 (Thread-192): handling poll request
2023-04-20 11:05:13.279155 (Thread-192): 11:05:13  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df7c0>]}
2023-04-20 11:05:13.279923 (Thread-192): sending response (<Response 51400 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:13.509574 (Thread-193): handling ps request
2023-04-20 11:05:13.510062 (Thread-193): 11:05:13  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1b8310>]}
2023-04-20 11:05:13.510795 (Thread-193): sending response (<Response 5458 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:13.982360 (Thread-194): handling status request
2023-04-20 11:05:13.982870 (Thread-194): 11:05:13  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1b8370>]}
2023-04-20 11:05:13.983345 (Thread-194): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:15.095248 (Thread-195): handling poll request
2023-04-20 11:05:15.095808 (Thread-195): 11:05:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1b8640>]}
2023-04-20 11:05:15.096589 (Thread-195): sending response (<Response 51400 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:15.877695 (Thread-196): handling status request
2023-04-20 11:05:15.878210 (Thread-196): 11:05:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1b8b80>]}
2023-04-20 11:05:15.878705 (Thread-196): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:09:25.195307 (Thread-197): handling status request
2023-04-20 11:09:25.197228 (Thread-197): 11:09:25  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df8e0>]}
2023-04-20 11:09:25.197699 (Thread-197): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:09:59.193495 (Thread-198): handling status request
2023-04-20 11:09:59.194031 (Thread-198): 11:09:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1c40a0>]}
2023-04-20 11:09:59.194522 (Thread-198): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:10:03.634887 (Thread-199): handling status request
2023-04-20 11:10:03.635378 (Thread-199): 11:10:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1c4250>]}
2023-04-20 11:10:03.635873 (Thread-199): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:10:07.023066 (Thread-200): handling status request
2023-04-20 11:10:07.023972 (Thread-200): 11:10:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d20d6d0>]}
2023-04-20 11:10:07.024442 (Thread-200): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:10:53.861934 (Thread-201): handling status request
2023-04-20 11:10:53.862435 (Thread-201): 11:10:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1dff10>]}
2023-04-20 11:10:53.862904 (Thread-201): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:11:34.198468 (Thread-202): handling status request
2023-04-20 11:11:34.198970 (Thread-202): 11:11:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1dfb50>]}
2023-04-20 11:11:34.199464 (Thread-202): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:12:58.624800 (Thread-203): handling status request
2023-04-20 11:12:58.626772 (Thread-203): 11:12:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df430>]}
2023-04-20 11:12:58.627250 (Thread-203): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:13:45.231807 (Thread-204): handling status request
2023-04-20 11:13:45.232315 (Thread-204): 11:13:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1dfc70>]}
2023-04-20 11:13:45.232784 (Thread-204): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:14:04.428647 (Thread-205): handling status request
2023-04-20 11:14:04.429145 (Thread-205): 11:14:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df1c0>]}
2023-04-20 11:14:04.429639 (Thread-205): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:14:26.654819 (Thread-206): 11:14:26  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:14:26.680446 (Thread-206): 11:14:26  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 11:14:26.680839 (Thread-206): 11:14:26  previous checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, current checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886
2023-04-20 11:14:26.681054 (Thread-206): 11:14:26  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564fdfcd0>]}
2023-04-20 11:14:27.070979 (Thread-207): handling status request
2023-04-20 11:14:27.071506 (Thread-207): 11:14:27  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9db8b0>]}
2023-04-20 11:14:27.071964 (Thread-207): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:14:28.386743 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 11:14:28.390445 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 11:14:28.394106 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 11:14:28.397199 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 11:14:28.400299 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 11:14:28.403148 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 11:14:28.406481 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 11:14:28.409649 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 11:14:28.412847 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 11:14:28.415792 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 11:14:28.419139 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 11:14:28.422148 (Thread-206): 11:14:28  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 11:14:28.424927 (Thread-206): 11:14:28  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 11:14:28.427840 (Thread-206): 11:14:28  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 11:14:28.431029 (Thread-206): 11:14:28  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:14:28.603725 (Thread-206): 11:14:28  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b8d30>]}
2023-04-20 11:14:29.840945 (Thread-208): handling status request
2023-04-20 11:14:29.841488 (Thread-208): 11:14:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b8be0>]}
2023-04-20 11:14:29.842131 (Thread-208): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:14:48.560204 (Thread-209): 11:14:48  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:14:48.873143 (Thread-209): 11:14:48  Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
2023-04-20 11:14:48.873726 (Thread-209): 11:14:48  Partial parsing: added file: dbsql_dbt_tpch://tests/not-null-founding-date.sql
2023-04-20 11:14:48.941612 (Thread-209): 11:14:48  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb555976940>]}
2023-04-20 11:14:49.000790 (Thread-210): handling status request
2023-04-20 11:14:49.001243 (Thread-210): 11:14:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559afbb0>]}
2023-04-20 11:14:49.001720 (Thread-210): sending response (<Response 1612 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:15:15.585116 (Thread-211): handling status request
2023-04-20 11:15:15.585614 (Thread-211): 11:15:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559afb20>]}
2023-04-20 11:15:15.586095 (Thread-211): sending response (<Response 1612 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:15:39.118389 (Thread-212): handling status request
2023-04-20 11:15:39.118924 (Thread-212): 11:15:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559af4f0>]}
2023-04-20 11:15:39.119419 (Thread-212): sending response (<Response 1612 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:15:55.326037 (Thread-213): 11:15:55  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:15:55.716882 (Thread-214): handling status request
2023-04-20 11:15:55.717429 (Thread-214): 11:15:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1c4bb0>]}
2023-04-20 11:15:55.717968 (Thread-214): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:15:55.932969 (Thread-213): 11:15:55  Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
2023-04-20 11:15:55.933464 (Thread-213): 11:15:55  Partial parsing: added file: dbsql_dbt_tpch://tests/null-founding-date.sql
2023-04-20 11:15:55.933648 (Thread-213): 11:15:55  Partial parsing: deleted file: dbsql_dbt_tpch://tests/not-null-founding-date.sql
2023-04-20 11:15:56.000710 (Thread-213): 11:15:56  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5649389a0>]}
2023-04-20 11:15:57.251007 (Thread-215): handling status request
2023-04-20 11:15:57.251544 (Thread-215): 11:15:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b13a0>]}
2023-04-20 11:15:57.252054 (Thread-215): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:16:34.074240 (Thread-216): handling status request
2023-04-20 11:16:34.074729 (Thread-216): 11:16:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1130>]}
2023-04-20 11:16:34.099355 (Thread-216): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:18:24.762112 (Thread-217): handling status request
2023-04-20 11:18:24.763986 (Thread-217): 11:18:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b17f0>]}
2023-04-20 11:18:24.764476 (Thread-217): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:19:25.094123 (Thread-218): handling status request
2023-04-20 11:19:25.094639 (Thread-218): 11:19:25  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1cd0>]}
2023-04-20 11:19:25.095129 (Thread-218): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:08.742042 (Thread-219): handling status request
2023-04-20 11:20:08.742542 (Thread-219): 11:20:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1730>]}
2023-04-20 11:20:08.743023 (Thread-219): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:31.462748 (Thread-220): handling status request
2023-04-20 11:20:31.463261 (Thread-220): 11:20:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1fa0>]}
2023-04-20 11:20:31.463794 (Thread-220): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:41.829746 (Thread-221): handling ps request
2023-04-20 11:20:41.830294 (Thread-221): 11:20:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1c40>]}
2023-04-20 11:20:41.831104 (Thread-221): sending response (<Response 5458 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:41.860655 (Thread-222): handling run_sql request
2023-04-20 11:20:41.861144 (Thread-222): 11:20:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1220>]}
2023-04-20 11:20:44.637900 (Thread-222): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:44.674551 (MainThread): 11:20:44  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5f9ebbff-f62f-413c-a4cb-dbdd0dd9fb0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd7a47e0220>]}
2023-04-20 11:20:44.675166 (MainThread): 11:20:44  Found 15 models, 2 tests, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:20:44.676568 (Thread-1): 11:20:44  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:20:44.676811 (Thread-1): 11:20:44  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:20:44.678800 (Thread-1): 11:20:44  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:20:44.676859 => 2023-04-20 11:20:44.678629
2023-04-20 11:20:44.679030 (Thread-1): 11:20:44  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:20:44.679402 (Thread-1): 11:20:44  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:20:44.679785 (Thread-1): 11:20:44  On rpc.dbsql_dbt_tpch.request: AND trim(substring(value, 99, 8)) <> ''
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:20:44.679943 (Thread-1): 11:20:44  Opening a new connection, currently in state init
2023-04-20 11:20:45.176631 (Thread-223): handling ps request
2023-04-20 11:20:45.177349 (Thread-223): 11:20:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b9b20>]}
2023-04-20 11:20:45.178698 (Thread-223): sending response (<Response 5966 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:45.206498 (Thread-224): handling ps request
2023-04-20 11:20:45.206878 (Thread-224): 11:20:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b98e0>]}
2023-04-20 11:20:45.207657 (Thread-224): sending response (<Response 5966 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:45.335716 (Thread-1): 11:20:45  Databricks adapter: Error while running:
AND trim(substring(value, 99, 8)) <> ''
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:20:45.335955 (Thread-1): 11:20:45  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'AND'.(line 1, pos 0)

== SQL ==
AND trim(substring(value, 99, 8)) <> ''
^^^
limit 500
/* limit added automatically by dbt cloud */

2023-04-20 11:20:45.336088 (Thread-1): 11:20:45  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'AND'.(line 1, pos 0)

== SQL ==
AND trim(substring(value, 99, 8)) <> ''
^^^
limit 500
/* limit added automatically by dbt cloud */

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'AND'.(line 1, pos 0)

== SQL ==
AND trim(substring(value, 99, 8)) <> ''
^^^
limit 500
/* limit added automatically by dbt cloud */

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 11:20:45.336205 (Thread-1): 11:20:45  Databricks adapter: operation-id: b'\x01\xed\xdfmd\xb5\x10-\x85\xe4ECx\xfb\x12\xab'
2023-04-20 11:20:45.336409 (Thread-1): 11:20:45  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:20:44.679079 => 2023-04-20 11:20:45.336276
2023-04-20 11:20:45.336611 (Thread-1): 11:20:45  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:20:45.625044 (Thread-1): Got an exception: Runtime Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'AND'.(line 1, pos 0)
  
  == SQL ==
  AND trim(substring(value, 99, 8)) <> ''
  ^^^
  limit 500
  /* limit added automatically by dbt cloud */
  
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'AND'.(line 1, pos 0)

== SQL ==
AND trim(substring(value, 99, 8)) <> ''
^^^
limit 500
/* limit added automatically by dbt cloud */


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'AND'.(line 1, pos 0)
  
  == SQL ==
  AND trim(substring(value, 99, 8)) <> ''
  ^^^
  limit 500
  /* limit added automatically by dbt cloud */
2023-04-20 11:20:45.626939 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': "Runtime Error in rpc request (from remote system.sql)\n  \n  [PARSE_SYNTAX_ERROR] Syntax error at or near 'AND'.(line 1, pos 0)\n  \n  == SQL ==\n  AND trim(substring(value, 99, 8)) <> ''\n  ^^^\n  limit 500\n  /* limit added automatically by dbt cloud */\n  ", 'raw_code': "AND trim(substring(value, 99, 8)) <> ''\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "AND trim(substring(value, 99, 8)) <> ''\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': "Runtime Error in rpc request (from remote system.sql)\n  \n  [PARSE_SYNTAX_ERROR] Syntax error at or near 'AND'.(line 1, pos 0)\n  \n  == SQL ==\n  AND trim(substring(value, 99, 8)) <> ''\n  ^^^\n  limit 500\n  /* limit added automatically by dbt cloud */\n  ", 'raw_code': "AND trim(substring(value, 99, 8)) <> ''\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "AND trim(substring(value, 99, 8)) <> ''\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 11:20:45.664605 (Thread-225): handling status request
2023-04-20 11:20:45.665180 (Thread-225): 11:20:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b9bb0>]}
2023-04-20 11:20:45.665756 (Thread-225): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:45.866693 (Thread-226): handling poll request
2023-04-20 11:20:45.867168 (Thread-226): 11:20:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b9a30>]}
2023-04-20 11:20:45.867857 (Thread-226): sending response (<Response 18150 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:45.870570 (Thread-227): handling poll request
2023-04-20 11:20:45.870904 (Thread-227): 11:20:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9ac460>]}
2023-04-20 11:20:45.871393 (Thread-227): sending response (<Response 18150 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:46.792842 (Thread-228): handling status request
2023-04-20 11:20:46.793339 (Thread-228): 11:20:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9acdc0>]}
2023-04-20 11:20:46.793835 (Thread-228): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:47.782674 (Thread-229): handling ps request
2023-04-20 11:20:47.783173 (Thread-229): 11:20:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5581fbb80>]}
2023-04-20 11:20:47.783982 (Thread-229): sending response (<Response 5989 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:48.203483 (Thread-230): handling status request
2023-04-20 11:20:48.204007 (Thread-230): 11:20:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559732b0>]}
2023-04-20 11:20:48.204489 (Thread-230): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:05.283370 (Thread-231): handling run_sql request
2023-04-20 11:21:05.283890 (Thread-231): 11:21:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5581fb790>]}
2023-04-20 11:21:05.339670 (Thread-232): handling ps request
2023-04-20 11:21:05.345418 (Thread-232): 11:21:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b9af0>]}
2023-04-20 11:21:05.347706 (Thread-232): sending response (<Response 6502 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:05.912466 (Thread-233): handling poll request
2023-04-20 11:21:05.912954 (Thread-233): 11:21:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1c70>]}
2023-04-20 11:21:05.913487 (Thread-233): sending response (<Response 422 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:06.334872 (Thread-234): handling status request
2023-04-20 11:21:06.335354 (Thread-234): 11:21:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1910>]}
2023-04-20 11:21:06.335934 (Thread-234): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:06.601055 (Thread-235): handling ps request
2023-04-20 11:21:06.601578 (Thread-235): 11:21:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b11c0>]}
2023-04-20 11:21:06.602449 (Thread-235): sending response (<Response 6502 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:08.112852 (Thread-231): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:08.149058 (MainThread): 11:21:08  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c3202b7c-9952-42de-8a8e-872d324aed9e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57bfa2a100>]}
2023-04-20 11:21:08.149660 (MainThread): 11:21:08  Found 15 models, 2 tests, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:21:08.151038 (Thread-1): 11:21:08  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:21:08.151277 (Thread-1): 11:21:08  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:21:08.155459 (Thread-1): 11:21:08  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:21:08.151326 => 2023-04-20 11:21:08.155288
2023-04-20 11:21:08.155786 (Thread-1): 11:21:08  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:21:08.156152 (Thread-1): 11:21:08  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:21:08.156600 (Thread-1): 11:21:08  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:21:08.156749 (Thread-1): 11:21:08  Opening a new connection, currently in state init
2023-04-20 11:21:08.450953 (Thread-236): handling poll request
2023-04-20 11:21:08.451450 (Thread-236): 11:21:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1be0>]}
2023-04-20 11:21:08.452076 (Thread-236): sending response (<Response 5758 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:08.636317 (Thread-237): handling ps request
2023-04-20 11:21:08.636799 (Thread-237): 11:21:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1d90>]}
2023-04-20 11:21:08.637585 (Thread-237): sending response (<Response 6497 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:08.674716 (Thread-238): handling ps request
2023-04-20 11:21:08.675169 (Thread-238): 11:21:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9c4130>]}
2023-04-20 11:21:08.675969 (Thread-238): sending response (<Response 6496 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:08.861644 (Thread-239): handling status request
2023-04-20 11:21:08.862146 (Thread-239): 11:21:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9c4cd0>]}
2023-04-20 11:21:08.862622 (Thread-239): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:09.220811 (Thread-240): handling poll request
2023-04-20 11:21:09.221311 (Thread-240): 11:21:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9c4280>]}
2023-04-20 11:21:09.221902 (Thread-240): sending response (<Response 5758 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:09.763613 (Thread-1): 11:21:09  SQL status: OK in 1.61 seconds
2023-04-20 11:21:09.794466 (Thread-1): 11:21:09  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:21:08.155841 => 2023-04-20 11:21:09.794248
2023-04-20 11:21:09.794712 (Thread-1): 11:21:09  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:21:11.491774 (Thread-241): handling ps request
2023-04-20 11:21:11.492529 (Thread-241): 11:21:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb57b95e850>]}
2023-04-20 11:21:11.493037 (Thread-242): handling poll request
2023-04-20 11:21:11.494015 (Thread-241): sending response (<Response 6522 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:11.494448 (Thread-242): 11:21:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1c47c0>]}
2023-04-20 11:21:11.500586 (Thread-242): sending response (<Response 175642 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:11.510484 (Thread-243): handling poll request
2023-04-20 11:21:11.510854 (Thread-243): 11:21:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52bf448b0>]}
2023-04-20 11:21:11.515723 (Thread-243): sending response (<Response 180985 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:11.966255 (Thread-244): handling status request
2023-04-20 11:21:11.966745 (Thread-244): 11:21:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56229f370>]}
2023-04-20 11:21:11.967232 (Thread-244): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:12.986842 (Thread-245): handling ps request
2023-04-20 11:21:12.987377 (Thread-245): 11:21:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1c4b20>]}
2023-04-20 11:21:12.988243 (Thread-245): sending response (<Response 6522 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:36.436492 (Thread-246): handling status request
2023-04-20 11:21:36.437000 (Thread-246): 11:21:36  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f9799d0>]}
2023-04-20 11:21:36.468398 (Thread-246): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:02.595441 (Thread-247): handling status request
2023-04-20 11:22:02.595968 (Thread-247): 11:22:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f979dc0>]}
2023-04-20 11:22:02.596451 (Thread-247): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:37.178183 (Thread-248): handling run_sql request
2023-04-20 11:22:37.178684 (Thread-248): 11:22:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f979a00>]}
2023-04-20 11:22:37.197067 (Thread-249): handling ps request
2023-04-20 11:22:37.203081 (Thread-249): 11:22:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8be0>]}
2023-04-20 11:22:37.205661 (Thread-249): sending response (<Response 7035 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:37.754623 (Thread-250): handling poll request
2023-04-20 11:22:37.755123 (Thread-250): 11:22:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8eb0>]}
2023-04-20 11:22:37.755709 (Thread-250): sending response (<Response 422 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:38.206397 (Thread-251): handling status request
2023-04-20 11:22:38.206890 (Thread-251): 11:22:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8c10>]}
2023-04-20 11:22:38.207432 (Thread-251): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:38.396396 (Thread-252): handling ps request
2023-04-20 11:22:38.396934 (Thread-252): 11:22:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8fa0>]}
2023-04-20 11:22:38.397791 (Thread-252): sending response (<Response 7035 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:39.968521 (Thread-248): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:40.008381 (MainThread): 11:22:40  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2339e5f5-7f32-49f2-ac12-b09ff95c0e77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c300a6160>]}
2023-04-20 11:22:40.008999 (MainThread): 11:22:40  Found 15 models, 2 tests, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:22:40.010375 (Thread-1): 11:22:40  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:22:40.010607 (Thread-1): 11:22:40  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:22:40.014882 (Thread-1): 11:22:40  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:22:40.010654 => 2023-04-20 11:22:40.014710
2023-04-20 11:22:40.015103 (Thread-1): 11:22:40  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:22:40.015467 (Thread-1): 11:22:40  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:22:40.015960 (Thread-1): 11:22:40  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(replace(trim(substring(value, 99, 8)),'','19010101'), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:22:40.016113 (Thread-1): 11:22:40  Opening a new connection, currently in state init
2023-04-20 11:22:40.419456 (Thread-253): handling poll request
2023-04-20 11:22:40.419977 (Thread-253): 11:22:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8700>]}
2023-04-20 11:22:40.420595 (Thread-253): sending response (<Response 5739 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:40.585618 (Thread-254): handling ps request
2023-04-20 11:22:40.586056 (Thread-254): 11:22:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8040>]}
2023-04-20 11:22:40.586901 (Thread-254): sending response (<Response 7030 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:40.642628 (Thread-255): handling ps request
2023-04-20 11:22:40.642982 (Thread-255): 11:22:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8250>]}
2023-04-20 11:22:40.643737 (Thread-255): sending response (<Response 7030 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:40.903182 (Thread-256): handling status request
2023-04-20 11:22:40.903673 (Thread-256): 11:22:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d165fd0>]}
2023-04-20 11:22:40.904148 (Thread-256): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:41.184069 (Thread-257): handling poll request
2023-04-20 11:22:41.184555 (Thread-257): 11:22:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1652b0>]}
2023-04-20 11:22:41.185133 (Thread-257): sending response (<Response 5739 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:41.232967 (Thread-1): 11:22:41  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(replace(trim(substring(value, 99, 8)),'','19010101'), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:22:41.233201 (Thread-1): 11:22:41  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:22:41.233333 (Thread-1): 11:22:41  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3768.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3768.0 (TID 30180) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3768.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3768.0 (TID 30180) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
Caused by: org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

2023-04-20 11:22:41.233445 (Thread-1): 11:22:41  Databricks adapter: operation-id: b'\x01\xed\xdfm\xa9n\x1c\xbe\xa5\x16\xa3rb\\~;'
2023-04-20 11:22:41.233641 (Thread-1): 11:22:41  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:22:40.015150 => 2023-04-20 11:22:41.233515
2023-04-20 11:22:41.233829 (Thread-1): 11:22:41  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:22:41.516705 (Thread-1): Got an exception: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:22:41.518533 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'\' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(replace(trim(substring(value, 99, 8)),'','19010101'), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP'\n       ) cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(replace(trim(substring(value, 99, 8)),'','19010101'), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP'\n       ) cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'\' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(replace(trim(substring(value, 99, 8)),'','19010101'), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP'\n       ) cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(replace(trim(substring(value, 99, 8)),'','19010101'), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP'\n       ) cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 11:22:42.787254 (Thread-258): handling poll request
2023-04-20 11:22:42.787792 (Thread-258): 11:22:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d12c1f0>]}
2023-04-20 11:22:42.788604 (Thread-258): sending response (<Response 51244 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:43.001158 (Thread-259): handling poll request
2023-04-20 11:22:43.001654 (Thread-259): 11:22:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d12cd60>]}
2023-04-20 11:22:43.002433 (Thread-259): sending response (<Response 51244 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:43.223856 (Thread-260): handling ps request
2023-04-20 11:22:43.224345 (Thread-260): 11:22:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1fa0>]}
2023-04-20 11:22:43.225202 (Thread-260): sending response (<Response 7053 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:43.822691 (Thread-261): handling status request
2023-04-20 11:22:43.823197 (Thread-261): 11:22:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1df0>]}
2023-04-20 11:22:43.823774 (Thread-261): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:18.192230 (Thread-262): handling run_sql request
2023-04-20 11:23:18.192721 (Thread-262): 11:23:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b11f0>]}
2023-04-20 11:23:18.252648 (Thread-263): handling ps request
2023-04-20 11:23:18.253588 (Thread-263): 11:23:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559557f0>]}
2023-04-20 11:23:18.254998 (Thread-263): sending response (<Response 7565 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:18.811943 (Thread-264): handling poll request
2023-04-20 11:23:18.812429 (Thread-264): 11:23:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9c4cd0>]}
2023-04-20 11:23:18.812940 (Thread-264): sending response (<Response 422 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:19.270419 (Thread-265): handling status request
2023-04-20 11:23:19.270901 (Thread-265): 11:23:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d12cbb0>]}
2023-04-20 11:23:19.271419 (Thread-265): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:19.726495 (Thread-266): handling ps request
2023-04-20 11:23:19.726970 (Thread-266): 11:23:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564936550>]}
2023-04-20 11:23:19.727856 (Thread-266): sending response (<Response 7566 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:20.992050 (Thread-262): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:21.028412 (MainThread): 11:23:21  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0b0d509f-6990-4c3f-b325-c4d9604505ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4ed79c100>]}
2023-04-20 11:23:21.029006 (MainThread): 11:23:21  Found 15 models, 2 tests, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:23:21.030348 (Thread-1): 11:23:21  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:23:21.030579 (Thread-1): 11:23:21  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:23:21.034766 (Thread-1): 11:23:21  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:23:21.030628 => 2023-04-20 11:23:21.034599
2023-04-20 11:23:21.034983 (Thread-1): 11:23:21  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:23:21.035337 (Thread-1): 11:23:21  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:23:21.035820 (Thread-1): 11:23:21  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:23:21.035967 (Thread-1): 11:23:21  Opening a new connection, currently in state init
2023-04-20 11:23:21.398554 (Thread-267): handling poll request
2023-04-20 11:23:21.399092 (Thread-267): 11:23:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d148a90>]}
2023-04-20 11:23:21.399741 (Thread-267): sending response (<Response 5758 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:21.565111 (Thread-268): handling ps request
2023-04-20 11:23:21.565588 (Thread-268): 11:23:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8eb0>]}
2023-04-20 11:23:21.566478 (Thread-268): sending response (<Response 7561 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:21.588458 (Thread-269): handling ps request
2023-04-20 11:23:21.588799 (Thread-269): 11:23:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56229f250>]}
2023-04-20 11:23:21.589534 (Thread-269): sending response (<Response 7561 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:21.836193 (Thread-270): handling status request
2023-04-20 11:23:21.836667 (Thread-270): 11:23:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d84c0>]}
2023-04-20 11:23:21.837152 (Thread-270): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:22.028127 (Thread-1): 11:23:22  SQL status: OK in 0.99 seconds
2023-04-20 11:23:22.059292 (Thread-1): 11:23:22  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:23:21.035030 => 2023-04-20 11:23:22.059068
2023-04-20 11:23:22.059575 (Thread-1): 11:23:22  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:23:22.176988 (Thread-271): handling poll request
2023-04-20 11:23:22.177489 (Thread-271): 11:23:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56229f310>]}
2023-04-20 11:23:22.178194 (Thread-271): sending response (<Response 6791 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:23.740016 (Thread-272): handling poll request
2023-04-20 11:23:23.740521 (Thread-272): 11:23:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d237100>]}
2023-04-20 11:23:23.746394 (Thread-272): sending response (<Response 174611 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:23.960357 (Thread-273): handling poll request
2023-04-20 11:23:23.960863 (Thread-273): 11:23:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d237430>]}
2023-04-20 11:23:23.966309 (Thread-273): sending response (<Response 180985 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:24.187677 (Thread-274): handling ps request
2023-04-20 11:23:24.188152 (Thread-274): 11:23:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa7bb50>]}
2023-04-20 11:23:24.189017 (Thread-274): sending response (<Response 7586 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:24.761465 (Thread-275): handling status request
2023-04-20 11:23:24.761981 (Thread-275): 11:23:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559ad610>]}
2023-04-20 11:23:24.762482 (Thread-275): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:24.997657 (Thread-276): handling ps request
2023-04-20 11:23:24.998171 (Thread-276): 11:23:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559ad5b0>]}
2023-04-20 11:23:25.021736 (Thread-276): sending response (<Response 7586 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:29.973443 (Thread-277): handling status request
2023-04-20 11:23:29.973962 (Thread-277): 11:23:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559ad4f0>]}
2023-04-20 11:23:29.974466 (Thread-277): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:30.497860 (Thread-278): handling cli_args request
2023-04-20 11:23:30.498462 (Thread-278): 11:23:30  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56229f370>]}
2023-04-20 11:23:33.094063 (Thread-279): 11:23:33  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:23:33.277996 (Thread-278): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:33.337291 (MainThread): 11:23:33  checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
2023-04-20 11:23:33.358159 (MainThread): 11:23:33  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 11:23:33.358336 (MainThread): 11:23:33  previous checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, current checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd
2023-04-20 11:23:33.358489 (MainThread): 11:23:33  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4d0f5e970>]}
2023-04-20 11:23:33.477649 (Thread-279): 11:23:33  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
2023-04-20 11:23:33.478393 (Thread-279): 11:23:33  Partial parsing: updated file: dbsql_dbt_tpch://models/silver/DimCompany.sql
2023-04-20 11:23:33.486916 (Thread-279): 11:23:33  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:23:33.547455 (Thread-279): 11:23:33  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb555a82670>]}
2023-04-20 11:23:33.570421 (Thread-280): handling status request
2023-04-20 11:23:33.570792 (Thread-280): 11:23:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5622c7d30>]}
2023-04-20 11:23:33.571374 (Thread-280): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:34.249421 (Thread-281): handling ps request
2023-04-20 11:23:34.249924 (Thread-281): 11:23:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5622c7c40>]}
2023-04-20 11:23:34.251001 (Thread-281): sending response (<Response 7968 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:34.329296 (Thread-282): handling poll request
2023-04-20 11:23:34.329805 (Thread-282): 11:23:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569924070>]}
2023-04-20 11:23:34.330348 (Thread-282): sending response (<Response 1863 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:34.610591 (Thread-283): handling ps request
2023-04-20 11:23:34.611131 (Thread-283): 11:23:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569924b20>]}
2023-04-20 11:23:34.612073 (Thread-283): sending response (<Response 7968 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:34.968995 (MainThread): 11:23:34  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 11:23:34.981556 (MainThread): 11:23:34  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 11:23:34.984655 (MainThread): 11:23:34  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 11:23:34.987677 (MainThread): 11:23:34  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 11:23:34.990582 (MainThread): 11:23:34  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 11:23:34.993405 (MainThread): 11:23:34  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 11:23:34.997018 (MainThread): 11:23:34  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 11:23:35.000079 (MainThread): 11:23:35  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 11:23:35.003110 (MainThread): 11:23:35  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 11:23:35.006046 (MainThread): 11:23:35  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 11:23:35.009401 (MainThread): 11:23:35  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 11:23:35.012273 (MainThread): 11:23:35  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 11:23:35.015178 (MainThread): 11:23:35  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 11:23:35.018013 (MainThread): 11:23:35  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 11:23:35.020872 (MainThread): 11:23:35  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:23:35.189010 (MainThread): 11:23:35  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4d05c76d0>]}
2023-04-20 11:23:35.224633 (MainThread): 11:23:35  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4d100fe20>]}
2023-04-20 11:23:35.225011 (MainThread): 11:23:35  Found 15 models, 2 tests, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:23:35.225160 (MainThread): 11:23:35  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4d11c4a90>]}
2023-04-20 11:23:35.227090 (MainThread): 11:23:35  
2023-04-20 11:23:35.228380 (MainThread): 11:23:35  Acquiring new databricks connection 'master'
2023-04-20 11:23:35.230108 (ThreadPoolExecutor-0_0): 11:23:35  Acquiring new databricks connection 'list_schemas'
2023-04-20 11:23:35.240990 (ThreadPoolExecutor-0_0): 11:23:35  Using databricks connection "list_schemas"
2023-04-20 11:23:35.241311 (ThreadPoolExecutor-0_0): 11:23:35  On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_schemas"} */

    show databases
  
2023-04-20 11:23:35.241470 (ThreadPoolExecutor-0_0): 11:23:35  Opening a new connection, currently in state init
2023-04-20 11:23:35.790849 (Thread-284): handling status request
2023-04-20 11:23:35.791329 (Thread-284): 11:23:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb555966220>]}
2023-04-20 11:23:35.791851 (Thread-284): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:36.310193 (ThreadPoolExecutor-0_0): 11:23:36  SQL status: OK in 1.07 seconds
2023-04-20 11:23:36.434620 (ThreadPoolExecutor-0_0): 11:23:36  On list_schemas: Close
2023-04-20 11:23:36.723590 (ThreadPoolExecutor-1_0): 11:23:36  Acquiring new databricks connection 'list_None_dbt_shabbirkdb'
2023-04-20 11:23:36.734015 (ThreadPoolExecutor-1_0): 11:23:36  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:36.734218 (ThreadPoolExecutor-1_0): 11:23:36  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 11:23:36.734376 (ThreadPoolExecutor-1_0): 11:23:36  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show tables in `dbt_shabbirkdb`
  
2023-04-20 11:23:36.734523 (ThreadPoolExecutor-1_0): 11:23:36  Opening a new connection, currently in state closed
2023-04-20 11:23:37.404691 (Thread-285): handling poll request
2023-04-20 11:23:37.405181 (Thread-285): 11:23:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5622b4220>]}
2023-04-20 11:23:37.405963 (Thread-285): sending response (<Response 11071 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:37.452849 (ThreadPoolExecutor-1_0): 11:23:37  SQL status: OK in 0.72 seconds
2023-04-20 11:23:37.462330 (ThreadPoolExecutor-1_0): 11:23:37  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 11:23:37.462544 (ThreadPoolExecutor-1_0): 11:23:37  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show views in `dbt_shabbirkdb`
  
2023-04-20 11:23:37.873220 (Thread-286): handling ps request
2023-04-20 11:23:37.872536 (ThreadPoolExecutor-1_0): 11:23:37  SQL status: OK in 0.41 seconds
2023-04-20 11:23:37.873994 (Thread-286): 11:23:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5622b41c0>]}
2023-04-20 11:23:37.874966 (Thread-286): sending response (<Response 7968 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:37.875905 (ThreadPoolExecutor-1_0): 11:23:37  On list_None_dbt_shabbirkdb: ROLLBACK
2023-04-20 11:23:37.876113 (ThreadPoolExecutor-1_0): 11:23:37  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:37.876257 (ThreadPoolExecutor-1_0): 11:23:37  On list_None_dbt_shabbirkdb: Close
2023-04-20 11:23:38.160297 (MainThread): 11:23:38  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4d0117850>]}
2023-04-20 11:23:38.160728 (MainThread): 11:23:38  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:38.160891 (MainThread): 11:23:38  Spark adapter: NotImplemented: commit
2023-04-20 11:23:38.161422 (MainThread): 11:23:38  Concurrency: 4 threads (target='default')
2023-04-20 11:23:38.161572 (MainThread): 11:23:38  
2023-04-20 11:23:38.164642 (Thread-1): 11:23:38  Began running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:23:38.165005 (Thread-1): 11:23:38  1 of 15 START sql table model dbt_shabbirkdb.DimBroker ......................... [RUN]
2023-04-20 11:23:38.165487 (Thread-1): 11:23:38  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
2023-04-20 11:23:38.165668 (Thread-1): 11:23:38  Began compiling node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:23:38.169746 (Thread-2): 11:23:38  Began running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:23:38.170093 (Thread-2): 11:23:38  2 of 15 START sql table model dbt_shabbirkdb.DimCompany ........................ [RUN]
2023-04-20 11:23:38.170660 (Thread-2): 11:23:38  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCompany'
2023-04-20 11:23:38.170944 (Thread-2): 11:23:38  Began compiling node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:23:38.175054 (Thread-3): 11:23:38  Began running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:23:38.175343 (Thread-3): 11:23:38  3 of 15 START sql table model dbt_shabbirkdb.DimCustomerStg .................... [RUN]
2023-04-20 11:23:38.175987 (Thread-3): 11:23:38  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomerStg'
2023-04-20 11:23:38.176169 (Thread-3): 11:23:38  Began compiling node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:23:38.180044 (Thread-1): 11:23:38  Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:23:38.180797 (Thread-2): 11:23:38  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:23:38.181481 (Thread-3): 11:23:38  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:23:38.181825 (Thread-4): 11:23:38  Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:23:38.182131 (Thread-4): 11:23:38  4 of 15 START sql table model dbt_shabbirkdb.tempDailyMarketHistorical ......... [RUN]
2023-04-20 11:23:38.182628 (Thread-4): 11:23:38  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempDailyMarketHistorical'
2023-04-20 11:23:38.182802 (Thread-4): 11:23:38  Began compiling node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:23:38.186391 (Thread-4): 11:23:38  Writing injected SQL for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:23:38.202438 (Thread-4): 11:23:38  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (compile): 2023-04-20 11:23:38.182845 => 2023-04-20 11:23:38.202265
2023-04-20 11:23:38.202660 (Thread-4): 11:23:38  Began executing node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:23:38.207800 (Thread-1): 11:23:38  Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 2023-04-20 11:23:38.165716 => 2023-04-20 11:23:38.207643
2023-04-20 11:23:38.208003 (Thread-1): 11:23:38  Began executing node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:23:38.212956 (Thread-2): 11:23:38  Timing info for model.dbsql_dbt_tpch.DimCompany (compile): 2023-04-20 11:23:38.171021 => 2023-04-20 11:23:38.212795
2023-04-20 11:23:38.213170 (Thread-2): 11:23:38  Began executing node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:23:38.235071 (Thread-1): 11:23:38  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:38.235246 (Thread-1): 11:23:38  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:23:38.235403 (Thread-1): 11:23:38  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

      describe extended `dbt_shabbirkdb`.`dimbroker`
  
2023-04-20 11:23:38.235559 (Thread-1): 11:23:38  Opening a new connection, currently in state closed
2023-04-20 11:23:38.235974 (Thread-3): 11:23:38  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (compile): 2023-04-20 11:23:38.176213 => 2023-04-20 11:23:38.235811
2023-04-20 11:23:38.236181 (Thread-3): 11:23:38  Began executing node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:23:38.241288 (Thread-4): 11:23:38  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:38.241452 (Thread-4): 11:23:38  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:23:38.241607 (Thread-4): 11:23:38  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

      describe extended `dbt_shabbirkdb`.`tempdailymarkethistorical`
  
2023-04-20 11:23:38.241729 (Thread-4): 11:23:38  Opening a new connection, currently in state init
2023-04-20 11:23:38.247067 (Thread-3): 11:23:38  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:38.247263 (Thread-3): 11:23:38  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:23:38.247417 (Thread-3): 11:23:38  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

      describe extended `dbt_shabbirkdb`.`dimcustomerstg`
  
2023-04-20 11:23:38.247572 (Thread-3): 11:23:38  Opening a new connection, currently in state init
2023-04-20 11:23:38.292329 (Thread-2): 11:23:38  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:23:38.307472 (Thread-2): 11:23:38  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:38.307674 (Thread-2): 11:23:38  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:23:38.307889 (Thread-2): 11:23:38  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCompany`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
  
2023-04-20 11:23:38.308014 (Thread-2): 11:23:38  Opening a new connection, currently in state init
2023-04-20 11:23:38.676571 (Thread-287): handling status request
2023-04-20 11:23:38.677097 (Thread-287): 11:23:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5650bca30>]}
2023-04-20 11:23:38.677611 (Thread-287): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:39.032437 (Thread-288): handling poll request
2023-04-20 11:23:39.032920 (Thread-288): 11:23:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb565079190>]}
2023-04-20 11:23:39.034033 (Thread-288): sending response (<Response 26162 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:39.050761 (Thread-1): 11:23:39  SQL status: OK in 0.82 seconds
2023-04-20 11:23:39.051219 (Thread-3): 11:23:39  SQL status: OK in 0.8 seconds
2023-04-20 11:23:39.054857 (Thread-4): 11:23:39  SQL status: OK in 0.81 seconds
2023-04-20 11:23:39.060767 (Thread-1): 11:23:39  Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:23:39.065131 (Thread-3): 11:23:39  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:23:39.069200 (Thread-4): 11:23:39  Writing runtime sql for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:23:39.076048 (Thread-1): 11:23:39  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:23:39.076260 (Thread-1): 11:23:39  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT
  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate`) effectivedate,
  date('9999-12-31') enddate
FROM  `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`HR`
WHERE employeejobcode = 314
  
2023-04-20 11:23:39.083720 (Thread-4): 11:23:39  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:23:39.083930 (Thread-4): 11:23:39  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempDailyMarketHistorical`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  dmh.*,
  sk_dateid,
  min(dm_low) OVER (
    PARTITION BY dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweeklow,
  max(dm_high) OVER (
    PARTITION by dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweekhigh
FROM (
  SELECT * FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketHistorical`
  UNION ALL
  SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketIncremental`) dmh
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
  ON d.datevalue = dm_date;
  
2023-04-20 11:23:39.091761 (Thread-3): 11:23:39  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:23:39.092088 (Thread-3): 11:23:39  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomerStg`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT * FROM (
  SELECT
    customerid,
    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) taxid,
    status,
    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) lastname,
    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) firstname,
    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) middleinitial,
    coalesce(gender, last_value(gender) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) gender,
    coalesce(tier, last_value(tier) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) tier,
    coalesce(dob, last_value(dob) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) dob,
    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline1,
    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline2,
    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) postalcode,
    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) CITY,
    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) stateprov,
    coalesce(country, last_value(country) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) country,
    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone1,
    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone2,
    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone3,
    coalesce(email1, last_value(email1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email1,
    coalesce(email2, last_value(email2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email2,
    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) LCL_TX_ID,
    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) NAT_TX_ID,
    batchid,
    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,
    date(update_ts) effectivedate,
    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate
  FROM (
    SELECT
      customerid,
      taxid,
      status,
      lastname,
      firstname,
      middleinitial,
      gender,
      tier,
      dob,
      addressline1,
      addressline2,
      postalcode,
      city,
      stateprov,
      country,
      phone1,
      phone2,
      phone3,
      email1,
      email2,
      lcl_tx_id,
      nat_tx_id,
      1 batchid,
      update_ts
    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')
    UNION ALL
    SELECT
      c.customerid,
      nullif(c.taxid, '') taxid,
      nullif(s.st_name, '') as status,
      nullif(c.lastname, '') lastname,
      nullif(c.firstname, '') firstname,
      nullif(c.middleinitial, '') middleinitial,
      gender,
      c.tier,
      c.dob,
      nullif(c.addressline1, '') addressline1,
      nullif(c.addressline2, '') addressline2,
      nullif(c.postalcode, '') postalcode,
      nullif(c.city, '') city,
      nullif(c.stateprov, '') stateprov,
      nullif(c.country, '') country,
      CASE
        WHEN isnull(c_local_1) then c_local_1
        ELSE concat(
          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),
          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),
          c_local_1,
          nvl(c_ext_1, '')) END as phone1,
      CASE
        WHEN isnull(c_local_2) then c_local_2
        ELSE concat(
          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),
          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),
          c_local_2,
          nvl(c_ext_2, '')) END as phone2,
      CASE
        WHEN isnull(c_local_3) then c_local_3
        ELSE concat(
          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),
          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),
          c_local_3,
          nvl(c_ext_3, '')) END as phone3,
      nullif(c.email1, '') email1,
      nullif(c.email2, '') email2,
      c.LCL_TX_ID, 
      c.NAT_TX_ID,
      c.batchid,
      timestamp(bd.batchdate) update_ts
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CustomerIncremental` c
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
      ON c.batchid = bd.batchid
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` s 
      ON c.status = s.st_id
  ) c
  )
  
2023-04-20 11:23:40.590840 (Thread-289): handling poll request
2023-04-20 11:23:40.591381 (Thread-289): 11:23:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5650797f0>]}
2023-04-20 11:23:40.592055 (Thread-289): sending response (<Response 12315 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:40.814313 (Thread-290): handling ps request
2023-04-20 11:23:40.814793 (Thread-290): 11:23:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500dc40>]}
2023-04-20 11:23:40.815708 (Thread-290): sending response (<Response 7969 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:41.239492 (Thread-291): handling status request
2023-04-20 11:23:41.239999 (Thread-291): 11:23:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569969340>]}
2023-04-20 11:23:41.240476 (Thread-291): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:42.090494 (Thread-2): 11:23:42  SQL status: OK in 3.78 seconds
2023-04-20 11:23:42.109983 (Thread-292): 11:23:42  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:23:42.115043 (Thread-2): 11:23:42  Timing info for model.dbsql_dbt_tpch.DimCompany (execute): 2023-04-20 11:23:38.213227 => 2023-04-20 11:23:42.114886
2023-04-20 11:23:42.115275 (Thread-2): 11:23:42  On model.dbsql_dbt_tpch.DimCompany: ROLLBACK
2023-04-20 11:23:42.115435 (Thread-2): 11:23:42  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:42.115600 (Thread-2): 11:23:42  On model.dbsql_dbt_tpch.DimCompany: Close
2023-04-20 11:23:42.130286 (Thread-292): 11:23:42  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 11:23:42.130648 (Thread-292): 11:23:42  previous checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, current checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886
2023-04-20 11:23:42.130848 (Thread-292): 11:23:42  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55817d3d0>]}
2023-04-20 11:23:42.139748 (Thread-293): handling poll request
2023-04-20 11:23:42.140182 (Thread-293): 11:23:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb558160310>]}
2023-04-20 11:23:42.140696 (Thread-293): sending response (<Response 2140 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:42.392126 (Thread-2): 11:23:42  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4c02ae640>]}
2023-04-20 11:23:42.392670 (Thread-2): 11:23:42  2 of 15 OK created sql table model dbt_shabbirkdb.DimCompany ................... [OK in 4.22s]
2023-04-20 11:23:42.393874 (Thread-2): 11:23:42  Finished running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:23:42.394737 (Thread-2): 11:23:42  Began running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:23:42.395056 (Thread-2): 11:23:42  5 of 15 START sql table model dbt_shabbirkdb.DimSecurity ....................... [RUN]
2023-04-20 11:23:42.395625 (Thread-2): 11:23:42  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimSecurity'
2023-04-20 11:23:42.395822 (Thread-2): 11:23:42  Began compiling node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:23:42.400551 (Thread-2): 11:23:42  Writing injected SQL for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:23:42.411545 (Thread-2): 11:23:42  Timing info for model.dbsql_dbt_tpch.DimSecurity (compile): 2023-04-20 11:23:42.395868 => 2023-04-20 11:23:42.411358
2023-04-20 11:23:42.411789 (Thread-2): 11:23:42  Began executing node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:23:42.415989 (Thread-2): 11:23:42  Writing runtime sql for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:23:42.425600 (Thread-2): 11:23:42  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:42.425781 (Thread-2): 11:23:42  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:23:42.426018 (Thread-2): 11:23:42  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 11:23:42.426151 (Thread-2): 11:23:42  Opening a new connection, currently in state closed
2023-04-20 11:23:42.571834 (Thread-1): 11:23:42  SQL status: OK in 3.5 seconds
2023-04-20 11:23:42.577425 (Thread-1): 11:23:42  Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 2023-04-20 11:23:38.208051 => 2023-04-20 11:23:42.577278
2023-04-20 11:23:42.577631 (Thread-1): 11:23:42  On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
2023-04-20 11:23:42.577770 (Thread-1): 11:23:42  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:42.589404 (Thread-294): handling status request
2023-04-20 11:23:42.577904 (Thread-1): 11:23:42  On model.dbsql_dbt_tpch.DimBroker: Close
2023-04-20 11:23:42.600292 (Thread-294): 11:23:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb555a503a0>]}
2023-04-20 11:23:42.626261 (Thread-294): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:42.857653 (Thread-1): 11:23:42  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4c012e8e0>]}
2023-04-20 11:23:42.858195 (Thread-1): 11:23:42  1 of 15 OK created sql table model dbt_shabbirkdb.DimBroker .................... [OK in 4.69s]
2023-04-20 11:23:42.858434 (Thread-1): 11:23:42  Finished running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:23:42.858717 (Thread-1): 11:23:42  Began running node model.dbsql_dbt_tpch.Financial
2023-04-20 11:23:42.859021 (Thread-1): 11:23:42  6 of 15 START sql table model dbt_shabbirkdb.Financial ......................... [RUN]
2023-04-20 11:23:42.859653 (Thread-1): 11:23:42  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Financial'
2023-04-20 11:23:42.859842 (Thread-1): 11:23:42  Began compiling node model.dbsql_dbt_tpch.Financial
2023-04-20 11:23:42.864765 (Thread-1): 11:23:42  Writing injected SQL for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:23:42.875820 (Thread-1): 11:23:42  Timing info for model.dbsql_dbt_tpch.Financial (compile): 2023-04-20 11:23:42.859888 => 2023-04-20 11:23:42.875628
2023-04-20 11:23:42.876095 (Thread-1): 11:23:42  Began executing node model.dbsql_dbt_tpch.Financial
2023-04-20 11:23:42.881690 (Thread-1): 11:23:42  Writing runtime sql for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:23:42.891879 (Thread-1): 11:23:42  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:42.892067 (Thread-1): 11:23:42  Using databricks connection "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:23:42.892292 (Thread-1): 11:23:42  On model.dbsql_dbt_tpch.Financial: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 11:23:42.892431 (Thread-1): 11:23:42  Opening a new connection, currently in state closed
2023-04-20 11:23:43.153721 (Thread-3): 11:23:43  SQL status: OK in 4.06 seconds
2023-04-20 11:23:43.156532 (Thread-3): 11:23:43  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (execute): 2023-04-20 11:23:38.236228 => 2023-04-20 11:23:43.156377
2023-04-20 11:23:43.156746 (Thread-3): 11:23:43  On model.dbsql_dbt_tpch.DimCustomerStg: ROLLBACK
2023-04-20 11:23:43.156886 (Thread-3): 11:23:43  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:43.157008 (Thread-3): 11:23:43  On model.dbsql_dbt_tpch.DimCustomerStg: Close
2023-04-20 11:23:43.201311 (Thread-2): 11:23:43  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 11:23:43.201522 (Thread-2): 11:23:43  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 78 pos 6
2023-04-20 11:23:43.201648 (Thread-2): 11:23:43  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 78 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 78 pos 6
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:23:43.201761 (Thread-2): 11:23:43  Databricks adapter: operation-id: b'\x01\xed\xdfm\xce\xa0\x11\x83\xa4\xd1\xf3\xfdB{\xdew'
2023-04-20 11:23:43.202008 (Thread-2): 11:23:43  Timing info for model.dbsql_dbt_tpch.DimSecurity (execute): 2023-04-20 11:23:42.411841 => 2023-04-20 11:23:43.201870
2023-04-20 11:23:43.202204 (Thread-2): 11:23:43  On model.dbsql_dbt_tpch.DimSecurity: ROLLBACK
2023-04-20 11:23:43.202334 (Thread-2): 11:23:43  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:43.202452 (Thread-2): 11:23:43  On model.dbsql_dbt_tpch.DimSecurity: Close
2023-04-20 11:23:43.434771 (Thread-3): 11:23:43  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4c02adb20>]}
2023-04-20 11:23:43.435290 (Thread-3): 11:23:43  3 of 15 OK created sql table model dbt_shabbirkdb.DimCustomerStg ............... [OK in 5.26s]
2023-04-20 11:23:43.435549 (Thread-3): 11:23:43  Finished running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:23:43.436395 (Thread-3): 11:23:43  Began running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:23:43.436694 (Thread-3): 11:23:43  7 of 15 START sql table model dbt_shabbirkdb.DimAccount ........................ [RUN]
2023-04-20 11:23:43.437170 (Thread-3): 11:23:43  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimAccount'
2023-04-20 11:23:43.437348 (Thread-3): 11:23:43  Began compiling node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:23:43.442563 (Thread-3): 11:23:43  Writing injected SQL for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:23:43.456971 (Thread-3): 11:23:43  Timing info for model.dbsql_dbt_tpch.DimAccount (compile): 2023-04-20 11:23:43.437394 => 2023-04-20 11:23:43.456810
2023-04-20 11:23:43.457189 (Thread-3): 11:23:43  Began executing node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:23:43.461196 (Thread-3): 11:23:43  Writing runtime sql for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:23:43.475660 (Thread-3): 11:23:43  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:43.475843 (Thread-3): 11:23:43  Using databricks connection "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:23:43.525739 (Thread-295): handling ps request
2023-04-20 11:23:43.476070 (Thread-3): 11:23:43  On model.dbsql_dbt_tpch.DimAccount: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 11:23:43.541558 (Thread-295): 11:23:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb565040e80>]}
2023-04-20 11:23:43.476202 (Thread-3): 11:23:43  Opening a new connection, currently in state closed
2023-04-20 11:23:43.567651 (Thread-295): sending response (<Response 7969 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:43.490743 (Thread-2): 11:23:43  Runtime Error in model DimSecurity (models/silver/DimSecurity.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 78 pos 6
2023-04-20 11:23:43.491215 (Thread-2): 11:23:43  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4c02ae130>]}
2023-04-20 11:23:43.491682 (Thread-2): 11:23:43  5 of 15 ERROR creating sql table model dbt_shabbirkdb.DimSecurity .............. [ERROR in 1.10s]
2023-04-20 11:23:43.491891 (Thread-2): 11:23:43  Finished running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:23:43.492166 (Thread-2): 11:23:43  Began running node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:23:43.492434 (Thread-2): 11:23:43  8 of 15 START sql table model dbt_shabbirkdb.Prospect .......................... [RUN]
2023-04-20 11:23:43.492896 (Thread-2): 11:23:43  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Prospect'
2023-04-20 11:23:43.493096 (Thread-2): 11:23:43  Began compiling node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:23:43.499073 (Thread-2): 11:23:43  Writing injected SQL for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:23:43.512769 (Thread-2): 11:23:43  Timing info for model.dbsql_dbt_tpch.Prospect (compile): 2023-04-20 11:23:43.493142 => 2023-04-20 11:23:43.512603
2023-04-20 11:23:43.513000 (Thread-2): 11:23:43  Began executing node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:23:43.518703 (Thread-2): 11:23:43  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:43.518903 (Thread-2): 11:23:43  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:23:43.519067 (Thread-2): 11:23:43  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

      describe extended `dbt_shabbirkdb`.`prospect`
  
2023-04-20 11:23:43.519196 (Thread-2): 11:23:43  Opening a new connection, currently in state closed
2023-04-20 11:23:43.622087 (Thread-1): 11:23:43  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 11:23:43.622300 (Thread-1): 11:23:43  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 61 pos 4
2023-04-20 11:23:43.730660 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 11:23:43.622423 (Thread-1): 11:23:43  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 61 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 61 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:23:43.734058 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 11:23:43.622534 (Thread-1): 11:23:43  Databricks adapter: operation-id: b'\x01\xed\xdfm\xce\xe5\x19\xc0\x84\xf9\x91\x066\x1c#\xe4'
2023-04-20 11:23:43.737579 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 11:23:43.622767 (Thread-1): 11:23:43  Timing info for model.dbsql_dbt_tpch.Financial (execute): 2023-04-20 11:23:42.876158 => 2023-04-20 11:23:43.622641
2023-04-20 11:23:43.740668 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 11:23:43.622945 (Thread-1): 11:23:43  On model.dbsql_dbt_tpch.Financial: ROLLBACK
2023-04-20 11:23:43.743901 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 11:23:43.623070 (Thread-1): 11:23:43  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:43.746868 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 11:23:43.623189 (Thread-1): 11:23:43  On model.dbsql_dbt_tpch.Financial: Close
2023-04-20 11:23:43.750241 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 11:23:43.753453 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 11:23:43.756459 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 11:23:43.759255 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 11:23:43.762514 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 11:23:43.765498 (Thread-292): 11:23:43  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 11:23:43.768175 (Thread-292): 11:23:43  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 11:23:43.770933 (Thread-292): 11:23:43  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 11:23:43.773784 (Thread-292): 11:23:43  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:23:43.796072 (Thread-296): handling status request
2023-04-20 11:23:43.801200 (Thread-296): 11:23:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564982520>]}
2023-04-20 11:23:43.801641 (Thread-296): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:43.899442 (Thread-1): 11:23:43  Runtime Error in model Financial (models/silver/Financial.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 61 pos 4
2023-04-20 11:23:43.899870 (Thread-1): 11:23:43  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4c0229940>]}
2023-04-20 11:23:43.900299 (Thread-1): 11:23:43  6 of 15 ERROR creating sql table model dbt_shabbirkdb.Financial ................ [ERROR in 1.04s]
2023-04-20 11:23:43.900509 (Thread-1): 11:23:43  Finished running node model.dbsql_dbt_tpch.Financial
2023-04-20 11:23:43.901374 (Thread-1): 11:23:43  Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 11:23:43.901631 (Thread-1): 11:23:43  9 of 15 SKIP relation dbt_shabbirkdb.tempSumpFiBasicEps ........................ [SKIP]
2023-04-20 11:23:43.901816 (Thread-1): 11:23:43  Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 11:23:43.928310 (Thread-292): 11:23:43  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb558281640>]}
2023-04-20 11:23:44.253850 (Thread-2): 11:23:44  SQL status: OK in 0.73 seconds
2023-04-20 11:23:44.260205 (Thread-2): 11:23:44  Writing runtime sql for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:23:44.267271 (Thread-3): 11:23:44  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 11:23:44.267448 (Thread-3): 11:23:44  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:23:44.267606 (Thread-3): 11:23:44  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:23:44.267727 (Thread-3): 11:23:44  Databricks adapter: operation-id: b'\x01\xed\xdfm\xcf?\x18"\xbc\x90\xd6GuF\x83\xa0'
2023-04-20 11:23:44.267952 (Thread-3): 11:23:44  Timing info for model.dbsql_dbt_tpch.DimAccount (execute): 2023-04-20 11:23:43.457238 => 2023-04-20 11:23:44.267826
2023-04-20 11:23:44.268124 (Thread-3): 11:23:44  On model.dbsql_dbt_tpch.DimAccount: ROLLBACK
2023-04-20 11:23:44.268246 (Thread-3): 11:23:44  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:44.268366 (Thread-3): 11:23:44  On model.dbsql_dbt_tpch.DimAccount: Close
2023-04-20 11:23:44.275102 (Thread-2): 11:23:44  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:23:44.275407 (Thread-2): 11:23:44  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Prospect`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  agencyid,
  recdate.sk_dateid sk_recorddateid,
  origdate.sk_dateid sk_updatedateid,
  p.batchid,
  nvl2(c.customerid, True, False) iscustomer, 
  p.lastname,
  p.firstname,
  p.middleinitial,
  p.gender,
  p.addressline1,
  p.addressline2,
  p.postalcode,
  city,
  state,
  country,
  phone,
  income,
  numbercars,
  numberchildren,
  maritalstatus,
  age,
  creditrating,
  ownorrentflag,
  employer,
  numbercreditcards,
  networth,
  if(
    isnotnull(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+","")),
    left(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+",""),
      length(
        if(networth > 1000000 or income > 200000,"HighValue+","") || 
        if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
        if(age > 45, "Boomer+", "") ||
        if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
        if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
        if(age < 25 and networth > 1000000, "Inherited+",""))
      -1),
    NULL) marketingnameplate
FROM (
  SELECT 
    * FROM (
    SELECT
      agencyid,
      max(batchid) recordbatchid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth,
      min(batchid) batchid
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`ProspectRaw` p
    GROUP BY
      agencyid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth)
  QUALIFY ROW_NUMBER() OVER (PARTITION BY agencyid ORDER BY batchid DESC) = 1) p
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) recdate
  ON p.recordbatchid = recdate.batchid
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) origdate
  ON p.batchid = origdate.batchid
LEFT JOIN (
  SELECT 
    customerid,
    lastname,
    firstname,
    addressline1,
    addressline2,
    postalcode
  FROM `dbt_shabbirkdb`.`DimCustomerStg`
  WHERE iscurrent) c
  ON 
    upper(p.LastName) = upper(c.lastname)
    and upper(p.FirstName) = upper(c.firstname)
    and upper(p.AddressLine1) = upper(c.addressline1)
    and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
    and upper(p.PostalCode) = upper(c.postalcode)
  
2023-04-20 11:23:44.293407 (Thread-297): handling poll request
2023-04-20 11:23:44.293824 (Thread-297): 11:23:44  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5649b6190>]}
2023-04-20 11:23:44.295551 (Thread-297): sending response (<Response 74900 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:44.557602 (Thread-3): 11:23:44  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:23:44.558187 (Thread-3): 11:23:44  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4c0214ca0>]}
2023-04-20 11:23:44.558821 (Thread-3): 11:23:44  7 of 15 ERROR creating sql table model dbt_shabbirkdb.DimAccount ............... [ERROR in 1.12s]
2023-04-20 11:23:44.559054 (Thread-3): 11:23:44  Finished running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:23:44.559859 (Thread-1): 11:23:44  Began running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 11:23:44.560119 (Thread-1): 11:23:44  10 of 15 SKIP relation dbt_shabbirkdb.DimTrade ................................. [SKIP]
2023-04-20 11:23:44.560309 (Thread-1): 11:23:44  Finished running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 11:23:44.560555 (Thread-1): 11:23:44  Began running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 11:23:44.560749 (Thread-1): 11:23:44  11 of 15 SKIP relation dbt_shabbirkdb.FactCashBalances ......................... [SKIP]
2023-04-20 11:23:44.560918 (Thread-1): 11:23:44  Finished running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 11:23:44.561658 (Thread-1): 11:23:44  Began running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 11:23:44.561874 (Thread-1): 11:23:44  12 of 15 SKIP relation dbt_shabbirkdb.FactHoldings ............................. [SKIP]
2023-04-20 11:23:44.562061 (Thread-1): 11:23:44  Finished running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 11:23:44.851966 (Thread-298): handling status request
2023-04-20 11:23:44.852447 (Thread-298): 11:23:44  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5649b6070>]}
2023-04-20 11:23:44.853062 (Thread-298): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:45.567111 (Thread-4): 11:23:45  SQL status: OK in 6.48 seconds
2023-04-20 11:23:45.852976 (Thread-299): handling poll request
2023-04-20 11:23:45.853576 (Thread-299): 11:23:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5649b62e0>]}
2023-04-20 11:23:45.854310 (Thread-299): sending response (<Response 7461 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:45.893870 (Thread-4): 11:23:45  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (execute): 2023-04-20 11:23:38.202710 => 2023-04-20 11:23:45.893687
2023-04-20 11:23:45.894141 (Thread-4): 11:23:45  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: ROLLBACK
2023-04-20 11:23:45.894288 (Thread-4): 11:23:45  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:45.894412 (Thread-4): 11:23:45  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: Close
2023-04-20 11:23:46.180582 (Thread-4): 11:23:46  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4a2e95190>]}
2023-04-20 11:23:46.181119 (Thread-4): 11:23:46  4 of 15 OK created sql table model dbt_shabbirkdb.tempDailyMarketHistorical .... [OK in 8.00s]
2023-04-20 11:23:46.181365 (Thread-4): 11:23:46  Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:23:46.182258 (Thread-1): 11:23:46  Began running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 11:23:46.182563 (Thread-1): 11:23:46  13 of 15 SKIP relation dbt_shabbirkdb.FactMarketHistory ........................ [SKIP]
2023-04-20 11:23:46.182756 (Thread-1): 11:23:46  Finished running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 11:23:46.383148 (Thread-300): handling ps request
2023-04-20 11:23:46.383647 (Thread-300): 11:23:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5649b6df0>]}
2023-04-20 11:23:46.384557 (Thread-300): sending response (<Response 7968 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:46.843699 (Thread-301): handling status request
2023-04-20 11:23:46.844196 (Thread-301): 11:23:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5649b6af0>]}
2023-04-20 11:23:46.844808 (Thread-301): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:47.500030 (Thread-302): handling poll request
2023-04-20 11:23:47.500525 (Thread-302): 11:23:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5649b6040>]}
2023-04-20 11:23:47.501129 (Thread-302): sending response (<Response 4955 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:48.303302 (Thread-2): 11:23:48  SQL status: OK in 4.03 seconds
2023-04-20 11:23:48.305786 (Thread-2): 11:23:48  Timing info for model.dbsql_dbt_tpch.Prospect (execute): 2023-04-20 11:23:43.513054 => 2023-04-20 11:23:48.305616
2023-04-20 11:23:48.306006 (Thread-2): 11:23:48  On model.dbsql_dbt_tpch.Prospect: ROLLBACK
2023-04-20 11:23:48.306146 (Thread-2): 11:23:48  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:48.306266 (Thread-2): 11:23:48  On model.dbsql_dbt_tpch.Prospect: Close
2023-04-20 11:23:48.550155 (Thread-303): handling status request
2023-04-20 11:23:48.550681 (Thread-303): 11:23:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb562203df0>]}
2023-04-20 11:23:48.551397 (Thread-303): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:48.589146 (Thread-2): 11:23:48  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4c028e5b0>]}
2023-04-20 11:23:48.589677 (Thread-2): 11:23:48  8 of 15 OK created sql table model dbt_shabbirkdb.Prospect ..................... [OK in 5.10s]
2023-04-20 11:23:48.589919 (Thread-2): 11:23:48  Finished running node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:23:48.591054 (Thread-4): 11:23:48  Began running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:23:48.591386 (Thread-4): 11:23:48  14 of 15 START sql table model dbt_shabbirkdb.DimCustomer ...................... [RUN]
2023-04-20 11:23:48.592023 (Thread-4): 11:23:48  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomer'
2023-04-20 11:23:48.592218 (Thread-4): 11:23:48  Began compiling node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:23:48.597511 (Thread-4): 11:23:48  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:23:48.611307 (Thread-4): 11:23:48  Timing info for model.dbsql_dbt_tpch.DimCustomer (compile): 2023-04-20 11:23:48.592265 => 2023-04-20 11:23:48.611133
2023-04-20 11:23:48.611562 (Thread-4): 11:23:48  Began executing node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:23:48.616044 (Thread-4): 11:23:48  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:23:48.631265 (Thread-4): 11:23:48  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:48.631454 (Thread-4): 11:23:48  Using databricks connection "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:23:48.631694 (Thread-4): 11:23:48  On model.dbsql_dbt_tpch.DimCustomer: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 11:23:48.631833 (Thread-4): 11:23:48  Opening a new connection, currently in state closed
2023-04-20 11:23:48.667852 (Thread-304): handling ps request
2023-04-20 11:23:48.668348 (Thread-304): 11:23:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb562203fa0>]}
2023-04-20 11:23:48.669259 (Thread-304): sending response (<Response 7969 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:48.942021 (Thread-305): handling ps request
2023-04-20 11:23:48.942542 (Thread-305): 11:23:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb562203250>]}
2023-04-20 11:23:48.966278 (Thread-305): sending response (<Response 7969 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:49.356507 (Thread-4): 11:23:49  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 11:23:49.356734 (Thread-4): 11:23:49  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:23:49.356858 (Thread-4): 11:23:49  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:23:49.356970 (Thread-4): 11:23:49  Databricks adapter: operation-id: b'\x01\xed\xdfm\xd2Q\x1dn\x9b\xc9g\xb7\xc3\x98\xc2\x1e'
2023-04-20 11:23:49.357203 (Thread-4): 11:23:49  Timing info for model.dbsql_dbt_tpch.DimCustomer (execute): 2023-04-20 11:23:48.611617 => 2023-04-20 11:23:49.357073
2023-04-20 11:23:49.357387 (Thread-4): 11:23:49  On model.dbsql_dbt_tpch.DimCustomer: ROLLBACK
2023-04-20 11:23:49.357512 (Thread-4): 11:23:49  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:49.357631 (Thread-4): 11:23:49  On model.dbsql_dbt_tpch.DimCustomer: Close
2023-04-20 11:23:49.631753 (Thread-4): 11:23:49  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:23:49.632149 (Thread-4): 11:23:49  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4c0223460>]}
2023-04-20 11:23:49.632730 (Thread-4): 11:23:49  14 of 15 ERROR creating sql table model dbt_shabbirkdb.DimCustomer ............. [ERROR in 1.04s]
2023-04-20 11:23:49.632956 (Thread-4): 11:23:49  Finished running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:23:49.634110 (Thread-3): 11:23:49  Began running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 11:23:49.634387 (Thread-3): 11:23:49  15 of 15 SKIP relation dbt_shabbirkdb.FactWatches .............................. [SKIP]
2023-04-20 11:23:49.634579 (Thread-3): 11:23:49  Finished running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 11:23:49.636355 (MainThread): 11:23:49  Acquiring new databricks connection 'master'
2023-04-20 11:23:49.636574 (MainThread): 11:23:49  On master: ROLLBACK
2023-04-20 11:23:49.636726 (MainThread): 11:23:49  Opening a new connection, currently in state init
2023-04-20 11:23:49.690996 (Thread-306): handling poll request
2023-04-20 11:23:49.691426 (Thread-306): 11:23:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564946070>]}
2023-04-20 11:23:49.692380 (Thread-306): sending response (<Response 22275 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:49.858151 (Thread-307): handling status request
2023-04-20 11:23:49.858604 (Thread-307): 11:23:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564946550>]}
2023-04-20 11:23:49.859204 (Thread-307): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:49.916055 (MainThread): 11:23:49  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:49.916291 (MainThread): 11:23:49  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:49.916426 (MainThread): 11:23:49  Spark adapter: NotImplemented: commit
2023-04-20 11:23:49.916584 (MainThread): 11:23:49  On master: ROLLBACK
2023-04-20 11:23:49.916715 (MainThread): 11:23:49  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:49.916851 (MainThread): 11:23:49  On master: Close
2023-04-20 11:23:50.189813 (MainThread): 11:23:50  Connection 'master' was properly closed.
2023-04-20 11:23:50.190017 (MainThread): 11:23:50  Connection 'model.dbsql_dbt_tpch.Financial' was properly closed.
2023-04-20 11:23:50.190127 (MainThread): 11:23:50  Connection 'model.dbsql_dbt_tpch.Prospect' was properly closed.
2023-04-20 11:23:50.190227 (MainThread): 11:23:50  Connection 'model.dbsql_dbt_tpch.DimAccount' was properly closed.
2023-04-20 11:23:50.190325 (MainThread): 11:23:50  Connection 'model.dbsql_dbt_tpch.DimCustomer' was properly closed.
2023-04-20 11:23:50.190632 (MainThread): 11:23:50  
2023-04-20 11:23:50.190781 (MainThread): 11:23:50  Finished running 15 table models in 0 hours 0 minutes and 14.96 seconds (14.96s).
2023-04-20 11:23:50.291430 (MainThread): 11:23:50  
2023-04-20 11:23:50.291708 (MainThread): 11:23:50  Completed with 4 errors and 0 warnings:
2023-04-20 11:23:50.291847 (MainThread): 11:23:50  
2023-04-20 11:23:50.291986 (MainThread): 11:23:50  Runtime Error in model DimSecurity (models/silver/DimSecurity.sql)
2023-04-20 11:23:50.292109 (MainThread): 11:23:50    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 78 pos 6
2023-04-20 11:23:50.292218 (MainThread): 11:23:50  
2023-04-20 11:23:50.292335 (MainThread): 11:23:50  Runtime Error in model Financial (models/silver/Financial.sql)
2023-04-20 11:23:50.292442 (MainThread): 11:23:50    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 61 pos 4
2023-04-20 11:23:50.292547 (MainThread): 11:23:50  
2023-04-20 11:23:50.292660 (MainThread): 11:23:50  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
2023-04-20 11:23:50.292767 (MainThread): 11:23:50    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:23:50.292870 (MainThread): 11:23:50  
2023-04-20 11:23:50.292981 (MainThread): 11:23:50  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
2023-04-20 11:23:50.293086 (MainThread): 11:23:50    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:23:50.293208 (MainThread): 11:23:50  
2023-04-20 11:23:50.293332 (MainThread): 11:23:50  Done. PASS=5 WARN=0 ERROR=4 SKIP=6 TOTAL=15
2023-04-20 11:23:51.172995 (Thread-308): handling status request
2023-04-20 11:23:51.173494 (Thread-308): 11:23:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500da30>]}
2023-04-20 11:23:51.174160 (Thread-308): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:51.308340 (Thread-309): handling ps request
2023-04-20 11:23:51.308835 (Thread-309): 11:23:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500df70>]}
2023-04-20 11:23:51.309752 (Thread-309): sending response (<Response 7993 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:51.714686 (Thread-310): handling poll request
2023-04-20 11:23:51.715183 (Thread-310): 11:23:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500d790>]}
2023-04-20 11:23:51.717741 (Thread-310): sending response (<Response 95532 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:51.942886 (Thread-311): handling status request
2023-04-20 11:23:51.943380 (Thread-311): 11:23:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500df70>]}
2023-04-20 11:23:51.944062 (Thread-311): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:24:37.984997 (Thread-312): handling status request
2023-04-20 11:24:37.985491 (Thread-312): 11:24:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500d7c0>]}
2023-04-20 11:24:37.986128 (Thread-312): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:24:38.127036 (Thread-313): handling ps request
2023-04-20 11:24:38.127548 (Thread-313): 11:24:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500d6d0>]}
2023-04-20 11:24:38.128446 (Thread-313): sending response (<Response 7993 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:05.171881 (Thread-314): handling status request
2023-04-20 11:25:05.172430 (Thread-314): 11:25:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500d670>]}
2023-04-20 11:25:05.173091 (Thread-314): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:05.297542 (Thread-315): handling status request
2023-04-20 11:25:05.298026 (Thread-315): 11:25:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500dc70>]}
2023-04-20 11:25:05.298673 (Thread-315): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:05.303165 (Thread-316): handling list request
2023-04-20 11:25:05.303483 (Thread-316): 11:25:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500ddf0>]}
2023-04-20 11:25:05.339912 (Thread-317): handling ps request
2023-04-20 11:25:05.340343 (Thread-317): 11:25:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56501bdf0>]}
2023-04-20 11:25:05.340905 (Thread-316): 11:25:05  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56501bf40>]}
2023-04-20 11:25:05.341797 (Thread-317): sending response (<Response 8265 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:05.342195 (Thread-316): 11:25:05  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:25:05.342807 (Thread-316): 11:25:05  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb565075ee0>]}
2023-04-20 11:25:05.345810 (Thread-316): sending response (<Response 6528 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:06.160116 (Thread-318): handling status request
2023-04-20 11:25:06.160605 (Thread-318): 11:25:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb553274d90>]}
2023-04-20 11:25:06.161265 (Thread-318): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:06.611085 (Thread-319): handling poll request
2023-04-20 11:25:06.611619 (Thread-319): 11:25:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5532746d0>]}
2023-04-20 11:25:06.612225 (Thread-319): sending response (<Response 303 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:11.218365 (Thread-320): handling status request
2023-04-20 11:25:11.218895 (Thread-320): 11:25:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56501beb0>]}
2023-04-20 11:25:11.219568 (Thread-320): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:11.224536 (Thread-321): handling list request
2023-04-20 11:25:11.224873 (Thread-321): 11:25:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56501bc70>]}
2023-04-20 11:25:11.254402 (Thread-321): 11:25:11  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5650c64f0>]}
2023-04-20 11:25:11.254914 (Thread-321): 11:25:11  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:25:11.255238 (Thread-321): 11:25:11  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f96eac0>]}
2023-04-20 11:25:11.257776 (Thread-321): sending response (<Response 6923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:16.881504 (Thread-322): handling status request
2023-04-20 11:25:16.882070 (Thread-322): 11:25:16  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5532746d0>]}
2023-04-20 11:25:16.882724 (Thread-322): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:16.887853 (Thread-323): handling list request
2023-04-20 11:25:16.888171 (Thread-323): 11:25:16  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569976820>]}
2023-04-20 11:25:16.927497 (Thread-323): 11:25:16  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb58983c610>]}
2023-04-20 11:25:16.928126 (Thread-323): 11:25:16  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:25:16.928479 (Thread-323): 11:25:16  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5650c6700>]}
2023-04-20 11:25:16.930895 (Thread-323): sending response (<Response 5535 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:31.790482 (Thread-324): 11:25:31  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:25:32.177789 (Thread-324): 11:25:32  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
2023-04-20 11:25:32.178402 (Thread-324): 11:25:32  Partial parsing: updated file: dbsql_dbt_tpch://models/silver/DimCompany.sql
2023-04-20 11:25:32.185959 (Thread-324): 11:25:32  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:25:32.199973 (Thread-325): handling status request
2023-04-20 11:25:32.200398 (Thread-325): 11:25:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb555a3b250>]}
2023-04-20 11:25:32.200803 (Thread-325): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:32.247672 (Thread-324): 11:25:32  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55aa0d910>]}
2023-04-20 11:25:32.473882 (Thread-326): handling status request
2023-04-20 11:25:32.474435 (Thread-326): 11:25:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56222aa90>]}
2023-04-20 11:25:32.474918 (Thread-326): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:32.479750 (Thread-327): handling list request
2023-04-20 11:25:32.480075 (Thread-327): 11:25:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56222aa60>]}
2023-04-20 11:25:32.513340 (Thread-327): 11:25:32  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55822a400>]}
2023-04-20 11:25:32.513817 (Thread-327): 11:25:32  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:25:32.514147 (Thread-327): 11:25:32  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5650c6c10>]}
2023-04-20 11:25:32.540039 (Thread-327): sending response (<Response 5535 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:33.489684 (Thread-328): handling status request
2023-04-20 11:25:33.490197 (Thread-328): 11:25:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56222aa00>]}
2023-04-20 11:25:33.490688 (Thread-328): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:33.495236 (Thread-329): handling list request
2023-04-20 11:25:33.495596 (Thread-329): 11:25:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb558281700>]}
2023-04-20 11:25:33.525722 (Thread-329): 11:25:33  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56222af70>]}
2023-04-20 11:25:33.526270 (Thread-329): 11:25:33  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:25:33.526595 (Thread-329): 11:25:33  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a6ed90>]}
2023-04-20 11:25:33.530847 (Thread-329): sending response (<Response 13286 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:33.673896 (Thread-330): handling status request
2023-04-20 11:25:33.674371 (Thread-330): 11:25:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55aa0dc70>]}
2023-04-20 11:25:33.674849 (Thread-330): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:34.717717 (Thread-331): 11:25:34  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:25:34.998013 (Thread-331): 11:25:34  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
2023-04-20 11:25:34.999102 (Thread-331): 11:25:34  Partial parsing: updated file: dbsql_dbt_tpch://models/sources.yml
2023-04-20 11:25:35.070641 (Thread-331): 11:25:35  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a1e070>]}
2023-04-20 11:25:35.148140 (Thread-332): handling status request
2023-04-20 11:25:35.148639 (Thread-332): 11:25:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a7db80>]}
2023-04-20 11:25:35.149138 (Thread-332): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:35.403274 (Thread-333): handling status request
2023-04-20 11:25:35.403807 (Thread-333): 11:25:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a7dbb0>]}
2023-04-20 11:25:35.404286 (Thread-333): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:35.409655 (Thread-334): handling list request
2023-04-20 11:25:35.409992 (Thread-334): 11:25:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a7dbe0>]}
2023-04-20 11:25:35.447901 (Thread-334): 11:25:35  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a7d700>]}
2023-04-20 11:25:35.448473 (Thread-334): 11:25:35  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:25:35.448802 (Thread-334): 11:25:35  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d157910>]}
2023-04-20 11:25:35.453021 (Thread-334): sending response (<Response 13286 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:37.464566 (Thread-335): handling status request
2023-04-20 11:25:37.465062 (Thread-335): 11:25:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55822a070>]}
2023-04-20 11:25:37.465536 (Thread-335): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:37.931932 (Thread-336): handling cli_args request
2023-04-20 11:25:37.932462 (Thread-336): 11:25:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a7d3d0>]}
2023-04-20 11:25:40.683276 (Thread-336): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:40.742563 (MainThread): 11:25:40  checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
2023-04-20 11:25:40.763727 (MainThread): 11:25:40  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 11:25:40.763901 (MainThread): 11:25:40  previous checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, current checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd
2023-04-20 11:25:40.764053 (MainThread): 11:25:40  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd24b33bac0>]}
2023-04-20 11:25:41.263395 (Thread-337): handling ps request
2023-04-20 11:25:41.264083 (Thread-337): 11:25:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5581a5e50>]}
2023-04-20 11:25:41.265761 (Thread-337): sending response (<Response 10127 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:41.346061 (Thread-338): handling status request
2023-04-20 11:25:41.346514 (Thread-338): 11:25:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a1e190>]}
2023-04-20 11:25:41.347013 (Thread-338): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:41.414194 (Thread-339): handling poll request
2023-04-20 11:25:41.414620 (Thread-339): 11:25:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a19a30>]}
2023-04-20 11:25:41.415170 (Thread-339): sending response (<Response 1863 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:41.630446 (Thread-340): handling ps request
2023-04-20 11:25:41.630935 (Thread-340): 11:25:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a19d30>]}
2023-04-20 11:25:41.631975 (Thread-340): sending response (<Response 10127 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:42.351039 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 11:25:42.363427 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 11:25:42.366562 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 11:25:42.369725 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 11:25:42.372746 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 11:25:42.375575 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 11:25:42.379378 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 11:25:42.382386 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 11:25:42.385414 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 11:25:42.388234 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 11:25:42.391609 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 11:25:42.394495 (MainThread): 11:25:42  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 11:25:42.397258 (MainThread): 11:25:42  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 11:25:42.400108 (MainThread): 11:25:42  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 11:25:42.403082 (MainThread): 11:25:42  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:25:42.564914 (MainThread): 11:25:42  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1f67f2c10>]}
2023-04-20 11:25:42.596669 (MainThread): 11:25:42  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd24b342580>]}
2023-04-20 11:25:42.596977 (MainThread): 11:25:42  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:25:42.597119 (MainThread): 11:25:42  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd24b5ab970>]}
2023-04-20 11:25:42.598935 (MainThread): 11:25:42  
2023-04-20 11:25:42.600179 (MainThread): 11:25:42  Acquiring new databricks connection 'master'
2023-04-20 11:25:42.601839 (ThreadPoolExecutor-0_0): 11:25:42  Acquiring new databricks connection 'list_schemas'
2023-04-20 11:25:42.612478 (ThreadPoolExecutor-0_0): 11:25:42  Using databricks connection "list_schemas"
2023-04-20 11:25:42.612794 (ThreadPoolExecutor-0_0): 11:25:42  On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_schemas"} */

    show databases
  
2023-04-20 11:25:42.612951 (ThreadPoolExecutor-0_0): 11:25:42  Opening a new connection, currently in state init
2023-04-20 11:25:43.376342 (Thread-341): handling poll request
2023-04-20 11:25:43.376835 (Thread-341): 11:25:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559f1400>]}
2023-04-20 11:25:43.377555 (Thread-341): sending response (<Response 8689 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:43.693658 (ThreadPoolExecutor-0_0): 11:25:43  SQL status: OK in 1.08 seconds
2023-04-20 11:25:43.819414 (ThreadPoolExecutor-0_0): 11:25:43  On list_schemas: Close
2023-04-20 11:25:44.095992 (ThreadPoolExecutor-1_0): 11:25:44  Acquiring new databricks connection 'list_None_dbt_shabbirkdb'
2023-04-20 11:25:44.106578 (ThreadPoolExecutor-1_0): 11:25:44  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:44.106791 (ThreadPoolExecutor-1_0): 11:25:44  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 11:25:44.106950 (ThreadPoolExecutor-1_0): 11:25:44  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show tables in `dbt_shabbirkdb`
  
2023-04-20 11:25:44.107096 (ThreadPoolExecutor-1_0): 11:25:44  Opening a new connection, currently in state closed
2023-04-20 11:25:44.259308 (Thread-342): handling ps request
2023-04-20 11:25:44.259870 (Thread-342): 11:25:44  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559f1040>]}
2023-04-20 11:25:44.260900 (Thread-342): sending response (<Response 10126 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:44.803816 (ThreadPoolExecutor-1_0): 11:25:44  SQL status: OK in 0.7 seconds
2023-04-20 11:25:44.813003 (ThreadPoolExecutor-1_0): 11:25:44  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 11:25:44.813228 (ThreadPoolExecutor-1_0): 11:25:44  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show views in `dbt_shabbirkdb`
  
2023-04-20 11:25:45.158334 (Thread-343): handling status request
2023-04-20 11:25:45.158845 (Thread-343): 11:25:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559f1cd0>]}
2023-04-20 11:25:45.159333 (Thread-343): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:45.281299 (ThreadPoolExecutor-1_0): 11:25:45  SQL status: OK in 0.47 seconds
2023-04-20 11:25:45.284957 (ThreadPoolExecutor-1_0): 11:25:45  On list_None_dbt_shabbirkdb: ROLLBACK
2023-04-20 11:25:45.285178 (ThreadPoolExecutor-1_0): 11:25:45  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:45.285326 (ThreadPoolExecutor-1_0): 11:25:45  On list_None_dbt_shabbirkdb: Close
2023-04-20 11:25:45.475070 (Thread-344): handling poll request
2023-04-20 11:25:45.475592 (Thread-344): 11:25:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5531c0100>]}
2023-04-20 11:25:45.476411 (Thread-344): sending response (<Response 5012 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:45.567866 (MainThread): 11:25:45  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1f67f2f10>]}
2023-04-20 11:25:45.568429 (MainThread): 11:25:45  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:45.568600 (MainThread): 11:25:45  Spark adapter: NotImplemented: commit
2023-04-20 11:25:45.569115 (MainThread): 11:25:45  Concurrency: 4 threads (target='default')
2023-04-20 11:25:45.569262 (MainThread): 11:25:45  
2023-04-20 11:25:45.572102 (Thread-1): 11:25:45  Began running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:25:45.572477 (Thread-1): 11:25:45  1 of 15 START sql table model dbt_shabbirkdb.DimBroker ......................... [RUN]
2023-04-20 11:25:45.573006 (Thread-1): 11:25:45  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
2023-04-20 11:25:45.573200 (Thread-1): 11:25:45  Began compiling node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:25:45.577242 (Thread-2): 11:25:45  Began running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:25:45.577554 (Thread-2): 11:25:45  2 of 15 START sql table model dbt_shabbirkdb.DimCompany ........................ [RUN]
2023-04-20 11:25:45.578071 (Thread-2): 11:25:45  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCompany'
2023-04-20 11:25:45.578247 (Thread-2): 11:25:45  Began compiling node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:25:45.582225 (Thread-3): 11:25:45  Began running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:25:45.582515 (Thread-3): 11:25:45  3 of 15 START sql table model dbt_shabbirkdb.DimCustomerStg .................... [RUN]
2023-04-20 11:25:45.583028 (Thread-3): 11:25:45  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomerStg'
2023-04-20 11:25:45.583202 (Thread-3): 11:25:45  Began compiling node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:25:45.588433 (Thread-1): 11:25:45  Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:25:45.588912 (Thread-2): 11:25:45  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:25:45.589274 (Thread-4): 11:25:45  Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:25:45.589564 (Thread-4): 11:25:45  4 of 15 START sql table model dbt_shabbirkdb.tempDailyMarketHistorical ......... [RUN]
2023-04-20 11:25:45.590082 (Thread-4): 11:25:45  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempDailyMarketHistorical'
2023-04-20 11:25:45.590259 (Thread-4): 11:25:45  Began compiling node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:25:45.594711 (Thread-3): 11:25:45  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:25:45.594813 (Thread-4): 11:25:45  Writing injected SQL for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:25:45.609071 (Thread-4): 11:25:45  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (compile): 2023-04-20 11:25:45.590303 => 2023-04-20 11:25:45.608890
2023-04-20 11:25:45.609328 (Thread-4): 11:25:45  Began executing node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:25:45.614421 (Thread-1): 11:25:45  Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 2023-04-20 11:25:45.574444 => 2023-04-20 11:25:45.614255
2023-04-20 11:25:45.614639 (Thread-1): 11:25:45  Began executing node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:25:45.630152 (Thread-3): 11:25:45  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (compile): 2023-04-20 11:25:45.583246 => 2023-04-20 11:25:45.629973
2023-04-20 11:25:45.630380 (Thread-3): 11:25:45  Began executing node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:25:45.642757 (Thread-1): 11:25:45  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:45.642946 (Thread-1): 11:25:45  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:25:45.643105 (Thread-1): 11:25:45  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

      describe extended `dbt_shabbirkdb`.`dimbroker`
  
2023-04-20 11:25:45.643236 (Thread-1): 11:25:45  Opening a new connection, currently in state closed
2023-04-20 11:25:45.643928 (Thread-3): 11:25:45  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:45.644106 (Thread-3): 11:25:45  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:25:45.644258 (Thread-3): 11:25:45  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

      describe extended `dbt_shabbirkdb`.`dimcustomerstg`
  
2023-04-20 11:25:45.644383 (Thread-3): 11:25:45  Opening a new connection, currently in state init
2023-04-20 11:25:45.644700 (Thread-2): 11:25:45  Timing info for model.dbsql_dbt_tpch.DimCompany (compile): 2023-04-20 11:25:45.578290 => 2023-04-20 11:25:45.644532
2023-04-20 11:25:45.644914 (Thread-2): 11:25:45  Began executing node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:25:45.640378 (Thread-4): 11:25:45  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:45.650749 (Thread-4): 11:25:45  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:25:45.650920 (Thread-4): 11:25:45  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

      describe extended `dbt_shabbirkdb`.`tempdailymarkethistorical`
  
2023-04-20 11:25:45.651048 (Thread-4): 11:25:45  Opening a new connection, currently in state init
2023-04-20 11:25:45.651335 (Thread-2): 11:25:45  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:45.651644 (Thread-2): 11:25:45  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:25:45.651822 (Thread-2): 11:25:45  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

      describe extended `dbt_shabbirkdb`.`dimcompany`
  
2023-04-20 11:25:45.651945 (Thread-2): 11:25:45  Opening a new connection, currently in state init
2023-04-20 11:25:46.412584 (Thread-3): 11:25:46  SQL status: OK in 0.77 seconds
2023-04-20 11:25:46.459940 (Thread-3): 11:25:46  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:25:46.460355 (Thread-1): 11:25:46  SQL status: OK in 0.82 seconds
2023-04-20 11:25:46.465461 (Thread-1): 11:25:46  Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:25:46.468268 (Thread-4): 11:25:46  SQL status: OK in 0.82 seconds
2023-04-20 11:25:46.468630 (Thread-2): 11:25:46  SQL status: OK in 0.82 seconds
2023-04-20 11:25:46.474684 (Thread-2): 11:25:46  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:25:46.478562 (Thread-4): 11:25:46  Writing runtime sql for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:25:46.485667 (Thread-1): 11:25:46  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:25:46.485890 (Thread-1): 11:25:46  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT
  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate`) effectivedate,
  date('9999-12-31') enddate
FROM  `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`HR`
WHERE employeejobcode = 314
  
2023-04-20 11:25:46.488231 (Thread-2): 11:25:46  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:25:46.488480 (Thread-2): 11:25:46  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCompany`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) sk_companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
  
2023-04-20 11:25:46.493704 (Thread-3): 11:25:46  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:25:46.494043 (Thread-3): 11:25:46  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomerStg`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT * FROM (
  SELECT
    customerid,
    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) taxid,
    status,
    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) lastname,
    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) firstname,
    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) middleinitial,
    coalesce(gender, last_value(gender) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) gender,
    coalesce(tier, last_value(tier) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) tier,
    coalesce(dob, last_value(dob) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) dob,
    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline1,
    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline2,
    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) postalcode,
    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) CITY,
    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) stateprov,
    coalesce(country, last_value(country) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) country,
    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone1,
    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone2,
    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone3,
    coalesce(email1, last_value(email1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email1,
    coalesce(email2, last_value(email2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email2,
    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) LCL_TX_ID,
    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) NAT_TX_ID,
    batchid,
    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,
    date(update_ts) effectivedate,
    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate
  FROM (
    SELECT
      customerid,
      taxid,
      status,
      lastname,
      firstname,
      middleinitial,
      gender,
      tier,
      dob,
      addressline1,
      addressline2,
      postalcode,
      city,
      stateprov,
      country,
      phone1,
      phone2,
      phone3,
      email1,
      email2,
      lcl_tx_id,
      nat_tx_id,
      1 batchid,
      update_ts
    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')
    UNION ALL
    SELECT
      c.customerid,
      nullif(c.taxid, '') taxid,
      nullif(s.st_name, '') as status,
      nullif(c.lastname, '') lastname,
      nullif(c.firstname, '') firstname,
      nullif(c.middleinitial, '') middleinitial,
      gender,
      c.tier,
      c.dob,
      nullif(c.addressline1, '') addressline1,
      nullif(c.addressline2, '') addressline2,
      nullif(c.postalcode, '') postalcode,
      nullif(c.city, '') city,
      nullif(c.stateprov, '') stateprov,
      nullif(c.country, '') country,
      CASE
        WHEN isnull(c_local_1) then c_local_1
        ELSE concat(
          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),
          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),
          c_local_1,
          nvl(c_ext_1, '')) END as phone1,
      CASE
        WHEN isnull(c_local_2) then c_local_2
        ELSE concat(
          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),
          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),
          c_local_2,
          nvl(c_ext_2, '')) END as phone2,
      CASE
        WHEN isnull(c_local_3) then c_local_3
        ELSE concat(
          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),
          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),
          c_local_3,
          nvl(c_ext_3, '')) END as phone3,
      nullif(c.email1, '') email1,
      nullif(c.email2, '') email2,
      c.LCL_TX_ID, 
      c.NAT_TX_ID,
      c.batchid,
      timestamp(bd.batchdate) update_ts
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CustomerIncremental` c
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
      ON c.batchid = bd.batchid
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` s 
      ON c.status = s.st_id
  ) c
  )
  
2023-04-20 11:25:46.494527 (Thread-4): 11:25:46  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:25:46.494721 (Thread-4): 11:25:46  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempDailyMarketHistorical`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  dmh.*,
  sk_dateid,
  min(dm_low) OVER (
    PARTITION BY dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweeklow,
  max(dm_high) OVER (
    PARTITION by dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweekhigh
FROM (
  SELECT * FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketHistorical`
  UNION ALL
  SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketIncremental`) dmh
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
  ON d.datevalue = dm_date;
  
2023-04-20 11:25:47.010224 (Thread-345): handling poll request
2023-04-20 11:25:47.010721 (Thread-345): 11:25:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559dca30>]}
2023-04-20 11:25:47.012007 (Thread-345): sending response (<Response 37156 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:47.268634 (Thread-346): handling ps request
2023-04-20 11:25:47.269127 (Thread-346): 11:25:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559dc9d0>]}
2023-04-20 11:25:47.270149 (Thread-346): sending response (<Response 10127 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:47.663201 (Thread-347): handling status request
2023-04-20 11:25:47.663709 (Thread-347): 11:25:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559dcfd0>]}
2023-04-20 11:25:47.664193 (Thread-347): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:48.550734 (Thread-348): handling poll request
2023-04-20 11:25:48.551229 (Thread-348): 11:25:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559dc910>]}
2023-04-20 11:25:48.551727 (Thread-348): sending response (<Response 291 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:49.259090 (Thread-2): 11:25:49  SQL status: OK in 2.77 seconds
2023-04-20 11:25:49.286078 (Thread-2): 11:25:49  Timing info for model.dbsql_dbt_tpch.DimCompany (execute): 2023-04-20 11:25:45.644961 => 2023-04-20 11:25:49.285920
2023-04-20 11:25:49.286305 (Thread-2): 11:25:49  On model.dbsql_dbt_tpch.DimCompany: ROLLBACK
2023-04-20 11:25:49.286450 (Thread-2): 11:25:49  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:49.286573 (Thread-2): 11:25:49  On model.dbsql_dbt_tpch.DimCompany: Close
2023-04-20 11:25:49.308087 (Thread-1): 11:25:49  SQL status: OK in 2.82 seconds
2023-04-20 11:25:49.310108 (Thread-1): 11:25:49  Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 2023-04-20 11:25:45.614688 => 2023-04-20 11:25:49.309976
2023-04-20 11:25:49.310294 (Thread-1): 11:25:49  On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
2023-04-20 11:25:49.310430 (Thread-1): 11:25:49  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:49.310548 (Thread-1): 11:25:49  On model.dbsql_dbt_tpch.DimBroker: Close
2023-04-20 11:25:49.571221 (Thread-2): 11:25:49  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd24ab0da30>]}
2023-04-20 11:25:49.572032 (Thread-2): 11:25:49  2 of 15 OK created sql table model dbt_shabbirkdb.DimCompany ................... [OK in 3.99s]
2023-04-20 11:25:49.573262 (Thread-2): 11:25:49  Finished running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:25:49.574360 (Thread-2): 11:25:49  Began running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:25:49.574693 (Thread-2): 11:25:49  5 of 15 START sql table model dbt_shabbirkdb.DimSecurity ....................... [RUN]
2023-04-20 11:25:49.575269 (Thread-2): 11:25:49  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimSecurity'
2023-04-20 11:25:49.575459 (Thread-2): 11:25:49  Began compiling node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:25:49.580453 (Thread-2): 11:25:49  Writing injected SQL for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:25:49.588178 (Thread-1): 11:25:49  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd2480fdbe0>]}
2023-04-20 11:25:49.588596 (Thread-1): 11:25:49  1 of 15 OK created sql table model dbt_shabbirkdb.DimBroker .................... [OK in 4.02s]
2023-04-20 11:25:49.588819 (Thread-1): 11:25:49  Finished running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:25:49.589088 (Thread-1): 11:25:49  Began running node model.dbsql_dbt_tpch.Financial
2023-04-20 11:25:49.589373 (Thread-1): 11:25:49  6 of 15 START sql table model dbt_shabbirkdb.Financial ......................... [RUN]
2023-04-20 11:25:49.589840 (Thread-1): 11:25:49  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Financial'
2023-04-20 11:25:49.590033 (Thread-1): 11:25:49  Began compiling node model.dbsql_dbt_tpch.Financial
2023-04-20 11:25:49.596365 (Thread-1): 11:25:49  Writing injected SQL for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:25:49.602908 (Thread-2): 11:25:49  Timing info for model.dbsql_dbt_tpch.DimSecurity (compile): 2023-04-20 11:25:49.575505 => 2023-04-20 11:25:49.602742
2023-04-20 11:25:49.603145 (Thread-2): 11:25:49  Began executing node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:25:49.607189 (Thread-2): 11:25:49  Writing runtime sql for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:25:49.613981 (Thread-1): 11:25:49  Timing info for model.dbsql_dbt_tpch.Financial (compile): 2023-04-20 11:25:49.590078 => 2023-04-20 11:25:49.613744
2023-04-20 11:25:49.614333 (Thread-1): 11:25:49  Began executing node model.dbsql_dbt_tpch.Financial
2023-04-20 11:25:49.619851 (Thread-1): 11:25:49  Writing runtime sql for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:25:49.626400 (Thread-2): 11:25:49  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:49.626585 (Thread-2): 11:25:49  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:25:49.626812 (Thread-2): 11:25:49  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 11:25:49.626944 (Thread-2): 11:25:49  Opening a new connection, currently in state closed
2023-04-20 11:25:49.634807 (Thread-1): 11:25:49  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:49.634989 (Thread-1): 11:25:49  Using databricks connection "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:25:49.635200 (Thread-1): 11:25:49  On model.dbsql_dbt_tpch.Financial: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 11:25:49.635326 (Thread-1): 11:25:49  Opening a new connection, currently in state closed
2023-04-20 11:25:49.803298 (Thread-349): handling ps request
2023-04-20 11:25:49.803816 (Thread-349): 11:25:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb555a20a00>]}
2023-04-20 11:25:49.804810 (Thread-349): sending response (<Response 10128 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:50.115312 (Thread-350): handling poll request
2023-04-20 11:25:50.115851 (Thread-350): 11:25:50  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb555a20c40>]}
2023-04-20 11:25:50.116738 (Thread-350): sending response (<Response 21225 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:50.218095 (Thread-351): handling status request
2023-04-20 11:25:50.218577 (Thread-351): 11:25:50  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb555a20cd0>]}
2023-04-20 11:25:50.219048 (Thread-351): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:50.393399 (Thread-2): 11:25:50  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 11:25:50.393637 (Thread-2): 11:25:50  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 86 pos 11
2023-04-20 11:25:50.393767 (Thread-2): 11:25:50  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 86 pos 11
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 86 pos 11
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:25:50.393891 (Thread-2): 11:25:50  Databricks adapter: operation-id: b'\x01\xed\xdfn\x1ap\x19K\xb1`\xe2L\xd6=\xb8t'
2023-04-20 11:25:50.394137 (Thread-2): 11:25:50  Timing info for model.dbsql_dbt_tpch.DimSecurity (execute): 2023-04-20 11:25:49.603197 => 2023-04-20 11:25:50.394005
2023-04-20 11:25:50.394324 (Thread-2): 11:25:50  On model.dbsql_dbt_tpch.DimSecurity: ROLLBACK
2023-04-20 11:25:50.394459 (Thread-2): 11:25:50  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:50.394590 (Thread-2): 11:25:50  On model.dbsql_dbt_tpch.DimSecurity: Close
2023-04-20 11:25:50.401764 (Thread-1): 11:25:50  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 11:25:50.401964 (Thread-1): 11:25:50  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 69 pos 9
2023-04-20 11:25:50.402093 (Thread-1): 11:25:50  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 69 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 69 pos 9
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:25:50.402210 (Thread-1): 11:25:50  Databricks adapter: operation-id: b'\x01\xed\xdfn\x1ap\x1e\xfd\xbch\xc7\xa3D$j\xd1'
2023-04-20 11:25:50.402419 (Thread-1): 11:25:50  Timing info for model.dbsql_dbt_tpch.Financial (execute): 2023-04-20 11:25:49.614420 => 2023-04-20 11:25:50.402305
2023-04-20 11:25:50.402584 (Thread-1): 11:25:50  On model.dbsql_dbt_tpch.Financial: ROLLBACK
2023-04-20 11:25:50.402706 (Thread-1): 11:25:50  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:50.402820 (Thread-1): 11:25:50  On model.dbsql_dbt_tpch.Financial: Close
2023-04-20 11:25:50.677257 (Thread-2): 11:25:50  Runtime Error in model DimSecurity (models/silver/DimSecurity.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 86 pos 11
2023-04-20 11:25:50.677891 (Thread-2): 11:25:50  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1f53e8580>]}
2023-04-20 11:25:50.678413 (Thread-2): 11:25:50  5 of 15 ERROR creating sql table model dbt_shabbirkdb.DimSecurity .............. [ERROR in 1.10s]
2023-04-20 11:25:50.678632 (Thread-2): 11:25:50  Finished running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:25:50.691340 (Thread-1): 11:25:50  Runtime Error in model Financial (models/silver/Financial.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 69 pos 9
2023-04-20 11:25:50.691692 (Thread-1): 11:25:50  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1f53f7100>]}
2023-04-20 11:25:50.692094 (Thread-1): 11:25:50  6 of 15 ERROR creating sql table model dbt_shabbirkdb.Financial ................ [ERROR in 1.10s]
2023-04-20 11:25:50.692295 (Thread-1): 11:25:50  Finished running node model.dbsql_dbt_tpch.Financial
2023-04-20 11:25:50.693269 (Thread-2): 11:25:50  Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 11:25:50.693564 (Thread-2): 11:25:50  7 of 15 SKIP relation dbt_shabbirkdb.tempSumpFiBasicEps ........................ [SKIP]
2023-04-20 11:25:50.693760 (Thread-2): 11:25:50  Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 11:25:50.736989 (Thread-3): 11:25:50  SQL status: OK in 4.24 seconds
2023-04-20 11:25:50.739480 (Thread-3): 11:25:50  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (execute): 2023-04-20 11:25:45.630429 => 2023-04-20 11:25:50.739312
2023-04-20 11:25:50.739760 (Thread-3): 11:25:50  On model.dbsql_dbt_tpch.DimCustomerStg: ROLLBACK
2023-04-20 11:25:50.739930 (Thread-3): 11:25:50  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:50.740077 (Thread-3): 11:25:50  On model.dbsql_dbt_tpch.DimCustomerStg: Close
2023-04-20 11:25:51.027773 (Thread-3): 11:25:51  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd24a9e85b0>]}
2023-04-20 11:25:51.028317 (Thread-3): 11:25:51  3 of 15 OK created sql table model dbt_shabbirkdb.DimCustomerStg ............... [OK in 5.44s]
2023-04-20 11:25:51.028554 (Thread-3): 11:25:51  Finished running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:25:51.029564 (Thread-2): 11:25:51  Began running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:25:51.029952 (Thread-2): 11:25:51  8 of 15 START sql table model dbt_shabbirkdb.DimAccount ........................ [RUN]
2023-04-20 11:25:51.030461 (Thread-2): 11:25:51  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimAccount'
2023-04-20 11:25:51.030648 (Thread-2): 11:25:51  Began compiling node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:25:51.034518 (Thread-1): 11:25:51  Began running node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:25:51.034799 (Thread-1): 11:25:51  9 of 15 START sql table model dbt_shabbirkdb.Prospect .......................... [RUN]
2023-04-20 11:25:51.035256 (Thread-1): 11:25:51  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Prospect'
2023-04-20 11:25:51.035445 (Thread-1): 11:25:51  Began compiling node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:25:51.040585 (Thread-1): 11:25:51  Writing injected SQL for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:25:51.042351 (Thread-2): 11:25:51  Writing injected SQL for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:25:51.056769 (Thread-1): 11:25:51  Timing info for model.dbsql_dbt_tpch.Prospect (compile): 2023-04-20 11:25:51.035504 => 2023-04-20 11:25:51.056591
2023-04-20 11:25:51.056997 (Thread-1): 11:25:51  Began executing node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:25:51.062238 (Thread-2): 11:25:51  Timing info for model.dbsql_dbt_tpch.DimAccount (compile): 2023-04-20 11:25:51.030695 => 2023-04-20 11:25:51.062024
2023-04-20 11:25:51.062472 (Thread-2): 11:25:51  Began executing node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:25:51.066376 (Thread-2): 11:25:51  Writing runtime sql for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:25:51.067055 (Thread-1): 11:25:51  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:51.067251 (Thread-1): 11:25:51  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:25:51.067414 (Thread-1): 11:25:51  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

      describe extended `dbt_shabbirkdb`.`prospect`
  
2023-04-20 11:25:51.067578 (Thread-1): 11:25:51  Opening a new connection, currently in state closed
2023-04-20 11:25:51.080091 (Thread-2): 11:25:51  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:51.080269 (Thread-2): 11:25:51  Using databricks connection "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:25:51.080489 (Thread-2): 11:25:51  On model.dbsql_dbt_tpch.DimAccount: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 11:25:51.080615 (Thread-2): 11:25:51  Opening a new connection, currently in state closed
2023-04-20 11:25:51.650585 (Thread-352): handling poll request
2023-04-20 11:25:51.651077 (Thread-352): 11:25:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a2f400>]}
2023-04-20 11:25:51.652197 (Thread-352): sending response (<Response 40777 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:51.825991 (Thread-1): 11:25:51  SQL status: OK in 0.76 seconds
2023-04-20 11:25:51.832037 (Thread-1): 11:25:51  Writing runtime sql for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:25:51.846552 (Thread-1): 11:25:51  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:25:51.846875 (Thread-1): 11:25:51  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Prospect`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  agencyid,
  recdate.sk_dateid sk_recorddateid,
  origdate.sk_dateid sk_updatedateid,
  p.batchid,
  nvl2(c.customerid, True, False) iscustomer, 
  p.lastname,
  p.firstname,
  p.middleinitial,
  p.gender,
  p.addressline1,
  p.addressline2,
  p.postalcode,
  city,
  state,
  country,
  phone,
  income,
  numbercars,
  numberchildren,
  maritalstatus,
  age,
  creditrating,
  ownorrentflag,
  employer,
  numbercreditcards,
  networth,
  if(
    isnotnull(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+","")),
    left(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+",""),
      length(
        if(networth > 1000000 or income > 200000,"HighValue+","") || 
        if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
        if(age > 45, "Boomer+", "") ||
        if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
        if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
        if(age < 25 and networth > 1000000, "Inherited+",""))
      -1),
    NULL) marketingnameplate
FROM (
  SELECT 
    * FROM (
    SELECT
      agencyid,
      max(batchid) recordbatchid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth,
      min(batchid) batchid
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`ProspectRaw` p
    GROUP BY
      agencyid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth)
  QUALIFY ROW_NUMBER() OVER (PARTITION BY agencyid ORDER BY batchid DESC) = 1) p
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) recdate
  ON p.recordbatchid = recdate.batchid
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) origdate
  ON p.batchid = origdate.batchid
LEFT JOIN (
  SELECT 
    customerid,
    lastname,
    firstname,
    addressline1,
    addressline2,
    postalcode
  FROM `dbt_shabbirkdb`.`DimCustomerStg`
  WHERE iscurrent) c
  ON 
    upper(p.LastName) = upper(c.lastname)
    and upper(p.FirstName) = upper(c.firstname)
    and upper(p.AddressLine1) = upper(c.addressline1)
    and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
    and upper(p.PostalCode) = upper(c.postalcode)
  
2023-04-20 11:25:51.864322 (Thread-2): 11:25:51  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 11:25:51.864520 (Thread-2): 11:25:51  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:25:51.864646 (Thread-2): 11:25:51  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:25:51.864761 (Thread-2): 11:25:51  Databricks adapter: operation-id: b'\x01\xed\xdfn\x1bM\x16\x14\x9f\x812\xb54\xb7\xfd\x0e'
2023-04-20 11:25:51.864990 (Thread-2): 11:25:51  Timing info for model.dbsql_dbt_tpch.DimAccount (execute): 2023-04-20 11:25:51.062522 => 2023-04-20 11:25:51.864863
2023-04-20 11:25:51.865167 (Thread-2): 11:25:51  On model.dbsql_dbt_tpch.DimAccount: ROLLBACK
2023-04-20 11:25:51.865293 (Thread-2): 11:25:51  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:51.865416 (Thread-2): 11:25:51  On model.dbsql_dbt_tpch.DimAccount: Close
2023-04-20 11:25:52.151849 (Thread-2): 11:25:52  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:25:52.152530 (Thread-2): 11:25:52  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1f5650dc0>]}
2023-04-20 11:25:52.153167 (Thread-2): 11:25:52  8 of 15 ERROR creating sql table model dbt_shabbirkdb.DimAccount ............... [ERROR in 1.12s]
2023-04-20 11:25:52.153409 (Thread-2): 11:25:52  Finished running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:25:52.154289 (Thread-3): 11:25:52  Began running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 11:25:52.154566 (Thread-3): 11:25:52  10 of 15 SKIP relation dbt_shabbirkdb.DimTrade ................................. [SKIP]
2023-04-20 11:25:52.154764 (Thread-3): 11:25:52  Finished running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 11:25:52.155135 (Thread-3): 11:25:52  Began running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 11:25:52.155501 (Thread-3): 11:25:52  11 of 15 SKIP relation dbt_shabbirkdb.FactCashBalances ......................... [SKIP]
2023-04-20 11:25:52.155865 (Thread-3): 11:25:52  Finished running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 11:25:52.156746 (Thread-3): 11:25:52  Began running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 11:25:52.156983 (Thread-3): 11:25:52  12 of 15 SKIP relation dbt_shabbirkdb.FactHoldings ............................. [SKIP]
2023-04-20 11:25:52.157164 (Thread-3): 11:25:52  Finished running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 11:25:52.345785 (Thread-353): handling ps request
2023-04-20 11:25:52.346307 (Thread-353): 11:25:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a2feb0>]}
2023-04-20 11:25:52.372189 (Thread-353): sending response (<Response 10128 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:52.813282 (Thread-354): handling status request
2023-04-20 11:25:52.813776 (Thread-354): 11:25:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a2fe80>]}
2023-04-20 11:25:52.814266 (Thread-354): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:52.997130 (Thread-4): 11:25:52  SQL status: OK in 6.5 seconds
2023-04-20 11:25:53.227386 (Thread-355): handling poll request
2023-04-20 11:25:53.227952 (Thread-355): 11:25:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a2ff40>]}
2023-04-20 11:25:53.228759 (Thread-355): sending response (<Response 22446 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:53.300525 (Thread-4): 11:25:53  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (execute): 2023-04-20 11:25:45.609379 => 2023-04-20 11:25:53.300336
2023-04-20 11:25:53.300797 (Thread-4): 11:25:53  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: ROLLBACK
2023-04-20 11:25:53.300945 (Thread-4): 11:25:53  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:53.301070 (Thread-4): 11:25:53  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: Close
2023-04-20 11:25:53.591942 (Thread-4): 11:25:53  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd24aaf87c0>]}
2023-04-20 11:25:53.592471 (Thread-4): 11:25:53  4 of 15 OK created sql table model dbt_shabbirkdb.tempDailyMarketHistorical .... [OK in 8.00s]
2023-04-20 11:25:53.592704 (Thread-4): 11:25:53  Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:25:53.593588 (Thread-3): 11:25:53  Began running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 11:25:53.593867 (Thread-3): 11:25:53  13 of 15 SKIP relation dbt_shabbirkdb.FactMarketHistory ........................ [SKIP]
2023-04-20 11:25:53.594075 (Thread-3): 11:25:53  Finished running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 11:25:54.804039 (Thread-356): handling poll request
2023-04-20 11:25:54.804534 (Thread-356): 11:25:54  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509836a0>]}
2023-04-20 11:25:54.805138 (Thread-356): sending response (<Response 4954 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:54.915307 (Thread-357): handling ps request
2023-04-20 11:25:54.915704 (Thread-357): 11:25:54  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550983bb0>]}
2023-04-20 11:25:54.916667 (Thread-357): sending response (<Response 10128 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:55.350231 (Thread-358): handling status request
2023-04-20 11:25:55.350720 (Thread-358): 11:25:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509833a0>]}
2023-04-20 11:25:55.351194 (Thread-358): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:56.238142 (Thread-1): 11:25:56  SQL status: OK in 4.39 seconds
2023-04-20 11:25:56.241212 (Thread-1): 11:25:56  Timing info for model.dbsql_dbt_tpch.Prospect (execute): 2023-04-20 11:25:51.057047 => 2023-04-20 11:25:56.241055
2023-04-20 11:25:56.241423 (Thread-1): 11:25:56  On model.dbsql_dbt_tpch.Prospect: ROLLBACK
2023-04-20 11:25:56.241564 (Thread-1): 11:25:56  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:56.241685 (Thread-1): 11:25:56  On model.dbsql_dbt_tpch.Prospect: Close
2023-04-20 11:25:56.345778 (Thread-359): handling poll request
2023-04-20 11:25:56.346284 (Thread-359): 11:25:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509837c0>]}
2023-04-20 11:25:56.346821 (Thread-359): sending response (<Response 2125 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:56.528914 (Thread-1): 11:25:56  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1f55d81c0>]}
2023-04-20 11:25:56.529653 (Thread-1): 11:25:56  9 of 15 OK created sql table model dbt_shabbirkdb.Prospect ..................... [OK in 5.49s]
2023-04-20 11:25:56.530050 (Thread-1): 11:25:56  Finished running node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:25:56.531279 (Thread-4): 11:25:56  Began running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:25:56.531638 (Thread-4): 11:25:56  14 of 15 START sql table model dbt_shabbirkdb.DimCustomer ...................... [RUN]
2023-04-20 11:25:56.532461 (Thread-4): 11:25:56  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomer'
2023-04-20 11:25:56.532755 (Thread-4): 11:25:56  Began compiling node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:25:56.538807 (Thread-4): 11:25:56  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:25:56.554593 (Thread-4): 11:25:56  Timing info for model.dbsql_dbt_tpch.DimCustomer (compile): 2023-04-20 11:25:56.532837 => 2023-04-20 11:25:56.554393
2023-04-20 11:25:56.554847 (Thread-4): 11:25:56  Began executing node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:25:56.560985 (Thread-4): 11:25:56  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:25:56.575540 (Thread-4): 11:25:56  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:56.575748 (Thread-4): 11:25:56  Using databricks connection "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:25:56.575963 (Thread-4): 11:25:56  On model.dbsql_dbt_tpch.DimCustomer: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 11:25:56.576096 (Thread-4): 11:25:56  Opening a new connection, currently in state closed
2023-04-20 11:25:57.318413 (Thread-4): 11:25:57  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 11:25:57.318660 (Thread-4): 11:25:57  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:25:57.318808 (Thread-4): 11:25:57  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:25:57.318940 (Thread-4): 11:25:57  Databricks adapter: operation-id: b'\x01\xed\xdfn\x1e\x97\x12\xbf\x8f\x91Z\xc8\\B\xce|'
2023-04-20 11:25:57.319228 (Thread-4): 11:25:57  Timing info for model.dbsql_dbt_tpch.DimCustomer (execute): 2023-04-20 11:25:56.554899 => 2023-04-20 11:25:57.319054
2023-04-20 11:25:57.319448 (Thread-4): 11:25:57  On model.dbsql_dbt_tpch.DimCustomer: ROLLBACK
2023-04-20 11:25:57.319635 (Thread-4): 11:25:57  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:57.319782 (Thread-4): 11:25:57  On model.dbsql_dbt_tpch.DimCustomer: Close
2023-04-20 11:25:57.439588 (Thread-360): handling ps request
2023-04-20 11:25:57.440079 (Thread-360): 11:25:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550989190>]}
2023-04-20 11:25:57.441084 (Thread-360): sending response (<Response 10128 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:57.601898 (Thread-4): 11:25:57  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:25:57.602322 (Thread-4): 11:25:57  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd248043eb0>]}
2023-04-20 11:25:57.602773 (Thread-4): 11:25:57  14 of 15 ERROR creating sql table model dbt_shabbirkdb.DimCustomer ............. [ERROR in 1.07s]
2023-04-20 11:25:57.602985 (Thread-4): 11:25:57  Finished running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:25:57.604314 (Thread-2): 11:25:57  Began running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 11:25:57.604668 (Thread-2): 11:25:57  15 of 15 SKIP relation dbt_shabbirkdb.FactWatches .............................. [SKIP]
2023-04-20 11:25:57.604872 (Thread-2): 11:25:57  Finished running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 11:25:57.606524 (MainThread): 11:25:57  Acquiring new databricks connection 'master'
2023-04-20 11:25:57.606741 (MainThread): 11:25:57  On master: ROLLBACK
2023-04-20 11:25:57.606892 (MainThread): 11:25:57  Opening a new connection, currently in state init
2023-04-20 11:25:57.874514 (Thread-361): handling status request
2023-04-20 11:25:57.875004 (Thread-361): 11:25:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509896d0>]}
2023-04-20 11:25:57.875495 (Thread-361): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:57.888687 (MainThread): 11:25:57  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:57.888931 (MainThread): 11:25:57  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:57.889071 (MainThread): 11:25:57  Spark adapter: NotImplemented: commit
2023-04-20 11:25:57.889228 (MainThread): 11:25:57  On master: ROLLBACK
2023-04-20 11:25:57.889378 (MainThread): 11:25:57  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:57.889517 (MainThread): 11:25:57  On master: Close
2023-04-20 11:25:57.903197 (Thread-362): handling poll request
2023-04-20 11:25:57.903582 (Thread-362): 11:25:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550989a00>]}
2023-04-20 11:25:57.904394 (Thread-362): sending response (<Response 22114 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:58.167015 (MainThread): 11:25:58  Connection 'master' was properly closed.
2023-04-20 11:25:58.167213 (MainThread): 11:25:58  Connection 'model.dbsql_dbt_tpch.Prospect' was properly closed.
2023-04-20 11:25:58.167322 (MainThread): 11:25:58  Connection 'model.dbsql_dbt_tpch.DimAccount' was properly closed.
2023-04-20 11:25:58.167426 (MainThread): 11:25:58  Connection 'model.dbsql_dbt_tpch.DimCustomerStg' was properly closed.
2023-04-20 11:25:58.167554 (MainThread): 11:25:58  Connection 'model.dbsql_dbt_tpch.DimCustomer' was properly closed.
2023-04-20 11:25:58.167856 (MainThread): 11:25:58  
2023-04-20 11:25:58.168002 (MainThread): 11:25:58  Finished running 15 table models in 0 hours 0 minutes and 15.57 seconds (15.57s).
2023-04-20 11:25:58.253557 (MainThread): 11:25:58  
2023-04-20 11:25:58.253795 (MainThread): 11:25:58  Completed with 4 errors and 0 warnings:
2023-04-20 11:25:58.253934 (MainThread): 11:25:58  
2023-04-20 11:25:58.254074 (MainThread): 11:25:58  Runtime Error in model DimSecurity (models/silver/DimSecurity.sql)
2023-04-20 11:25:58.254199 (MainThread): 11:25:58    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 86 pos 11
2023-04-20 11:25:58.254310 (MainThread): 11:25:58  
2023-04-20 11:25:58.254428 (MainThread): 11:25:58  Runtime Error in model Financial (models/silver/Financial.sql)
2023-04-20 11:25:58.254537 (MainThread): 11:25:58    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 69 pos 9
2023-04-20 11:25:58.254643 (MainThread): 11:25:58  
2023-04-20 11:25:58.254754 (MainThread): 11:25:58  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
2023-04-20 11:25:58.254859 (MainThread): 11:25:58    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:25:58.254962 (MainThread): 11:25:58  
2023-04-20 11:25:58.255073 (MainThread): 11:25:58  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
2023-04-20 11:25:58.255177 (MainThread): 11:25:58    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:25:58.255298 (MainThread): 11:25:58  
2023-04-20 11:25:58.255421 (MainThread): 11:25:58  Done. PASS=5 WARN=0 ERROR=4 SKIP=6 TOTAL=15
2023-04-20 11:25:59.597215 (Thread-363): handling poll request
2023-04-20 11:25:59.597716 (Thread-363): 11:25:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509b2c40>]}
2023-04-20 11:25:59.600011 (Thread-363): sending response (<Response 93853 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:59.983641 (Thread-364): handling ps request
2023-04-20 11:25:59.984128 (Thread-364): 11:25:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509b2d30>]}
2023-04-20 11:25:59.985141 (Thread-364): sending response (<Response 10152 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:26:00.452926 (Thread-365): handling status request
2023-04-20 11:26:00.453413 (Thread-365): 11:26:00  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509b2f70>]}
2023-04-20 11:26:00.453889 (Thread-365): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:26:20.467945 (Thread-366): handling status request
2023-04-20 11:26:20.468446 (Thread-366): 11:26:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509b2c70>]}
2023-04-20 11:26:20.468922 (Thread-366): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:26:20.475738 (Thread-367): handling list request
2023-04-20 11:26:20.476079 (Thread-367): 11:26:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509bb040>]}
2023-04-20 11:26:20.508389 (Thread-367): 11:26:20  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509a6460>]}
2023-04-20 11:26:20.508951 (Thread-367): 11:26:20  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:26:20.509322 (Thread-367): 11:26:20  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55aa2f460>]}
2023-04-20 11:26:20.511679 (Thread-367): sending response (<Response 5200 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:28:18.506029 (Thread-368): handling status request
2023-04-20 11:28:18.507932 (Thread-368): 11:28:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509bb0d0>]}
2023-04-20 11:28:18.508429 (Thread-368): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:28:21.945541 (Thread-369): handling status request
2023-04-20 11:28:21.946050 (Thread-369): 11:28:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509bb850>]}
2023-04-20 11:28:21.946529 (Thread-369): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:28:21.951587 (Thread-370): handling list request
2023-04-20 11:28:21.951952 (Thread-370): 11:28:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509bbe20>]}
2023-04-20 11:28:21.990373 (Thread-370): 11:28:21  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb553176490>]}
2023-04-20 11:28:21.991064 (Thread-370): 11:28:21  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:28:21.991420 (Thread-370): 11:28:21  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5649b4a90>]}
2023-04-20 11:28:21.993784 (Thread-370): sending response (<Response 5535 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:28:57.250909 (Thread-371): handling status request
2023-04-20 11:28:57.251399 (Thread-371): 11:28:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509fd610>]}
2023-04-20 11:28:57.251907 (Thread-371): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:14.729881 (Thread-372): handling status request
2023-04-20 11:29:14.730387 (Thread-372): 11:29:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509fdb20>]}
2023-04-20 11:29:14.730853 (Thread-372): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:14.868358 (Thread-373): handling ps request
2023-04-20 11:29:14.868806 (Thread-373): 11:29:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509fd5e0>]}
2023-04-20 11:29:14.869838 (Thread-373): sending response (<Response 10736 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:15.772652 (Thread-374): handling ps request
2023-04-20 11:29:15.773136 (Thread-374): 11:29:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509fdf40>]}
2023-04-20 11:29:15.774197 (Thread-374): sending response (<Response 10736 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:15.834253 (Thread-375): handling run_sql request
2023-04-20 11:29:15.834714 (Thread-375): 11:29:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509fdc40>]}
2023-04-20 11:29:18.573808 (Thread-375): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:18.609418 (MainThread): 11:29:18  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '668db86d-14d7-483c-a0c7-fac05e891036', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f273942bd30>]}
2023-04-20 11:29:18.610000 (MainThread): 11:29:18  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:29:18.611335 (Thread-1): 11:29:18  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:29:18.611597 (Thread-1): 11:29:18  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:29:18.615750 (Thread-1): 11:29:18  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:29:18.611648 => 2023-04-20 11:29:18.615577
2023-04-20 11:29:18.615965 (Thread-1): 11:29:18  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:29:18.616388 (Thread-1): 11:29:18  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:29:18.616754 (Thread-1): 11:29:18  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) sk_companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(replace(substring(value, 99, 8),'',NULL), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
      -- AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:29:18.616895 (Thread-1): 11:29:18  Opening a new connection, currently in state init
2023-04-20 11:29:19.259963 (Thread-376): handling ps request
2023-04-20 11:29:19.260620 (Thread-376): 11:29:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509d8dc0>]}
2023-04-20 11:29:19.262389 (Thread-376): sending response (<Response 11244 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:19.273978 (Thread-377): handling ps request
2023-04-20 11:29:19.274340 (Thread-377): 11:29:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509d8ee0>]}
2023-04-20 11:29:19.275192 (Thread-377): sending response (<Response 11244 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:19.803668 (Thread-378): handling status request
2023-04-20 11:29:19.804157 (Thread-378): 11:29:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc6a0>]}
2023-04-20 11:29:19.804666 (Thread-378): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:19.876212 (Thread-379): handling poll request
2023-04-20 11:29:19.876686 (Thread-379): 11:29:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc100>]}
2023-04-20 11:29:19.899397 (Thread-379): sending response (<Response 5779 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:20.051275 (Thread-1): 11:29:20  SQL status: OK in 1.43 seconds
2023-04-20 11:29:20.082037 (Thread-1): 11:29:20  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:29:18.616011 => 2023-04-20 11:29:20.081787
2023-04-20 11:29:20.082301 (Thread-1): 11:29:20  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:29:20.246259 (Thread-380): handling poll request
2023-04-20 11:29:20.246744 (Thread-380): 11:29:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc0d0>]}
2023-04-20 11:29:20.247347 (Thread-380): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:20.324639 (Thread-381): handling status request
2023-04-20 11:29:20.325128 (Thread-381): 11:29:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc8b0>]}
2023-04-20 11:29:20.325612 (Thread-381): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:21.778150 (Thread-382): handling poll request
2023-04-20 11:29:21.778642 (Thread-382): 11:29:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dcd00>]}
2023-04-20 11:29:21.783393 (Thread-382): sending response (<Response 171202 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:21.999738 (Thread-383): handling ps request
2023-04-20 11:29:22.000245 (Thread-383): 11:29:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc1c0>]}
2023-04-20 11:29:22.001321 (Thread-383): sending response (<Response 11269 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:22.441807 (Thread-384): handling status request
2023-04-20 11:29:22.442320 (Thread-384): 11:29:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc2b0>]}
2023-04-20 11:29:22.442857 (Thread-384): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:22.721170 (Thread-385): handling poll request
2023-04-20 11:29:22.721660 (Thread-385): 11:29:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dcd30>]}
2023-04-20 11:29:22.726412 (Thread-385): sending response (<Response 177597 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:23.661313 (Thread-386): handling status request
2023-04-20 11:29:23.661836 (Thread-386): 11:29:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dcfa0>]}
2023-04-20 11:29:23.662340 (Thread-386): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:23.958077 (Thread-387): handling ps request
2023-04-20 11:29:23.958575 (Thread-387): 11:29:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc130>]}
2023-04-20 11:29:23.959657 (Thread-387): sending response (<Response 11269 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:02.800258 (Thread-388): handling status request
2023-04-20 11:30:02.800774 (Thread-388): 11:30:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc8e0>]}
2023-04-20 11:30:02.801241 (Thread-388): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:28.897839 (Thread-389): handling run_sql request
2023-04-20 11:30:28.898363 (Thread-389): 11:30:28  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc760>]}
2023-04-20 11:30:28.935656 (Thread-390): handling ps request
2023-04-20 11:30:28.936562 (Thread-390): 11:30:28  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550927520>]}
2023-04-20 11:30:28.938425 (Thread-390): sending response (<Response 11782 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:29.455800 (Thread-391): handling poll request
2023-04-20 11:30:29.456279 (Thread-391): 11:30:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550927790>]}
2023-04-20 11:30:29.456805 (Thread-391): sending response (<Response 422 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:29.886633 (Thread-392): handling status request
2023-04-20 11:30:29.887113 (Thread-392): 11:30:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550927730>]}
2023-04-20 11:30:29.887661 (Thread-392): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:30.082616 (Thread-393): handling ps request
2023-04-20 11:30:30.083106 (Thread-393): 11:30:30  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550927850>]}
2023-04-20 11:30:30.084215 (Thread-393): sending response (<Response 11782 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:31.692840 (Thread-389): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:31.732487 (MainThread): 11:30:31  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '477a930a-62c6-4b88-bfd0-ae4ed9344772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f558e691e20>]}
2023-04-20 11:30:31.733105 (MainThread): 11:30:31  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:30:31.734494 (Thread-1): 11:30:31  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:30:31.734742 (Thread-1): 11:30:31  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:30:31.739426 (Thread-1): 11:30:31  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:30:31.734791 => 2023-04-20 11:30:31.739245
2023-04-20 11:30:31.739699 (Thread-1): 11:30:31  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:30:31.740064 (Thread-1): 11:30:31  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:30:31.740545 (Thread-1): 11:30:31  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) sk_companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
      -- AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:30:31.740700 (Thread-1): 11:30:31  Opening a new connection, currently in state init
2023-04-20 11:30:32.234945 (Thread-394): handling poll request
2023-04-20 11:30:32.235453 (Thread-394): 11:30:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509279d0>]}
2023-04-20 11:30:32.236126 (Thread-394): sending response (<Response 5806 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:32.250043 (Thread-395): handling ps request
2023-04-20 11:30:32.250558 (Thread-395): 11:30:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550927bb0>]}
2023-04-20 11:30:32.251737 (Thread-395): sending response (<Response 11777 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:32.292532 (Thread-396): handling ps request
2023-04-20 11:30:32.293005 (Thread-396): 11:30:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550927e20>]}
2023-04-20 11:30:32.294445 (Thread-396): sending response (<Response 11777 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:32.454760 (Thread-1): 11:30:32  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) sk_companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
      -- AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:30:32.454981 (Thread-1): 11:30:32  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 41, pos 51)

== SQL ==

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) sk_companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
---------------------------------------------------^^^
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
      -- AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */

2023-04-20 11:30:32.455107 (Thread-1): 11:30:32  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 41, pos 51)

== SQL ==

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) sk_companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
---------------------------------------------------^^^
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
      -- AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 41, pos 51)

== SQL ==

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) sk_companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
---------------------------------------------------^^^
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
      -- AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 11:30:32.455221 (Thread-1): 11:30:32  Databricks adapter: operation-id: b'\x01\xed\xdfn\xc2\x9e\x19I\x9a\xe6\xfbZ\x0e\x1a\xe8\xfa'
2023-04-20 11:30:32.455418 (Thread-1): 11:30:32  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:30:31.739751 => 2023-04-20 11:30:32.455294
2023-04-20 11:30:32.455640 (Thread-1): 11:30:32  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:30:32.689674 (Thread-397): handling status request
2023-04-20 11:30:32.690198 (Thread-397): 11:30:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550927970>]}
2023-04-20 11:30:32.690686 (Thread-397): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:32.781302 (Thread-1): Got an exception: Runtime Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 41, pos 51)
  
  == SQL ==
  
  SELECT 
    * FROM (
    SELECT
      cast(cik as BIGINT) sk_companyid,
      st.st_name status,
      companyname name,
      ind.in_name industry,
      if(
        SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
        SPrating, 
        cast(null as string)) sprating, 
      CASE
        WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
        WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
        ELSE cast(null as boolean)
        END as islowgrade, 
      ceoname ceo,
      addrline1 addressline1,
      addrline2 addressline2,
      postalcode,
      city,
      stateprovince stateprov,
      country,
      description,
      foundingdate,
      nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
      1 batchid,
      date(pts) effectivedate,
      coalesce(
        lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
        cast('9999-12-31' as date)) enddate
    FROM (
      SELECT
        to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
        trim(substring(value, 19, 60)) AS CompanyName,
        trim(substring(value, 79, 10)) AS CIK,
        trim(substring(value, 89, 4)) AS Status,
        trim(substring(value, 93, 2)) AS IndustryID,
        trim(substring(value, 95, 4)) AS SPrating,
        to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
  ---------------------------------------------------^^^
        trim(substring(value, 107, 80)) AS AddrLine1,
        trim(substring(value, 187, 80)) AS AddrLine2,
        trim(substring(value, 267, 12)) AS PostalCode,
        trim(substring(value, 279, 25)) AS City,
        trim(substring(value, 304, 20)) AS StateProvince,
        trim(substring(value, 324, 24)) AS Country,
        trim(substring(value, 348, 46)) AS CEOname,
        trim(substring(value, 394, 150)) AS Description
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'CMP'
        -- AND trim(substring(value, 99, 8)) <> ''
         ) cmp
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
  )
  limit 500
  /* limit added automatically by dbt cloud */
  
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 41, pos 51)

== SQL ==

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) sk_companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
---------------------------------------------------^^^
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
      -- AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 41, pos 51)
  
  == SQL ==
  
  SELECT 
    * FROM (
    SELECT
      cast(cik as BIGINT) sk_companyid,
      st.st_name status,
      companyname name,
      ind.in_name industry,
      if(
        SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
        SPrating, 
        cast(null as string)) sprating, 
      CASE
        WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
        WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
        ELSE cast(null as boolean)
        END as islowgrade, 
      ceoname ceo,
      addrline1 addressline1,
      addrline2 addressline2,
      postalcode,
      city,
      stateprovince stateprov,
      country,
      description,
      foundingdate,
      nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
      1 batchid,
      date(pts) effectivedate,
      coalesce(
        lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
        cast('9999-12-31' as date)) enddate
    FROM (
      SELECT
        to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
        trim(substring(value, 19, 60)) AS CompanyName,
        trim(substring(value, 79, 10)) AS CIK,
        trim(substring(value, 89, 4)) AS Status,
        trim(substring(value, 93, 2)) AS IndustryID,
        trim(substring(value, 95, 4)) AS SPrating,
        to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
  ---------------------------------------------------^^^
        trim(substring(value, 107, 80)) AS AddrLine1,
        trim(substring(value, 187, 80)) AS AddrLine2,
        trim(substring(value, 267, 12)) AS PostalCode,
        trim(substring(value, 279, 25)) AS City,
        trim(substring(value, 304, 20)) AS StateProvince,
        trim(substring(value, 324, 24)) AS Country,
        trim(substring(value, 348, 46)) AS CEOname,
        trim(substring(value, 394, 150)) AS Description
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'CMP'
        -- AND trim(substring(value, 99, 8)) <> ''
         ) cmp
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
  )
  limit 500
  /* limit added automatically by dbt cloud */
2023-04-20 11:30:32.783243 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': "Runtime Error in rpc request (from remote system.sql)\n  \n  [PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 41, pos 51)\n  \n  == SQL ==\n  \n  SELECT \n    * FROM (\n    SELECT\n      cast(cik as BIGINT) sk_companyid,\n      st.st_name status,\n      companyname name,\n      ind.in_name industry,\n      if(\n        SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n        SPrating, \n        cast(null as string)) sprating, \n      CASE\n        WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n        WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n        ELSE cast(null as boolean)\n        END as islowgrade, \n      ceoname ceo,\n      addrline1 addressline1,\n      addrline2 addressline2,\n      postalcode,\n      city,\n      stateprovince stateprov,\n      country,\n      description,\n      foundingdate,\n      nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n      1 batchid,\n      date(pts) effectivedate,\n      coalesce(\n        lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n        cast('9999-12-31' as date)) enddate\n    FROM (\n      SELECT\n        to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n        trim(substring(value, 19, 60)) AS CompanyName,\n        trim(substring(value, 79, 10)) AS CIK,\n        trim(substring(value, 89, 4)) AS Status,\n        trim(substring(value, 93, 2)) AS IndustryID,\n        trim(substring(value, 95, 4)) AS SPrating,\n        to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n  ---------------------------------------------------^^^\n        trim(substring(value, 107, 80)) AS AddrLine1,\n        trim(substring(value, 187, 80)) AS AddrLine2,\n        trim(substring(value, 267, 12)) AS PostalCode,\n        trim(substring(value, 279, 25)) AS City,\n        trim(substring(value, 304, 20)) AS StateProvince,\n        trim(substring(value, 324, 24)) AS Country,\n        trim(substring(value, 348, 46)) AS CEOname,\n        trim(substring(value, 394, 150)) AS Description\n      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n      WHERE rectype = 'CMP'\n        -- AND trim(substring(value, 99, 8)) <> ''\n         ) cmp\n    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n  )\n  limit 500\n  /* limit added automatically by dbt cloud */\n  ", 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) sk_companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP'\n      -- AND trim(substring(value, 99, 8)) <> ''\n       ) cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) sk_companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP'\n      -- AND trim(substring(value, 99, 8)) <> ''\n       ) cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': "Runtime Error in rpc request (from remote system.sql)\n  \n  [PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 41, pos 51)\n  \n  == SQL ==\n  \n  SELECT \n    * FROM (\n    SELECT\n      cast(cik as BIGINT) sk_companyid,\n      st.st_name status,\n      companyname name,\n      ind.in_name industry,\n      if(\n        SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n        SPrating, \n        cast(null as string)) sprating, \n      CASE\n        WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n        WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n        ELSE cast(null as boolean)\n        END as islowgrade, \n      ceoname ceo,\n      addrline1 addressline1,\n      addrline2 addressline2,\n      postalcode,\n      city,\n      stateprovince stateprov,\n      country,\n      description,\n      foundingdate,\n      nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n      1 batchid,\n      date(pts) effectivedate,\n      coalesce(\n        lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n        cast('9999-12-31' as date)) enddate\n    FROM (\n      SELECT\n        to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n        trim(substring(value, 19, 60)) AS CompanyName,\n        trim(substring(value, 79, 10)) AS CIK,\n        trim(substring(value, 89, 4)) AS Status,\n        trim(substring(value, 93, 2)) AS IndustryID,\n        trim(substring(value, 95, 4)) AS SPrating,\n        to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n  ---------------------------------------------------^^^\n        trim(substring(value, 107, 80)) AS AddrLine1,\n        trim(substring(value, 187, 80)) AS AddrLine2,\n        trim(substring(value, 267, 12)) AS PostalCode,\n        trim(substring(value, 279, 25)) AS City,\n        trim(substring(value, 304, 20)) AS StateProvince,\n        trim(substring(value, 324, 24)) AS Country,\n        trim(substring(value, 348, 46)) AS CEOname,\n        trim(substring(value, 394, 150)) AS Description\n      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n      WHERE rectype = 'CMP'\n        -- AND trim(substring(value, 99, 8)) <> ''\n         ) cmp\n    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n  )\n  limit 500\n  /* limit added automatically by dbt cloud */\n  ", 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) sk_companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP'\n      -- AND trim(substring(value, 99, 8)) <> ''\n       ) cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) sk_companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP'\n      -- AND trim(substring(value, 99, 8)) <> ''\n       ) cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 11:30:33.018925 (Thread-398): handling poll request
2023-04-20 11:30:33.019448 (Thread-398): 11:30:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550927400>]}
2023-04-20 11:30:33.020350 (Thread-398): sending response (<Response 50579 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:34.590274 (Thread-399): handling poll request
2023-04-20 11:30:34.590781 (Thread-399): 11:30:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509273d0>]}
2023-04-20 11:30:34.591642 (Thread-399): sending response (<Response 57684 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:34.739259 (Thread-400): handling poll request
2023-04-20 11:30:34.739786 (Thread-400): 11:30:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc760>]}
2023-04-20 11:30:34.740631 (Thread-400): sending response (<Response 57684 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:35.095295 (Thread-401): handling ps request
2023-04-20 11:30:35.095904 (Thread-401): 11:30:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509278e0>]}
2023-04-20 11:30:35.097190 (Thread-401): sending response (<Response 11800 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:35.718890 (Thread-402): handling status request
2023-04-20 11:30:35.719403 (Thread-402): 11:30:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc280>]}
2023-04-20 11:30:35.719920 (Thread-402): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:51.904094 (Thread-403): handling run_sql request
2023-04-20 11:30:51.904671 (Thread-403): 11:30:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dcd90>]}
2023-04-20 11:30:51.906383 (Thread-404): handling ps request
2023-04-20 11:30:51.907013 (Thread-404): 11:30:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2ae8e0>]}
2023-04-20 11:30:51.908221 (Thread-404): sending response (<Response 12313 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:52.463254 (Thread-405): handling poll request
2023-04-20 11:30:52.464241 (Thread-405): 11:30:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2aeee0>]}
2023-04-20 11:30:52.465104 (Thread-405): sending response (<Response 421 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:52.870797 (Thread-406): handling status request
2023-04-20 11:30:52.871316 (Thread-406): 11:30:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2ae940>]}
2023-04-20 11:30:52.871866 (Thread-406): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:53.144420 (Thread-407): handling ps request
2023-04-20 11:30:53.144902 (Thread-407): 11:30:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2b5070>]}
2023-04-20 11:30:53.146477 (Thread-407): sending response (<Response 12313 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:54.671072 (Thread-403): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:54.708040 (MainThread): 11:30:54  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7a6925f4-d697-48be-ac1b-171a367fd662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd002133e20>]}
2023-04-20 11:30:54.708650 (MainThread): 11:30:54  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:30:54.710031 (Thread-1): 11:30:54  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:30:54.710262 (Thread-1): 11:30:54  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:30:54.714934 (Thread-1): 11:30:54  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:30:54.710310 => 2023-04-20 11:30:54.714765
2023-04-20 11:30:54.715155 (Thread-1): 11:30:54  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:30:54.715551 (Thread-1): 11:30:54  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:30:54.716017 (Thread-1): 11:30:54  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) sk_companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
      -- AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:30:54.716165 (Thread-1): 11:30:54  Opening a new connection, currently in state init
2023-04-20 11:30:55.010505 (Thread-408): handling poll request
2023-04-20 11:30:55.010983 (Thread-408): 11:30:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2b5130>]}
2023-04-20 11:30:55.011603 (Thread-408): sending response (<Response 5805 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:55.241411 (Thread-409): handling ps request
2023-04-20 11:30:55.241900 (Thread-409): 11:30:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2b57c0>]}
2023-04-20 11:30:55.265885 (Thread-409): sending response (<Response 12308 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:55.300630 (Thread-410): handling ps request
2023-04-20 11:30:55.301053 (Thread-410): 11:30:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2b55b0>]}
2023-04-20 11:30:55.302101 (Thread-410): sending response (<Response 12308 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:55.428540 (Thread-411): handling status request
2023-04-20 11:30:55.429013 (Thread-411): 11:30:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2b5b50>]}
2023-04-20 11:30:55.429485 (Thread-411): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:56.033257 (Thread-412): handling poll request
2023-04-20 11:30:56.033751 (Thread-412): 11:30:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2b5310>]}
2023-04-20 11:30:56.034349 (Thread-412): sending response (<Response 5805 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:56.101486 (Thread-1): 11:30:56  SQL status: OK in 1.38 seconds
2023-04-20 11:30:56.132607 (Thread-1): 11:30:56  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:30:54.715200 => 2023-04-20 11:30:56.132383
2023-04-20 11:30:56.132860 (Thread-1): 11:30:56  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:30:57.588262 (Thread-413): handling poll request
2023-04-20 11:30:57.588734 (Thread-413): 11:30:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2aef70>]}
2023-04-20 11:30:57.594356 (Thread-413): sending response (<Response 181479 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:57.644027 (Thread-414): handling poll request
2023-04-20 11:30:57.644439 (Thread-414): 11:30:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2aee20>]}
2023-04-20 11:30:57.649523 (Thread-414): sending response (<Response 176089 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:57.863189 (Thread-415): handling ps request
2023-04-20 11:30:57.863682 (Thread-415): 11:30:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2aefd0>]}
2023-04-20 11:30:57.864778 (Thread-415): sending response (<Response 12333 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:58.514944 (Thread-416): handling status request
2023-04-20 11:30:58.515457 (Thread-416): 11:30:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2ae340>]}
2023-04-20 11:30:58.515983 (Thread-416): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:58.739606 (Thread-417): handling ps request
2023-04-20 11:30:58.740074 (Thread-417): 11:30:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2ae490>]}
2023-04-20 11:30:58.741468 (Thread-417): sending response (<Response 12333 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:31:09.027237 (Thread-418): 11:31:09  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:31:09.051973 (Thread-418): 11:31:09  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 11:31:09.052382 (Thread-418): 11:31:09  previous checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, current checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886
2023-04-20 11:31:09.052593 (Thread-418): 11:31:09  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2ae550>]}
2023-04-20 11:31:09.522040 (Thread-419): handling status request
2023-04-20 11:31:09.522556 (Thread-419): 11:31:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559ad2e0>]}
2023-04-20 11:31:09.523011 (Thread-419): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:31:10.877985 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 11:31:10.881386 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 11:31:10.884657 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 11:31:10.887586 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 11:31:10.890552 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 11:31:10.893269 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 11:31:10.896396 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 11:31:10.899544 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 11:31:10.902585 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 11:31:10.905358 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 11:31:10.908650 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 11:31:10.911550 (Thread-418): 11:31:10  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 11:31:10.914167 (Thread-418): 11:31:10  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 11:31:10.916944 (Thread-418): 11:31:10  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 11:31:10.920035 (Thread-418): 11:31:10  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:31:11.072956 (Thread-418): 11:31:11  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdad1c0>]}
2023-04-20 11:31:11.920850 (Thread-420): handling status request
2023-04-20 11:31:11.921382 (Thread-420): 11:31:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdad8b0>]}
2023-04-20 11:31:11.922019 (Thread-420): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:31:13.103293 (Thread-421): 11:31:13  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:31:13.383899 (Thread-421): 11:31:13  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
2023-04-20 11:31:13.384540 (Thread-421): 11:31:13  Partial parsing: updated file: dbsql_dbt_tpch://models/silver/DimCompany.sql
2023-04-20 11:31:13.394538 (Thread-421): 11:31:13  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:31:13.456603 (Thread-421): 11:31:13  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd2b8b0>]}
2023-04-20 11:31:13.613511 (Thread-422): handling status request
2023-04-20 11:31:13.614014 (Thread-422): 11:31:13  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd1dd60>]}
2023-04-20 11:31:13.614505 (Thread-422): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:32:53.740450 (Thread-423): handling status request
2023-04-20 11:32:53.742397 (Thread-423): 11:32:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd1d700>]}
2023-04-20 11:32:53.742877 (Thread-423): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:33:00.512514 (Thread-424): 11:33:00  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:33:00.941303 (Thread-424): 11:33:00  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
2023-04-20 11:33:00.941916 (Thread-424): 11:33:00  Partial parsing: updated file: dbsql_dbt_tpch://models/silver/DimCompany.sql
2023-04-20 11:33:00.949506 (Thread-424): 11:33:00  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:33:01.010538 (Thread-424): 11:33:01  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb6a9d0>]}
2023-04-20 11:33:01.038727 (Thread-425): handling status request
2023-04-20 11:33:01.039124 (Thread-425): 11:33:01  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb64a60>]}
2023-04-20 11:33:01.039607 (Thread-425): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:33:37.926550 (Thread-426): handling status request
2023-04-20 11:33:37.927047 (Thread-426): 11:33:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb64b80>]}
2023-04-20 11:33:37.927544 (Thread-426): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:33:38.023009 (Thread-427): handling ps request
2023-04-20 11:33:38.023430 (Thread-427): 11:33:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb64a30>]}
2023-04-20 11:33:38.024567 (Thread-427): sending response (<Response 12333 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:37:32.897275 (Thread-428): handling status request
2023-04-20 11:37:32.899177 (Thread-428): 11:37:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb67370>]}
2023-04-20 11:37:32.899694 (Thread-428): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:38:36.571249 (Thread-429): handling status request
2023-04-20 11:38:36.573101 (Thread-429): 11:38:36  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb64f10>]}
2023-04-20 11:38:36.573581 (Thread-429): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:39:38.301114 (Thread-430): handling status request
2023-04-20 11:39:38.301622 (Thread-430): 11:39:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb675e0>]}
2023-04-20 11:39:38.302124 (Thread-430): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:40:17.390838 (Thread-431): 11:40:17  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:40:17.666052 (Thread-431): 11:40:17  Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
2023-04-20 11:40:17.666334 (Thread-431): 11:40:17  Partial parsing enabled, no changes found, skipping parsing
2023-04-20 11:40:17.674329 (Thread-431): 11:40:17  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54b98cd90>]}
2023-04-20 11:40:17.790638 (Thread-432): handling status request
2023-04-20 11:40:17.791102 (Thread-432): 11:40:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb641f0>]}
2023-04-20 11:40:17.791587 (Thread-432): sending response (<Response 1593 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:44:05.762616 (Thread-433): handling status request
2023-04-20 11:44:05.764496 (Thread-433): 11:44:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb678e0>]}
2023-04-20 11:44:05.764970 (Thread-433): sending response (<Response 1593 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:44:16.123683 (Thread-434): handling poll request
2023-04-20 11:44:16.124175 (Thread-434): 11:44:16  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb67730>]}
2023-04-20 11:44:16.129432 (Thread-434): sending response (<Response 181479 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:44:16.771926 (Thread-435): handling status request
2023-04-20 11:44:16.772418 (Thread-435): 11:44:16  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb67af0>]}
2023-04-20 11:44:16.772890 (Thread-435): sending response (<Response 1593 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:44:17.009692 (Thread-436): handling ps request
2023-04-20 11:44:17.010182 (Thread-436): 11:44:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb67640>]}
2023-04-20 11:44:17.011293 (Thread-436): sending response (<Response 12333 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:45:06.706796 (Thread-437): handling run_sql request
2023-04-20 11:45:06.707298 (Thread-437): 11:45:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbc0eb0>]}
2023-04-20 11:45:06.753532 (Thread-438): handling ps request
2023-04-20 11:45:06.754543 (Thread-438): 11:45:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e14ce50>]}
2023-04-20 11:45:06.784841 (Thread-438): sending response (<Response 12846 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:45:07.655178 (Thread-439): handling poll request
2023-04-20 11:45:07.655717 (Thread-439): 11:45:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e14cdf0>]}
2023-04-20 11:45:07.656259 (Thread-439): sending response (<Response 422 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:45:08.086167 (Thread-440): handling status request
2023-04-20 11:45:08.086653 (Thread-440): 11:45:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e14ca00>]}
2023-04-20 11:45:08.087171 (Thread-440): sending response (<Response 1593 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:45:08.304779 (Thread-441): handling ps request
2023-04-20 11:45:08.305273 (Thread-441): 11:45:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e14cb80>]}
2023-04-20 11:45:08.306442 (Thread-441): sending response (<Response 12846 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:45:09.546330 (Thread-437): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:45:09.586288 (MainThread): 11:45:09  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '146d4d23-ce93-4769-b556-c9ac8f5f4315', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5910f98dc0>]}
2023-04-20 11:45:09.586872 (MainThread): 11:45:09  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:45:09.588254 (Thread-1): 11:45:09  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:45:09.588493 (Thread-1): 11:45:09  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:45:09.593203 (Thread-1): 11:45:09  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:45:09.588543 => 2023-04-20 11:45:09.593040
2023-04-20 11:45:09.593348 (Thread-1): Got an exception: Compilation Error in rpc request (from remote system.sql)
  'dict object' has no attribute 'generate_surrogate_key'. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 513, in catch_jinja
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 545, in render_template
    return template.render(ctx)
  File "/usr/local/lib/python3.8/dist-packages/jinja2/environment.py", line 1301, in render
    self.environment.handle_exception()
  File "/usr/local/lib/python3.8/dist-packages/jinja2/environment.py", line 936, in handle_exception
    raise rewrite_traceback_stack(source=source)
  File "<template>", line 9, in top-level template code
  File "/usr/local/lib/python3.8/dist-packages/jinja2/sandbox.py", line 391, in call
    if not __self.is_safe_callable(__obj):
  File "/usr/local/lib/python3.8/dist-packages/jinja2/sandbox.py", line 275, in is_safe_callable
    getattr(obj, "unsafe_callable", False) or getattr(obj, "alters_data", False)
jinja2.exceptions.UndefinedError: 'dict object' has no attribute 'generate_surrogate_key'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 304, in compile_and_execute
    ctx.node = self.compile(manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 57, in compile
    return compiler.compile_node(self.node, manifest, {}, write=False)
  File "/usr/local/lib/python3.8/dist-packages/dbt/compilation.py", line 524, in compile_node
    node = self._compile_code(node, manifest, extra_context)
  File "/usr/local/lib/python3.8/dist-packages/dbt/compilation.py", line 370, in _compile_code
    node.compiled_code = jinja.get_rendered(
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 590, in get_rendered
    return render_template(template, ctx, node)
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 545, in render_template
    return template.render(ctx)
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/clients/jinja.py", line 518, in catch_jinja
    raise UndefinedMacroError(str(e), node) from e
dbt.exceptions.UndefinedMacroError: Compilation Error in rpc request (from remote system.sql)
  'dict object' has no attribute 'generate_surrogate_key'. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
2023-04-20 11:45:09.594888 (Thread-1): Got exception RPCException(10004, Compilation Error, {'type': 'UndefinedMacroError', 'message': 'Compilation Error in rpc request (from remote system.sql)\n  \'dict object\' has no attribute \'generate_surrogate_key\'. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    {{ dbt_utils.generate_surrogate_key(['companyid']) }} sk_companyid,\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP'\n       ) cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': None, 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10004, Compilation Error, {'type': 'UndefinedMacroError', 'message': 'Compilation Error in rpc request (from remote system.sql)\n  \'dict object\' has no attribute \'generate_surrogate_key\'. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    {{ dbt_utils.generate_surrogate_key(['companyid']) }} sk_companyid,\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP'\n       ) cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': None, 'tags': None}, None)
2023-04-20 11:45:10.126269 (Thread-442): handling ps request
2023-04-20 11:45:10.126760 (Thread-442): 11:45:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e14ceb0>]}
2023-04-20 11:45:10.127946 (Thread-442): sending response (<Response 12864 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:45:10.143501 (Thread-443): handling ps request
2023-04-20 11:45:10.143893 (Thread-443): 11:45:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e14c670>]}
2023-04-20 11:45:10.144929 (Thread-443): sending response (<Response 12864 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:45:10.301156 (Thread-444): handling poll request
2023-04-20 11:45:10.301664 (Thread-444): 11:45:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e14ca30>]}
2023-04-20 11:45:10.302264 (Thread-444): sending response (<Response 14481 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:45:10.590475 (Thread-445): handling status request
2023-04-20 11:45:10.590967 (Thread-445): 11:45:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e14c580>]}
2023-04-20 11:45:10.591438 (Thread-445): sending response (<Response 1593 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:45:10.791780 (Thread-446): handling poll request
2023-04-20 11:45:10.792266 (Thread-446): 11:45:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e14cb80>]}
2023-04-20 11:45:10.792823 (Thread-446): sending response (<Response 14481 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:46:24.510354 (Thread-447): handling status request
2023-04-20 11:46:24.510890 (Thread-447): 11:46:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a96adf0>]}
2023-04-20 11:46:24.511354 (Thread-447): sending response (<Response 1593 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:46:45.512416 (Thread-448): handling run_sql request
2023-04-20 11:46:45.512909 (Thread-448): 11:46:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be46e80>]}
2023-04-20 11:46:45.544305 (Thread-449): handling ps request
2023-04-20 11:46:45.545483 (Thread-449): 11:46:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559a6c40>]}
2023-04-20 11:46:45.548586 (Thread-449): sending response (<Response 13377 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:46:46.147335 (Thread-450): handling poll request
2023-04-20 11:46:46.147889 (Thread-450): 11:46:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559a6580>]}
2023-04-20 11:46:46.148427 (Thread-450): sending response (<Response 422 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:46:46.585819 (Thread-451): handling status request
2023-04-20 11:46:46.586325 (Thread-451): 11:46:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdad130>]}
2023-04-20 11:46:46.586854 (Thread-451): sending response (<Response 1593 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:46:46.844359 (Thread-452): handling ps request
2023-04-20 11:46:46.844842 (Thread-452): 11:46:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdad070>]}
2023-04-20 11:46:46.846055 (Thread-452): sending response (<Response 13377 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:46:48.254605 (Thread-448): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:46:48.293781 (MainThread): 11:46:48  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04688d89-eefe-48fb-b467-8cb1781fb7c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdbfe834e20>]}
2023-04-20 11:46:48.294410 (MainThread): 11:46:48  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:46:48.295812 (Thread-1): 11:46:48  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:46:48.296041 (Thread-1): 11:46:48  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:46:48.300479 (Thread-1): 11:46:48  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:46:48.296089 => 2023-04-20 11:46:48.300305
2023-04-20 11:46:48.300703 (Thread-1): 11:46:48  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:46:48.301135 (Thread-1): 11:46:48  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:46:48.301504 (Thread-1): 11:46:48  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    md5(companyid) sk_companyid,
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:46:48.301647 (Thread-1): 11:46:48  Opening a new connection, currently in state init
2023-04-20 11:46:48.822748 (Thread-453): handling ps request
2023-04-20 11:46:48.823266 (Thread-453): 11:46:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdad130>]}
2023-04-20 11:46:48.824467 (Thread-453): sending response (<Response 13372 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:46:48.894806 (Thread-454): handling poll request
2023-04-20 11:46:48.895247 (Thread-454): 11:46:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbc0940>]}
2023-04-20 11:46:48.895902 (Thread-454): sending response (<Response 5786 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:46:48.963881 (Thread-455): handling ps request
2023-04-20 11:46:48.964258 (Thread-455): 11:46:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbc02b0>]}
2023-04-20 11:46:48.965352 (Thread-455): sending response (<Response 13372 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:46:49.124350 (Thread-1): 11:46:49  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    md5(companyid) sk_companyid,
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:46:49.124636 (Thread-1): 11:46:49  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `cmp`.`PTS`, `ind`.`in_id`, `st`.`st_id`].; line 5 pos 8
2023-04-20 11:46:49.124770 (Thread-1): 11:46:49  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `cmp`.`PTS`, `ind`.`in_id`, `st`.`st_id`].; line 5 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `cmp`.`PTS`, `ind`.`in_id`, `st`.`st_id`].; line 5 pos 8
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:46:49.124884 (Thread-1): 11:46:49  Databricks adapter: operation-id: b'\x01\xed\xdfq\x08\xba\x10\x12\xab\xb7\x96\xcb\x94b\xaf\x97'
2023-04-20 11:46:49.125081 (Thread-1): 11:46:49  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:46:48.300751 => 2023-04-20 11:46:49.124957
2023-04-20 11:46:49.125267 (Thread-1): 11:46:49  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:46:49.353427 (Thread-456): handling status request
2023-04-20 11:46:49.353960 (Thread-456): 11:46:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbc0280>]}
2023-04-20 11:46:49.354457 (Thread-456): sending response (<Response 1593 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:46:49.402133 (Thread-1): Got an exception: Runtime Error
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `cmp`.`PTS`, `ind`.`in_id`, `st`.`st_id`].; line 5 pos 8
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `cmp`.`PTS`, `ind`.`in_id`, `st`.`st_id`].; line 5 pos 8

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `cmp`.`PTS`, `ind`.`in_id`, `st`.`st_id`].; line 5 pos 8
2023-04-20 11:46:49.403983 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `cmp`.`PTS`, `ind`.`in_id`, `st`.`st_id`].; line 5 pos 8', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    md5(companyid) sk_companyid,\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP'\n       ) cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    md5(companyid) sk_companyid,\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP'\n       ) cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `cmp`.`PTS`, `ind`.`in_id`, `st`.`st_id`].; line 5 pos 8', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    md5(companyid) sk_companyid,\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP'\n       ) cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    md5(companyid) sk_companyid,\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP'\n       ) cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 11:46:49.683799 (Thread-457): handling poll request
2023-04-20 11:46:49.684257 (Thread-457): 11:46:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbc0070>]}
2023-04-20 11:46:49.684957 (Thread-457): sending response (<Response 33496 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:46:51.507009 (Thread-458): handling poll request
2023-04-20 11:46:51.507507 (Thread-458): 11:46:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb647f0>]}
2023-04-20 11:46:51.508440 (Thread-458): sending response (<Response 33496 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:46:51.969519 (Thread-459): handling ps request
2023-04-20 11:46:51.969998 (Thread-459): 11:46:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be46f70>]}
2023-04-20 11:46:51.971197 (Thread-459): sending response (<Response 13395 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:46:52.457456 (Thread-460): handling status request
2023-04-20 11:46:52.457959 (Thread-460): 11:46:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd1deb0>]}
2023-04-20 11:46:52.458444 (Thread-460): sending response (<Response 1593 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:02.861467 (Thread-461): handling ps request
2023-04-20 11:47:02.862046 (Thread-461): 11:47:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd1dd00>]}
2023-04-20 11:47:02.863218 (Thread-461): sending response (<Response 13395 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:02.872463 (Thread-462): handling run_sql request
2023-04-20 11:47:02.872855 (Thread-462): 11:47:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be469a0>]}
2023-04-20 11:47:05.633206 (Thread-462): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:05.668780 (MainThread): 11:47:05  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e5e52b47-2663-4152-b14e-9b51e389738b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa311336d30>]}
2023-04-20 11:47:05.669375 (MainThread): 11:47:05  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:47:05.670767 (Thread-1): 11:47:05  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:47:05.671002 (Thread-1): 11:47:05  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:47:05.675195 (Thread-1): 11:47:05  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:47:05.671050 => 2023-04-20 11:47:05.675028
2023-04-20 11:47:05.675435 (Thread-1): 11:47:05  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:47:05.675883 (Thread-1): 11:47:05  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:47:05.676256 (Thread-1): 11:47:05  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    md5(cik) sk_companyid,
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:47:05.676403 (Thread-1): 11:47:05  Opening a new connection, currently in state init
2023-04-20 11:47:06.242376 (Thread-463): handling ps request
2023-04-20 11:47:06.243086 (Thread-463): 11:47:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be46550>]}
2023-04-20 11:47:06.245143 (Thread-463): sending response (<Response 13901 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:06.291433 (Thread-464): handling ps request
2023-04-20 11:47:06.291833 (Thread-464): 11:47:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e108a90>]}
2023-04-20 11:47:06.292907 (Thread-464): sending response (<Response 13903 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:06.934675 (Thread-465): handling status request
2023-04-20 11:47:06.935165 (Thread-465): 11:47:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd2ba60>]}
2023-04-20 11:47:06.935709 (Thread-465): sending response (<Response 1593 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:06.978656 (Thread-466): handling poll request
2023-04-20 11:47:06.979053 (Thread-466): 11:47:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd2b6d0>]}
2023-04-20 11:47:06.979661 (Thread-466): sending response (<Response 5780 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:07.008225 (Thread-467): handling poll request
2023-04-20 11:47:07.008609 (Thread-467): 11:47:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd2b580>]}
2023-04-20 11:47:07.009137 (Thread-467): sending response (<Response 5780 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:07.178760 (Thread-1): 11:47:07  SQL status: OK in 1.5 seconds
2023-04-20 11:47:07.212075 (Thread-1): 11:47:07  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:47:05.675482 => 2023-04-20 11:47:07.211829
2023-04-20 11:47:07.212345 (Thread-1): 11:47:07  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:47:07.463013 (Thread-468): handling status request
2023-04-20 11:47:07.463539 (Thread-468): 11:47:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54ba92070>]}
2023-04-20 11:47:07.490476 (Thread-468): sending response (<Response 1593 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:08.555709 (Thread-469): handling poll request
2023-04-20 11:47:08.556217 (Thread-469): 11:47:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55aa2fb50>]}
2023-04-20 11:47:08.562042 (Thread-469): sending response (<Response 194001 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:08.938258 (Thread-470): handling ps request
2023-04-20 11:47:08.938771 (Thread-470): 11:47:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb76df0>]}
2023-04-20 11:47:08.940054 (Thread-470): sending response (<Response 13928 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:09.374829 (Thread-471): handling status request
2023-04-20 11:47:09.375317 (Thread-471): 11:47:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e166970>]}
2023-04-20 11:47:09.375824 (Thread-471): sending response (<Response 1593 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:09.643234 (Thread-472): handling poll request
2023-04-20 11:47:09.643709 (Thread-472): 11:47:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb6c7f0>]}
2023-04-20 11:47:09.648961 (Thread-472): sending response (<Response 199366 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:10.803967 (Thread-473): handling status request
2023-04-20 11:47:10.804501 (Thread-473): 11:47:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb6c700>]}
2023-04-20 11:47:10.805003 (Thread-473): sending response (<Response 1593 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:11.043198 (Thread-474): handling ps request
2023-04-20 11:47:11.043709 (Thread-474): 11:47:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54b98ce50>]}
2023-04-20 11:47:11.044900 (Thread-474): sending response (<Response 13928 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:32.835403 (Thread-475): handling status request
2023-04-20 11:47:32.835933 (Thread-475): 11:47:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54b98d460>]}
2023-04-20 11:47:32.836397 (Thread-475): sending response (<Response 1593 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:46.793686 (Thread-476): handling status request
2023-04-20 11:47:46.794209 (Thread-476): 11:47:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54b98d0a0>]}
2023-04-20 11:47:46.794675 (Thread-476): sending response (<Response 1593 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:46.888727 (Thread-477): handling ps request
2023-04-20 11:47:46.889219 (Thread-477): 11:47:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54b98da60>]}
2023-04-20 11:47:46.890399 (Thread-477): sending response (<Response 13928 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:48.344601 (Thread-478): 11:47:48  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:47:48.632813 (Thread-478): 11:47:48  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
2023-04-20 11:47:48.633475 (Thread-478): 11:47:48  Partial parsing: updated file: dbsql_dbt_tpch://models/silver/DimCompany.sql
2023-04-20 11:47:48.642128 (Thread-478): 11:47:48  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:47:48.720950 (Thread-478): 11:47:48  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52859f580>]}
2023-04-20 11:47:48.793199 (Thread-479): handling status request
2023-04-20 11:47:48.793625 (Thread-479): 11:47:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbfbc70>]}
2023-04-20 11:47:48.794106 (Thread-479): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:51.832277 (Thread-480): handling status request
2023-04-20 11:47:51.832774 (Thread-480): 11:47:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbfbac0>]}
2023-04-20 11:47:51.833255 (Thread-480): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:52.559892 (Thread-481): handling cli_args request
2023-04-20 11:47:52.560415 (Thread-481): 11:47:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbfb970>]}
2023-04-20 11:47:55.299170 (Thread-481): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:55.359087 (MainThread): 11:47:55  checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
2023-04-20 11:47:55.381324 (MainThread): 11:47:55  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 11:47:55.381509 (MainThread): 11:47:55  previous checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, current checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd
2023-04-20 11:47:55.381662 (MainThread): 11:47:55  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'cbd58d16-85cf-467c-b2b3-8367c8d46fe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3dc6297bb0>]}
2023-04-20 11:47:55.907060 (Thread-482): handling ps request
2023-04-20 11:47:55.907818 (Thread-482): 11:47:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbfb550>]}
2023-04-20 11:47:55.909977 (Thread-482): sending response (<Response 14310 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:56.027448 (Thread-483): handling poll request
2023-04-20 11:47:56.027955 (Thread-483): 11:47:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd31e50>]}
2023-04-20 11:47:56.028600 (Thread-483): sending response (<Response 1862 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:56.072755 (Thread-484): handling status request
2023-04-20 11:47:56.073227 (Thread-484): 11:47:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbfbbb0>]}
2023-04-20 11:47:56.073734 (Thread-484): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:56.423776 (Thread-485): handling ps request
2023-04-20 11:47:56.424265 (Thread-485): 11:47:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbfbfd0>]}
2023-04-20 11:47:56.425505 (Thread-485): sending response (<Response 14310 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:57.014505 (MainThread): 11:47:57  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 11:47:57.027193 (MainThread): 11:47:57  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 11:47:57.030332 (MainThread): 11:47:57  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 11:47:57.033309 (MainThread): 11:47:57  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 11:47:57.036378 (MainThread): 11:47:57  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 11:47:57.039167 (MainThread): 11:47:57  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 11:47:57.042774 (MainThread): 11:47:57  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 11:47:57.045882 (MainThread): 11:47:57  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 11:47:57.049042 (MainThread): 11:47:57  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 11:47:57.051807 (MainThread): 11:47:57  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 11:47:57.055161 (MainThread): 11:47:57  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 11:47:57.058047 (MainThread): 11:47:57  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 11:47:57.060854 (MainThread): 11:47:57  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 11:47:57.063699 (MainThread): 11:47:57  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 11:47:57.066830 (MainThread): 11:47:57  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:47:57.234051 (MainThread): 11:47:57  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cbd58d16-85cf-467c-b2b3-8367c8d46fe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d9176ac70>]}
2023-04-20 11:47:57.274293 (MainThread): 11:47:57  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cbd58d16-85cf-467c-b2b3-8367c8d46fe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3dc6292610>]}
2023-04-20 11:47:57.274634 (MainThread): 11:47:57  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:47:57.274779 (MainThread): 11:47:57  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cbd58d16-85cf-467c-b2b3-8367c8d46fe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3dc6501970>]}
2023-04-20 11:47:57.276704 (MainThread): 11:47:57  
2023-04-20 11:47:57.277939 (MainThread): 11:47:57  Acquiring new databricks connection 'master'
2023-04-20 11:47:57.279659 (ThreadPoolExecutor-0_0): 11:47:57  Acquiring new databricks connection 'list_schemas'
2023-04-20 11:47:57.290761 (ThreadPoolExecutor-0_0): 11:47:57  Using databricks connection "list_schemas"
2023-04-20 11:47:57.291122 (ThreadPoolExecutor-0_0): 11:47:57  On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_schemas"} */

    show databases
  
2023-04-20 11:47:57.291283 (ThreadPoolExecutor-0_0): 11:47:57  Opening a new connection, currently in state init
2023-04-20 11:47:58.282355 (Thread-486): handling poll request
2023-04-20 11:47:58.282835 (Thread-486): 11:47:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbb74f0>]}
2023-04-20 11:47:58.283585 (Thread-486): sending response (<Response 8689 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:58.330779 (ThreadPoolExecutor-0_0): 11:47:58  SQL status: OK in 1.04 seconds
2023-04-20 11:47:58.455768 (ThreadPoolExecutor-0_0): 11:47:58  On list_schemas: Close
2023-04-20 11:47:58.743808 (ThreadPoolExecutor-1_0): 11:47:58  Acquiring new databricks connection 'list_None_dbt_shabbirkdb'
2023-04-20 11:47:58.754313 (ThreadPoolExecutor-1_0): 11:47:58  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:47:58.754497 (ThreadPoolExecutor-1_0): 11:47:58  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 11:47:58.754654 (ThreadPoolExecutor-1_0): 11:47:58  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show tables in `dbt_shabbirkdb`
  
2023-04-20 11:47:58.754800 (ThreadPoolExecutor-1_0): 11:47:58  Opening a new connection, currently in state closed
2023-04-20 11:47:59.267945 (Thread-487): handling ps request
2023-04-20 11:47:59.268514 (Thread-487): 11:47:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbb7a00>]}
2023-04-20 11:47:59.269766 (Thread-487): sending response (<Response 14310 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:47:59.466190 (ThreadPoolExecutor-1_0): 11:47:59  SQL status: OK in 0.71 seconds
2023-04-20 11:47:59.475299 (ThreadPoolExecutor-1_0): 11:47:59  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 11:47:59.475512 (ThreadPoolExecutor-1_0): 11:47:59  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show views in `dbt_shabbirkdb`
  
2023-04-20 11:47:59.877506 (ThreadPoolExecutor-1_0): 11:47:59  SQL status: OK in 0.4 seconds
2023-04-20 11:47:59.881103 (ThreadPoolExecutor-1_0): 11:47:59  On list_None_dbt_shabbirkdb: ROLLBACK
2023-04-20 11:47:59.881314 (ThreadPoolExecutor-1_0): 11:47:59  Databricks adapter: NotImplemented: rollback
2023-04-20 11:47:59.881461 (ThreadPoolExecutor-1_0): 11:47:59  On list_None_dbt_shabbirkdb: Close
2023-04-20 11:48:00.160390 (MainThread): 11:48:00  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cbd58d16-85cf-467c-b2b3-8367c8d46fe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3dc6297a60>]}
2023-04-20 11:48:00.160807 (MainThread): 11:48:00  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:48:00.160965 (MainThread): 11:48:00  Spark adapter: NotImplemented: commit
2023-04-20 11:48:00.161464 (MainThread): 11:48:00  Concurrency: 4 threads (target='default')
2023-04-20 11:48:00.161612 (MainThread): 11:48:00  
2023-04-20 11:48:00.164378 (Thread-1): 11:48:00  Began running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:48:00.164737 (Thread-1): 11:48:00  1 of 15 START sql table model dbt_shabbirkdb.DimBroker ......................... [RUN]
2023-04-20 11:48:00.165246 (Thread-1): 11:48:00  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
2023-04-20 11:48:00.165436 (Thread-1): 11:48:00  Began compiling node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:48:00.169560 (Thread-2): 11:48:00  Began running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:48:00.169890 (Thread-2): 11:48:00  2 of 15 START sql table model dbt_shabbirkdb.DimCompany ........................ [RUN]
2023-04-20 11:48:00.170409 (Thread-2): 11:48:00  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCompany'
2023-04-20 11:48:00.170585 (Thread-2): 11:48:00  Began compiling node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:48:00.175435 (Thread-1): 11:48:00  Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:48:00.175844 (Thread-3): 11:48:00  Began running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:48:00.176148 (Thread-3): 11:48:00  3 of 15 START sql table model dbt_shabbirkdb.DimCustomerStg .................... [RUN]
2023-04-20 11:48:00.176677 (Thread-3): 11:48:00  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomerStg'
2023-04-20 11:48:00.176854 (Thread-3): 11:48:00  Began compiling node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:48:00.181066 (Thread-2): 11:48:00  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:48:00.181576 (Thread-4): 11:48:00  Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:48:00.181884 (Thread-4): 11:48:00  4 of 15 START sql table model dbt_shabbirkdb.tempDailyMarketHistorical ......... [RUN]
2023-04-20 11:48:00.182402 (Thread-4): 11:48:00  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempDailyMarketHistorical'
2023-04-20 11:48:00.182576 (Thread-4): 11:48:00  Began compiling node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:48:00.187120 (Thread-3): 11:48:00  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:48:00.186322 (Thread-4): 11:48:00  Writing injected SQL for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:48:00.193995 (Thread-1): 11:48:00  Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 2023-04-20 11:48:00.166673 => 2023-04-20 11:48:00.193820
2023-04-20 11:48:00.194227 (Thread-1): 11:48:00  Began executing node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:48:00.210397 (Thread-1): 11:48:00  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:48:00.210607 (Thread-1): 11:48:00  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:48:00.210769 (Thread-1): 11:48:00  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

      describe extended `dbt_shabbirkdb`.`dimbroker`
  
2023-04-20 11:48:00.210901 (Thread-1): 11:48:00  Opening a new connection, currently in state closed
2023-04-20 11:48:00.216432 (Thread-4): 11:48:00  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (compile): 2023-04-20 11:48:00.182619 => 2023-04-20 11:48:00.216232
2023-04-20 11:48:00.216682 (Thread-4): 11:48:00  Began executing node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:48:00.221013 (Thread-4): 11:48:00  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:48:00.221188 (Thread-4): 11:48:00  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:48:00.221343 (Thread-4): 11:48:00  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

      describe extended `dbt_shabbirkdb`.`tempdailymarkethistorical`
  
2023-04-20 11:48:00.221469 (Thread-4): 11:48:00  Opening a new connection, currently in state init
2023-04-20 11:48:00.221904 (Thread-2): 11:48:00  Timing info for model.dbsql_dbt_tpch.DimCompany (compile): 2023-04-20 11:48:00.170628 => 2023-04-20 11:48:00.221713
2023-04-20 11:48:00.222131 (Thread-2): 11:48:00  Began executing node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:48:00.226156 (Thread-2): 11:48:00  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:48:00.226425 (Thread-2): 11:48:00  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:48:00.226589 (Thread-2): 11:48:00  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

      describe extended `dbt_shabbirkdb`.`dimcompany`
  
2023-04-20 11:48:00.226717 (Thread-2): 11:48:00  Opening a new connection, currently in state init
2023-04-20 11:48:00.227098 (Thread-3): 11:48:00  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (compile): 2023-04-20 11:48:00.176897 => 2023-04-20 11:48:00.226917
2023-04-20 11:48:00.227313 (Thread-3): 11:48:00  Began executing node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:48:00.232914 (Thread-3): 11:48:00  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:48:00.233136 (Thread-3): 11:48:00  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:48:00.233373 (Thread-3): 11:48:00  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

      describe extended `dbt_shabbirkdb`.`dimcustomerstg`
  
2023-04-20 11:48:00.233538 (Thread-3): 11:48:00  Opening a new connection, currently in state init
2023-04-20 11:48:00.270800 (Thread-488): handling status request
2023-04-20 11:48:00.271295 (Thread-488): 11:48:00  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbfe130>]}
2023-04-20 11:48:00.271854 (Thread-488): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:00.621146 (Thread-489): handling poll request
2023-04-20 11:48:00.621673 (Thread-489): 11:48:00  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbeec40>]}
2023-04-20 11:48:00.623061 (Thread-489): sending response (<Response 25844 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:00.953547 (Thread-1): 11:48:00  SQL status: OK in 0.74 seconds
2023-04-20 11:48:01.001568 (Thread-1): 11:48:01  Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:48:01.003427 (Thread-4): 11:48:01  SQL status: OK in 0.78 seconds
2023-04-20 11:48:01.003962 (Thread-2): 11:48:01  SQL status: OK in 0.78 seconds
2023-04-20 11:48:01.004324 (Thread-3): 11:48:01  SQL status: OK in 0.77 seconds
2023-04-20 11:48:01.012552 (Thread-2): 11:48:01  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:48:01.016607 (Thread-4): 11:48:01  Writing runtime sql for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:48:01.020223 (Thread-3): 11:48:01  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:48:01.020665 (Thread-1): 11:48:01  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:48:01.020871 (Thread-1): 11:48:01  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT
  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate`) effectivedate,
  date('9999-12-31') enddate
FROM  `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`HR`
WHERE employeejobcode = 314
  
2023-04-20 11:48:01.028799 (Thread-2): 11:48:01  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:48:01.029058 (Thread-2): 11:48:01  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCompany`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  * FROM (
  SELECT
    md5(cik) sk_companyid,
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
  
2023-04-20 11:48:01.033428 (Thread-4): 11:48:01  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:48:01.033637 (Thread-4): 11:48:01  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempDailyMarketHistorical`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  dmh.*,
  sk_dateid,
  min(dm_low) OVER (
    PARTITION BY dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweeklow,
  max(dm_high) OVER (
    PARTITION by dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweekhigh
FROM (
  SELECT * FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketHistorical`
  UNION ALL
  SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketIncremental`) dmh
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
  ON d.datevalue = dm_date;
  
2023-04-20 11:48:01.048719 (Thread-3): 11:48:01  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:48:01.049055 (Thread-3): 11:48:01  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomerStg`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT * FROM (
  SELECT
    customerid,
    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) taxid,
    status,
    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) lastname,
    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) firstname,
    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) middleinitial,
    coalesce(gender, last_value(gender) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) gender,
    coalesce(tier, last_value(tier) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) tier,
    coalesce(dob, last_value(dob) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) dob,
    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline1,
    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline2,
    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) postalcode,
    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) CITY,
    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) stateprov,
    coalesce(country, last_value(country) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) country,
    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone1,
    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone2,
    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone3,
    coalesce(email1, last_value(email1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email1,
    coalesce(email2, last_value(email2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email2,
    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) LCL_TX_ID,
    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) NAT_TX_ID,
    batchid,
    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,
    date(update_ts) effectivedate,
    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate
  FROM (
    SELECT
      customerid,
      taxid,
      status,
      lastname,
      firstname,
      middleinitial,
      gender,
      tier,
      dob,
      addressline1,
      addressline2,
      postalcode,
      city,
      stateprov,
      country,
      phone1,
      phone2,
      phone3,
      email1,
      email2,
      lcl_tx_id,
      nat_tx_id,
      1 batchid,
      update_ts
    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')
    UNION ALL
    SELECT
      c.customerid,
      nullif(c.taxid, '') taxid,
      nullif(s.st_name, '') as status,
      nullif(c.lastname, '') lastname,
      nullif(c.firstname, '') firstname,
      nullif(c.middleinitial, '') middleinitial,
      gender,
      c.tier,
      c.dob,
      nullif(c.addressline1, '') addressline1,
      nullif(c.addressline2, '') addressline2,
      nullif(c.postalcode, '') postalcode,
      nullif(c.city, '') city,
      nullif(c.stateprov, '') stateprov,
      nullif(c.country, '') country,
      CASE
        WHEN isnull(c_local_1) then c_local_1
        ELSE concat(
          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),
          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),
          c_local_1,
          nvl(c_ext_1, '')) END as phone1,
      CASE
        WHEN isnull(c_local_2) then c_local_2
        ELSE concat(
          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),
          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),
          c_local_2,
          nvl(c_ext_2, '')) END as phone2,
      CASE
        WHEN isnull(c_local_3) then c_local_3
        ELSE concat(
          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),
          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),
          c_local_3,
          nvl(c_ext_3, '')) END as phone3,
      nullif(c.email1, '') email1,
      nullif(c.email2, '') email2,
      c.LCL_TX_ID, 
      c.NAT_TX_ID,
      c.batchid,
      timestamp(bd.batchdate) update_ts
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CustomerIncremental` c
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
      ON c.batchid = bd.batchid
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` s 
      ON c.status = s.st_id
  ) c
  )
  
2023-04-20 11:48:02.356152 (Thread-490): handling poll request
2023-04-20 11:48:02.356641 (Thread-490): 11:48:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb4ab20>]}
2023-04-20 11:48:02.357373 (Thread-490): sending response (<Response 16344 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:02.767478 (Thread-491): handling ps request
2023-04-20 11:48:02.767985 (Thread-491): 11:48:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb4ac10>]}
2023-04-20 11:48:02.769182 (Thread-491): sending response (<Response 14310 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:03.337807 (Thread-492): handling status request
2023-04-20 11:48:03.338342 (Thread-492): 11:48:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbc1670>]}
2023-04-20 11:48:03.338833 (Thread-492): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:03.891313 (Thread-2): 11:48:03  SQL status: OK in 2.86 seconds
2023-04-20 11:48:03.918169 (Thread-2): 11:48:03  Timing info for model.dbsql_dbt_tpch.DimCompany (execute): 2023-04-20 11:48:00.222179 => 2023-04-20 11:48:03.917996
2023-04-20 11:48:03.918423 (Thread-2): 11:48:03  On model.dbsql_dbt_tpch.DimCompany: ROLLBACK
2023-04-20 11:48:03.918572 (Thread-2): 11:48:03  Databricks adapter: NotImplemented: rollback
2023-04-20 11:48:03.918694 (Thread-2): 11:48:03  On model.dbsql_dbt_tpch.DimCompany: Close
2023-04-20 11:48:04.072999 (Thread-493): handling poll request
2023-04-20 11:48:04.073511 (Thread-493): 11:48:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb4af10>]}
2023-04-20 11:48:04.074092 (Thread-493): sending response (<Response 2141 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:04.200527 (Thread-2): 11:48:04  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cbd58d16-85cf-467c-b2b3-8367c8d46fe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d917b3a30>]}
2023-04-20 11:48:04.201052 (Thread-2): 11:48:04  2 of 15 OK created sql table model dbt_shabbirkdb.DimCompany ................... [OK in 4.03s]
2023-04-20 11:48:04.202300 (Thread-2): 11:48:04  Finished running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:48:04.203145 (Thread-2): 11:48:04  Began running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:48:04.203443 (Thread-2): 11:48:04  5 of 15 START sql table model dbt_shabbirkdb.DimSecurity ....................... [RUN]
2023-04-20 11:48:04.203974 (Thread-2): 11:48:04  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimSecurity'
2023-04-20 11:48:04.204155 (Thread-2): 11:48:04  Began compiling node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:48:04.208760 (Thread-2): 11:48:04  Writing injected SQL for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:48:04.222468 (Thread-2): 11:48:04  Timing info for model.dbsql_dbt_tpch.DimSecurity (compile): 2023-04-20 11:48:04.204198 => 2023-04-20 11:48:04.222307
2023-04-20 11:48:04.222692 (Thread-2): 11:48:04  Began executing node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:48:04.226756 (Thread-2): 11:48:04  Writing runtime sql for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:48:04.241003 (Thread-2): 11:48:04  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:48:04.241184 (Thread-2): 11:48:04  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:48:04.241411 (Thread-2): 11:48:04  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 11:48:04.241542 (Thread-2): 11:48:04  Opening a new connection, currently in state closed
2023-04-20 11:48:04.330636 (Thread-1): 11:48:04  SQL status: OK in 3.31 seconds
2023-04-20 11:48:04.334339 (Thread-1): 11:48:04  Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 2023-04-20 11:48:00.194277 => 2023-04-20 11:48:04.334207
2023-04-20 11:48:04.334521 (Thread-1): 11:48:04  On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
2023-04-20 11:48:04.334659 (Thread-1): 11:48:04  Databricks adapter: NotImplemented: rollback
2023-04-20 11:48:04.334778 (Thread-1): 11:48:04  On model.dbsql_dbt_tpch.DimBroker: Close
2023-04-20 11:48:04.615396 (Thread-1): 11:48:04  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cbd58d16-85cf-467c-b2b3-8367c8d46fe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3dc59df070>]}
2023-04-20 11:48:04.615949 (Thread-1): 11:48:04  1 of 15 OK created sql table model dbt_shabbirkdb.DimBroker .................... [OK in 4.45s]
2023-04-20 11:48:04.616186 (Thread-1): 11:48:04  Finished running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:48:04.616468 (Thread-1): 11:48:04  Began running node model.dbsql_dbt_tpch.Financial
2023-04-20 11:48:04.616732 (Thread-1): 11:48:04  6 of 15 START sql table model dbt_shabbirkdb.Financial ......................... [RUN]
2023-04-20 11:48:04.617225 (Thread-1): 11:48:04  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Financial'
2023-04-20 11:48:04.617402 (Thread-1): 11:48:04  Began compiling node model.dbsql_dbt_tpch.Financial
2023-04-20 11:48:04.622071 (Thread-1): 11:48:04  Writing injected SQL for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:48:04.636492 (Thread-1): 11:48:04  Timing info for model.dbsql_dbt_tpch.Financial (compile): 2023-04-20 11:48:04.617447 => 2023-04-20 11:48:04.636329
2023-04-20 11:48:04.636711 (Thread-1): 11:48:04  Began executing node model.dbsql_dbt_tpch.Financial
2023-04-20 11:48:04.640747 (Thread-1): 11:48:04  Writing runtime sql for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:48:04.654060 (Thread-1): 11:48:04  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:48:04.654245 (Thread-1): 11:48:04  Using databricks connection "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:48:04.654460 (Thread-1): 11:48:04  On model.dbsql_dbt_tpch.Financial: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 11:48:04.654593 (Thread-1): 11:48:04  Opening a new connection, currently in state closed
2023-04-20 11:48:04.900102 (Thread-3): 11:48:04  SQL status: OK in 3.85 seconds
2023-04-20 11:48:04.902592 (Thread-3): 11:48:04  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (execute): 2023-04-20 11:48:00.227360 => 2023-04-20 11:48:04.902434
2023-04-20 11:48:04.902804 (Thread-3): 11:48:04  On model.dbsql_dbt_tpch.DimCustomerStg: ROLLBACK
2023-04-20 11:48:04.902945 (Thread-3): 11:48:04  Databricks adapter: NotImplemented: rollback
2023-04-20 11:48:04.903066 (Thread-3): 11:48:04  On model.dbsql_dbt_tpch.DimCustomerStg: Close
2023-04-20 11:48:05.191464 (Thread-3): 11:48:05  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cbd58d16-85cf-467c-b2b3-8367c8d46fe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d917bba60>]}
2023-04-20 11:48:05.192038 (Thread-3): 11:48:05  3 of 15 OK created sql table model dbt_shabbirkdb.DimCustomerStg ............... [OK in 5.01s]
2023-04-20 11:48:05.192282 (Thread-3): 11:48:05  Finished running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:48:05.193083 (Thread-3): 11:48:05  Began running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:48:05.193387 (Thread-3): 11:48:05  7 of 15 START sql table model dbt_shabbirkdb.DimAccount ........................ [RUN]
2023-04-20 11:48:05.193902 (Thread-3): 11:48:05  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimAccount'
2023-04-20 11:48:05.194082 (Thread-3): 11:48:05  Began compiling node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:48:05.199367 (Thread-3): 11:48:05  Writing injected SQL for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:48:05.213066 (Thread-3): 11:48:05  Timing info for model.dbsql_dbt_tpch.DimAccount (compile): 2023-04-20 11:48:05.194126 => 2023-04-20 11:48:05.212901
2023-04-20 11:48:05.213285 (Thread-3): 11:48:05  Began executing node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:48:05.217463 (Thread-3): 11:48:05  Writing runtime sql for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:48:05.234197 (Thread-3): 11:48:05  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:48:05.234434 (Thread-3): 11:48:05  Using databricks connection "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:48:05.234679 (Thread-3): 11:48:05  On model.dbsql_dbt_tpch.DimAccount: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 11:48:05.234815 (Thread-3): 11:48:05  Opening a new connection, currently in state closed
2023-04-20 11:48:05.425077 (Thread-494): handling ps request
2023-04-20 11:48:05.425628 (Thread-494): 11:48:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54b9b4580>]}
2023-04-20 11:48:05.426872 (Thread-494): sending response (<Response 14311 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:05.710313 (Thread-495): handling poll request
2023-04-20 11:48:05.710801 (Thread-495): 11:48:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54b9b47c0>]}
2023-04-20 11:48:05.711858 (Thread-495): sending response (<Response 30236 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:05.864269 (Thread-496): handling status request
2023-04-20 11:48:05.864774 (Thread-496): 11:48:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54b9b4760>]}
2023-04-20 11:48:05.865264 (Thread-496): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:06.005726 (Thread-3): 11:48:06  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 11:48:06.006045 (Thread-3): 11:48:06  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:48:06.006242 (Thread-3): 11:48:06  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:48:06.006418 (Thread-3): 11:48:06  Databricks adapter: operation-id: b'\x01\xed\xdfq6\x86\x1b#\xa1\xec%\x08\xb0\x0e\x90\xed'
2023-04-20 11:48:06.006776 (Thread-3): 11:48:06  Timing info for model.dbsql_dbt_tpch.DimAccount (execute): 2023-04-20 11:48:05.213336 => 2023-04-20 11:48:06.006571
2023-04-20 11:48:06.007043 (Thread-3): 11:48:06  On model.dbsql_dbt_tpch.DimAccount: ROLLBACK
2023-04-20 11:48:06.007232 (Thread-3): 11:48:06  Databricks adapter: NotImplemented: rollback
2023-04-20 11:48:06.007427 (Thread-3): 11:48:06  On model.dbsql_dbt_tpch.DimAccount: Close
2023-04-20 11:48:06.280127 (Thread-3): 11:48:06  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:48:06.280701 (Thread-3): 11:48:06  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cbd58d16-85cf-467c-b2b3-8367c8d46fe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d904ecc40>]}
2023-04-20 11:48:06.281145 (Thread-3): 11:48:06  7 of 15 ERROR creating sql table model dbt_shabbirkdb.DimAccount ............... [ERROR in 1.09s]
2023-04-20 11:48:06.281357 (Thread-3): 11:48:06  Finished running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:48:06.281636 (Thread-3): 11:48:06  Began running node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:48:06.281912 (Thread-3): 11:48:06  8 of 15 START sql table model dbt_shabbirkdb.Prospect .......................... [RUN]
2023-04-20 11:48:06.282494 (Thread-3): 11:48:06  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Prospect'
2023-04-20 11:48:06.282725 (Thread-3): 11:48:06  Began compiling node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:48:06.288661 (Thread-3): 11:48:06  Writing injected SQL for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:48:06.302852 (Thread-3): 11:48:06  Timing info for model.dbsql_dbt_tpch.Prospect (compile): 2023-04-20 11:48:06.282778 => 2023-04-20 11:48:06.302681
2023-04-20 11:48:06.303075 (Thread-3): 11:48:06  Began executing node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:48:06.308942 (Thread-3): 11:48:06  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:48:06.309121 (Thread-3): 11:48:06  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:48:06.309293 (Thread-3): 11:48:06  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

      describe extended `dbt_shabbirkdb`.`prospect`
  
2023-04-20 11:48:06.309423 (Thread-3): 11:48:06  Opening a new connection, currently in state closed
2023-04-20 11:48:06.812677 (Thread-2): 11:48:06  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 11:48:06.812897 (Thread-2): 11:48:06  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [CAST_INVALID_INPUT] The value 'BELJVClFSUDWQNFrxaHxDDEMSOMNqTeWbFtIh' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:48:06.813033 (Thread-2): 11:48:06  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: [CAST_INVALID_INPUT] The value 'BELJVClFSUDWQNFrxaHxDDEMSOMNqTeWbFtIh' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: [CAST_INVALID_INPUT] The value 'BELJVClFSUDWQNFrxaHxDDEMSOMNqTeWbFtIh' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$1(TransactionalWriteEdge.scala:584)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:645)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:407)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:405)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:402)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:450)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:435)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:640)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:559)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:520)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:63)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:145)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:104)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:316)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:1961)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:315)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:348)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:342)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:625)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:615)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:226)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:223)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:367)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:151)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:169)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:645)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:407)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:405)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:402)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:450)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:435)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:640)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:559)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:520)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:63)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:145)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:104)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:120)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:257)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:86)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:119)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:907)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:86)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:866)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:557)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1747)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:538)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:567)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:533)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:222)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:278)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:238)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:237)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:220)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:226)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:226)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:226)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:171)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:249)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:456)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:475)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:708)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:535)
	... 21 more
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'BELJVClFSUDWQNFrxaHxDDEMSOMNqTeWbFtIh' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:190)
	at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:137)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:234)
	... 182 more
	Suppressed: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4069.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4069.0 (TID 31770) (10.5.234.83 executor 1): com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: long,BELJVClFSUDWQNFrxaHxDDEMSOMNqTeWbFtIh
	at 0x89a2b7e <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_3/photon/common/status.cc:137)
	at 0x5ded45d <photon>.OnInvalidInput(external/workspace_spark_3_3/photon/exprs/cast-functions.h:622)
	at 0x5ded0f2 <photon>.ProjectBatch(external/workspace_spark_3_3/photon/exprs/compute-function.h:1117)
	at 0x5deccac <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:69)
	at 0x5df978c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x850c561 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/coalesce-expr.cc:55)
	at 0x4c5466f <photon>.EvalResult(external/workspace_spark_3_3/photon/exec-nodes/window-node.cc:166)
	at 0x4c53d22 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/window-node.cc:118)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4b83940 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/filter-node.cc:73)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/sort-node.cc:98)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonResultStageExec.$anonfun$doExecuteColumnar$2(PhotonExec.scala:848)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonResultStageExec.timeit(PhotonExec.scala:838)
	at com.databricks.photon.PhotonResultStageExec.$anonfun$doExecuteColumnar$1(PhotonExec.scala:848)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1172)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1160)
		at org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2737)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2720)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:611)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:236)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:336)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:226)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:115)
		at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:70)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$12(TransactionalWriteEdge.scala:584)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)
		... 182 more
	Caused by: com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: long,BELJVClFSUDWQNFrxaHxDDEMSOMNqTeWbFtIh
		at 0x89a2b7e <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_3/photon/common/status.cc:137)
		at 0x5ded45d <photon>.OnInvalidInput(external/workspace_spark_3_3/photon/exprs/cast-functions.h:622)
		at 0x5ded0f2 <photon>.ProjectBatch(external/workspace_spark_3_3/photon/exprs/compute-function.h:1117)
		at 0x5deccac <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:69)
		at 0x5df978c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
		at 0x850c561 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/coalesce-expr.cc:55)
		at 0x4c5466f <photon>.EvalResult(external/workspace_spark_3_3/photon/exec-nodes/window-node.cc:166)
		at 0x4c53d22 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/window-node.cc:118)
		at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
		at 0x4b83940 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/filter-node.cc:73)
		at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
		at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
		at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
		at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
		at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
		at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/sort-node.cc:98)
		at com.databricks.photon.JniApiImpl.open(Native Method)
		at com.databricks.photon.JniApi.open(JniApi.scala)
		at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
		at com.databricks.photon.PhotonResultStageExec.$anonfun$doExecuteColumnar$2(PhotonExec.scala:848)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonResultStageExec.timeit(PhotonExec.scala:838)
		at com.databricks.photon.PhotonResultStageExec.$anonfun$doExecuteColumnar$1(PhotonExec.scala:848)
		at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
		at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)
		at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
		at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
		at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
		at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
		at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
		at scala.util.Using$.resource(Using.scala:269)
		at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
		at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.Task.run(Task.scala:97)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
		... 3 more

2023-04-20 11:48:06.813150 (Thread-2): 11:48:06  Databricks adapter: operation-id: b'\x01\xed\xdfq5\xef\x15\x92\x9f5"\x9f1BUf'
2023-04-20 11:48:06.813393 (Thread-2): 11:48:06  Timing info for model.dbsql_dbt_tpch.DimSecurity (execute): 2023-04-20 11:48:04.222741 => 2023-04-20 11:48:06.813258
2023-04-20 11:48:06.813576 (Thread-2): 11:48:06  On model.dbsql_dbt_tpch.DimSecurity: ROLLBACK
2023-04-20 11:48:06.813699 (Thread-2): 11:48:06  Databricks adapter: NotImplemented: rollback
2023-04-20 11:48:06.813820 (Thread-2): 11:48:06  On model.dbsql_dbt_tpch.DimSecurity: Close
2023-04-20 11:48:07.039865 (Thread-3): 11:48:07  SQL status: OK in 0.73 seconds
2023-04-20 11:48:07.045921 (Thread-3): 11:48:07  Writing runtime sql for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:48:07.060534 (Thread-3): 11:48:07  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:48:07.060913 (Thread-3): 11:48:07  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Prospect`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  agencyid,
  recdate.sk_dateid sk_recorddateid,
  origdate.sk_dateid sk_updatedateid,
  p.batchid,
  nvl2(c.customerid, True, False) iscustomer, 
  p.lastname,
  p.firstname,
  p.middleinitial,
  p.gender,
  p.addressline1,
  p.addressline2,
  p.postalcode,
  city,
  state,
  country,
  phone,
  income,
  numbercars,
  numberchildren,
  maritalstatus,
  age,
  creditrating,
  ownorrentflag,
  employer,
  numbercreditcards,
  networth,
  if(
    isnotnull(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+","")),
    left(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+",""),
      length(
        if(networth > 1000000 or income > 200000,"HighValue+","") || 
        if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
        if(age > 45, "Boomer+", "") ||
        if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
        if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
        if(age < 25 and networth > 1000000, "Inherited+",""))
      -1),
    NULL) marketingnameplate
FROM (
  SELECT 
    * FROM (
    SELECT
      agencyid,
      max(batchid) recordbatchid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth,
      min(batchid) batchid
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`ProspectRaw` p
    GROUP BY
      agencyid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth)
  QUALIFY ROW_NUMBER() OVER (PARTITION BY agencyid ORDER BY batchid DESC) = 1) p
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) recdate
  ON p.recordbatchid = recdate.batchid
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) origdate
  ON p.batchid = origdate.batchid
LEFT JOIN (
  SELECT 
    customerid,
    lastname,
    firstname,
    addressline1,
    addressline2,
    postalcode
  FROM `dbt_shabbirkdb`.`DimCustomerStg`
  WHERE iscurrent) c
  ON 
    upper(p.LastName) = upper(c.lastname)
    and upper(p.FirstName) = upper(c.firstname)
    and upper(p.AddressLine1) = upper(c.addressline1)
    and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
    and upper(p.PostalCode) = upper(c.postalcode)
  
2023-04-20 11:48:07.097249 (Thread-2): 11:48:07  Runtime Error in model DimSecurity (models/silver/DimSecurity.sql)
  [CAST_INVALID_INPUT] The value 'BELJVClFSUDWQNFrxaHxDDEMSOMNqTeWbFtIh' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:48:07.097764 (Thread-2): 11:48:07  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cbd58d16-85cf-467c-b2b3-8367c8d46fe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d9040f730>]}
2023-04-20 11:48:07.098276 (Thread-2): 11:48:07  5 of 15 ERROR creating sql table model dbt_shabbirkdb.DimSecurity .............. [ERROR in 2.89s]
2023-04-20 11:48:07.098490 (Thread-2): 11:48:07  Finished running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:48:07.098772 (Thread-2): 11:48:07  Began running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 11:48:07.098993 (Thread-2): 11:48:07  9 of 15 SKIP relation dbt_shabbirkdb.FactCashBalances .......................... [SKIP]
2023-04-20 11:48:07.099171 (Thread-2): 11:48:07  Finished running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 11:48:07.100039 (Thread-2): 11:48:07  Began running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 11:48:07.100259 (Thread-2): 11:48:07  10 of 15 SKIP relation dbt_shabbirkdb.DimTrade ................................. [SKIP]
2023-04-20 11:48:07.100436 (Thread-2): 11:48:07  Finished running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 11:48:07.100952 (Thread-2): 11:48:07  Began running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 11:48:07.101160 (Thread-2): 11:48:07  11 of 15 SKIP relation dbt_shabbirkdb.FactHoldings ............................. [SKIP]
2023-04-20 11:48:07.101334 (Thread-2): 11:48:07  Finished running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 11:48:07.281093 (Thread-497): handling poll request
2023-04-20 11:48:07.281578 (Thread-497): 11:48:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52859db20>]}
2023-04-20 11:48:07.282748 (Thread-497): sending response (<Response 70640 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:07.529198 (Thread-4): 11:48:07  SQL status: OK in 6.5 seconds
2023-04-20 11:48:07.847175 (Thread-4): 11:48:07  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (execute): 2023-04-20 11:48:00.216733 => 2023-04-20 11:48:07.846989
2023-04-20 11:48:07.847440 (Thread-4): 11:48:07  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: ROLLBACK
2023-04-20 11:48:07.847628 (Thread-4): 11:48:07  Databricks adapter: NotImplemented: rollback
2023-04-20 11:48:07.847760 (Thread-4): 11:48:07  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: Close
2023-04-20 11:48:08.025604 (Thread-498): handling ps request
2023-04-20 11:48:08.026103 (Thread-498): 11:48:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52859de20>]}
2023-04-20 11:48:08.052165 (Thread-498): sending response (<Response 14310 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:08.132030 (Thread-4): 11:48:08  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cbd58d16-85cf-467c-b2b3-8367c8d46fe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3dc4052cd0>]}
2023-04-20 11:48:08.132540 (Thread-4): 11:48:08  4 of 15 OK created sql table model dbt_shabbirkdb.tempDailyMarketHistorical .... [OK in 7.95s]
2023-04-20 11:48:08.132800 (Thread-4): 11:48:08  Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:48:08.551599 (Thread-499): handling status request
2023-04-20 11:48:08.552094 (Thread-499): 11:48:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52859d6d0>]}
2023-04-20 11:48:08.552675 (Thread-499): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:09.011481 (Thread-500): handling poll request
2023-04-20 11:48:09.011971 (Thread-500): 11:48:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb4ab20>]}
2023-04-20 11:48:09.012614 (Thread-500): sending response (<Response 3604 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:10.632597 (Thread-501): handling ps request
2023-04-20 11:48:10.633134 (Thread-501): 11:48:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbc1460>]}
2023-04-20 11:48:10.634374 (Thread-501): sending response (<Response 14311 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:10.636133 (Thread-502): handling poll request
2023-04-20 11:48:10.636475 (Thread-502): 11:48:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbc1160>]}
2023-04-20 11:48:10.636853 (Thread-502): sending response (<Response 291 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:10.949536 (Thread-1): 11:48:10  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 11:48:10.949758 (Thread-1): 11:48:10  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: Task failed while writing rows.
2023-04-20 11:48:10.949914 (Thread-1): 11:48:10  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4071.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4071.0 (TID 31787) (10.5.234.83 executor 1): org.apache.spark.SparkException: Task failed while writing rows.
	at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:109)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'WIFHmdYSMPHKWkoqPrQTYs' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:190)
	at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:137)
	at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:106)
	... 31 more
	Suppressed: com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: long,WIFHmdYSMPHKWkoqPrQTYs
		at 0x89a2b7e <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_3/photon/common/status.cc:137)
		at 0x5ded45d <photon>.OnInvalidInput(external/workspace_spark_3_3/photon/exprs/cast-functions.h:622)
		at 0x5ded0f2 <photon>.ProjectBatch(external/workspace_spark_3_3/photon/exprs/compute-function.h:1117)
		at 0x5deccac <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:69)
		at 0x5df978c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
		at 0x850c561 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/coalesce-expr.cc:55)
		at 0x7dcf64c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/is-null-expr.h:90)
		at 0x85d4a3e <photon>.Eval(external/workspace_spark_3_3/photon/exprs/logical-functions.cc:284)
		at 0x4c56aee <photon>.EvalAllFilters(external/workspace_spark_3_3/photon/io/parquet-reader/data-filter.cc:186)
		at 0x4c86441 <photon>.ScanAndFilterRowsUntilNonEmptyBatch(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:936)
		at 0x4c86da0 <photon>.YieldRowsWithFilter(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:989)
		at 0x4c8782b <photon>.Next(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:1068)
		at 0x4c78105 <photon>.Next(external/workspace_spark_3_3/photon/io/parquet-reader/file-reader.cc:428)
		at 0x4b7bd32 <photon>.operator()(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:728)
		at 0x4b795a0 <photon>.TryPullNextBatch(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:728)
		at 0x4b795a0 <photon>.DoHasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:508)
		at 0x4b7932e <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:336)
		at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
		at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
		at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
		at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
		at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
		at com.databricks.photon.JniApiImpl.hasNext(Native Method)
		at com.databricks.photon.JniApi.hasNext(JniApi.scala)
		at com.databricks.photon.JniExecNode.hasNext(JniExecNode.java:68)
		at com.databricks.photon.PhotonResultStageExec$$anon$2.hasNext(PhotonExec.scala:853)
		at com.databricks.photon.PhotonExec$$anon$1.$anonfun$hasNext$1(PhotonExec.scala:582)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonResultStageExec.timeit(PhotonExec.scala:838)
		at com.databricks.photon.PhotonExec$$anon$1.hasNext(PhotonExec.scala:582)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)
		at org.apache.spark.sql.execution.RowToColumnarExec$$anon$1.hasNext(Columnar.scala:503)
		at com.databricks.photon.CloseableIterator$$anon$5.hasNext(CloseableIterator.scala:62)
		at com.databricks.photon.NativeColumnBatchIterator.hasNext(NativeColumnBatchIterator.java:47)
		at 0x899c692 <photon>.HasNext(external/workspace_spark_3_3/photon/jni-wrappers/jni-native-column-batch-iterator.cc:62)
		at 0x4abefd9 <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/file-writer-node.cc:161)
		at com.databricks.photon.JniApiImpl.open(Native Method)
		at com.databricks.photon.JniApi.open(JniApi.scala)
		at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$5(PhotonWriteStageExec.scala:89)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonWriteStageExec.timeit(PhotonWriteStageExec.scala:38)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$4(PhotonWriteStageExec.scala:89)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1747)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:86)
		... 31 more

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4071.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4071.0 (TID 31787) (10.5.234.83 executor 1): org.apache.spark.SparkException: Task failed while writing rows.
	at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:109)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'WIFHmdYSMPHKWkoqPrQTYs' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:190)
	at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:137)
	at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:106)
	... 31 more
	Suppressed: com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: long,WIFHmdYSMPHKWkoqPrQTYs
		at 0x89a2b7e <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_3/photon/common/status.cc:137)
		at 0x5ded45d <photon>.OnInvalidInput(external/workspace_spark_3_3/photon/exprs/cast-functions.h:622)
		at 0x5ded0f2 <photon>.ProjectBatch(external/workspace_spark_3_3/photon/exprs/compute-function.h:1117)
		at 0x5deccac <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:69)
		at 0x5df978c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
		at 0x850c561 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/coalesce-expr.cc:55)
		at 0x7dcf64c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/is-null-expr.h:90)
		at 0x85d4a3e <photon>.Eval(external/workspace_spark_3_3/photon/exprs/logical-functions.cc:284)
		at 0x4c56aee <photon>.EvalAllFilters(external/workspace_spark_3_3/photon/io/parquet-reader/data-filter.cc:186)
		at 0x4c86441 <photon>.ScanAndFilterRowsUntilNonEmptyBatch(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:936)
		at 0x4c86da0 <photon>.YieldRowsWithFilter(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:989)
		at 0x4c8782b <photon>.Next(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:1068)
		at 0x4c78105 <photon>.Next(external/workspace_spark_3_3/photon/io/parquet-reader/file-reader.cc:428)
		at 0x4b7bd32 <photon>.operator()(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:728)
		at 0x4b795a0 <photon>.TryPullNextBatch(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:728)
		at 0x4b795a0 <photon>.DoHasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:508)
		at 0x4b7932e <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:336)
		at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
		at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
		at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
		at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
		at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
		at com.databricks.photon.JniApiImpl.hasNext(Native Method)
		at com.databricks.photon.JniApi.hasNext(JniApi.scala)
		at com.databricks.photon.JniExecNode.hasNext(JniExecNode.java:68)
		at com.databricks.photon.PhotonResultStageExec$$anon$2.hasNext(PhotonExec.scala:853)
		at com.databricks.photon.PhotonExec$$anon$1.$anonfun$hasNext$1(PhotonExec.scala:582)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonResultStageExec.timeit(PhotonExec.scala:838)
		at com.databricks.photon.PhotonExec$$anon$1.hasNext(PhotonExec.scala:582)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)
		at org.apache.spark.sql.execution.RowToColumnarExec$$anon$1.hasNext(Columnar.scala:503)
		at com.databricks.photon.CloseableIterator$$anon$5.hasNext(CloseableIterator.scala:62)
		at com.databricks.photon.NativeColumnBatchIterator.hasNext(NativeColumnBatchIterator.java:47)
		at 0x899c692 <photon>.HasNext(external/workspace_spark_3_3/photon/jni-wrappers/jni-native-column-batch-iterator.cc:62)
		at 0x4abefd9 <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/file-writer-node.cc:161)
		at com.databricks.photon.JniApiImpl.open(Native Method)
		at com.databricks.photon.JniApi.open(JniApi.scala)
		at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$5(PhotonWriteStageExec.scala:89)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonWriteStageExec.timeit(PhotonWriteStageExec.scala:38)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$4(PhotonWriteStageExec.scala:89)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1747)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:86)
		... 31 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1172)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1160)
	at org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2720)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:611)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:236)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:336)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:226)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:115)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:70)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$12(TransactionalWriteEdge.scala:584)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$1(TransactionalWriteEdge.scala:584)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:645)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:407)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:405)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:402)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:450)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:435)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:640)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:559)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:520)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:63)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:145)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:104)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:316)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:1961)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:315)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:348)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:342)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:625)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:615)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:226)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:223)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:367)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:151)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:169)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:645)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:407)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:405)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:402)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:450)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:435)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:640)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:559)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:520)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:63)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:145)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:104)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:120)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:257)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:86)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:119)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:907)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:86)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:866)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:557)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1747)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:538)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:567)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:533)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:222)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:278)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:238)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:237)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:220)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:226)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:226)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:226)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:171)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:249)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:456)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:475)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:708)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:535)
	... 21 more
Caused by: org.apache.spark.SparkException: Task failed while writing rows.
	at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:109)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	... 3 more
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'WIFHmdYSMPHKWkoqPrQTYs' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:190)
	at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:137)
	at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:106)
	... 31 more
	Suppressed: com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: long,WIFHmdYSMPHKWkoqPrQTYs
		at 0x89a2b7e <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_3/photon/common/status.cc:137)
		at 0x5ded45d <photon>.OnInvalidInput(external/workspace_spark_3_3/photon/exprs/cast-functions.h:622)
		at 0x5ded0f2 <photon>.ProjectBatch(external/workspace_spark_3_3/photon/exprs/compute-function.h:1117)
		at 0x5deccac <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:69)
		at 0x5df978c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
		at 0x850c561 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/coalesce-expr.cc:55)
		at 0x7dcf64c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/is-null-expr.h:90)
		at 0x85d4a3e <photon>.Eval(external/workspace_spark_3_3/photon/exprs/logical-functions.cc:284)
		at 0x4c56aee <photon>.EvalAllFilters(external/workspace_spark_3_3/photon/io/parquet-reader/data-filter.cc:186)
		at 0x4c86441 <photon>.ScanAndFilterRowsUntilNonEmptyBatch(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:936)
		at 0x4c86da0 <photon>.YieldRowsWithFilter(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:989)
		at 0x4c8782b <photon>.Next(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:1068)
		at 0x4c78105 <photon>.Next(external/workspace_spark_3_3/photon/io/parquet-reader/file-reader.cc:428)
		at 0x4b7bd32 <photon>.operator()(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:728)
		at 0x4b795a0 <photon>.TryPullNextBatch(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:728)
		at 0x4b795a0 <photon>.DoHasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:508)
		at 0x4b7932e <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:336)
		at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
		at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
		at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
		at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
		at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
		at com.databricks.photon.JniApiImpl.hasNext(Native Method)
		at com.databricks.photon.JniApi.hasNext(JniApi.scala)
		at com.databricks.photon.JniExecNode.hasNext(JniExecNode.java:68)
		at com.databricks.photon.PhotonResultStageExec$$anon$2.hasNext(PhotonExec.scala:853)
		at com.databricks.photon.PhotonExec$$anon$1.$anonfun$hasNext$1(PhotonExec.scala:582)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonResultStageExec.timeit(PhotonExec.scala:838)
		at com.databricks.photon.PhotonExec$$anon$1.hasNext(PhotonExec.scala:582)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)
		at org.apache.spark.sql.execution.RowToColumnarExec$$anon$1.hasNext(Columnar.scala:503)
		at com.databricks.photon.CloseableIterator$$anon$5.hasNext(CloseableIterator.scala:62)
		at com.databricks.photon.NativeColumnBatchIterator.hasNext(NativeColumnBatchIterator.java:47)
		at 0x899c692 <photon>.HasNext(external/workspace_spark_3_3/photon/jni-wrappers/jni-native-column-batch-iterator.cc:62)
		at 0x4abefd9 <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/file-writer-node.cc:161)
		at com.databricks.photon.JniApiImpl.open(Native Method)
		at com.databricks.photon.JniApi.open(JniApi.scala)
		at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$5(PhotonWriteStageExec.scala:89)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonWriteStageExec.timeit(PhotonWriteStageExec.scala:38)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$4(PhotonWriteStageExec.scala:89)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1747)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:86)
		... 31 more

2023-04-20 11:48:10.950041 (Thread-1): 11:48:10  Databricks adapter: operation-id: b'\x01\xed\xdfq6.\x19\xe8\xb3\xd3)\xb0\xdb_\xb14'
2023-04-20 11:48:10.950278 (Thread-1): 11:48:10  Timing info for model.dbsql_dbt_tpch.Financial (execute): 2023-04-20 11:48:04.636760 => 2023-04-20 11:48:10.950147
2023-04-20 11:48:10.950459 (Thread-1): 11:48:10  On model.dbsql_dbt_tpch.Financial: ROLLBACK
2023-04-20 11:48:10.950580 (Thread-1): 11:48:10  Databricks adapter: NotImplemented: rollback
2023-04-20 11:48:10.950701 (Thread-1): 11:48:10  On model.dbsql_dbt_tpch.Financial: Close
2023-04-20 11:48:11.106772 (Thread-503): handling status request
2023-04-20 11:48:11.119779 (Thread-503): 11:48:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285953a0>]}
2023-04-20 11:48:11.120254 (Thread-503): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:11.243034 (Thread-1): 11:48:11  Runtime Error in model Financial (models/silver/Financial.sql)
  Job aborted due to stage failure: Task failed while writing rows.
2023-04-20 11:48:11.243433 (Thread-1): 11:48:11  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cbd58d16-85cf-467c-b2b3-8367c8d46fe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d9054dfa0>]}
2023-04-20 11:48:11.244042 (Thread-1): 11:48:11  6 of 15 ERROR creating sql table model dbt_shabbirkdb.Financial ................ [ERROR in 6.63s]
2023-04-20 11:48:11.244259 (Thread-1): 11:48:11  Finished running node model.dbsql_dbt_tpch.Financial
2023-04-20 11:48:11.245117 (Thread-2): 11:48:11  Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 11:48:11.245400 (Thread-2): 11:48:11  12 of 15 SKIP relation dbt_shabbirkdb.tempSumpFiBasicEps ....................... [SKIP]
2023-04-20 11:48:11.245599 (Thread-2): 11:48:11  Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 11:48:11.246222 (Thread-1): 11:48:11  Began running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 11:48:11.246451 (Thread-1): 11:48:11  13 of 15 SKIP relation dbt_shabbirkdb.FactMarketHistory ........................ [SKIP]
2023-04-20 11:48:11.246632 (Thread-1): 11:48:11  Finished running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 11:48:11.928941 (Thread-3): 11:48:11  SQL status: OK in 4.87 seconds
2023-04-20 11:48:11.932503 (Thread-3): 11:48:11  Timing info for model.dbsql_dbt_tpch.Prospect (execute): 2023-04-20 11:48:06.303128 => 2023-04-20 11:48:11.932286
2023-04-20 11:48:11.932800 (Thread-3): 11:48:11  On model.dbsql_dbt_tpch.Prospect: ROLLBACK
2023-04-20 11:48:11.933010 (Thread-3): 11:48:11  Databricks adapter: NotImplemented: rollback
2023-04-20 11:48:11.933202 (Thread-3): 11:48:11  On model.dbsql_dbt_tpch.Prospect: Close
2023-04-20 11:48:12.214355 (Thread-3): 11:48:12  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cbd58d16-85cf-467c-b2b3-8367c8d46fe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d904ec6d0>]}
2023-04-20 11:48:12.214872 (Thread-3): 11:48:12  8 of 15 OK created sql table model dbt_shabbirkdb.Prospect ..................... [OK in 5.93s]
2023-04-20 11:48:12.215115 (Thread-3): 11:48:12  Finished running node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:48:12.216336 (Thread-4): 11:48:12  Began running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:48:12.216674 (Thread-4): 11:48:12  14 of 15 START sql table model dbt_shabbirkdb.DimCustomer ...................... [RUN]
2023-04-20 11:48:12.217235 (Thread-4): 11:48:12  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomer'
2023-04-20 11:48:12.217444 (Thread-4): 11:48:12  Began compiling node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:48:12.222682 (Thread-4): 11:48:12  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:48:12.237464 (Thread-4): 11:48:12  Timing info for model.dbsql_dbt_tpch.DimCustomer (compile): 2023-04-20 11:48:12.217493 => 2023-04-20 11:48:12.237293
2023-04-20 11:48:12.237689 (Thread-4): 11:48:12  Began executing node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:48:12.241931 (Thread-4): 11:48:12  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:48:12.256869 (Thread-4): 11:48:12  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:48:12.257097 (Thread-4): 11:48:12  Using databricks connection "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:48:12.257331 (Thread-4): 11:48:12  On model.dbsql_dbt_tpch.DimCustomer: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 11:48:12.257471 (Thread-4): 11:48:12  Opening a new connection, currently in state closed
2023-04-20 11:48:12.269758 (Thread-504): handling poll request
2023-04-20 11:48:12.270193 (Thread-504): 11:48:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52858f130>]}
2023-04-20 11:48:12.271252 (Thread-504): sending response (<Response 68309 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:12.981558 (Thread-4): 11:48:12  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 11:48:12.981803 (Thread-4): 11:48:12  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:48:12.981939 (Thread-4): 11:48:12  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:48:12.982056 (Thread-4): 11:48:12  Databricks adapter: operation-id: b'\x01\xed\xdfq:\xb5\x13\x02\x96\x8b\xb3\xa9\xd7\x02]6'
2023-04-20 11:48:12.982293 (Thread-4): 11:48:12  Timing info for model.dbsql_dbt_tpch.DimCustomer (execute): 2023-04-20 11:48:12.237740 => 2023-04-20 11:48:12.982162
2023-04-20 11:48:12.982471 (Thread-4): 11:48:12  On model.dbsql_dbt_tpch.DimCustomer: ROLLBACK
2023-04-20 11:48:12.982594 (Thread-4): 11:48:12  Databricks adapter: NotImplemented: rollback
2023-04-20 11:48:12.982713 (Thread-4): 11:48:12  On model.dbsql_dbt_tpch.DimCustomer: Close
2023-04-20 11:48:13.273003 (Thread-4): 11:48:13  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:48:13.273405 (Thread-4): 11:48:13  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cbd58d16-85cf-467c-b2b3-8367c8d46fe6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d905aa670>]}
2023-04-20 11:48:13.273992 (Thread-4): 11:48:13  14 of 15 ERROR creating sql table model dbt_shabbirkdb.DimCustomer ............. [ERROR in 1.06s]
2023-04-20 11:48:13.274209 (Thread-4): 11:48:13  Finished running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:48:13.275262 (Thread-2): 11:48:13  Began running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 11:48:13.275553 (Thread-2): 11:48:13  15 of 15 SKIP relation dbt_shabbirkdb.FactWatches .............................. [SKIP]
2023-04-20 11:48:13.275767 (Thread-2): 11:48:13  Finished running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 11:48:13.277433 (MainThread): 11:48:13  Acquiring new databricks connection 'master'
2023-04-20 11:48:13.277655 (MainThread): 11:48:13  On master: ROLLBACK
2023-04-20 11:48:13.277810 (MainThread): 11:48:13  Opening a new connection, currently in state init
2023-04-20 11:48:13.292522 (Thread-505): handling ps request
2023-04-20 11:48:13.292977 (Thread-505): 11:48:13  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52858f910>]}
2023-04-20 11:48:13.294203 (Thread-505): sending response (<Response 14311 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:13.553949 (MainThread): 11:48:13  Databricks adapter: NotImplemented: rollback
2023-04-20 11:48:13.554196 (MainThread): 11:48:13  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:48:13.554336 (MainThread): 11:48:13  Spark adapter: NotImplemented: commit
2023-04-20 11:48:13.554496 (MainThread): 11:48:13  On master: ROLLBACK
2023-04-20 11:48:13.554628 (MainThread): 11:48:13  Databricks adapter: NotImplemented: rollback
2023-04-20 11:48:13.554765 (MainThread): 11:48:13  On master: Close
2023-04-20 11:48:13.776084 (Thread-506): handling status request
2023-04-20 11:48:13.776568 (Thread-506): 11:48:13  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52858fd60>]}
2023-04-20 11:48:13.777042 (Thread-506): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:13.833932 (MainThread): 11:48:13  Connection 'master' was properly closed.
2023-04-20 11:48:13.840309 (Thread-507): handling poll request
2023-04-20 11:48:13.840831 (Thread-507): 11:48:13  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52858ffa0>]}
2023-04-20 11:48:13.841554 (Thread-507): sending response (<Response 14727 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:13.834125 (MainThread): 11:48:13  Connection 'model.dbsql_dbt_tpch.Financial' was properly closed.
2023-04-20 11:48:13.834234 (MainThread): 11:48:13  Connection 'model.dbsql_dbt_tpch.DimSecurity' was properly closed.
2023-04-20 11:48:13.834335 (MainThread): 11:48:13  Connection 'model.dbsql_dbt_tpch.Prospect' was properly closed.
2023-04-20 11:48:13.834434 (MainThread): 11:48:13  Connection 'model.dbsql_dbt_tpch.DimCustomer' was properly closed.
2023-04-20 11:48:13.834759 (MainThread): 11:48:13  
2023-04-20 11:48:13.834907 (MainThread): 11:48:13  Finished running 15 table models in 0 hours 0 minutes and 16.56 seconds (16.56s).
2023-04-20 11:48:13.943248 (MainThread): 11:48:13  
2023-04-20 11:48:13.943549 (MainThread): 11:48:13  Completed with 4 errors and 0 warnings:
2023-04-20 11:48:13.943699 (MainThread): 11:48:13  
2023-04-20 11:48:13.943845 (MainThread): 11:48:13  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
2023-04-20 11:48:13.943971 (MainThread): 11:48:13    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:48:13.944083 (MainThread): 11:48:13  
2023-04-20 11:48:13.944202 (MainThread): 11:48:13  Runtime Error in model DimSecurity (models/silver/DimSecurity.sql)
2023-04-20 11:48:13.944312 (MainThread): 11:48:13    [CAST_INVALID_INPUT] The value 'BELJVClFSUDWQNFrxaHxDDEMSOMNqTeWbFtIh' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:48:13.944418 (MainThread): 11:48:13  
2023-04-20 11:48:13.944533 (MainThread): 11:48:13  Runtime Error in model Financial (models/silver/Financial.sql)
2023-04-20 11:48:13.944640 (MainThread): 11:48:13    Job aborted due to stage failure: Task failed while writing rows.
2023-04-20 11:48:13.944745 (MainThread): 11:48:13  
2023-04-20 11:48:13.944858 (MainThread): 11:48:13  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
2023-04-20 11:48:13.944964 (MainThread): 11:48:13    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:48:13.945088 (MainThread): 11:48:13  
2023-04-20 11:48:13.945213 (MainThread): 11:48:13  Done. PASS=5 WARN=0 ERROR=4 SKIP=6 TOTAL=15
2023-04-20 11:48:15.531401 (Thread-508): handling poll request
2023-04-20 11:48:15.531919 (Thread-508): 11:48:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52855a220>]}
2023-04-20 11:48:15.534219 (Thread-508): sending response (<Response 92663 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:15.937570 (Thread-509): handling ps request
2023-04-20 11:48:15.938082 (Thread-509): 11:48:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52855a310>]}
2023-04-20 11:48:15.939348 (Thread-509): sending response (<Response 14335 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:48:16.465091 (Thread-510): handling status request
2023-04-20 11:48:16.465577 (Thread-510): 11:48:16  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52855a580>]}
2023-04-20 11:48:16.466069 (Thread-510): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:49:17.381649 (Thread-511): handling status request
2023-04-20 11:49:17.382173 (Thread-511): 11:49:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52855a610>]}
2023-04-20 11:49:17.382658 (Thread-511): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:49:47.371994 (Thread-512): handling ps request
2023-04-20 11:49:47.372490 (Thread-512): 11:49:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52855a7f0>]}
2023-04-20 11:49:47.373687 (Thread-512): sending response (<Response 14335 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:49:47.374814 (Thread-513): handling run_sql request
2023-04-20 11:49:47.375208 (Thread-513): 11:49:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdc2c70>]}
2023-04-20 11:49:50.159178 (Thread-513): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:49:50.196041 (MainThread): 11:49:50  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0b96f8d6-30ef-4394-b4e6-22d33e9d5714', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8e2005f40>]}
2023-04-20 11:49:50.196648 (MainThread): 11:49:50  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:49:50.198027 (Thread-1): 11:49:50  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:49:50.198266 (Thread-1): 11:49:50  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:49:50.200210 (Thread-1): 11:49:50  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:49:50.198316 => 2023-04-20 11:49:50.200043
2023-04-20 11:49:50.200432 (Thread-1): 11:49:50  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:49:50.200806 (Thread-1): 11:49:50  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:49:50.201124 (Thread-1): 11:49:50  On rpc.dbsql_dbt_tpch.request: sk_companyid
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:49:50.201267 (Thread-1): 11:49:50  Opening a new connection, currently in state init
2023-04-20 11:49:50.706414 (Thread-514): handling ps request
2023-04-20 11:49:50.707086 (Thread-514): 11:49:50  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528552700>]}
2023-04-20 11:49:50.709195 (Thread-514): sending response (<Response 14845 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:49:50.730331 (Thread-515): handling ps request
2023-04-20 11:49:50.730720 (Thread-515): 11:49:50  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528552970>]}
2023-04-20 11:49:50.731886 (Thread-515): sending response (<Response 14845 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:49:50.814996 (Thread-1): 11:49:50  Databricks adapter: Error while running:
sk_companyid
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:49:50.815230 (Thread-1): 11:49:50  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'sk_companyid'.(line 1, pos 0)

== SQL ==
sk_companyid
^^^
limit 500
/* limit added automatically by dbt cloud */

2023-04-20 11:49:50.815360 (Thread-1): 11:49:50  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'sk_companyid'.(line 1, pos 0)

== SQL ==
sk_companyid
^^^
limit 500
/* limit added automatically by dbt cloud */

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'sk_companyid'.(line 1, pos 0)

== SQL ==
sk_companyid
^^^
limit 500
/* limit added automatically by dbt cloud */

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 11:49:50.815474 (Thread-1): 11:49:50  Databricks adapter: operation-id: b'\x01\xed\xdfqu\x19\x1a\x89\xb9\xda\xf79\xbafV\x9a'
2023-04-20 11:49:50.815705 (Thread-1): 11:49:50  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:49:50.200481 => 2023-04-20 11:49:50.815572
2023-04-20 11:49:50.815905 (Thread-1): 11:49:50  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:49:51.102121 (Thread-1): Got an exception: Runtime Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'sk_companyid'.(line 1, pos 0)
  
  == SQL ==
  sk_companyid
  ^^^
  limit 500
  /* limit added automatically by dbt cloud */
  
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'sk_companyid'.(line 1, pos 0)

== SQL ==
sk_companyid
^^^
limit 500
/* limit added automatically by dbt cloud */


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'sk_companyid'.(line 1, pos 0)
  
  == SQL ==
  sk_companyid
  ^^^
  limit 500
  /* limit added automatically by dbt cloud */
2023-04-20 11:49:51.104014 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': "Runtime Error in rpc request (from remote system.sql)\n  \n  [PARSE_SYNTAX_ERROR] Syntax error at or near 'sk_companyid'.(line 1, pos 0)\n  \n  == SQL ==\n  sk_companyid\n  ^^^\n  limit 500\n  /* limit added automatically by dbt cloud */\n  ", 'raw_code': 'sk_companyid\nlimit 500\n/* limit added automatically by dbt cloud */', 'compiled_code': 'sk_companyid\nlimit 500\n/* limit added automatically by dbt cloud */', 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': "Runtime Error in rpc request (from remote system.sql)\n  \n  [PARSE_SYNTAX_ERROR] Syntax error at or near 'sk_companyid'.(line 1, pos 0)\n  \n  == SQL ==\n  sk_companyid\n  ^^^\n  limit 500\n  /* limit added automatically by dbt cloud */\n  ", 'raw_code': 'sk_companyid\nlimit 500\n/* limit added automatically by dbt cloud */', 'compiled_code': 'sk_companyid\nlimit 500\n/* limit added automatically by dbt cloud */', 'tags': None}, None)
2023-04-20 11:49:51.147446 (Thread-516): handling status request
2023-04-20 11:49:51.147950 (Thread-516): 11:49:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528552e50>]}
2023-04-20 11:49:51.148492 (Thread-516): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:49:51.413141 (Thread-517): handling poll request
2023-04-20 11:49:51.413784 (Thread-517): 11:49:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285527f0>]}
2023-04-20 11:49:51.414513 (Thread-517): sending response (<Response 17769 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:49:51.537761 (Thread-518): handling poll request
2023-04-20 11:49:51.538257 (Thread-518): 11:49:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285440d0>]}
2023-04-20 11:49:51.538879 (Thread-518): sending response (<Response 17769 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:49:52.614643 (Thread-519): handling status request
2023-04-20 11:49:52.615153 (Thread-519): 11:49:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528544490>]}
2023-04-20 11:49:52.615705 (Thread-519): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:49:53.855424 (Thread-520): handling ps request
2023-04-20 11:49:53.856004 (Thread-520): 11:49:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528552b20>]}
2023-04-20 11:49:53.857269 (Thread-520): sending response (<Response 14868 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:49:54.280651 (Thread-521): handling status request
2023-04-20 11:49:54.281141 (Thread-521): 11:49:54  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285447f0>]}
2023-04-20 11:49:54.281620 (Thread-521): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:50:52.016893 (Thread-522): handling status request
2023-04-20 11:50:52.017389 (Thread-522): 11:50:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528544880>]}
2023-04-20 11:50:52.017870 (Thread-522): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:50:52.060557 (Thread-523): handling ps request
2023-04-20 11:50:52.060954 (Thread-523): 11:50:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528544a30>]}
2023-04-20 11:50:52.062150 (Thread-523): sending response (<Response 14868 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:50:53.169143 (Thread-524): handling ps request
2023-04-20 11:50:53.169642 (Thread-524): 11:50:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528544c40>]}
2023-04-20 11:50:53.170907 (Thread-524): sending response (<Response 14868 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:50:53.178370 (Thread-525): handling compile_sql request
2023-04-20 11:50:53.178698 (Thread-525): 11:50:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528544700>]}
2023-04-20 11:50:55.898767 (Thread-525): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:50:55.937892 (MainThread): 11:50:55  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '59d024ae-83ab-4bd6-bff0-3bece21b3ab6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa6359ef10>]}
2023-04-20 11:50:55.938476 (MainThread): 11:50:55  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:50:55.939830 (Thread-1): 11:50:55  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:50:55.940060 (Thread-1): 11:50:55  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:50:55.944474 (Thread-1): 11:50:55  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:50:55.940110 => 2023-04-20 11:50:55.944311
2023-04-20 11:50:55.944687 (Thread-1): 11:50:55  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:50:55.944845 (Thread-1): 11:50:55  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:50:55.944732 => 2023-04-20 11:50:55.944747
2023-04-20 11:50:56.439980 (Thread-526): handling ps request
2023-04-20 11:50:56.441364 (Thread-526): 11:50:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285443d0>]}
2023-04-20 11:50:56.441935 (Thread-527): handling ps request
2023-04-20 11:50:56.444079 (Thread-526): sending response (<Response 15411 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:50:56.444488 (Thread-527): 11:50:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52855a2b0>]}
2023-04-20 11:50:56.446059 (Thread-527): sending response (<Response 15411 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:50:56.964021 (Thread-528): handling status request
2023-04-20 11:50:56.964504 (Thread-528): 11:50:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528539cd0>]}
2023-04-20 11:50:56.990937 (Thread-528): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:50:57.105859 (Thread-529): handling poll request
2023-04-20 11:50:57.106327 (Thread-529): 11:50:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528539e80>]}
2023-04-20 11:50:57.107289 (Thread-529): sending response (<Response 14761 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:50:57.275800 (Thread-530): handling poll request
2023-04-20 11:50:57.276337 (Thread-530): 11:50:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528539fa0>]}
2023-04-20 11:50:57.277161 (Thread-530): sending response (<Response 14761 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:50:58.349962 (Thread-531): handling status request
2023-04-20 11:50:58.350456 (Thread-531): 11:50:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528552c40>]}
2023-04-20 11:50:58.350976 (Thread-531): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:54:28.680230 (Thread-532): handling status request
2023-04-20 11:54:28.682078 (Thread-532): 11:54:28  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52855a190>]}
2023-04-20 11:54:28.682576 (Thread-532): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:54:54.131157 (Thread-533): handling status request
2023-04-20 11:54:54.131707 (Thread-533): 11:54:54  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52855a160>]}
2023-04-20 11:54:54.132217 (Thread-533): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:55:32.281649 (Thread-534): handling status request
2023-04-20 11:55:32.282158 (Thread-534): 11:55:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285399a0>]}
2023-04-20 11:55:32.282677 (Thread-534): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:56:20.081550 (Thread-535): handling status request
2023-04-20 11:56:20.082047 (Thread-535): 11:56:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285395b0>]}
2023-04-20 11:56:20.082524 (Thread-535): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:56:26.551629 (Thread-536): handling status request
2023-04-20 11:56:26.552133 (Thread-536): 11:56:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528564760>]}
2023-04-20 11:56:26.552619 (Thread-536): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:56:26.686001 (Thread-537): handling ps request
2023-04-20 11:56:26.686496 (Thread-537): 11:56:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528564340>]}
2023-04-20 11:56:26.687837 (Thread-537): sending response (<Response 15411 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:56:27.549442 (Thread-538): handling run_sql request
2023-04-20 11:56:27.549942 (Thread-538): 11:56:27  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528564400>]}
2023-04-20 11:56:27.573837 (Thread-539): handling ps request
2023-04-20 11:56:27.575669 (Thread-539): 11:56:27  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52853b550>]}
2023-04-20 11:56:27.578826 (Thread-539): sending response (<Response 15926 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:56:28.193850 (Thread-540): handling poll request
2023-04-20 11:56:28.194332 (Thread-540): 11:56:28  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52853b790>]}
2023-04-20 11:56:28.194849 (Thread-540): sending response (<Response 424 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:56:28.684031 (Thread-541): handling status request
2023-04-20 11:56:28.684519 (Thread-541): 11:56:28  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52853b730>]}
2023-04-20 11:56:28.685071 (Thread-541): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:56:30.310891 (Thread-538): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:56:30.349033 (MainThread): 11:56:30  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fde8556f-1ada-497d-a09a-c22bac31fdc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd4734a6e50>]}
2023-04-20 11:56:30.349639 (MainThread): 11:56:30  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:56:30.351002 (Thread-1): 11:56:30  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:56:30.351236 (Thread-1): 11:56:30  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:56:30.355764 (Thread-1): 11:56:30  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:56:30.351283 => 2023-04-20 11:56:30.355593
2023-04-20 11:56:30.355980 (Thread-1): 11:56:30  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:56:30.356331 (Thread-1): 11:56:30  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:56:30.356783 (Thread-1): 11:56:30  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(iff(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:56:30.356928 (Thread-1): 11:56:30  Opening a new connection, currently in state init
2023-04-20 11:56:30.800151 (Thread-542): handling poll request
2023-04-20 11:56:30.800670 (Thread-542): 11:56:30  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52853b9d0>]}
2023-04-20 11:56:30.801297 (Thread-542): sending response (<Response 5944 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:56:30.927215 (Thread-543): handling ps request
2023-04-20 11:56:30.927752 (Thread-543): 11:56:30  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52853b8e0>]}
2023-04-20 11:56:30.929108 (Thread-543): sending response (<Response 15921 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:56:30.934602 (Thread-544): handling ps request
2023-04-20 11:56:30.934918 (Thread-544): 11:56:30  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52853bbe0>]}
2023-04-20 11:56:30.936044 (Thread-544): sending response (<Response 15921 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:56:31.150516 (Thread-1): 11:56:31  Databricks adapter: Error while running:

SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(iff(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:56:31.150760 (Thread-1): 11:56:31  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'AS'.(line 54, pos 44)

== SQL ==

SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(iff(substring(value, 161, 60)) AS conameorcik
--------------------------------------------^^^
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
limit 500
/* limit added automatically by dbt cloud */

2023-04-20 11:56:31.150888 (Thread-1): 11:56:31  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'AS'.(line 54, pos 44)

== SQL ==

SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(iff(substring(value, 161, 60)) AS conameorcik
--------------------------------------------^^^
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
limit 500
/* limit added automatically by dbt cloud */

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'AS'.(line 54, pos 44)

== SQL ==

SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(iff(substring(value, 161, 60)) AS conameorcik
--------------------------------------------^^^
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
limit 500
/* limit added automatically by dbt cloud */

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 11:56:31.151001 (Thread-1): 11:56:31  Databricks adapter: operation-id: b'\x01\xed\xdfrc\xa5\x18O\x92\x03#\x80N\x16\r?'
2023-04-20 11:56:31.151204 (Thread-1): 11:56:31  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:56:30.356026 => 2023-04-20 11:56:31.151071
2023-04-20 11:56:31.151397 (Thread-1): 11:56:31  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:56:31.248679 (Thread-545): handling status request
2023-04-20 11:56:31.249155 (Thread-545): 11:56:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52853bf40>]}
2023-04-20 11:56:31.249629 (Thread-545): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:56:31.468160 (Thread-1): Got an exception: Runtime Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'AS'.(line 54, pos 44)
  
  == SQL ==
  
  SELECT 
    Symbol,
    issue,
    status,
    Name,
    exchangeid,
    sk_companyid,
    sharesoutstanding,
    firsttrade,
    firsttradeonexchange,
    Dividend,
    if(enddate = date('9999-12-31'), True, False) iscurrent,
    1 batchid,
    effectivedate,
    enddate
  FROM (
    SELECT 
      fws.Symbol,
      fws.issue,
      fws.status,
      fws.Name,
      fws.exchangeid,
      dc.sk_companyid,
      fws.sharesoutstanding,
      fws.firsttrade,
      fws.firsttradeonexchange,
      fws.Dividend,
      if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
      if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
    FROM (
      SELECT 
        fws.* except(Status, conameorcik),
        nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,
        s.ST_NAME as status,
        coalesce(
          lead(effectivedate) OVER (
            PARTITION BY symbol
            ORDER BY effectivedate),
          date('9999-12-31')
        ) enddate
      FROM (
        SELECT
          date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
          trim(substring(value, 19, 15)) AS Symbol,
          trim(substring(value, 34, 6)) AS issue,
          trim(substring(value, 40, 4)) AS Status,
          trim(substring(value, 44, 70)) AS Name,
          trim(substring(value, 114, 6)) AS exchangeid,
          cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
          to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
          to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
          cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
          trim(iff(substring(value, 161, 60)) AS conameorcik
  --------------------------------------------^^^
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
        WHERE rectype = 'SEC'
        ) fws
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
        ON s.ST_ID = fws.status
      ) fws
    JOIN (
      SELECT 
        sk_companyid,
        name conameorcik,
        EffectiveDate,
        EndDate
      FROM `dbt_shabbirkdb`.`DimCompany`
      UNION ALL
      SELECT 
        sk_companyid,
        cast(companyid as string) conameorcik,
        EffectiveDate,
        EndDate
      FROM `dbt_shabbirkdb`.`DimCompany`
    ) dc 
    ON
      fws.conameorcik = dc.conameorcik 
      AND fws.EffectiveDate < dc.EndDate
      AND fws.EndDate > dc.EffectiveDate
  ) fws
  WHERE effectivedate != enddate
  limit 500
  /* limit added automatically by dbt cloud */
  
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'AS'.(line 54, pos 44)

== SQL ==

SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(iff(substring(value, 161, 60)) AS conameorcik
--------------------------------------------^^^
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
limit 500
/* limit added automatically by dbt cloud */


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'AS'.(line 54, pos 44)
  
  == SQL ==
  
  SELECT 
    Symbol,
    issue,
    status,
    Name,
    exchangeid,
    sk_companyid,
    sharesoutstanding,
    firsttrade,
    firsttradeonexchange,
    Dividend,
    if(enddate = date('9999-12-31'), True, False) iscurrent,
    1 batchid,
    effectivedate,
    enddate
  FROM (
    SELECT 
      fws.Symbol,
      fws.issue,
      fws.status,
      fws.Name,
      fws.exchangeid,
      dc.sk_companyid,
      fws.sharesoutstanding,
      fws.firsttrade,
      fws.firsttradeonexchange,
      fws.Dividend,
      if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
      if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
    FROM (
      SELECT 
        fws.* except(Status, conameorcik),
        nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,
        s.ST_NAME as status,
        coalesce(
          lead(effectivedate) OVER (
            PARTITION BY symbol
            ORDER BY effectivedate),
          date('9999-12-31')
        ) enddate
      FROM (
        SELECT
          date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
          trim(substring(value, 19, 15)) AS Symbol,
          trim(substring(value, 34, 6)) AS issue,
          trim(substring(value, 40, 4)) AS Status,
          trim(substring(value, 44, 70)) AS Name,
          trim(substring(value, 114, 6)) AS exchangeid,
          cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
          to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
          to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
          cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
          trim(iff(substring(value, 161, 60)) AS conameorcik
  --------------------------------------------^^^
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
        WHERE rectype = 'SEC'
        ) fws
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
        ON s.ST_ID = fws.status
      ) fws
    JOIN (
      SELECT 
        sk_companyid,
        name conameorcik,
        EffectiveDate,
        EndDate
      FROM `dbt_shabbirkdb`.`DimCompany`
      UNION ALL
      SELECT 
        sk_companyid,
        cast(companyid as string) conameorcik,
        EffectiveDate,
        EndDate
      FROM `dbt_shabbirkdb`.`DimCompany`
    ) dc 
    ON
      fws.conameorcik = dc.conameorcik 
      AND fws.EffectiveDate < dc.EndDate
      AND fws.EndDate > dc.EffectiveDate
  ) fws
  WHERE effectivedate != enddate
  limit 500
  /* limit added automatically by dbt cloud */
2023-04-20 11:56:31.470103 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': "Runtime Error in rpc request (from remote system.sql)\n  \n  [PARSE_SYNTAX_ERROR] Syntax error at or near 'AS'.(line 54, pos 44)\n  \n  == SQL ==\n  \n  SELECT \n    Symbol,\n    issue,\n    status,\n    Name,\n    exchangeid,\n    sk_companyid,\n    sharesoutstanding,\n    firsttrade,\n    firsttradeonexchange,\n    Dividend,\n    if(enddate = date('9999-12-31'), True, False) iscurrent,\n    1 batchid,\n    effectivedate,\n    enddate\n  FROM (\n    SELECT \n      fws.Symbol,\n      fws.issue,\n      fws.status,\n      fws.Name,\n      fws.exchangeid,\n      dc.sk_companyid,\n      fws.sharesoutstanding,\n      fws.firsttrade,\n      fws.firsttradeonexchange,\n      fws.Dividend,\n      if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,\n      if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate\n    FROM (\n      SELECT \n        fws.* except(Status, conameorcik),\n        nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,\n        s.ST_NAME as status,\n        coalesce(\n          lead(effectivedate) OVER (\n            PARTITION BY symbol\n            ORDER BY effectivedate),\n          date('9999-12-31')\n        ) enddate\n      FROM (\n        SELECT\n          date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,\n          trim(substring(value, 19, 15)) AS Symbol,\n          trim(substring(value, 34, 6)) AS issue,\n          trim(substring(value, 40, 4)) AS Status,\n          trim(substring(value, 44, 70)) AS Name,\n          trim(substring(value, 114, 6)) AS exchangeid,\n          cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,\n          to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,\n          to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,\n          cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,\n          trim(iff(substring(value, 161, 60)) AS conameorcik\n  --------------------------------------------^^^\n        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n        WHERE rectype = 'SEC'\n        ) fws\n      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s \n        ON s.ST_ID = fws.status\n      ) fws\n    JOIN (\n      SELECT \n        sk_companyid,\n        name conameorcik,\n        EffectiveDate,\n        EndDate\n      FROM `dbt_shabbirkdb`.`DimCompany`\n      UNION ALL\n      SELECT \n        sk_companyid,\n        cast(companyid as string) conameorcik,\n        EffectiveDate,\n        EndDate\n      FROM `dbt_shabbirkdb`.`DimCompany`\n    ) dc \n    ON\n      fws.conameorcik = dc.conameorcik \n      AND fws.EffectiveDate < dc.EndDate\n      AND fws.EndDate > dc.EffectiveDate\n  ) fws\n  WHERE effectivedate != enddate\n  limit 500\n  /* limit added automatically by dbt cloud */\n  ", 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  Symbol,\n  issue,\n  status,\n  Name,\n  exchangeid,\n  sk_companyid,\n  sharesoutstanding,\n  firsttrade,\n  firsttradeonexchange,\n  Dividend,\n  if(enddate = date('9999-12-31'), True, False) iscurrent,\n  1 batchid,\n  effectivedate,\n  enddate\nFROM (\n  SELECT \n    fws.Symbol,\n    fws.issue,\n    fws.status,\n    fws.Name,\n    fws.exchangeid,\n    dc.sk_companyid,\n    fws.sharesoutstanding,\n    fws.firsttrade,\n    fws.firsttradeonexchange,\n    fws.Dividend,\n    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,\n    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate\n  FROM (\n    SELECT \n      fws.* except(Status, conameorcik),\n      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,\n      s.ST_NAME as status,\n      coalesce(\n        lead(effectivedate) OVER (\n          PARTITION BY symbol\n          ORDER BY effectivedate),\n        date('9999-12-31')\n      ) enddate\n    FROM (\n      SELECT\n        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,\n        trim(substring(value, 19, 15)) AS Symbol,\n        trim(substring(value, 34, 6)) AS issue,\n        trim(substring(value, 40, 4)) AS Status,\n        trim(substring(value, 44, 70)) AS Name,\n        trim(substring(value, 114, 6)) AS exchangeid,\n        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,\n        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,\n        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,\n        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,\n        trim(iff(substring(value, 161, 60)) AS conameorcik\n      FROM {{ source('tpcdi', 'FinWire') }}\n      WHERE rectype = 'SEC'\n      ) fws\n    JOIN {{ source('tpcdi', 'StatusType') }}s \n      ON s.ST_ID = fws.status\n    ) fws\n  JOIN (\n    SELECT \n      sk_companyid,\n      name conameorcik,\n      EffectiveDate,\n      EndDate\n    FROM {{ ref('DimCompany') }}\n    UNION ALL\n    SELECT \n      sk_companyid,\n      cast(companyid as string) conameorcik,\n      EffectiveDate,\n      EndDate\n    FROM {{ ref('DimCompany') }}\n  ) dc \n  ON\n    fws.conameorcik = dc.conameorcik \n    AND fws.EffectiveDate < dc.EndDate\n    AND fws.EndDate > dc.EffectiveDate\n) fws\nWHERE effectivedate != enddate\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  Symbol,\n  issue,\n  status,\n  Name,\n  exchangeid,\n  sk_companyid,\n  sharesoutstanding,\n  firsttrade,\n  firsttradeonexchange,\n  Dividend,\n  if(enddate = date('9999-12-31'), True, False) iscurrent,\n  1 batchid,\n  effectivedate,\n  enddate\nFROM (\n  SELECT \n    fws.Symbol,\n    fws.issue,\n    fws.status,\n    fws.Name,\n    fws.exchangeid,\n    dc.sk_companyid,\n    fws.sharesoutstanding,\n    fws.firsttrade,\n    fws.firsttradeonexchange,\n    fws.Dividend,\n    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,\n    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate\n  FROM (\n    SELECT \n      fws.* except(Status, conameorcik),\n      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,\n      s.ST_NAME as status,\n      coalesce(\n        lead(effectivedate) OVER (\n          PARTITION BY symbol\n          ORDER BY effectivedate),\n        date('9999-12-31')\n      ) enddate\n    FROM (\n      SELECT\n        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,\n        trim(substring(value, 19, 15)) AS Symbol,\n        trim(substring(value, 34, 6)) AS issue,\n        trim(substring(value, 40, 4)) AS Status,\n        trim(substring(value, 44, 70)) AS Name,\n        trim(substring(value, 114, 6)) AS exchangeid,\n        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,\n        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,\n        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,\n        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,\n        trim(iff(substring(value, 161, 60)) AS conameorcik\n      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n      WHERE rectype = 'SEC'\n      ) fws\n    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s \n      ON s.ST_ID = fws.status\n    ) fws\n  JOIN (\n    SELECT \n      sk_companyid,\n      name conameorcik,\n      EffectiveDate,\n      EndDate\n    FROM `dbt_shabbirkdb`.`DimCompany`\n    UNION ALL\n    SELECT \n      sk_companyid,\n      cast(companyid as string) conameorcik,\n      EffectiveDate,\n      EndDate\n    FROM `dbt_shabbirkdb`.`DimCompany`\n  ) dc \n  ON\n    fws.conameorcik = dc.conameorcik \n    AND fws.EffectiveDate < dc.EndDate\n    AND fws.EndDate > dc.EffectiveDate\n) fws\nWHERE effectivedate != enddate\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': "Runtime Error in rpc request (from remote system.sql)\n  \n  [PARSE_SYNTAX_ERROR] Syntax error at or near 'AS'.(line 54, pos 44)\n  \n  == SQL ==\n  \n  SELECT \n    Symbol,\n    issue,\n    status,\n    Name,\n    exchangeid,\n    sk_companyid,\n    sharesoutstanding,\n    firsttrade,\n    firsttradeonexchange,\n    Dividend,\n    if(enddate = date('9999-12-31'), True, False) iscurrent,\n    1 batchid,\n    effectivedate,\n    enddate\n  FROM (\n    SELECT \n      fws.Symbol,\n      fws.issue,\n      fws.status,\n      fws.Name,\n      fws.exchangeid,\n      dc.sk_companyid,\n      fws.sharesoutstanding,\n      fws.firsttrade,\n      fws.firsttradeonexchange,\n      fws.Dividend,\n      if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,\n      if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate\n    FROM (\n      SELECT \n        fws.* except(Status, conameorcik),\n        nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,\n        s.ST_NAME as status,\n        coalesce(\n          lead(effectivedate) OVER (\n            PARTITION BY symbol\n            ORDER BY effectivedate),\n          date('9999-12-31')\n        ) enddate\n      FROM (\n        SELECT\n          date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,\n          trim(substring(value, 19, 15)) AS Symbol,\n          trim(substring(value, 34, 6)) AS issue,\n          trim(substring(value, 40, 4)) AS Status,\n          trim(substring(value, 44, 70)) AS Name,\n          trim(substring(value, 114, 6)) AS exchangeid,\n          cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,\n          to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,\n          to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,\n          cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,\n          trim(iff(substring(value, 161, 60)) AS conameorcik\n  --------------------------------------------^^^\n        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n        WHERE rectype = 'SEC'\n        ) fws\n      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s \n        ON s.ST_ID = fws.status\n      ) fws\n    JOIN (\n      SELECT \n        sk_companyid,\n        name conameorcik,\n        EffectiveDate,\n        EndDate\n      FROM `dbt_shabbirkdb`.`DimCompany`\n      UNION ALL\n      SELECT \n        sk_companyid,\n        cast(companyid as string) conameorcik,\n        EffectiveDate,\n        EndDate\n      FROM `dbt_shabbirkdb`.`DimCompany`\n    ) dc \n    ON\n      fws.conameorcik = dc.conameorcik \n      AND fws.EffectiveDate < dc.EndDate\n      AND fws.EndDate > dc.EffectiveDate\n  ) fws\n  WHERE effectivedate != enddate\n  limit 500\n  /* limit added automatically by dbt cloud */\n  ", 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  Symbol,\n  issue,\n  status,\n  Name,\n  exchangeid,\n  sk_companyid,\n  sharesoutstanding,\n  firsttrade,\n  firsttradeonexchange,\n  Dividend,\n  if(enddate = date('9999-12-31'), True, False) iscurrent,\n  1 batchid,\n  effectivedate,\n  enddate\nFROM (\n  SELECT \n    fws.Symbol,\n    fws.issue,\n    fws.status,\n    fws.Name,\n    fws.exchangeid,\n    dc.sk_companyid,\n    fws.sharesoutstanding,\n    fws.firsttrade,\n    fws.firsttradeonexchange,\n    fws.Dividend,\n    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,\n    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate\n  FROM (\n    SELECT \n      fws.* except(Status, conameorcik),\n      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,\n      s.ST_NAME as status,\n      coalesce(\n        lead(effectivedate) OVER (\n          PARTITION BY symbol\n          ORDER BY effectivedate),\n        date('9999-12-31')\n      ) enddate\n    FROM (\n      SELECT\n        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,\n        trim(substring(value, 19, 15)) AS Symbol,\n        trim(substring(value, 34, 6)) AS issue,\n        trim(substring(value, 40, 4)) AS Status,\n        trim(substring(value, 44, 70)) AS Name,\n        trim(substring(value, 114, 6)) AS exchangeid,\n        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,\n        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,\n        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,\n        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,\n        trim(iff(substring(value, 161, 60)) AS conameorcik\n      FROM {{ source('tpcdi', 'FinWire') }}\n      WHERE rectype = 'SEC'\n      ) fws\n    JOIN {{ source('tpcdi', 'StatusType') }}s \n      ON s.ST_ID = fws.status\n    ) fws\n  JOIN (\n    SELECT \n      sk_companyid,\n      name conameorcik,\n      EffectiveDate,\n      EndDate\n    FROM {{ ref('DimCompany') }}\n    UNION ALL\n    SELECT \n      sk_companyid,\n      cast(companyid as string) conameorcik,\n      EffectiveDate,\n      EndDate\n    FROM {{ ref('DimCompany') }}\n  ) dc \n  ON\n    fws.conameorcik = dc.conameorcik \n    AND fws.EffectiveDate < dc.EndDate\n    AND fws.EndDate > dc.EffectiveDate\n) fws\nWHERE effectivedate != enddate\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  Symbol,\n  issue,\n  status,\n  Name,\n  exchangeid,\n  sk_companyid,\n  sharesoutstanding,\n  firsttrade,\n  firsttradeonexchange,\n  Dividend,\n  if(enddate = date('9999-12-31'), True, False) iscurrent,\n  1 batchid,\n  effectivedate,\n  enddate\nFROM (\n  SELECT \n    fws.Symbol,\n    fws.issue,\n    fws.status,\n    fws.Name,\n    fws.exchangeid,\n    dc.sk_companyid,\n    fws.sharesoutstanding,\n    fws.firsttrade,\n    fws.firsttradeonexchange,\n    fws.Dividend,\n    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,\n    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate\n  FROM (\n    SELECT \n      fws.* except(Status, conameorcik),\n      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,\n      s.ST_NAME as status,\n      coalesce(\n        lead(effectivedate) OVER (\n          PARTITION BY symbol\n          ORDER BY effectivedate),\n        date('9999-12-31')\n      ) enddate\n    FROM (\n      SELECT\n        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,\n        trim(substring(value, 19, 15)) AS Symbol,\n        trim(substring(value, 34, 6)) AS issue,\n        trim(substring(value, 40, 4)) AS Status,\n        trim(substring(value, 44, 70)) AS Name,\n        trim(substring(value, 114, 6)) AS exchangeid,\n        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,\n        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,\n        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,\n        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,\n        trim(iff(substring(value, 161, 60)) AS conameorcik\n      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n      WHERE rectype = 'SEC'\n      ) fws\n    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s \n      ON s.ST_ID = fws.status\n    ) fws\n  JOIN (\n    SELECT \n      sk_companyid,\n      name conameorcik,\n      EffectiveDate,\n      EndDate\n    FROM `dbt_shabbirkdb`.`DimCompany`\n    UNION ALL\n    SELECT \n      sk_companyid,\n      cast(companyid as string) conameorcik,\n      EffectiveDate,\n      EndDate\n    FROM `dbt_shabbirkdb`.`DimCompany`\n  ) dc \n  ON\n    fws.conameorcik = dc.conameorcik \n    AND fws.EffectiveDate < dc.EndDate\n    AND fws.EndDate > dc.EffectiveDate\n) fws\nWHERE effectivedate != enddate\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 11:56:31.679929 (Thread-546): handling poll request
2023-04-20 11:56:31.680433 (Thread-546): 11:56:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52853bca0>]}
2023-04-20 11:56:31.681342 (Thread-546): sending response (<Response 52653 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:56:33.332167 (Thread-547): handling poll request
2023-04-20 11:56:33.332662 (Thread-547): 11:56:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52853bd30>]}
2023-04-20 11:56:33.333494 (Thread-547): sending response (<Response 60201 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:56:33.425683 (Thread-548): handling poll request
2023-04-20 11:56:33.426178 (Thread-548): 11:56:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5284f23d0>]}
2023-04-20 11:56:33.426947 (Thread-548): sending response (<Response 60201 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:56:33.573768 (Thread-549): handling ps request
2023-04-20 11:56:33.574272 (Thread-549): 11:56:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5284f2790>]}
2023-04-20 11:56:33.576250 (Thread-549): sending response (<Response 15944 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:56:34.476621 (Thread-550): handling status request
2023-04-20 11:56:34.477120 (Thread-550): 11:56:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52853b820>]}
2023-04-20 11:56:34.477633 (Thread-550): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:08.811448 (Thread-551): handling ps request
2023-04-20 11:57:08.811980 (Thread-551): 11:57:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52853b730>]}
2023-04-20 11:57:08.813261 (Thread-551): sending response (<Response 15944 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:08.838330 (Thread-552): handling run_sql request
2023-04-20 11:57:08.838770 (Thread-552): 11:57:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52853b760>]}
2023-04-20 11:57:11.583731 (Thread-552): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:11.618430 (MainThread): 11:57:11  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a361be05-7327-4e35-9051-cfeb015ebb7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7dfd552f10>]}
2023-04-20 11:57:11.619032 (MainThread): 11:57:11  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:57:11.620404 (Thread-1): 11:57:11  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:57:11.620637 (Thread-1): 11:57:11  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:57:11.625162 (Thread-1): 11:57:11  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:57:11.620693 => 2023-04-20 11:57:11.624991
2023-04-20 11:57:11.625379 (Thread-1): 11:57:11  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:57:11.625738 (Thread-1): 11:57:11  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:57:11.626210 (Thread-1): 11:57:11  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:57:11.626358 (Thread-1): 11:57:11  Opening a new connection, currently in state init
2023-04-20 11:57:12.154178 (Thread-553): handling ps request
2023-04-20 11:57:12.154839 (Thread-553): 11:57:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528539400>]}
2023-04-20 11:57:12.157008 (Thread-553): sending response (<Response 16454 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:12.175627 (Thread-554): handling ps request
2023-04-20 11:57:12.175981 (Thread-554): 11:57:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528539d30>]}
2023-04-20 11:57:12.177122 (Thread-554): sending response (<Response 16454 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:12.575706 (Thread-555): handling status request
2023-04-20 11:57:12.576265 (Thread-555): 11:57:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52853bac0>]}
2023-04-20 11:57:12.576809 (Thread-555): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:12.846243 (Thread-556): handling poll request
2023-04-20 11:57:12.846737 (Thread-556): 11:57:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528539c70>]}
2023-04-20 11:57:12.847369 (Thread-556): sending response (<Response 5939 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:12.957200 (Thread-1): 11:57:12  SQL status: OK in 1.33 seconds
2023-04-20 11:57:12.985583 (Thread-557): handling poll request
2023-04-20 11:57:12.986061 (Thread-557): 11:57:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528539c10>]}
2023-04-20 11:57:12.986654 (Thread-557): sending response (<Response 6213 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:12.989124 (Thread-1): 11:57:12  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:57:11.625427 => 2023-04-20 11:57:12.988895
2023-04-20 11:57:12.989387 (Thread-1): 11:57:12  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:57:13.400412 (Thread-558): handling status request
2023-04-20 11:57:13.400931 (Thread-558): 11:57:13  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52858fa30>]}
2023-04-20 11:57:13.428708 (Thread-558): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:14.482855 (Thread-559): handling poll request
2023-04-20 11:57:14.483418 (Thread-559): 11:57:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5622c77c0>]}
2023-04-20 11:57:14.489649 (Thread-559): sending response (<Response 120934 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:14.720320 (Thread-560): handling ps request
2023-04-20 11:57:14.720766 (Thread-560): 11:57:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbcb640>]}
2023-04-20 11:57:14.722086 (Thread-560): sending response (<Response 16479 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:15.173337 (Thread-561): handling status request
2023-04-20 11:57:15.173839 (Thread-561): 11:57:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bda7580>]}
2023-04-20 11:57:15.174342 (Thread-561): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:15.564638 (Thread-562): handling poll request
2023-04-20 11:57:15.565132 (Thread-562): 11:57:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb180d0>]}
2023-04-20 11:57:15.571083 (Thread-562): sending response (<Response 126457 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:16.589077 (Thread-563): handling status request
2023-04-20 11:57:16.589613 (Thread-563): 11:57:16  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbcb6d0>]}
2023-04-20 11:57:16.590122 (Thread-563): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:16.834236 (Thread-564): handling ps request
2023-04-20 11:57:16.834735 (Thread-564): 11:57:16  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbfb850>]}
2023-04-20 11:57:16.836076 (Thread-564): sending response (<Response 16479 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:20.559152 (Thread-565): handling status request
2023-04-20 11:57:20.559704 (Thread-565): 11:57:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdadee0>]}
2023-04-20 11:57:20.560213 (Thread-565): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:21.176951 (Thread-566): handling cli_args request
2023-04-20 11:57:21.177425 (Thread-566): 11:57:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5284f2a00>]}
2023-04-20 11:57:24.029580 (Thread-566): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:24.088770 (MainThread): 11:57:24  checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
2023-04-20 11:57:24.407874 (MainThread): 11:57:24  Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
2023-04-20 11:57:24.408151 (MainThread): 11:57:24  Partial parsing enabled, no changes found, skipping parsing
2023-04-20 11:57:24.416738 (MainThread): 11:57:24  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3e55f136-6a70-4d86-aacd-1d698687ef4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab0a19e850>]}
2023-04-20 11:57:24.454454 (MainThread): 11:57:24  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3e55f136-6a70-4d86-aacd-1d698687ef4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab0a207e80>]}
2023-04-20 11:57:24.454846 (MainThread): 11:57:24  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:57:24.455000 (MainThread): 11:57:24  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3e55f136-6a70-4d86-aacd-1d698687ef4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab0a207c70>]}
2023-04-20 11:57:24.457159 (MainThread): 11:57:24  
2023-04-20 11:57:24.458490 (MainThread): 11:57:24  Acquiring new databricks connection 'master'
2023-04-20 11:57:24.460318 (ThreadPoolExecutor-0_0): 11:57:24  Acquiring new databricks connection 'list_schemas'
2023-04-20 11:57:24.541831 (ThreadPoolExecutor-0_0): 11:57:24  Using databricks connection "list_schemas"
2023-04-20 11:57:24.542213 (ThreadPoolExecutor-0_0): 11:57:24  On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_schemas"} */

    show databases
  
2023-04-20 11:57:24.542385 (ThreadPoolExecutor-0_0): 11:57:24  Opening a new connection, currently in state init
2023-04-20 11:57:24.703510 (Thread-567): handling ps request
2023-04-20 11:57:24.704220 (Thread-567): 11:57:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbc1e50>]}
2023-04-20 11:57:24.706563 (Thread-567): sending response (<Response 16861 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:24.818304 (Thread-568): handling poll request
2023-04-20 11:57:24.818794 (Thread-568): 11:57:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbc1970>]}
2023-04-20 11:57:24.819505 (Thread-568): sending response (<Response 5012 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:24.834092 (Thread-569): handling status request
2023-04-20 11:57:24.834482 (Thread-569): 11:57:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbc1fa0>]}
2023-04-20 11:57:24.834972 (Thread-569): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:25.213104 (Thread-570): handling ps request
2023-04-20 11:57:25.213598 (Thread-570): 11:57:25  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bbc1be0>]}
2023-04-20 11:57:25.214958 (Thread-570): sending response (<Response 16861 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:25.526564 (ThreadPoolExecutor-0_0): 11:57:25  SQL status: OK in 0.98 seconds
2023-04-20 11:57:25.587985 (ThreadPoolExecutor-0_0): 11:57:25  On list_schemas: Close
2023-04-20 11:57:25.878329 (ThreadPoolExecutor-1_0): 11:57:25  Acquiring new databricks connection 'list_None_dbt_shabbirkdb'
2023-04-20 11:57:25.889055 (ThreadPoolExecutor-1_0): 11:57:25  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:57:25.889262 (ThreadPoolExecutor-1_0): 11:57:25  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 11:57:25.889423 (ThreadPoolExecutor-1_0): 11:57:25  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show tables in `dbt_shabbirkdb`
  
2023-04-20 11:57:25.889576 (ThreadPoolExecutor-1_0): 11:57:25  Opening a new connection, currently in state closed
2023-04-20 11:57:26.605228 (ThreadPoolExecutor-1_0): 11:57:26  SQL status: OK in 0.72 seconds
2023-04-20 11:57:26.614319 (ThreadPoolExecutor-1_0): 11:57:26  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 11:57:26.614536 (ThreadPoolExecutor-1_0): 11:57:26  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show views in `dbt_shabbirkdb`
  
2023-04-20 11:57:27.034505 (ThreadPoolExecutor-1_0): 11:57:27  SQL status: OK in 0.42 seconds
2023-04-20 11:57:27.038098 (ThreadPoolExecutor-1_0): 11:57:27  On list_None_dbt_shabbirkdb: ROLLBACK
2023-04-20 11:57:27.038314 (ThreadPoolExecutor-1_0): 11:57:27  Databricks adapter: NotImplemented: rollback
2023-04-20 11:57:27.038482 (ThreadPoolExecutor-1_0): 11:57:27  On list_None_dbt_shabbirkdb: Close
2023-04-20 11:57:27.074657 (Thread-571): handling poll request
2023-04-20 11:57:27.075122 (Thread-571): 11:57:27  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54b98d130>]}
2023-04-20 11:57:27.075785 (Thread-571): sending response (<Response 5013 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:27.330441 (MainThread): 11:57:27  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3e55f136-6a70-4d86-aacd-1d698687ef4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab0b00b9a0>]}
2023-04-20 11:57:27.330853 (MainThread): 11:57:27  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:57:27.331006 (MainThread): 11:57:27  Spark adapter: NotImplemented: commit
2023-04-20 11:57:27.331551 (MainThread): 11:57:27  Concurrency: 4 threads (target='default')
2023-04-20 11:57:27.331712 (MainThread): 11:57:27  
2023-04-20 11:57:27.334477 (Thread-1): 11:57:27  Began running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:57:27.334839 (Thread-1): 11:57:27  1 of 15 START sql table model dbt_shabbirkdb.DimBroker ......................... [RUN]
2023-04-20 11:57:27.335321 (Thread-1): 11:57:27  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
2023-04-20 11:57:27.335507 (Thread-1): 11:57:27  Began compiling node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:57:27.339568 (Thread-2): 11:57:27  Began running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:57:27.339879 (Thread-2): 11:57:27  2 of 15 START sql table model dbt_shabbirkdb.DimCompany ........................ [RUN]
2023-04-20 11:57:27.340392 (Thread-2): 11:57:27  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCompany'
2023-04-20 11:57:27.340571 (Thread-2): 11:57:27  Began compiling node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:57:27.344630 (Thread-3): 11:57:27  Began running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:57:27.344916 (Thread-3): 11:57:27  3 of 15 START sql table model dbt_shabbirkdb.DimCustomerStg .................... [RUN]
2023-04-20 11:57:27.345407 (Thread-3): 11:57:27  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomerStg'
2023-04-20 11:57:27.345581 (Thread-3): 11:57:27  Began compiling node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:57:27.349772 (Thread-1): 11:57:27  Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:57:27.350243 (Thread-2): 11:57:27  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:57:27.350613 (Thread-4): 11:57:27  Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:57:27.350903 (Thread-4): 11:57:27  4 of 15 START sql table model dbt_shabbirkdb.tempDailyMarketHistorical ......... [RUN]
2023-04-20 11:57:27.351396 (Thread-4): 11:57:27  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempDailyMarketHistorical'
2023-04-20 11:57:27.351602 (Thread-4): 11:57:27  Began compiling node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:57:27.355348 (Thread-4): 11:57:27  Writing injected SQL for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:57:27.357184 (Thread-3): 11:57:27  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:57:27.369181 (Thread-2): 11:57:27  Timing info for model.dbsql_dbt_tpch.DimCompany (compile): 2023-04-20 11:57:27.340616 => 2023-04-20 11:57:27.368988
2023-04-20 11:57:27.369426 (Thread-2): 11:57:27  Began executing node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:57:27.374640 (Thread-1): 11:57:27  Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 2023-04-20 11:57:27.335587 => 2023-04-20 11:57:27.374460
2023-04-20 11:57:27.374862 (Thread-1): 11:57:27  Began executing node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:57:27.379759 (Thread-4): 11:57:27  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (compile): 2023-04-20 11:57:27.351647 => 2023-04-20 11:57:27.379577
2023-04-20 11:57:27.379983 (Thread-4): 11:57:27  Began executing node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:57:27.406665 (Thread-4): 11:57:27  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:57:27.406864 (Thread-4): 11:57:27  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:57:27.407026 (Thread-4): 11:57:27  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

      describe extended `dbt_shabbirkdb`.`tempdailymarkethistorical`
  
2023-04-20 11:57:27.407161 (Thread-4): 11:57:27  Opening a new connection, currently in state init
2023-04-20 11:57:27.409477 (Thread-2): 11:57:27  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:57:27.409658 (Thread-2): 11:57:27  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:57:27.409813 (Thread-2): 11:57:27  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

      describe extended `dbt_shabbirkdb`.`dimcompany`
  
2023-04-20 11:57:27.409955 (Thread-2): 11:57:27  Opening a new connection, currently in state init
2023-04-20 11:57:27.412398 (Thread-1): 11:57:27  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:57:27.412577 (Thread-1): 11:57:27  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:57:27.412733 (Thread-1): 11:57:27  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

      describe extended `dbt_shabbirkdb`.`dimbroker`
  
2023-04-20 11:57:27.412860 (Thread-1): 11:57:27  Opening a new connection, currently in state closed
2023-04-20 11:57:27.427206 (Thread-3): 11:57:27  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (compile): 2023-04-20 11:57:27.345623 => 2023-04-20 11:57:27.427012
2023-04-20 11:57:27.427465 (Thread-3): 11:57:27  Began executing node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:57:27.431949 (Thread-3): 11:57:27  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:57:27.432130 (Thread-3): 11:57:27  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:57:27.432283 (Thread-3): 11:57:27  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

      describe extended `dbt_shabbirkdb`.`dimcustomerstg`
  
2023-04-20 11:57:27.432498 (Thread-3): 11:57:27  Opening a new connection, currently in state init
2023-04-20 11:57:28.047794 (Thread-572): handling ps request
2023-04-20 11:57:28.048285 (Thread-572): 11:57:28  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285842b0>]}
2023-04-20 11:57:28.049644 (Thread-572): sending response (<Response 16861 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:28.187785 (Thread-4): 11:57:28  SQL status: OK in 0.78 seconds
2023-04-20 11:57:28.239605 (Thread-4): 11:57:28  Writing runtime sql for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:57:28.240000 (Thread-2): 11:57:28  SQL status: OK in 0.83 seconds
2023-04-20 11:57:28.240399 (Thread-3): 11:57:28  SQL status: OK in 0.81 seconds
2023-04-20 11:57:28.240901 (Thread-1): 11:57:28  SQL status: OK in 0.83 seconds
2023-04-20 11:57:28.248948 (Thread-3): 11:57:28  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:57:28.253641 (Thread-1): 11:57:28  Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:57:28.326895 (Thread-2): 11:57:28  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:57:28.329658 (Thread-1): 11:57:28  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:57:28.329910 (Thread-1): 11:57:28  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT
  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate`) effectivedate,
  date('9999-12-31') enddate
FROM  `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`HR`
WHERE employeejobcode = 314
  
2023-04-20 11:57:28.333458 (Thread-4): 11:57:28  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:57:28.333687 (Thread-4): 11:57:28  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempDailyMarketHistorical`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  dmh.*,
  sk_dateid,
  min(dm_low) OVER (
    PARTITION BY dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweeklow,
  max(dm_high) OVER (
    PARTITION by dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweekhigh
FROM (
  SELECT * FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketHistorical`
  UNION ALL
  SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketIncremental`) dmh
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
  ON d.datevalue = dm_date;
  
2023-04-20 11:57:28.340397 (Thread-3): 11:57:28  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:57:28.340736 (Thread-3): 11:57:28  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomerStg`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT * FROM (
  SELECT
    customerid,
    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) taxid,
    status,
    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) lastname,
    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) firstname,
    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) middleinitial,
    coalesce(gender, last_value(gender) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) gender,
    coalesce(tier, last_value(tier) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) tier,
    coalesce(dob, last_value(dob) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) dob,
    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline1,
    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline2,
    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) postalcode,
    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) CITY,
    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) stateprov,
    coalesce(country, last_value(country) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) country,
    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone1,
    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone2,
    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone3,
    coalesce(email1, last_value(email1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email1,
    coalesce(email2, last_value(email2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email2,
    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) LCL_TX_ID,
    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) NAT_TX_ID,
    batchid,
    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,
    date(update_ts) effectivedate,
    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate
  FROM (
    SELECT
      customerid,
      taxid,
      status,
      lastname,
      firstname,
      middleinitial,
      gender,
      tier,
      dob,
      addressline1,
      addressline2,
      postalcode,
      city,
      stateprov,
      country,
      phone1,
      phone2,
      phone3,
      email1,
      email2,
      lcl_tx_id,
      nat_tx_id,
      1 batchid,
      update_ts
    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')
    UNION ALL
    SELECT
      c.customerid,
      nullif(c.taxid, '') taxid,
      nullif(s.st_name, '') as status,
      nullif(c.lastname, '') lastname,
      nullif(c.firstname, '') firstname,
      nullif(c.middleinitial, '') middleinitial,
      gender,
      c.tier,
      c.dob,
      nullif(c.addressline1, '') addressline1,
      nullif(c.addressline2, '') addressline2,
      nullif(c.postalcode, '') postalcode,
      nullif(c.city, '') city,
      nullif(c.stateprov, '') stateprov,
      nullif(c.country, '') country,
      CASE
        WHEN isnull(c_local_1) then c_local_1
        ELSE concat(
          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),
          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),
          c_local_1,
          nvl(c_ext_1, '')) END as phone1,
      CASE
        WHEN isnull(c_local_2) then c_local_2
        ELSE concat(
          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),
          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),
          c_local_2,
          nvl(c_ext_2, '')) END as phone2,
      CASE
        WHEN isnull(c_local_3) then c_local_3
        ELSE concat(
          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),
          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),
          c_local_3,
          nvl(c_ext_3, '')) END as phone3,
      nullif(c.email1, '') email1,
      nullif(c.email2, '') email2,
      c.LCL_TX_ID, 
      c.NAT_TX_ID,
      c.batchid,
      timestamp(bd.batchdate) update_ts
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CustomerIncremental` c
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
      ON c.batchid = bd.batchid
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` s 
      ON c.status = s.st_id
  ) c
  )
  
2023-04-20 11:57:28.341902 (Thread-2): 11:57:28  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:57:28.342157 (Thread-2): 11:57:28  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCompany`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  * FROM (
  SELECT
    md5(cik) sk_companyid,
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
  
2023-04-20 11:57:29.130479 (Thread-573): handling status request
2023-04-20 11:57:29.130971 (Thread-573): 11:57:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54b98d910>]}
2023-04-20 11:57:29.131457 (Thread-573): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:29.497792 (Thread-574): handling poll request
2023-04-20 11:57:29.498285 (Thread-574): 11:57:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528595c10>]}
2023-04-20 11:57:29.499544 (Thread-574): sending response (<Response 37176 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:31.110639 (Thread-575): handling poll request
2023-04-20 11:57:31.111133 (Thread-575): 11:57:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52859d610>]}
2023-04-20 11:57:31.111648 (Thread-575): sending response (<Response 290 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:31.290223 (Thread-1): 11:57:31  SQL status: OK in 2.96 seconds
2023-04-20 11:57:31.296405 (Thread-576): handling ps request
2023-04-20 11:57:31.296943 (Thread-576): 11:57:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e14cdc0>]}
2023-04-20 11:57:31.298256 (Thread-576): sending response (<Response 16862 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:31.317228 (Thread-1): 11:57:31  Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 2023-04-20 11:57:27.374910 => 2023-04-20 11:57:31.317065
2023-04-20 11:57:31.317464 (Thread-1): 11:57:31  On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
2023-04-20 11:57:31.317610 (Thread-1): 11:57:31  Databricks adapter: NotImplemented: rollback
2023-04-20 11:57:31.317733 (Thread-1): 11:57:31  On model.dbsql_dbt_tpch.DimBroker: Close
2023-04-20 11:57:31.596601 (Thread-1): 11:57:31  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3e55f136-6a70-4d86-aacd-1d698687ef4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab08f9a910>]}
2023-04-20 11:57:31.597113 (Thread-1): 11:57:31  1 of 15 OK created sql table model dbt_shabbirkdb.DimBroker .................... [OK in 4.26s]
2023-04-20 11:57:31.598288 (Thread-1): 11:57:31  Finished running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:57:31.755185 (Thread-577): handling status request
2023-04-20 11:57:31.755697 (Thread-577): 11:57:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54ba00ee0>]}
2023-04-20 11:57:31.756192 (Thread-577): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:31.917397 (Thread-2): 11:57:31  SQL status: OK in 3.57 seconds
2023-04-20 11:57:31.919749 (Thread-2): 11:57:31  Timing info for model.dbsql_dbt_tpch.DimCompany (execute): 2023-04-20 11:57:27.369475 => 2023-04-20 11:57:31.919587
2023-04-20 11:57:31.919968 (Thread-2): 11:57:31  On model.dbsql_dbt_tpch.DimCompany: ROLLBACK
2023-04-20 11:57:31.920110 (Thread-2): 11:57:31  Databricks adapter: NotImplemented: rollback
2023-04-20 11:57:31.920232 (Thread-2): 11:57:31  On model.dbsql_dbt_tpch.DimCompany: Close
2023-04-20 11:57:32.194344 (Thread-3): 11:57:32  SQL status: OK in 3.85 seconds
2023-04-20 11:57:32.196916 (Thread-3): 11:57:32  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (execute): 2023-04-20 11:57:27.427538 => 2023-04-20 11:57:32.196759
2023-04-20 11:57:32.197127 (Thread-3): 11:57:32  On model.dbsql_dbt_tpch.DimCustomerStg: ROLLBACK
2023-04-20 11:57:32.197268 (Thread-3): 11:57:32  Databricks adapter: NotImplemented: rollback
2023-04-20 11:57:32.197389 (Thread-3): 11:57:32  On model.dbsql_dbt_tpch.DimCustomerStg: Close
2023-04-20 11:57:32.205969 (Thread-2): 11:57:32  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3e55f136-6a70-4d86-aacd-1d698687ef4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab08f98df0>]}
2023-04-20 11:57:32.206527 (Thread-2): 11:57:32  2 of 15 OK created sql table model dbt_shabbirkdb.DimCompany ................... [OK in 4.87s]
2023-04-20 11:57:32.206848 (Thread-2): 11:57:32  Finished running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:57:32.207892 (Thread-1): 11:57:32  Began running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:57:32.208252 (Thread-1): 11:57:32  5 of 15 START sql table model dbt_shabbirkdb.DimSecurity ....................... [RUN]
2023-04-20 11:57:32.208744 (Thread-1): 11:57:32  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimSecurity'
2023-04-20 11:57:32.208929 (Thread-1): 11:57:32  Began compiling node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:57:32.213142 (Thread-2): 11:57:32  Began running node model.dbsql_dbt_tpch.Financial
2023-04-20 11:57:32.213443 (Thread-2): 11:57:32  6 of 15 START sql table model dbt_shabbirkdb.Financial ......................... [RUN]
2023-04-20 11:57:32.213963 (Thread-2): 11:57:32  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Financial'
2023-04-20 11:57:32.214145 (Thread-2): 11:57:32  Began compiling node model.dbsql_dbt_tpch.Financial
2023-04-20 11:57:32.219561 (Thread-1): 11:57:32  Writing injected SQL for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:57:32.219918 (Thread-2): 11:57:32  Writing injected SQL for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:57:32.233918 (Thread-1): 11:57:32  Timing info for model.dbsql_dbt_tpch.DimSecurity (compile): 2023-04-20 11:57:32.208974 => 2023-04-20 11:57:32.233714
2023-04-20 11:57:32.234164 (Thread-1): 11:57:32  Began executing node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:57:32.238318 (Thread-1): 11:57:32  Writing runtime sql for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:57:32.238709 (Thread-2): 11:57:32  Timing info for model.dbsql_dbt_tpch.Financial (compile): 2023-04-20 11:57:32.214189 => 2023-04-20 11:57:32.238536
2023-04-20 11:57:32.238925 (Thread-2): 11:57:32  Began executing node model.dbsql_dbt_tpch.Financial
2023-04-20 11:57:32.242714 (Thread-2): 11:57:32  Writing runtime sql for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:57:32.255926 (Thread-2): 11:57:32  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:57:32.256123 (Thread-2): 11:57:32  Using databricks connection "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:57:32.256344 (Thread-2): 11:57:32  On model.dbsql_dbt_tpch.Financial: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 11:57:32.256481 (Thread-2): 11:57:32  Opening a new connection, currently in state closed
2023-04-20 11:57:32.257647 (Thread-1): 11:57:32  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:57:32.257845 (Thread-1): 11:57:32  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:57:32.258100 (Thread-1): 11:57:32  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 11:57:32.258247 (Thread-1): 11:57:32  Opening a new connection, currently in state closed
2023-04-20 11:57:32.482413 (Thread-3): 11:57:32  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3e55f136-6a70-4d86-aacd-1d698687ef4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab08c9ceb0>]}
2023-04-20 11:57:32.483021 (Thread-3): 11:57:32  3 of 15 OK created sql table model dbt_shabbirkdb.DimCustomerStg ............... [OK in 5.14s]
2023-04-20 11:57:32.483270 (Thread-3): 11:57:32  Finished running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:57:32.484076 (Thread-3): 11:57:32  Began running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:57:32.484368 (Thread-3): 11:57:32  7 of 15 START sql table model dbt_shabbirkdb.DimAccount ........................ [RUN]
2023-04-20 11:57:32.484849 (Thread-3): 11:57:32  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimAccount'
2023-04-20 11:57:32.485031 (Thread-3): 11:57:32  Began compiling node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:57:32.491193 (Thread-3): 11:57:32  Writing injected SQL for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:57:32.505810 (Thread-3): 11:57:32  Timing info for model.dbsql_dbt_tpch.DimAccount (compile): 2023-04-20 11:57:32.485076 => 2023-04-20 11:57:32.505636
2023-04-20 11:57:32.506052 (Thread-3): 11:57:32  Began executing node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:57:32.509962 (Thread-3): 11:57:32  Writing runtime sql for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:57:32.524470 (Thread-3): 11:57:32  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:57:32.524652 (Thread-3): 11:57:32  Using databricks connection "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:57:32.524880 (Thread-3): 11:57:32  On model.dbsql_dbt_tpch.DimAccount: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 11:57:32.525014 (Thread-3): 11:57:32  Opening a new connection, currently in state closed
2023-04-20 11:57:32.697202 (Thread-578): handling poll request
2023-04-20 11:57:32.697686 (Thread-578): 11:57:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285c7940>]}
2023-04-20 11:57:32.698792 (Thread-578): sending response (<Response 32088 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:33.313142 (Thread-3): 11:57:33  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 11:57:33.313376 (Thread-3): 11:57:33  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:57:33.313502 (Thread-3): 11:57:33  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:57:33.313616 (Thread-3): 11:57:33  Databricks adapter: operation-id: b'\x01\xed\xdfr\x88\xa6\x1b\xa8\xa8\xe4^b\x0b$\xb12'
2023-04-20 11:57:33.313873 (Thread-3): 11:57:33  Timing info for model.dbsql_dbt_tpch.DimAccount (execute): 2023-04-20 11:57:32.506102 => 2023-04-20 11:57:33.313731
2023-04-20 11:57:33.314061 (Thread-3): 11:57:33  On model.dbsql_dbt_tpch.DimAccount: ROLLBACK
2023-04-20 11:57:33.314187 (Thread-3): 11:57:33  Databricks adapter: NotImplemented: rollback
2023-04-20 11:57:33.314309 (Thread-3): 11:57:33  On model.dbsql_dbt_tpch.DimAccount: Close
2023-04-20 11:57:33.606118 (Thread-3): 11:57:33  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:57:33.606679 (Thread-3): 11:57:33  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3e55f136-6a70-4d86-aacd-1d698687ef4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab0a1c1c40>]}
2023-04-20 11:57:33.607151 (Thread-3): 11:57:33  7 of 15 ERROR creating sql table model dbt_shabbirkdb.DimAccount ............... [ERROR in 1.12s]
2023-04-20 11:57:33.607387 (Thread-3): 11:57:33  Finished running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:57:33.607725 (Thread-3): 11:57:33  Began running node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:57:33.608006 (Thread-3): 11:57:33  8 of 15 START sql table model dbt_shabbirkdb.Prospect .......................... [RUN]
2023-04-20 11:57:33.608528 (Thread-3): 11:57:33  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Prospect'
2023-04-20 11:57:33.608709 (Thread-3): 11:57:33  Began compiling node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:57:33.614644 (Thread-3): 11:57:33  Writing injected SQL for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:57:33.629270 (Thread-3): 11:57:33  Timing info for model.dbsql_dbt_tpch.Prospect (compile): 2023-04-20 11:57:33.608754 => 2023-04-20 11:57:33.629105
2023-04-20 11:57:33.629492 (Thread-3): 11:57:33  Began executing node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:57:33.633789 (Thread-3): 11:57:33  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:57:33.633972 (Thread-3): 11:57:33  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:57:33.634128 (Thread-3): 11:57:33  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

      describe extended `dbt_shabbirkdb`.`prospect`
  
2023-04-20 11:57:33.634259 (Thread-3): 11:57:33  Opening a new connection, currently in state closed
2023-04-20 11:57:33.848516 (Thread-579): handling ps request
2023-04-20 11:57:33.849022 (Thread-579): 11:57:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be8a310>]}
2023-04-20 11:57:33.850360 (Thread-579): sending response (<Response 16862 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:34.181359 (Thread-1): 11:57:34  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 11:57:34.181594 (Thread-1): 11:57:34  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [CAST_INVALID_INPUT] The value 'BELJVClFSUDWQNFrxaHxDDEMSOMNqTeWbFtIh' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:57:34.181729 (Thread-1): 11:57:34  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: [CAST_INVALID_INPUT] The value 'BELJVClFSUDWQNFrxaHxDDEMSOMNqTeWbFtIh' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: [CAST_INVALID_INPUT] The value 'BELJVClFSUDWQNFrxaHxDDEMSOMNqTeWbFtIh' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$1(TransactionalWriteEdge.scala:584)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:645)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:407)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:405)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:402)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:450)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:435)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:640)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:559)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:520)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:63)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:145)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:104)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:316)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:1961)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:315)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:348)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:342)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:625)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:615)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:226)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:223)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:367)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:151)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:169)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:645)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:407)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:405)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:402)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:450)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:435)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:640)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:559)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:520)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:63)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:145)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:104)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:120)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:257)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:86)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:119)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:907)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:86)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:866)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:557)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1747)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:538)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:567)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:533)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:222)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:278)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:238)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:237)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:220)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:226)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:226)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:226)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:171)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:249)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:456)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:475)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:708)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:535)
	... 21 more
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'BELJVClFSUDWQNFrxaHxDDEMSOMNqTeWbFtIh' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:190)
	at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:137)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:234)
	... 182 more
	Suppressed: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4183.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4183.0 (TID 32389) (10.5.178.81 executor 6): com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: long,BELJVClFSUDWQNFrxaHxDDEMSOMNqTeWbFtIh
	at 0x89a2b7e <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_3/photon/common/status.cc:137)
	at 0x5ded45d <photon>.OnInvalidInput(external/workspace_spark_3_3/photon/exprs/cast-functions.h:622)
	at 0x5ded0f2 <photon>.ProjectBatch(external/workspace_spark_3_3/photon/exprs/compute-function.h:1117)
	at 0x5deccac <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:69)
	at 0x5df978c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x850c561 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/coalesce-expr.cc:55)
	at 0x4c5466f <photon>.EvalResult(external/workspace_spark_3_3/photon/exec-nodes/window-node.cc:166)
	at 0x4c53d22 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/window-node.cc:118)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4b83940 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/filter-node.cc:73)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/sort-node.cc:98)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonResultStageExec.$anonfun$doExecuteColumnar$2(PhotonExec.scala:848)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonResultStageExec.timeit(PhotonExec.scala:838)
	at com.databricks.photon.PhotonResultStageExec.$anonfun$doExecuteColumnar$1(PhotonExec.scala:848)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1172)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1160)
		at org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2737)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2720)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:611)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:236)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:336)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:226)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:115)
		at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:70)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$12(TransactionalWriteEdge.scala:584)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)
		... 182 more
	Caused by: com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: long,BELJVClFSUDWQNFrxaHxDDEMSOMNqTeWbFtIh
		at 0x89a2b7e <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_3/photon/common/status.cc:137)
		at 0x5ded45d <photon>.OnInvalidInput(external/workspace_spark_3_3/photon/exprs/cast-functions.h:622)
		at 0x5ded0f2 <photon>.ProjectBatch(external/workspace_spark_3_3/photon/exprs/compute-function.h:1117)
		at 0x5deccac <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:69)
		at 0x5df978c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
		at 0x850c561 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/coalesce-expr.cc:55)
		at 0x4c5466f <photon>.EvalResult(external/workspace_spark_3_3/photon/exec-nodes/window-node.cc:166)
		at 0x4c53d22 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/window-node.cc:118)
		at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
		at 0x4b83940 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/filter-node.cc:73)
		at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
		at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
		at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
		at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
		at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
		at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/sort-node.cc:98)
		at com.databricks.photon.JniApiImpl.open(Native Method)
		at com.databricks.photon.JniApi.open(JniApi.scala)
		at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
		at com.databricks.photon.PhotonResultStageExec.$anonfun$doExecuteColumnar$2(PhotonExec.scala:848)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonResultStageExec.timeit(PhotonExec.scala:838)
		at com.databricks.photon.PhotonResultStageExec.$anonfun$doExecuteColumnar$1(PhotonExec.scala:848)
		at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
		at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)
		at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
		at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
		at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
		at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
		at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
		at scala.util.Using$.resource(Using.scala:269)
		at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
		at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.Task.run(Task.scala:97)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
		... 3 more

2023-04-20 11:57:34.181857 (Thread-1): 11:57:34  Databricks adapter: operation-id: b'\x01\xed\xdfr\x88\x82\x10^\x8eK\xe3=iD\xd2k'
2023-04-20 11:57:34.182106 (Thread-1): 11:57:34  Timing info for model.dbsql_dbt_tpch.DimSecurity (execute): 2023-04-20 11:57:32.234214 => 2023-04-20 11:57:34.181973
2023-04-20 11:57:34.182288 (Thread-1): 11:57:34  On model.dbsql_dbt_tpch.DimSecurity: ROLLBACK
2023-04-20 11:57:34.182412 (Thread-1): 11:57:34  Databricks adapter: NotImplemented: rollback
2023-04-20 11:57:34.182533 (Thread-1): 11:57:34  On model.dbsql_dbt_tpch.DimSecurity: Close
2023-04-20 11:57:34.303612 (Thread-580): handling status request
2023-04-20 11:57:34.304058 (Thread-580): 11:57:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be8aa90>]}
2023-04-20 11:57:34.304524 (Thread-580): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:34.311410 (Thread-581): handling poll request
2023-04-20 11:57:34.311750 (Thread-581): 11:57:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be8abe0>]}
2023-04-20 11:57:34.312580 (Thread-581): sending response (<Response 58035 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:34.348060 (Thread-3): 11:57:34  SQL status: OK in 0.71 seconds
2023-04-20 11:57:34.353800 (Thread-3): 11:57:34  Writing runtime sql for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:57:34.369189 (Thread-3): 11:57:34  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:57:34.369499 (Thread-3): 11:57:34  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Prospect`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  agencyid,
  recdate.sk_dateid sk_recorddateid,
  origdate.sk_dateid sk_updatedateid,
  p.batchid,
  nvl2(c.customerid, True, False) iscustomer, 
  p.lastname,
  p.firstname,
  p.middleinitial,
  p.gender,
  p.addressline1,
  p.addressline2,
  p.postalcode,
  city,
  state,
  country,
  phone,
  income,
  numbercars,
  numberchildren,
  maritalstatus,
  age,
  creditrating,
  ownorrentflag,
  employer,
  numbercreditcards,
  networth,
  if(
    isnotnull(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+","")),
    left(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+",""),
      length(
        if(networth > 1000000 or income > 200000,"HighValue+","") || 
        if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
        if(age > 45, "Boomer+", "") ||
        if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
        if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
        if(age < 25 and networth > 1000000, "Inherited+",""))
      -1),
    NULL) marketingnameplate
FROM (
  SELECT 
    * FROM (
    SELECT
      agencyid,
      max(batchid) recordbatchid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth,
      min(batchid) batchid
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`ProspectRaw` p
    GROUP BY
      agencyid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth)
  QUALIFY ROW_NUMBER() OVER (PARTITION BY agencyid ORDER BY batchid DESC) = 1) p
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) recdate
  ON p.recordbatchid = recdate.batchid
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) origdate
  ON p.batchid = origdate.batchid
LEFT JOIN (
  SELECT 
    customerid,
    lastname,
    firstname,
    addressline1,
    addressline2,
    postalcode
  FROM `dbt_shabbirkdb`.`DimCustomerStg`
  WHERE iscurrent) c
  ON 
    upper(p.LastName) = upper(c.lastname)
    and upper(p.FirstName) = upper(c.firstname)
    and upper(p.AddressLine1) = upper(c.addressline1)
    and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
    and upper(p.PostalCode) = upper(c.postalcode)
  
2023-04-20 11:57:34.455986 (Thread-1): 11:57:34  Runtime Error in model DimSecurity (models/silver/DimSecurity.sql)
  [CAST_INVALID_INPUT] The value 'BELJVClFSUDWQNFrxaHxDDEMSOMNqTeWbFtIh' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:57:34.456452 (Thread-1): 11:57:34  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3e55f136-6a70-4d86-aacd-1d698687ef4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab0a1c2130>]}
2023-04-20 11:57:34.456873 (Thread-1): 11:57:34  5 of 15 ERROR creating sql table model dbt_shabbirkdb.DimSecurity .............. [ERROR in 2.25s]
2023-04-20 11:57:34.457085 (Thread-1): 11:57:34  Finished running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:57:34.457364 (Thread-1): 11:57:34  Began running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 11:57:34.457609 (Thread-1): 11:57:34  9 of 15 SKIP relation dbt_shabbirkdb.FactCashBalances .......................... [SKIP]
2023-04-20 11:57:34.457788 (Thread-1): 11:57:34  Finished running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 11:57:34.458646 (Thread-1): 11:57:34  Began running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 11:57:34.458865 (Thread-1): 11:57:34  10 of 15 SKIP relation dbt_shabbirkdb.DimTrade ................................. [SKIP]
2023-04-20 11:57:34.459039 (Thread-1): 11:57:34  Finished running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 11:57:34.459586 (Thread-1): 11:57:34  Began running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 11:57:34.459799 (Thread-1): 11:57:34  11 of 15 SKIP relation dbt_shabbirkdb.FactHoldings ............................. [SKIP]
2023-04-20 11:57:34.459973 (Thread-1): 11:57:34  Finished running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 11:57:34.937750 (Thread-4): 11:57:34  SQL status: OK in 6.6 seconds
2023-04-20 11:57:35.272211 (Thread-4): 11:57:35  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (execute): 2023-04-20 11:57:27.380032 => 2023-04-20 11:57:35.272004
2023-04-20 11:57:35.272497 (Thread-4): 11:57:35  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: ROLLBACK
2023-04-20 11:57:35.272651 (Thread-4): 11:57:35  Databricks adapter: NotImplemented: rollback
2023-04-20 11:57:35.272777 (Thread-4): 11:57:35  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: Close
2023-04-20 11:57:35.551863 (Thread-4): 11:57:35  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3e55f136-6a70-4d86-aacd-1d698687ef4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab08b0d8b0>]}
2023-04-20 11:57:35.552446 (Thread-4): 11:57:35  4 of 15 OK created sql table model dbt_shabbirkdb.tempDailyMarketHistorical .... [OK in 8.20s]
2023-04-20 11:57:35.552691 (Thread-4): 11:57:35  Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:57:36.016083 (Thread-582): handling poll request
2023-04-20 11:57:36.016571 (Thread-582): 11:57:36  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdc2a60>]}
2023-04-20 11:57:36.017325 (Thread-582): sending response (<Response 16208 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:36.168375 (Thread-2): 11:57:36  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 11:57:36.168603 (Thread-2): 11:57:36  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: Task failed while writing rows.
2023-04-20 11:57:36.168744 (Thread-2): 11:57:36  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4179.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4179.0 (TID 32403) (10.5.143.207 executor 2): org.apache.spark.SparkException: Task failed while writing rows.
	at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:109)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'WIFHmdYSMPHKWkoqPrQTYs' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:190)
	at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:137)
	at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:106)
	... 31 more
	Suppressed: com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: long,WIFHmdYSMPHKWkoqPrQTYs
		at 0x89a2b7e <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_3/photon/common/status.cc:137)
		at 0x5ded45d <photon>.OnInvalidInput(external/workspace_spark_3_3/photon/exprs/cast-functions.h:622)
		at 0x5ded0f2 <photon>.ProjectBatch(external/workspace_spark_3_3/photon/exprs/compute-function.h:1117)
		at 0x5deccac <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:69)
		at 0x5df978c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
		at 0x850c561 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/coalesce-expr.cc:55)
		at 0x7dcf64c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/is-null-expr.h:90)
		at 0x85d4a3e <photon>.Eval(external/workspace_spark_3_3/photon/exprs/logical-functions.cc:284)
		at 0x4c56aee <photon>.EvalAllFilters(external/workspace_spark_3_3/photon/io/parquet-reader/data-filter.cc:186)
		at 0x4c86441 <photon>.ScanAndFilterRowsUntilNonEmptyBatch(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:936)
		at 0x4c86da0 <photon>.YieldRowsWithFilter(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:989)
		at 0x4c8782b <photon>.Next(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:1068)
		at 0x4c78105 <photon>.Next(external/workspace_spark_3_3/photon/io/parquet-reader/file-reader.cc:428)
		at 0x4b7bd32 <photon>.operator()(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:728)
		at 0x4b795a0 <photon>.TryPullNextBatch(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:728)
		at 0x4b795a0 <photon>.DoHasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:508)
		at 0x4b7932e <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:336)
		at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
		at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
		at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
		at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
		at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
		at com.databricks.photon.JniApiImpl.hasNext(Native Method)
		at com.databricks.photon.JniApi.hasNext(JniApi.scala)
		at com.databricks.photon.JniExecNode.hasNext(JniExecNode.java:68)
		at com.databricks.photon.PhotonResultStageExec$$anon$2.hasNext(PhotonExec.scala:853)
		at com.databricks.photon.PhotonExec$$anon$1.$anonfun$hasNext$1(PhotonExec.scala:582)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonResultStageExec.timeit(PhotonExec.scala:838)
		at com.databricks.photon.PhotonExec$$anon$1.hasNext(PhotonExec.scala:582)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)
		at org.apache.spark.sql.execution.RowToColumnarExec$$anon$1.hasNext(Columnar.scala:503)
		at com.databricks.photon.CloseableIterator$$anon$5.hasNext(CloseableIterator.scala:62)
		at com.databricks.photon.NativeColumnBatchIterator.hasNext(NativeColumnBatchIterator.java:47)
		at 0x899c692 <photon>.HasNext(external/workspace_spark_3_3/photon/jni-wrappers/jni-native-column-batch-iterator.cc:62)
		at 0x4abefd9 <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/file-writer-node.cc:161)
		at com.databricks.photon.JniApiImpl.open(Native Method)
		at com.databricks.photon.JniApi.open(JniApi.scala)
		at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$5(PhotonWriteStageExec.scala:89)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonWriteStageExec.timeit(PhotonWriteStageExec.scala:38)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$4(PhotonWriteStageExec.scala:89)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1747)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:86)
		... 31 more

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4179.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4179.0 (TID 32403) (10.5.143.207 executor 2): org.apache.spark.SparkException: Task failed while writing rows.
	at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:109)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'WIFHmdYSMPHKWkoqPrQTYs' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:190)
	at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:137)
	at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:106)
	... 31 more
	Suppressed: com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: long,WIFHmdYSMPHKWkoqPrQTYs
		at 0x89a2b7e <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_3/photon/common/status.cc:137)
		at 0x5ded45d <photon>.OnInvalidInput(external/workspace_spark_3_3/photon/exprs/cast-functions.h:622)
		at 0x5ded0f2 <photon>.ProjectBatch(external/workspace_spark_3_3/photon/exprs/compute-function.h:1117)
		at 0x5deccac <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:69)
		at 0x5df978c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
		at 0x850c561 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/coalesce-expr.cc:55)
		at 0x7dcf64c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/is-null-expr.h:90)
		at 0x85d4a3e <photon>.Eval(external/workspace_spark_3_3/photon/exprs/logical-functions.cc:284)
		at 0x4c56aee <photon>.EvalAllFilters(external/workspace_spark_3_3/photon/io/parquet-reader/data-filter.cc:186)
		at 0x4c86441 <photon>.ScanAndFilterRowsUntilNonEmptyBatch(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:936)
		at 0x4c86da0 <photon>.YieldRowsWithFilter(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:989)
		at 0x4c8782b <photon>.Next(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:1068)
		at 0x4c78105 <photon>.Next(external/workspace_spark_3_3/photon/io/parquet-reader/file-reader.cc:428)
		at 0x4b7bd32 <photon>.operator()(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:728)
		at 0x4b795a0 <photon>.TryPullNextBatch(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:728)
		at 0x4b795a0 <photon>.DoHasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:508)
		at 0x4b7932e <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:336)
		at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
		at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
		at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
		at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
		at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
		at com.databricks.photon.JniApiImpl.hasNext(Native Method)
		at com.databricks.photon.JniApi.hasNext(JniApi.scala)
		at com.databricks.photon.JniExecNode.hasNext(JniExecNode.java:68)
		at com.databricks.photon.PhotonResultStageExec$$anon$2.hasNext(PhotonExec.scala:853)
		at com.databricks.photon.PhotonExec$$anon$1.$anonfun$hasNext$1(PhotonExec.scala:582)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonResultStageExec.timeit(PhotonExec.scala:838)
		at com.databricks.photon.PhotonExec$$anon$1.hasNext(PhotonExec.scala:582)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)
		at org.apache.spark.sql.execution.RowToColumnarExec$$anon$1.hasNext(Columnar.scala:503)
		at com.databricks.photon.CloseableIterator$$anon$5.hasNext(CloseableIterator.scala:62)
		at com.databricks.photon.NativeColumnBatchIterator.hasNext(NativeColumnBatchIterator.java:47)
		at 0x899c692 <photon>.HasNext(external/workspace_spark_3_3/photon/jni-wrappers/jni-native-column-batch-iterator.cc:62)
		at 0x4abefd9 <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/file-writer-node.cc:161)
		at com.databricks.photon.JniApiImpl.open(Native Method)
		at com.databricks.photon.JniApi.open(JniApi.scala)
		at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$5(PhotonWriteStageExec.scala:89)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonWriteStageExec.timeit(PhotonWriteStageExec.scala:38)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$4(PhotonWriteStageExec.scala:89)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1747)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:86)
		... 31 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1172)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1160)
	at org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2720)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:611)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:236)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:336)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:226)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:115)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:70)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$12(TransactionalWriteEdge.scala:584)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$1(TransactionalWriteEdge.scala:584)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:645)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:407)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:405)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:402)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:450)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:435)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:640)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:559)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:520)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:63)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:145)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:104)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:316)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:1961)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:315)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:348)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:342)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:625)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:615)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:226)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:223)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:367)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:151)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:169)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:645)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:407)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:405)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:402)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:450)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:435)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:640)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:559)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:520)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:63)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:145)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:104)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:120)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:257)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:86)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:119)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:907)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:86)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:866)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:557)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1747)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:538)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:567)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:533)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:222)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:278)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:238)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:237)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:220)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:226)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:226)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:226)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:171)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:249)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:456)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:475)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:708)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:535)
	... 21 more
Caused by: org.apache.spark.SparkException: Task failed while writing rows.
	at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:109)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	... 3 more
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'WIFHmdYSMPHKWkoqPrQTYs' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:190)
	at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:137)
	at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:106)
	... 31 more
	Suppressed: com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: long,WIFHmdYSMPHKWkoqPrQTYs
		at 0x89a2b7e <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_3/photon/common/status.cc:137)
		at 0x5ded45d <photon>.OnInvalidInput(external/workspace_spark_3_3/photon/exprs/cast-functions.h:622)
		at 0x5ded0f2 <photon>.ProjectBatch(external/workspace_spark_3_3/photon/exprs/compute-function.h:1117)
		at 0x5deccac <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:69)
		at 0x5df978c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
		at 0x850c561 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/coalesce-expr.cc:55)
		at 0x7dcf64c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/is-null-expr.h:90)
		at 0x85d4a3e <photon>.Eval(external/workspace_spark_3_3/photon/exprs/logical-functions.cc:284)
		at 0x4c56aee <photon>.EvalAllFilters(external/workspace_spark_3_3/photon/io/parquet-reader/data-filter.cc:186)
		at 0x4c86441 <photon>.ScanAndFilterRowsUntilNonEmptyBatch(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:936)
		at 0x4c86da0 <photon>.YieldRowsWithFilter(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:989)
		at 0x4c8782b <photon>.Next(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:1068)
		at 0x4c78105 <photon>.Next(external/workspace_spark_3_3/photon/io/parquet-reader/file-reader.cc:428)
		at 0x4b7bd32 <photon>.operator()(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:728)
		at 0x4b795a0 <photon>.TryPullNextBatch(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:728)
		at 0x4b795a0 <photon>.DoHasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:508)
		at 0x4b7932e <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:336)
		at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
		at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
		at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
		at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
		at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
		at com.databricks.photon.JniApiImpl.hasNext(Native Method)
		at com.databricks.photon.JniApi.hasNext(JniApi.scala)
		at com.databricks.photon.JniExecNode.hasNext(JniExecNode.java:68)
		at com.databricks.photon.PhotonResultStageExec$$anon$2.hasNext(PhotonExec.scala:853)
		at com.databricks.photon.PhotonExec$$anon$1.$anonfun$hasNext$1(PhotonExec.scala:582)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonResultStageExec.timeit(PhotonExec.scala:838)
		at com.databricks.photon.PhotonExec$$anon$1.hasNext(PhotonExec.scala:582)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)
		at org.apache.spark.sql.execution.RowToColumnarExec$$anon$1.hasNext(Columnar.scala:503)
		at com.databricks.photon.CloseableIterator$$anon$5.hasNext(CloseableIterator.scala:62)
		at com.databricks.photon.NativeColumnBatchIterator.hasNext(NativeColumnBatchIterator.java:47)
		at 0x899c692 <photon>.HasNext(external/workspace_spark_3_3/photon/jni-wrappers/jni-native-column-batch-iterator.cc:62)
		at 0x4abefd9 <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/file-writer-node.cc:161)
		at com.databricks.photon.JniApiImpl.open(Native Method)
		at com.databricks.photon.JniApi.open(JniApi.scala)
		at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$5(PhotonWriteStageExec.scala:89)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonWriteStageExec.timeit(PhotonWriteStageExec.scala:38)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$4(PhotonWriteStageExec.scala:89)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1747)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:86)
		... 31 more

2023-04-20 11:57:36.168867 (Thread-2): 11:57:36  Databricks adapter: operation-id: b'\x01\xed\xdfr\x88\x80\x15\x83\x84\x02\xcf\xec\xb7\xfa\x0be'
2023-04-20 11:57:36.169128 (Thread-2): 11:57:36  Timing info for model.dbsql_dbt_tpch.Financial (execute): 2023-04-20 11:57:32.238973 => 2023-04-20 11:57:36.168978
2023-04-20 11:57:36.169343 (Thread-2): 11:57:36  On model.dbsql_dbt_tpch.Financial: ROLLBACK
2023-04-20 11:57:36.169478 (Thread-2): 11:57:36  Databricks adapter: NotImplemented: rollback
2023-04-20 11:57:36.169601 (Thread-2): 11:57:36  On model.dbsql_dbt_tpch.Financial: Close
2023-04-20 11:57:36.460060 (Thread-2): 11:57:36  Runtime Error in model Financial (models/silver/Financial.sql)
  Job aborted due to stage failure: Task failed while writing rows.
2023-04-20 11:57:36.460488 (Thread-2): 11:57:36  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3e55f136-6a70-4d86-aacd-1d698687ef4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab08fa12b0>]}
2023-04-20 11:57:36.461055 (Thread-2): 11:57:36  6 of 15 ERROR creating sql table model dbt_shabbirkdb.Financial ................ [ERROR in 4.25s]
2023-04-20 11:57:36.461297 (Thread-2): 11:57:36  Finished running node model.dbsql_dbt_tpch.Financial
2023-04-20 11:57:36.462186 (Thread-1): 11:57:36  Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 11:57:36.462476 (Thread-1): 11:57:36  12 of 15 SKIP relation dbt_shabbirkdb.tempSumpFiBasicEps ....................... [SKIP]
2023-04-20 11:57:36.462668 (Thread-1): 11:57:36  Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 11:57:36.463246 (Thread-2): 11:57:36  Began running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 11:57:36.463472 (Thread-2): 11:57:36  13 of 15 SKIP relation dbt_shabbirkdb.FactMarketHistory ........................ [SKIP]
2023-04-20 11:57:36.463686 (Thread-2): 11:57:36  Finished running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 11:57:36.516116 (Thread-583): handling ps request
2023-04-20 11:57:36.516757 (Thread-583): 11:57:36  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdc2820>]}
2023-04-20 11:57:36.518084 (Thread-583): sending response (<Response 16862 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:36.949600 (Thread-584): handling status request
2023-04-20 11:57:36.950099 (Thread-584): 11:57:36  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be8a910>]}
2023-04-20 11:57:36.950587 (Thread-584): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:37.673464 (Thread-585): handling poll request
2023-04-20 11:57:37.673959 (Thread-585): 11:57:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be8a8b0>]}
2023-04-20 11:57:37.674842 (Thread-585): sending response (<Response 58818 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:38.430129 (Thread-3): 11:57:38  SQL status: OK in 4.06 seconds
2023-04-20 11:57:38.432586 (Thread-3): 11:57:38  Timing info for model.dbsql_dbt_tpch.Prospect (execute): 2023-04-20 11:57:33.629540 => 2023-04-20 11:57:38.432430
2023-04-20 11:57:38.432796 (Thread-3): 11:57:38  On model.dbsql_dbt_tpch.Prospect: ROLLBACK
2023-04-20 11:57:38.432939 (Thread-3): 11:57:38  Databricks adapter: NotImplemented: rollback
2023-04-20 11:57:38.433060 (Thread-3): 11:57:38  On model.dbsql_dbt_tpch.Prospect: Close
2023-04-20 11:57:38.720946 (Thread-3): 11:57:38  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3e55f136-6a70-4d86-aacd-1d698687ef4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab08fc83d0>]}
2023-04-20 11:57:38.721476 (Thread-3): 11:57:38  8 of 15 OK created sql table model dbt_shabbirkdb.Prospect ..................... [OK in 5.11s]
2023-04-20 11:57:38.721719 (Thread-3): 11:57:38  Finished running node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:57:38.722882 (Thread-4): 11:57:38  Began running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:57:38.723226 (Thread-4): 11:57:38  14 of 15 START sql table model dbt_shabbirkdb.DimCustomer ...................... [RUN]
2023-04-20 11:57:38.723807 (Thread-4): 11:57:38  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomer'
2023-04-20 11:57:38.724001 (Thread-4): 11:57:38  Began compiling node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:57:38.730827 (Thread-4): 11:57:38  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:57:38.744331 (Thread-4): 11:57:38  Timing info for model.dbsql_dbt_tpch.DimCustomer (compile): 2023-04-20 11:57:38.724048 => 2023-04-20 11:57:38.744137
2023-04-20 11:57:38.744578 (Thread-4): 11:57:38  Began executing node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:57:38.748573 (Thread-4): 11:57:38  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:57:38.763082 (Thread-4): 11:57:38  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:57:38.763278 (Thread-4): 11:57:38  Using databricks connection "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:57:38.763483 (Thread-4): 11:57:38  On model.dbsql_dbt_tpch.DimCustomer: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 11:57:38.763644 (Thread-4): 11:57:38  Opening a new connection, currently in state closed
2023-04-20 11:57:39.153454 (Thread-586): handling ps request
2023-04-20 11:57:39.153966 (Thread-586): 11:57:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdc7730>]}
2023-04-20 11:57:39.155316 (Thread-586): sending response (<Response 16862 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:39.225103 (Thread-587): handling poll request
2023-04-20 11:57:39.225558 (Thread-587): 11:57:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdc79a0>]}
2023-04-20 11:57:39.226260 (Thread-587): sending response (<Response 9798 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:39.499299 (Thread-4): 11:57:39  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 11:57:39.499611 (Thread-4): 11:57:39  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:57:39.499756 (Thread-4): 11:57:39  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:57:39.499875 (Thread-4): 11:57:39  Databricks adapter: operation-id: b'\x01\xed\xdfr\x8ca\x175\xa9\xa4%\x0b\xe7\x1f\x13e'
2023-04-20 11:57:39.500106 (Thread-4): 11:57:39  Timing info for model.dbsql_dbt_tpch.DimCustomer (execute): 2023-04-20 11:57:38.744629 => 2023-04-20 11:57:39.499979
2023-04-20 11:57:39.500289 (Thread-4): 11:57:39  On model.dbsql_dbt_tpch.DimCustomer: ROLLBACK
2023-04-20 11:57:39.500436 (Thread-4): 11:57:39  Databricks adapter: NotImplemented: rollback
2023-04-20 11:57:39.500558 (Thread-4): 11:57:39  On model.dbsql_dbt_tpch.DimCustomer: Close
2023-04-20 11:57:39.578853 (Thread-588): handling status request
2023-04-20 11:57:39.579325 (Thread-588): 11:57:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdc7f10>]}
2023-04-20 11:57:39.602331 (Thread-588): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:39.781934 (Thread-4): 11:57:39  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:57:39.782335 (Thread-4): 11:57:39  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3e55f136-6a70-4d86-aacd-1d698687ef4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab08fc9100>]}
2023-04-20 11:57:39.782995 (Thread-4): 11:57:39  14 of 15 ERROR creating sql table model dbt_shabbirkdb.DimCustomer ............. [ERROR in 1.06s]
2023-04-20 11:57:39.783245 (Thread-4): 11:57:39  Finished running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:57:39.784244 (Thread-1): 11:57:39  Began running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 11:57:39.784493 (Thread-1): 11:57:39  15 of 15 SKIP relation dbt_shabbirkdb.FactWatches .............................. [SKIP]
2023-04-20 11:57:39.784680 (Thread-1): 11:57:39  Finished running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 11:57:39.786380 (MainThread): 11:57:39  Acquiring new databricks connection 'master'
2023-04-20 11:57:39.786603 (MainThread): 11:57:39  On master: ROLLBACK
2023-04-20 11:57:39.786758 (MainThread): 11:57:39  Opening a new connection, currently in state init
2023-04-20 11:57:40.067107 (MainThread): 11:57:40  Databricks adapter: NotImplemented: rollback
2023-04-20 11:57:40.067357 (MainThread): 11:57:40  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:57:40.067494 (MainThread): 11:57:40  Spark adapter: NotImplemented: commit
2023-04-20 11:57:40.067686 (MainThread): 11:57:40  On master: ROLLBACK
2023-04-20 11:57:40.067822 (MainThread): 11:57:40  Databricks adapter: NotImplemented: rollback
2023-04-20 11:57:40.067958 (MainThread): 11:57:40  On master: Close
2023-04-20 11:57:40.356079 (MainThread): 11:57:40  Connection 'master' was properly closed.
2023-04-20 11:57:40.356284 (MainThread): 11:57:40  Connection 'model.dbsql_dbt_tpch.DimSecurity' was properly closed.
2023-04-20 11:57:40.356394 (MainThread): 11:57:40  Connection 'model.dbsql_dbt_tpch.Financial' was properly closed.
2023-04-20 11:57:40.356497 (MainThread): 11:57:40  Connection 'model.dbsql_dbt_tpch.Prospect' was properly closed.
2023-04-20 11:57:40.356599 (MainThread): 11:57:40  Connection 'model.dbsql_dbt_tpch.DimCustomer' was properly closed.
2023-04-20 11:57:40.356902 (MainThread): 11:57:40  
2023-04-20 11:57:40.357050 (MainThread): 11:57:40  Finished running 15 table models in 0 hours 0 minutes and 15.90 seconds (15.90s).
2023-04-20 11:57:40.459479 (MainThread): 11:57:40  
2023-04-20 11:57:40.459786 (MainThread): 11:57:40  Completed with 4 errors and 0 warnings:
2023-04-20 11:57:40.459924 (MainThread): 11:57:40  
2023-04-20 11:57:40.460066 (MainThread): 11:57:40  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
2023-04-20 11:57:40.460193 (MainThread): 11:57:40    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:57:40.460303 (MainThread): 11:57:40  
2023-04-20 11:57:40.460421 (MainThread): 11:57:40  Runtime Error in model DimSecurity (models/silver/DimSecurity.sql)
2023-04-20 11:57:40.460531 (MainThread): 11:57:40    [CAST_INVALID_INPUT] The value 'BELJVClFSUDWQNFrxaHxDDEMSOMNqTeWbFtIh' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:57:40.460636 (MainThread): 11:57:40  
2023-04-20 11:57:40.460750 (MainThread): 11:57:40  Runtime Error in model Financial (models/silver/Financial.sql)
2023-04-20 11:57:40.460877 (MainThread): 11:57:40    Job aborted due to stage failure: Task failed while writing rows.
2023-04-20 11:57:40.460987 (MainThread): 11:57:40  
2023-04-20 11:57:40.461103 (MainThread): 11:57:40  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
2023-04-20 11:57:40.461211 (MainThread): 11:57:40    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:57:40.461334 (MainThread): 11:57:40  
2023-04-20 11:57:40.461461 (MainThread): 11:57:40  Done. PASS=5 WARN=0 ERROR=4 SKIP=6 TOTAL=15
2023-04-20 11:57:41.024735 (Thread-589): handling poll request
2023-04-20 11:57:41.025221 (Thread-589): 11:57:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528564550>]}
2023-04-20 11:57:41.027775 (Thread-589): sending response (<Response 107094 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:41.890051 (Thread-590): handling ps request
2023-04-20 11:57:41.890631 (Thread-590): 11:57:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285398e0>]}
2023-04-20 11:57:41.892059 (Thread-590): sending response (<Response 16886 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:42.355481 (Thread-591): handling status request
2023-04-20 11:57:42.355992 (Thread-591): 11:57:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52853bc10>]}
2023-04-20 11:57:42.356472 (Thread-591): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:46.912038 (Thread-592): 11:57:46  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:57:46.934295 (Thread-592): 11:57:46  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 11:57:46.934690 (Thread-592): 11:57:46  previous checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, current checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886
2023-04-20 11:57:46.934899 (Thread-592): 11:57:46  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb77730>]}
2023-04-20 11:57:47.519723 (Thread-593): handling status request
2023-04-20 11:57:47.535512 (Thread-593): 11:57:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52832a1c0>]}
2023-04-20 11:57:47.541107 (Thread-593): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:48.481738 (Thread-592): 11:57:48  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 11:57:48.485179 (Thread-592): 11:57:48  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 11:57:48.488943 (Thread-592): 11:57:48  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 11:57:48.492440 (Thread-592): 11:57:48  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 11:57:48.496085 (Thread-592): 11:57:48  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 11:57:48.499398 (Thread-592): 11:57:48  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 11:57:48.504373 (Thread-592): 11:57:48  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 11:57:48.509160 (Thread-592): 11:57:48  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 11:57:48.513110 (Thread-592): 11:57:48  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 11:57:48.516504 (Thread-592): 11:57:48  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 11:57:48.520839 (Thread-592): 11:57:48  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 11:57:48.524325 (Thread-592): 11:57:48  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 11:57:48.527619 (Thread-592): 11:57:48  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 11:57:48.531068 (Thread-592): 11:57:48  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 11:57:48.534481 (Thread-592): 11:57:48  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:57:48.865153 (Thread-594): handling status request
2023-04-20 11:57:48.865635 (Thread-594): 11:57:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d237850>]}
2023-04-20 11:57:48.866070 (Thread-594): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:48.946813 (Thread-592): 11:57:48  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5282a2310>]}
2023-04-20 11:57:49.815799 (Thread-595): handling status request
2023-04-20 11:57:49.816348 (Thread-595): 11:57:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52836f850>]}
2023-04-20 11:57:49.816963 (Thread-595): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:53.338017 (Thread-596): handling status request
2023-04-20 11:57:53.338511 (Thread-596): 11:57:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528286c70>]}
2023-04-20 11:57:53.339116 (Thread-596): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:53.894597 (Thread-597): handling cli_args request
2023-04-20 11:57:53.895087 (Thread-597): 11:57:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528286430>]}
2023-04-20 11:57:56.680644 (Thread-597): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:56.739274 (MainThread): 11:57:56  checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
2023-04-20 11:57:56.761929 (MainThread): 11:57:56  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 11:57:56.762106 (MainThread): 11:57:56  previous checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, current checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd
2023-04-20 11:57:56.762260 (MainThread): 11:57:56  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e3074c5c-c059-4ae3-8ef8-d9d708ab92e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdef5be0ac0>]}
2023-04-20 11:57:57.277223 (Thread-598): handling ps request
2023-04-20 11:57:57.277958 (Thread-598): 11:57:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52836fb80>]}
2023-04-20 11:57:57.280300 (Thread-598): sending response (<Response 17268 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:57.458079 (Thread-599): handling poll request
2023-04-20 11:57:57.458612 (Thread-599): 11:57:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52825b2b0>]}
2023-04-20 11:57:57.459184 (Thread-599): sending response (<Response 1862 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:57.502804 (Thread-600): handling status request
2023-04-20 11:57:57.503242 (Thread-600): 11:57:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52826a670>]}
2023-04-20 11:57:57.503977 (Thread-600): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:57.802326 (Thread-601): handling ps request
2023-04-20 11:57:57.802878 (Thread-601): 11:57:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52836fb80>]}
2023-04-20 11:57:57.804286 (Thread-601): sending response (<Response 17268 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:58.373584 (MainThread): 11:57:58  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 11:57:58.386115 (MainThread): 11:57:58  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 11:57:58.389316 (MainThread): 11:57:58  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 11:57:58.392373 (MainThread): 11:57:58  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 11:57:58.395313 (MainThread): 11:57:58  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 11:57:58.398069 (MainThread): 11:57:58  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 11:57:58.401202 (MainThread): 11:57:58  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 11:57:58.404335 (MainThread): 11:57:58  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 11:57:58.407361 (MainThread): 11:57:58  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 11:57:58.410321 (MainThread): 11:57:58  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 11:57:58.414571 (MainThread): 11:57:58  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 11:57:58.417625 (MainThread): 11:57:58  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 11:57:58.420536 (MainThread): 11:57:58  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 11:57:58.423540 (MainThread): 11:57:58  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 11:57:58.426839 (MainThread): 11:57:58  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:57:58.596907 (MainThread): 11:57:58  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e3074c5c-c059-4ae3-8ef8-d9d708ab92e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fded4fdfca0>]}
2023-04-20 11:57:58.629308 (MainThread): 11:57:58  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e3074c5c-c059-4ae3-8ef8-d9d708ab92e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdef5be7580>]}
2023-04-20 11:57:58.629635 (MainThread): 11:57:58  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:57:58.629781 (MainThread): 11:57:58  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e3074c5c-c059-4ae3-8ef8-d9d708ab92e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdef5da6a00>]}
2023-04-20 11:57:58.631644 (MainThread): 11:57:58  
2023-04-20 11:57:58.633058 (MainThread): 11:57:58  Acquiring new databricks connection 'master'
2023-04-20 11:57:58.634750 (ThreadPoolExecutor-0_0): 11:57:58  Acquiring new databricks connection 'list_schemas'
2023-04-20 11:57:58.645507 (ThreadPoolExecutor-0_0): 11:57:58  Using databricks connection "list_schemas"
2023-04-20 11:57:58.645830 (ThreadPoolExecutor-0_0): 11:57:58  On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_schemas"} */

    show databases
  
2023-04-20 11:57:58.646003 (ThreadPoolExecutor-0_0): 11:57:58  Opening a new connection, currently in state init
2023-04-20 11:57:59.660594 (ThreadPoolExecutor-0_0): 11:57:59  SQL status: OK in 1.01 seconds
2023-04-20 11:57:59.778182 (Thread-602): handling poll request
2023-04-20 11:57:59.778690 (Thread-602): 11:57:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54baa5e50>]}
2023-04-20 11:57:59.779420 (Thread-602): sending response (<Response 8976 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:57:59.788985 (ThreadPoolExecutor-0_0): 11:57:59  On list_schemas: Close
2023-04-20 11:58:00.083060 (ThreadPoolExecutor-1_0): 11:58:00  Acquiring new databricks connection 'list_None_dbt_shabbirkdb'
2023-04-20 11:58:00.093630 (ThreadPoolExecutor-1_0): 11:58:00  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:58:00.093835 (ThreadPoolExecutor-1_0): 11:58:00  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 11:58:00.094014 (ThreadPoolExecutor-1_0): 11:58:00  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show tables in `dbt_shabbirkdb`
  
2023-04-20 11:58:00.094169 (ThreadPoolExecutor-1_0): 11:58:00  Opening a new connection, currently in state closed
2023-04-20 11:58:00.714304 (Thread-603): handling ps request
2023-04-20 11:58:00.714849 (Thread-603): 11:58:00  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bcd0070>]}
2023-04-20 11:58:00.716284 (Thread-603): sending response (<Response 17268 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:00.809401 (ThreadPoolExecutor-1_0): 11:58:00  SQL status: OK in 0.72 seconds
2023-04-20 11:58:00.818916 (ThreadPoolExecutor-1_0): 11:58:00  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 11:58:00.819140 (ThreadPoolExecutor-1_0): 11:58:00  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show views in `dbt_shabbirkdb`
  
2023-04-20 11:58:01.229781 (ThreadPoolExecutor-1_0): 11:58:01  SQL status: OK in 0.41 seconds
2023-04-20 11:58:01.233496 (ThreadPoolExecutor-1_0): 11:58:01  On list_None_dbt_shabbirkdb: ROLLBACK
2023-04-20 11:58:01.233708 (ThreadPoolExecutor-1_0): 11:58:01  Databricks adapter: NotImplemented: rollback
2023-04-20 11:58:01.233864 (ThreadPoolExecutor-1_0): 11:58:01  On list_None_dbt_shabbirkdb: Close
2023-04-20 11:58:01.521905 (MainThread): 11:58:01  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e3074c5c-c059-4ae3-8ef8-d9d708ab92e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdef51f7070>]}
2023-04-20 11:58:01.522301 (MainThread): 11:58:01  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:58:01.522456 (MainThread): 11:58:01  Spark adapter: NotImplemented: commit
2023-04-20 11:58:01.522960 (MainThread): 11:58:01  Concurrency: 4 threads (target='default')
2023-04-20 11:58:01.523109 (MainThread): 11:58:01  
2023-04-20 11:58:01.525856 (Thread-1): 11:58:01  Began running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:58:01.526241 (Thread-1): 11:58:01  1 of 15 START sql table model dbt_shabbirkdb.DimBroker ......................... [RUN]
2023-04-20 11:58:01.526771 (Thread-1): 11:58:01  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
2023-04-20 11:58:01.526968 (Thread-1): 11:58:01  Began compiling node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:58:01.530913 (Thread-2): 11:58:01  Began running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:58:01.531221 (Thread-2): 11:58:01  2 of 15 START sql table model dbt_shabbirkdb.DimCompany ........................ [RUN]
2023-04-20 11:58:01.531912 (Thread-2): 11:58:01  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCompany'
2023-04-20 11:58:01.532101 (Thread-2): 11:58:01  Began compiling node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:58:01.537058 (Thread-1): 11:58:01  Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:58:01.537433 (Thread-3): 11:58:01  Began running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:58:01.537743 (Thread-3): 11:58:01  3 of 15 START sql table model dbt_shabbirkdb.DimCustomerStg .................... [RUN]
2023-04-20 11:58:01.538259 (Thread-3): 11:58:01  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomerStg'
2023-04-20 11:58:01.538439 (Thread-3): 11:58:01  Began compiling node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:58:01.542687 (Thread-2): 11:58:01  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:58:01.543093 (Thread-4): 11:58:01  Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:58:01.543399 (Thread-4): 11:58:01  4 of 15 START sql table model dbt_shabbirkdb.tempDailyMarketHistorical ......... [RUN]
2023-04-20 11:58:01.543922 (Thread-4): 11:58:01  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempDailyMarketHistorical'
2023-04-20 11:58:01.544100 (Thread-4): 11:58:01  Began compiling node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:58:01.547700 (Thread-4): 11:58:01  Writing injected SQL for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:58:01.548353 (Thread-3): 11:58:01  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:58:01.565333 (Thread-2): 11:58:01  Timing info for model.dbsql_dbt_tpch.DimCompany (compile): 2023-04-20 11:58:01.532147 => 2023-04-20 11:58:01.565169
2023-04-20 11:58:01.565560 (Thread-2): 11:58:01  Began executing node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:58:01.570509 (Thread-1): 11:58:01  Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 2023-04-20 11:58:01.527263 => 2023-04-20 11:58:01.570341
2023-04-20 11:58:01.570723 (Thread-1): 11:58:01  Began executing node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:58:01.586224 (Thread-3): 11:58:01  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (compile): 2023-04-20 11:58:01.538482 => 2023-04-20 11:58:01.586057
2023-04-20 11:58:01.586434 (Thread-3): 11:58:01  Began executing node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:58:01.591337 (Thread-4): 11:58:01  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (compile): 2023-04-20 11:58:01.544143 => 2023-04-20 11:58:01.591182
2023-04-20 11:58:01.591566 (Thread-4): 11:58:01  Began executing node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:58:01.598604 (Thread-3): 11:58:01  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:58:01.598775 (Thread-3): 11:58:01  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:58:01.598933 (Thread-3): 11:58:01  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

      describe extended `dbt_shabbirkdb`.`dimcustomerstg`
  
2023-04-20 11:58:01.599063 (Thread-3): 11:58:01  Opening a new connection, currently in state init
2023-04-20 11:58:01.601454 (Thread-1): 11:58:01  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:58:01.601630 (Thread-1): 11:58:01  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:58:01.601783 (Thread-1): 11:58:01  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

      describe extended `dbt_shabbirkdb`.`dimbroker`
  
2023-04-20 11:58:01.601917 (Thread-1): 11:58:01  Opening a new connection, currently in state closed
2023-04-20 11:58:01.604868 (Thread-2): 11:58:01  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:58:01.605039 (Thread-2): 11:58:01  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:58:01.605189 (Thread-2): 11:58:01  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

      describe extended `dbt_shabbirkdb`.`dimcompany`
  
2023-04-20 11:58:01.605311 (Thread-2): 11:58:01  Opening a new connection, currently in state init
2023-04-20 11:58:01.605979 (Thread-4): 11:58:01  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:58:01.606149 (Thread-4): 11:58:01  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:58:01.606298 (Thread-4): 11:58:01  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

      describe extended `dbt_shabbirkdb`.`tempdailymarkethistorical`
  
2023-04-20 11:58:01.606416 (Thread-4): 11:58:01  Opening a new connection, currently in state init
2023-04-20 11:58:02.275431 (Thread-604): handling status request
2023-04-20 11:58:02.276272 (Thread-604): 11:58:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bc461f0>]}
2023-04-20 11:58:02.276914 (Thread-604): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:02.370547 (Thread-4): 11:58:02  SQL status: OK in 0.76 seconds
2023-04-20 11:58:02.420082 (Thread-4): 11:58:02  Writing runtime sql for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:58:02.421900 (Thread-3): 11:58:02  SQL status: OK in 0.82 seconds
2023-04-20 11:58:02.423198 (Thread-2): 11:58:02  SQL status: OK in 0.82 seconds
2023-04-20 11:58:02.423884 (Thread-1): 11:58:02  SQL status: OK in 0.82 seconds
2023-04-20 11:58:02.431253 (Thread-3): 11:58:02  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:58:02.435143 (Thread-2): 11:58:02  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:58:02.438782 (Thread-1): 11:58:02  Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:58:02.442939 (Thread-4): 11:58:02  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:58:02.443159 (Thread-4): 11:58:02  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempDailyMarketHistorical`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  dmh.*,
  sk_dateid,
  min(dm_low) OVER (
    PARTITION BY dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweeklow,
  max(dm_high) OVER (
    PARTITION by dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweekhigh
FROM (
  SELECT * FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketHistorical`
  UNION ALL
  SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketIncremental`) dmh
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
  ON d.datevalue = dm_date;
  
2023-04-20 11:58:02.450545 (Thread-2): 11:58:02  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:58:02.450799 (Thread-2): 11:58:02  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCompany`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  * FROM (
  SELECT
    md5(cik) sk_companyid,
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
  
2023-04-20 11:58:02.453593 (Thread-1): 11:58:02  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:58:02.453795 (Thread-1): 11:58:02  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT
  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate`) effectivedate,
  date('9999-12-31') enddate
FROM  `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`HR`
WHERE employeejobcode = 314
  
2023-04-20 11:58:02.464244 (Thread-3): 11:58:02  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:58:02.464586 (Thread-3): 11:58:02  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomerStg`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT * FROM (
  SELECT
    customerid,
    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) taxid,
    status,
    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) lastname,
    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) firstname,
    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) middleinitial,
    coalesce(gender, last_value(gender) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) gender,
    coalesce(tier, last_value(tier) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) tier,
    coalesce(dob, last_value(dob) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) dob,
    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline1,
    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline2,
    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) postalcode,
    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) CITY,
    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) stateprov,
    coalesce(country, last_value(country) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) country,
    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone1,
    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone2,
    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone3,
    coalesce(email1, last_value(email1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email1,
    coalesce(email2, last_value(email2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email2,
    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) LCL_TX_ID,
    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) NAT_TX_ID,
    batchid,
    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,
    date(update_ts) effectivedate,
    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate
  FROM (
    SELECT
      customerid,
      taxid,
      status,
      lastname,
      firstname,
      middleinitial,
      gender,
      tier,
      dob,
      addressline1,
      addressline2,
      postalcode,
      city,
      stateprov,
      country,
      phone1,
      phone2,
      phone3,
      email1,
      email2,
      lcl_tx_id,
      nat_tx_id,
      1 batchid,
      update_ts
    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')
    UNION ALL
    SELECT
      c.customerid,
      nullif(c.taxid, '') taxid,
      nullif(s.st_name, '') as status,
      nullif(c.lastname, '') lastname,
      nullif(c.firstname, '') firstname,
      nullif(c.middleinitial, '') middleinitial,
      gender,
      c.tier,
      c.dob,
      nullif(c.addressline1, '') addressline1,
      nullif(c.addressline2, '') addressline2,
      nullif(c.postalcode, '') postalcode,
      nullif(c.city, '') city,
      nullif(c.stateprov, '') stateprov,
      nullif(c.country, '') country,
      CASE
        WHEN isnull(c_local_1) then c_local_1
        ELSE concat(
          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),
          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),
          c_local_1,
          nvl(c_ext_1, '')) END as phone1,
      CASE
        WHEN isnull(c_local_2) then c_local_2
        ELSE concat(
          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),
          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),
          c_local_2,
          nvl(c_ext_2, '')) END as phone2,
      CASE
        WHEN isnull(c_local_3) then c_local_3
        ELSE concat(
          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),
          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),
          c_local_3,
          nvl(c_ext_3, '')) END as phone3,
      nullif(c.email1, '') email1,
      nullif(c.email2, '') email2,
      c.LCL_TX_ID, 
      c.NAT_TX_ID,
      c.batchid,
      timestamp(bd.batchdate) update_ts
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CustomerIncremental` c
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
      ON c.batchid = bd.batchid
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` s 
      ON c.status = s.st_id
  ) c
  )
  
2023-04-20 11:58:02.642022 (Thread-605): handling poll request
2023-04-20 11:58:02.642504 (Thread-605): 11:58:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bcea1f0>]}
2023-04-20 11:58:02.644020 (Thread-605): sending response (<Response 41614 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:03.967938 (Thread-606): handling ps request
2023-04-20 11:58:03.968433 (Thread-606): 11:58:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54ba9e970>]}
2023-04-20 11:58:03.969799 (Thread-606): sending response (<Response 17269 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:04.252247 (Thread-607): handling poll request
2023-04-20 11:58:04.252738 (Thread-607): 11:58:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54ba9ed60>]}
2023-04-20 11:58:04.253222 (Thread-607): sending response (<Response 291 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:04.409490 (Thread-608): handling status request
2023-04-20 11:58:04.409981 (Thread-608): 11:58:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bc46a60>]}
2023-04-20 11:58:04.410580 (Thread-608): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:05.352574 (Thread-1): 11:58:05  SQL status: OK in 2.9 seconds
2023-04-20 11:58:05.380016 (Thread-1): 11:58:05  Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 2023-04-20 11:58:01.570772 => 2023-04-20 11:58:05.379840
2023-04-20 11:58:05.380266 (Thread-1): 11:58:05  On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
2023-04-20 11:58:05.380409 (Thread-1): 11:58:05  Databricks adapter: NotImplemented: rollback
2023-04-20 11:58:05.380533 (Thread-1): 11:58:05  On model.dbsql_dbt_tpch.DimBroker: Close
2023-04-20 11:58:05.661120 (Thread-1): 11:58:05  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e3074c5c-c059-4ae3-8ef8-d9d708ab92e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fded4355cd0>]}
2023-04-20 11:58:05.661853 (Thread-1): 11:58:05  1 of 15 OK created sql table model dbt_shabbirkdb.DimBroker .................... [OK in 4.13s]
2023-04-20 11:58:05.663349 (Thread-1): 11:58:05  Finished running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:58:05.685294 (Thread-2): 11:58:05  SQL status: OK in 3.23 seconds
2023-04-20 11:58:05.687571 (Thread-2): 11:58:05  Timing info for model.dbsql_dbt_tpch.DimCompany (execute): 2023-04-20 11:58:01.565609 => 2023-04-20 11:58:05.687405
2023-04-20 11:58:05.687774 (Thread-2): 11:58:05  On model.dbsql_dbt_tpch.DimCompany: ROLLBACK
2023-04-20 11:58:05.687916 (Thread-2): 11:58:05  Databricks adapter: NotImplemented: rollback
2023-04-20 11:58:05.688038 (Thread-2): 11:58:05  On model.dbsql_dbt_tpch.DimCompany: Close
2023-04-20 11:58:05.842383 (Thread-609): handling poll request
2023-04-20 11:58:05.842871 (Thread-609): 11:58:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bc46610>]}
2023-04-20 11:58:05.843497 (Thread-609): sending response (<Response 5264 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:05.971549 (Thread-2): 11:58:05  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e3074c5c-c059-4ae3-8ef8-d9d708ab92e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdef5315130>]}
2023-04-20 11:58:05.972077 (Thread-2): 11:58:05  2 of 15 OK created sql table model dbt_shabbirkdb.DimCompany ................... [OK in 4.44s]
2023-04-20 11:58:05.972309 (Thread-2): 11:58:05  Finished running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:58:05.973276 (Thread-1): 11:58:05  Began running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:58:05.973749 (Thread-1): 11:58:05  5 of 15 START sql table model dbt_shabbirkdb.DimSecurity ....................... [RUN]
2023-04-20 11:58:05.974322 (Thread-1): 11:58:05  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimSecurity'
2023-04-20 11:58:05.974512 (Thread-1): 11:58:05  Began compiling node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:58:05.978342 (Thread-2): 11:58:05  Began running node model.dbsql_dbt_tpch.Financial
2023-04-20 11:58:05.978623 (Thread-2): 11:58:05  6 of 15 START sql table model dbt_shabbirkdb.Financial ......................... [RUN]
2023-04-20 11:58:05.979082 (Thread-2): 11:58:05  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Financial'
2023-04-20 11:58:05.979259 (Thread-2): 11:58:05  Began compiling node model.dbsql_dbt_tpch.Financial
2023-04-20 11:58:05.984754 (Thread-1): 11:58:05  Writing injected SQL for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:58:05.984994 (Thread-2): 11:58:05  Writing injected SQL for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:58:06.000837 (Thread-2): 11:58:06  Timing info for model.dbsql_dbt_tpch.Financial (compile): 2023-04-20 11:58:05.979304 => 2023-04-20 11:58:06.000657
2023-04-20 11:58:06.001095 (Thread-2): 11:58:06  Began executing node model.dbsql_dbt_tpch.Financial
2023-04-20 11:58:06.005934 (Thread-1): 11:58:06  Timing info for model.dbsql_dbt_tpch.DimSecurity (compile): 2023-04-20 11:58:05.974559 => 2023-04-20 11:58:06.005750
2023-04-20 11:58:06.006157 (Thread-1): 11:58:06  Began executing node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:58:06.010319 (Thread-1): 11:58:06  Writing runtime sql for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:58:06.011126 (Thread-2): 11:58:06  Writing runtime sql for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:58:06.030398 (Thread-2): 11:58:06  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:58:06.030609 (Thread-2): 11:58:06  Using databricks connection "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:58:06.030834 (Thread-2): 11:58:06  On model.dbsql_dbt_tpch.Financial: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 11:58:06.030972 (Thread-2): 11:58:06  Opening a new connection, currently in state closed
2023-04-20 11:58:06.031451 (Thread-1): 11:58:06  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:58:06.031666 (Thread-1): 11:58:06  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:58:06.031891 (Thread-1): 11:58:06  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 11:58:06.032019 (Thread-1): 11:58:06  Opening a new connection, currently in state closed
2023-04-20 11:58:06.387621 (Thread-3): 11:58:06  SQL status: OK in 3.92 seconds
2023-04-20 11:58:06.390078 (Thread-3): 11:58:06  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (execute): 2023-04-20 11:58:01.586483 => 2023-04-20 11:58:06.389881
2023-04-20 11:58:06.390394 (Thread-3): 11:58:06  On model.dbsql_dbt_tpch.DimCustomerStg: ROLLBACK
2023-04-20 11:58:06.390556 (Thread-3): 11:58:06  Databricks adapter: NotImplemented: rollback
2023-04-20 11:58:06.390685 (Thread-3): 11:58:06  On model.dbsql_dbt_tpch.DimCustomerStg: Close
2023-04-20 11:58:06.577815 (Thread-610): handling ps request
2023-04-20 11:58:06.578314 (Thread-610): 11:58:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bc7b9d0>]}
2023-04-20 11:58:06.579736 (Thread-610): sending response (<Response 17269 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:06.666814 (Thread-3): 11:58:06  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e3074c5c-c059-4ae3-8ef8-d9d708ab92e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdef52fda30>]}
2023-04-20 11:58:06.667340 (Thread-3): 11:58:06  3 of 15 OK created sql table model dbt_shabbirkdb.DimCustomerStg ............... [OK in 5.13s]
2023-04-20 11:58:06.667643 (Thread-3): 11:58:06  Finished running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:58:06.668485 (Thread-3): 11:58:06  Began running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:58:06.668922 (Thread-3): 11:58:06  7 of 15 START sql table model dbt_shabbirkdb.DimAccount ........................ [RUN]
2023-04-20 11:58:06.669668 (Thread-3): 11:58:06  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimAccount'
2023-04-20 11:58:06.669870 (Thread-3): 11:58:06  Began compiling node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:58:06.675390 (Thread-3): 11:58:06  Writing injected SQL for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:58:06.688966 (Thread-3): 11:58:06  Timing info for model.dbsql_dbt_tpch.DimAccount (compile): 2023-04-20 11:58:06.669919 => 2023-04-20 11:58:06.688806
2023-04-20 11:58:06.689184 (Thread-3): 11:58:06  Began executing node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:58:06.693249 (Thread-3): 11:58:06  Writing runtime sql for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:58:06.706538 (Thread-3): 11:58:06  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:58:06.706721 (Thread-3): 11:58:06  Using databricks connection "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:58:06.706950 (Thread-3): 11:58:06  On model.dbsql_dbt_tpch.DimAccount: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 11:58:06.707081 (Thread-3): 11:58:06  Opening a new connection, currently in state closed
2023-04-20 11:58:07.080717 (Thread-611): handling status request
2023-04-20 11:58:07.081217 (Thread-611): 11:58:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bc7bb20>]}
2023-04-20 11:58:07.081844 (Thread-611): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:07.532051 (Thread-3): 11:58:07  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 11:58:07.532274 (Thread-3): 11:58:07  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:58:07.532400 (Thread-3): 11:58:07  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:58:07.532515 (Thread-3): 11:58:07  Databricks adapter: operation-id: b'\x01\xed\xdfr\x9d\t\x15\x16\x9b\xc1`\xf4v\xa3 \x9a'
2023-04-20 11:58:07.532756 (Thread-3): 11:58:07  Timing info for model.dbsql_dbt_tpch.DimAccount (execute): 2023-04-20 11:58:06.689233 => 2023-04-20 11:58:07.532626
2023-04-20 11:58:07.532946 (Thread-3): 11:58:07  On model.dbsql_dbt_tpch.DimAccount: ROLLBACK
2023-04-20 11:58:07.533069 (Thread-3): 11:58:07  Databricks adapter: NotImplemented: rollback
2023-04-20 11:58:07.533190 (Thread-3): 11:58:07  On model.dbsql_dbt_tpch.DimAccount: Close
2023-04-20 11:58:07.601435 (Thread-612): handling poll request
2023-04-20 11:58:07.601925 (Thread-612): 11:58:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bc4b700>]}
2023-04-20 11:58:07.603152 (Thread-612): sending response (<Response 36427 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:07.832602 (Thread-3): 11:58:07  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:58:07.833183 (Thread-3): 11:58:07  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e3074c5c-c059-4ae3-8ef8-d9d708ab92e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fded44abd30>]}
2023-04-20 11:58:07.833732 (Thread-3): 11:58:07  7 of 15 ERROR creating sql table model dbt_shabbirkdb.DimAccount ............... [ERROR in 1.16s]
2023-04-20 11:58:07.833990 (Thread-3): 11:58:07  Finished running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:58:07.834284 (Thread-3): 11:58:07  Began running node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:58:07.834566 (Thread-3): 11:58:07  8 of 15 START sql table model dbt_shabbirkdb.Prospect .......................... [RUN]
2023-04-20 11:58:07.835202 (Thread-3): 11:58:07  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Prospect'
2023-04-20 11:58:07.835445 (Thread-3): 11:58:07  Began compiling node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:58:07.841461 (Thread-3): 11:58:07  Writing injected SQL for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:58:07.855028 (Thread-3): 11:58:07  Timing info for model.dbsql_dbt_tpch.Prospect (compile): 2023-04-20 11:58:07.835500 => 2023-04-20 11:58:07.854834
2023-04-20 11:58:07.855281 (Thread-3): 11:58:07  Began executing node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:58:07.861409 (Thread-3): 11:58:07  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:58:07.861587 (Thread-3): 11:58:07  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:58:07.861745 (Thread-3): 11:58:07  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

      describe extended `dbt_shabbirkdb`.`prospect`
  
2023-04-20 11:58:07.861888 (Thread-3): 11:58:07  Opening a new connection, currently in state closed
2023-04-20 11:58:08.614576 (Thread-3): 11:58:08  SQL status: OK in 0.75 seconds
2023-04-20 11:58:08.620885 (Thread-3): 11:58:08  Writing runtime sql for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:58:08.636677 (Thread-3): 11:58:08  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:58:08.637054 (Thread-3): 11:58:08  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Prospect`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  agencyid,
  recdate.sk_dateid sk_recorddateid,
  origdate.sk_dateid sk_updatedateid,
  p.batchid,
  nvl2(c.customerid, True, False) iscustomer, 
  p.lastname,
  p.firstname,
  p.middleinitial,
  p.gender,
  p.addressline1,
  p.addressline2,
  p.postalcode,
  city,
  state,
  country,
  phone,
  income,
  numbercars,
  numberchildren,
  maritalstatus,
  age,
  creditrating,
  ownorrentflag,
  employer,
  numbercreditcards,
  networth,
  if(
    isnotnull(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+","")),
    left(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+",""),
      length(
        if(networth > 1000000 or income > 200000,"HighValue+","") || 
        if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
        if(age > 45, "Boomer+", "") ||
        if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
        if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
        if(age < 25 and networth > 1000000, "Inherited+",""))
      -1),
    NULL) marketingnameplate
FROM (
  SELECT 
    * FROM (
    SELECT
      agencyid,
      max(batchid) recordbatchid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth,
      min(batchid) batchid
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`ProspectRaw` p
    GROUP BY
      agencyid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth)
  QUALIFY ROW_NUMBER() OVER (PARTITION BY agencyid ORDER BY batchid DESC) = 1) p
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) recdate
  ON p.recordbatchid = recdate.batchid
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) origdate
  ON p.batchid = origdate.batchid
LEFT JOIN (
  SELECT 
    customerid,
    lastname,
    firstname,
    addressline1,
    addressline2,
    postalcode
  FROM `dbt_shabbirkdb`.`DimCustomerStg`
  WHERE iscurrent) c
  ON 
    upper(p.LastName) = upper(c.lastname)
    and upper(p.FirstName) = upper(c.firstname)
    and upper(p.AddressLine1) = upper(c.addressline1)
    and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
    and upper(p.PostalCode) = upper(c.postalcode)
  
2023-04-20 11:58:09.192983 (Thread-613): handling poll request
2023-04-20 11:58:09.193502 (Thread-613): 11:58:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bc4b280>]}
2023-04-20 11:58:09.194237 (Thread-613): sending response (<Response 12566 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:09.255163 (Thread-614): handling ps request
2023-04-20 11:58:09.255690 (Thread-614): 11:58:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bc4b400>]}
2023-04-20 11:58:09.257125 (Thread-614): sending response (<Response 17269 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:09.308757 (Thread-4): 11:58:09  SQL status: OK in 6.87 seconds
2023-04-20 11:58:09.627297 (Thread-4): 11:58:09  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (execute): 2023-04-20 11:58:01.591617 => 2023-04-20 11:58:09.627102
2023-04-20 11:58:09.627623 (Thread-4): 11:58:09  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: ROLLBACK
2023-04-20 11:58:09.627783 (Thread-4): 11:58:09  Databricks adapter: NotImplemented: rollback
2023-04-20 11:58:09.627913 (Thread-4): 11:58:09  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: Close
2023-04-20 11:58:09.694289 (Thread-615): handling status request
2023-04-20 11:58:09.694762 (Thread-615): 11:58:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bc58100>]}
2023-04-20 11:58:09.695358 (Thread-615): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:09.893048 (Thread-4): 11:58:09  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e3074c5c-c059-4ae3-8ef8-d9d708ab92e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fded4274490>]}
2023-04-20 11:58:09.893585 (Thread-4): 11:58:09  4 of 15 OK created sql table model dbt_shabbirkdb.tempDailyMarketHistorical .... [OK in 8.35s]
2023-04-20 11:58:09.893823 (Thread-4): 11:58:09  Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:58:09.894123 (Thread-4): 11:58:09  Began running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 11:58:09.894349 (Thread-4): 11:58:09  9 of 15 SKIP relation dbt_shabbirkdb.FactCashBalances .......................... [SKIP]
2023-04-20 11:58:09.894528 (Thread-4): 11:58:09  Finished running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 11:58:10.029407 (Thread-1): 11:58:10  SQL status: OK in 4.0 seconds
2023-04-20 11:58:10.031661 (Thread-1): 11:58:10  Timing info for model.dbsql_dbt_tpch.DimSecurity (execute): 2023-04-20 11:58:06.006205 => 2023-04-20 11:58:10.031486
2023-04-20 11:58:10.031868 (Thread-1): 11:58:10  On model.dbsql_dbt_tpch.DimSecurity: ROLLBACK
2023-04-20 11:58:10.032009 (Thread-1): 11:58:10  Databricks adapter: NotImplemented: rollback
2023-04-20 11:58:10.032130 (Thread-1): 11:58:10  On model.dbsql_dbt_tpch.DimSecurity: Close
2023-04-20 11:58:10.309033 (Thread-1): 11:58:10  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e3074c5c-c059-4ae3-8ef8-d9d708ab92e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fded4621760>]}
2023-04-20 11:58:10.309536 (Thread-1): 11:58:10  5 of 15 OK created sql table model dbt_shabbirkdb.DimSecurity .................. [OK in 4.33s]
2023-04-20 11:58:10.309767 (Thread-1): 11:58:10  Finished running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:58:10.310657 (Thread-4): 11:58:10  Began running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 11:58:10.310928 (Thread-4): 11:58:10  10 of 15 SKIP relation dbt_shabbirkdb.DimTrade ................................. [SKIP]
2023-04-20 11:58:10.311116 (Thread-4): 11:58:10  Finished running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 11:58:10.311763 (Thread-4): 11:58:10  Began running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 11:58:10.311986 (Thread-4): 11:58:10  11 of 15 SKIP relation dbt_shabbirkdb.FactHoldings ............................. [SKIP]
2023-04-20 11:58:10.312163 (Thread-4): 11:58:10  Finished running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 11:58:10.782081 (Thread-616): handling poll request
2023-04-20 11:58:10.782569 (Thread-616): 11:58:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bc9fd90>]}
2023-04-20 11:58:10.783362 (Thread-616): sending response (<Response 11685 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:11.223465 (Thread-2): 11:58:11  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 11:58:11.223725 (Thread-2): 11:58:11  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: Task failed while writing rows.
2023-04-20 11:58:11.223869 (Thread-2): 11:58:11  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4247.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4247.0 (TID 32581) (10.5.234.83 executor 1): org.apache.spark.SparkException: Task failed while writing rows.
	at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:109)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'WIFHmdYSMPHKWkoqPrQTYs' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:190)
	at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:137)
	at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:106)
	... 31 more
	Suppressed: com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: long,WIFHmdYSMPHKWkoqPrQTYs
		at 0x89a2b7e <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_3/photon/common/status.cc:137)
		at 0x5ded45d <photon>.OnInvalidInput(external/workspace_spark_3_3/photon/exprs/cast-functions.h:622)
		at 0x5ded0f2 <photon>.ProjectBatch(external/workspace_spark_3_3/photon/exprs/compute-function.h:1117)
		at 0x5deccac <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:69)
		at 0x5df978c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
		at 0x850c561 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/coalesce-expr.cc:55)
		at 0x7dcf64c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/is-null-expr.h:90)
		at 0x85d4a3e <photon>.Eval(external/workspace_spark_3_3/photon/exprs/logical-functions.cc:284)
		at 0x4c56aee <photon>.EvalAllFilters(external/workspace_spark_3_3/photon/io/parquet-reader/data-filter.cc:186)
		at 0x4c86441 <photon>.ScanAndFilterRowsUntilNonEmptyBatch(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:936)
		at 0x4c86da0 <photon>.YieldRowsWithFilter(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:989)
		at 0x4c8782b <photon>.Next(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:1068)
		at 0x4c78105 <photon>.Next(external/workspace_spark_3_3/photon/io/parquet-reader/file-reader.cc:428)
		at 0x4b7bd32 <photon>.operator()(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:728)
		at 0x4b795a0 <photon>.TryPullNextBatch(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:728)
		at 0x4b795a0 <photon>.DoHasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:508)
		at 0x4b7932e <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:336)
		at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
		at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
		at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
		at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
		at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
		at com.databricks.photon.JniApiImpl.hasNext(Native Method)
		at com.databricks.photon.JniApi.hasNext(JniApi.scala)
		at com.databricks.photon.JniExecNode.hasNext(JniExecNode.java:68)
		at com.databricks.photon.PhotonResultStageExec$$anon$2.hasNext(PhotonExec.scala:853)
		at com.databricks.photon.PhotonExec$$anon$1.$anonfun$hasNext$1(PhotonExec.scala:582)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonResultStageExec.timeit(PhotonExec.scala:838)
		at com.databricks.photon.PhotonExec$$anon$1.hasNext(PhotonExec.scala:582)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)
		at org.apache.spark.sql.execution.RowToColumnarExec$$anon$1.hasNext(Columnar.scala:503)
		at com.databricks.photon.CloseableIterator$$anon$5.hasNext(CloseableIterator.scala:62)
		at com.databricks.photon.NativeColumnBatchIterator.hasNext(NativeColumnBatchIterator.java:47)
		at 0x899c692 <photon>.HasNext(external/workspace_spark_3_3/photon/jni-wrappers/jni-native-column-batch-iterator.cc:62)
		at 0x4abefd9 <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/file-writer-node.cc:161)
		at com.databricks.photon.JniApiImpl.open(Native Method)
		at com.databricks.photon.JniApi.open(JniApi.scala)
		at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$5(PhotonWriteStageExec.scala:89)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonWriteStageExec.timeit(PhotonWriteStageExec.scala:38)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$4(PhotonWriteStageExec.scala:89)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1747)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:86)
		... 31 more

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4247.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4247.0 (TID 32581) (10.5.234.83 executor 1): org.apache.spark.SparkException: Task failed while writing rows.
	at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:109)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'WIFHmdYSMPHKWkoqPrQTYs' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:190)
	at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:137)
	at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:106)
	... 31 more
	Suppressed: com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: long,WIFHmdYSMPHKWkoqPrQTYs
		at 0x89a2b7e <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_3/photon/common/status.cc:137)
		at 0x5ded45d <photon>.OnInvalidInput(external/workspace_spark_3_3/photon/exprs/cast-functions.h:622)
		at 0x5ded0f2 <photon>.ProjectBatch(external/workspace_spark_3_3/photon/exprs/compute-function.h:1117)
		at 0x5deccac <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:69)
		at 0x5df978c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
		at 0x850c561 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/coalesce-expr.cc:55)
		at 0x7dcf64c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/is-null-expr.h:90)
		at 0x85d4a3e <photon>.Eval(external/workspace_spark_3_3/photon/exprs/logical-functions.cc:284)
		at 0x4c56aee <photon>.EvalAllFilters(external/workspace_spark_3_3/photon/io/parquet-reader/data-filter.cc:186)
		at 0x4c86441 <photon>.ScanAndFilterRowsUntilNonEmptyBatch(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:936)
		at 0x4c86da0 <photon>.YieldRowsWithFilter(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:989)
		at 0x4c8782b <photon>.Next(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:1068)
		at 0x4c78105 <photon>.Next(external/workspace_spark_3_3/photon/io/parquet-reader/file-reader.cc:428)
		at 0x4b7bd32 <photon>.operator()(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:728)
		at 0x4b795a0 <photon>.TryPullNextBatch(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:728)
		at 0x4b795a0 <photon>.DoHasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:508)
		at 0x4b7932e <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:336)
		at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
		at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
		at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
		at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
		at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
		at com.databricks.photon.JniApiImpl.hasNext(Native Method)
		at com.databricks.photon.JniApi.hasNext(JniApi.scala)
		at com.databricks.photon.JniExecNode.hasNext(JniExecNode.java:68)
		at com.databricks.photon.PhotonResultStageExec$$anon$2.hasNext(PhotonExec.scala:853)
		at com.databricks.photon.PhotonExec$$anon$1.$anonfun$hasNext$1(PhotonExec.scala:582)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonResultStageExec.timeit(PhotonExec.scala:838)
		at com.databricks.photon.PhotonExec$$anon$1.hasNext(PhotonExec.scala:582)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)
		at org.apache.spark.sql.execution.RowToColumnarExec$$anon$1.hasNext(Columnar.scala:503)
		at com.databricks.photon.CloseableIterator$$anon$5.hasNext(CloseableIterator.scala:62)
		at com.databricks.photon.NativeColumnBatchIterator.hasNext(NativeColumnBatchIterator.java:47)
		at 0x899c692 <photon>.HasNext(external/workspace_spark_3_3/photon/jni-wrappers/jni-native-column-batch-iterator.cc:62)
		at 0x4abefd9 <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/file-writer-node.cc:161)
		at com.databricks.photon.JniApiImpl.open(Native Method)
		at com.databricks.photon.JniApi.open(JniApi.scala)
		at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$5(PhotonWriteStageExec.scala:89)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonWriteStageExec.timeit(PhotonWriteStageExec.scala:38)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$4(PhotonWriteStageExec.scala:89)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1747)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:86)
		... 31 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1172)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1160)
	at org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2720)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:611)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:236)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:336)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:226)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:115)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:70)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$12(TransactionalWriteEdge.scala:584)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$1(TransactionalWriteEdge.scala:584)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:645)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:407)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:405)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:402)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:450)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:435)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:640)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:559)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:520)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:63)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:145)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:104)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:316)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:1961)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:315)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:348)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:342)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:625)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:615)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:226)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:223)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:142)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:367)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:151)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:169)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:645)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:407)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:405)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:402)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:450)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:435)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:640)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:559)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:550)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:520)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:23)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:63)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:145)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:104)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:54)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:120)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:257)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:86)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:119)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:907)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:86)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:866)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:557)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1747)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:538)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:567)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:533)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:222)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:278)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:238)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:237)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:220)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:226)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:226)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:226)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:171)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:249)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:456)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:475)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:708)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:535)
	... 21 more
Caused by: org.apache.spark.SparkException: Task failed while writing rows.
	at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:109)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	... 3 more
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'WIFHmdYSMPHKWkoqPrQTYs' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:190)
	at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:137)
	at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:106)
	... 31 more
	Suppressed: com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: long,WIFHmdYSMPHKWkoqPrQTYs
		at 0x89a2b7e <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_3/photon/common/status.cc:137)
		at 0x5ded45d <photon>.OnInvalidInput(external/workspace_spark_3_3/photon/exprs/cast-functions.h:622)
		at 0x5ded0f2 <photon>.ProjectBatch(external/workspace_spark_3_3/photon/exprs/compute-function.h:1117)
		at 0x5deccac <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:69)
		at 0x5df978c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
		at 0x850c561 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/coalesce-expr.cc:55)
		at 0x7dcf64c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/is-null-expr.h:90)
		at 0x85d4a3e <photon>.Eval(external/workspace_spark_3_3/photon/exprs/logical-functions.cc:284)
		at 0x4c56aee <photon>.EvalAllFilters(external/workspace_spark_3_3/photon/io/parquet-reader/data-filter.cc:186)
		at 0x4c86441 <photon>.ScanAndFilterRowsUntilNonEmptyBatch(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:936)
		at 0x4c86da0 <photon>.YieldRowsWithFilter(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:989)
		at 0x4c8782b <photon>.Next(external/workspace_spark_3_3/photon/io/parquet-reader/row-group-reader.cc:1068)
		at 0x4c78105 <photon>.Next(external/workspace_spark_3_3/photon/io/parquet-reader/file-reader.cc:428)
		at 0x4b7bd32 <photon>.operator()(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:728)
		at 0x4b795a0 <photon>.TryPullNextBatch(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:728)
		at 0x4b795a0 <photon>.DoHasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:508)
		at 0x4b7932e <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/file-scan-node.cc:336)
		at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
		at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
		at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
		at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
		at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
		at com.databricks.photon.JniApiImpl.hasNext(Native Method)
		at com.databricks.photon.JniApi.hasNext(JniApi.scala)
		at com.databricks.photon.JniExecNode.hasNext(JniExecNode.java:68)
		at com.databricks.photon.PhotonResultStageExec$$anon$2.hasNext(PhotonExec.scala:853)
		at com.databricks.photon.PhotonExec$$anon$1.$anonfun$hasNext$1(PhotonExec.scala:582)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonResultStageExec.timeit(PhotonExec.scala:838)
		at com.databricks.photon.PhotonExec$$anon$1.hasNext(PhotonExec.scala:582)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)
		at org.apache.spark.sql.execution.RowToColumnarExec$$anon$1.hasNext(Columnar.scala:503)
		at com.databricks.photon.CloseableIterator$$anon$5.hasNext(CloseableIterator.scala:62)
		at com.databricks.photon.NativeColumnBatchIterator.hasNext(NativeColumnBatchIterator.java:47)
		at 0x899c692 <photon>.HasNext(external/workspace_spark_3_3/photon/jni-wrappers/jni-native-column-batch-iterator.cc:62)
		at 0x4abefd9 <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/file-writer-node.cc:161)
		at com.databricks.photon.JniApiImpl.open(Native Method)
		at com.databricks.photon.JniApi.open(JniApi.scala)
		at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$5(PhotonWriteStageExec.scala:89)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
		at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
		at com.databricks.photon.PhotonWriteStageExec.timeit(PhotonWriteStageExec.scala:38)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$4(PhotonWriteStageExec.scala:89)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1747)
		at com.databricks.photon.PhotonWriteStageExec.$anonfun$executeWrite$2(PhotonWriteStageExec.scala:86)
		... 31 more

2023-04-20 11:58:11.223988 (Thread-2): 11:58:11  Databricks adapter: operation-id: b'\x01\xed\xdfr\x9c\x9f\x126\x82\xf5\x81\xadB?\xc3\xa1'
2023-04-20 11:58:11.224250 (Thread-2): 11:58:11  Timing info for model.dbsql_dbt_tpch.Financial (execute): 2023-04-20 11:58:06.001150 => 2023-04-20 11:58:11.224114
2023-04-20 11:58:11.224439 (Thread-2): 11:58:11  On model.dbsql_dbt_tpch.Financial: ROLLBACK
2023-04-20 11:58:11.224565 (Thread-2): 11:58:11  Databricks adapter: NotImplemented: rollback
2023-04-20 11:58:11.224704 (Thread-2): 11:58:11  On model.dbsql_dbt_tpch.Financial: Close
2023-04-20 11:58:11.520994 (Thread-2): 11:58:11  Runtime Error in model Financial (models/silver/Financial.sql)
  Job aborted due to stage failure: Task failed while writing rows.
2023-04-20 11:58:11.521390 (Thread-2): 11:58:11  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e3074c5c-c059-4ae3-8ef8-d9d708ab92e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fded462af70>]}
2023-04-20 11:58:11.521816 (Thread-2): 11:58:11  6 of 15 ERROR creating sql table model dbt_shabbirkdb.Financial ................ [ERROR in 5.54s]
2023-04-20 11:58:11.522040 (Thread-2): 11:58:11  Finished running node model.dbsql_dbt_tpch.Financial
2023-04-20 11:58:11.522980 (Thread-4): 11:58:11  Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 11:58:11.523256 (Thread-4): 11:58:11  12 of 15 SKIP relation dbt_shabbirkdb.tempSumpFiBasicEps ....................... [SKIP]
2023-04-20 11:58:11.523447 (Thread-4): 11:58:11  Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 11:58:11.524094 (Thread-2): 11:58:11  Began running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 11:58:11.524341 (Thread-2): 11:58:11  13 of 15 SKIP relation dbt_shabbirkdb.FactMarketHistory ........................ [SKIP]
2023-04-20 11:58:11.524524 (Thread-2): 11:58:11  Finished running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 11:58:11.840235 (Thread-617): handling ps request
2023-04-20 11:58:11.840952 (Thread-617): 11:58:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bc581f0>]}
2023-04-20 11:58:11.868236 (Thread-617): sending response (<Response 17269 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:12.352793 (Thread-618): handling status request
2023-04-20 11:58:12.353310 (Thread-618): 11:58:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bc9f1c0>]}
2023-04-20 11:58:12.353949 (Thread-618): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:12.405726 (Thread-619): handling poll request
2023-04-20 11:58:12.406138 (Thread-619): 11:58:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bc9f130>]}
2023-04-20 11:58:12.407001 (Thread-619): sending response (<Response 58808 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:12.620191 (Thread-3): 11:58:12  SQL status: OK in 3.98 seconds
2023-04-20 11:58:12.622590 (Thread-3): 11:58:12  Timing info for model.dbsql_dbt_tpch.Prospect (execute): 2023-04-20 11:58:07.855333 => 2023-04-20 11:58:12.622424
2023-04-20 11:58:12.622801 (Thread-3): 11:58:12  On model.dbsql_dbt_tpch.Prospect: ROLLBACK
2023-04-20 11:58:12.622945 (Thread-3): 11:58:12  Databricks adapter: NotImplemented: rollback
2023-04-20 11:58:12.623068 (Thread-3): 11:58:12  On model.dbsql_dbt_tpch.Prospect: Close
2023-04-20 11:58:12.899107 (Thread-3): 11:58:12  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e3074c5c-c059-4ae3-8ef8-d9d708ab92e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdef5df1e50>]}
2023-04-20 11:58:12.899882 (Thread-3): 11:58:12  8 of 15 OK created sql table model dbt_shabbirkdb.Prospect ..................... [OK in 5.06s]
2023-04-20 11:58:12.900159 (Thread-3): 11:58:12  Finished running node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:58:12.901431 (Thread-1): 11:58:12  Began running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:58:12.901953 (Thread-1): 11:58:12  14 of 15 START sql table model dbt_shabbirkdb.DimCustomer ...................... [RUN]
2023-04-20 11:58:12.902794 (Thread-1): 11:58:12  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomer'
2023-04-20 11:58:12.903107 (Thread-1): 11:58:12  Began compiling node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:58:12.911642 (Thread-1): 11:58:12  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:58:12.925777 (Thread-1): 11:58:12  Timing info for model.dbsql_dbt_tpch.DimCustomer (compile): 2023-04-20 11:58:12.903185 => 2023-04-20 11:58:12.925602
2023-04-20 11:58:12.926018 (Thread-1): 11:58:12  Began executing node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:58:12.930583 (Thread-1): 11:58:12  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:58:12.945038 (Thread-1): 11:58:12  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:58:12.945224 (Thread-1): 11:58:12  Using databricks connection "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:58:12.945422 (Thread-1): 11:58:12  On model.dbsql_dbt_tpch.DimCustomer: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 11:58:12.945556 (Thread-1): 11:58:12  Opening a new connection, currently in state closed
2023-04-20 11:58:13.683223 (Thread-1): 11:58:13  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 11:58:13.683452 (Thread-1): 11:58:13  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:58:13.683610 (Thread-1): 11:58:13  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:58:13.683731 (Thread-1): 11:58:13  Databricks adapter: operation-id: b'\x01\xed\xdfr\xa0\xc2\x1c\xbb\xae\xcb\xaa{\xd5:\x03\x81'
2023-04-20 11:58:13.683972 (Thread-1): 11:58:13  Timing info for model.dbsql_dbt_tpch.DimCustomer (execute): 2023-04-20 11:58:12.926071 => 2023-04-20 11:58:13.683840
2023-04-20 11:58:13.684161 (Thread-1): 11:58:13  On model.dbsql_dbt_tpch.DimCustomer: ROLLBACK
2023-04-20 11:58:13.684284 (Thread-1): 11:58:13  Databricks adapter: NotImplemented: rollback
2023-04-20 11:58:13.684404 (Thread-1): 11:58:13  On model.dbsql_dbt_tpch.DimCustomer: Close
2023-04-20 11:58:13.966661 (Thread-1): 11:58:13  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:58:13.967068 (Thread-1): 11:58:13  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e3074c5c-c059-4ae3-8ef8-d9d708ab92e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fded4355940>]}
2023-04-20 11:58:13.967700 (Thread-1): 11:58:13  14 of 15 ERROR creating sql table model dbt_shabbirkdb.DimCustomer ............. [ERROR in 1.06s]
2023-04-20 11:58:13.967918 (Thread-1): 11:58:13  Finished running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:58:13.969047 (Thread-4): 11:58:13  Began running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 11:58:13.969323 (Thread-4): 11:58:13  15 of 15 SKIP relation dbt_shabbirkdb.FactWatches .............................. [SKIP]
2023-04-20 11:58:13.969514 (Thread-4): 11:58:13  Finished running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 11:58:13.971190 (MainThread): 11:58:13  Acquiring new databricks connection 'master'
2023-04-20 11:58:13.971409 (MainThread): 11:58:13  On master: ROLLBACK
2023-04-20 11:58:13.971593 (MainThread): 11:58:13  Opening a new connection, currently in state init
2023-04-20 11:58:14.144976 (Thread-620): handling poll request
2023-04-20 11:58:14.145484 (Thread-620): 11:58:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bcfd310>]}
2023-04-20 11:58:14.146521 (Thread-620): sending response (<Response 22279 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:14.265984 (MainThread): 11:58:14  Databricks adapter: NotImplemented: rollback
2023-04-20 11:58:14.266232 (MainThread): 11:58:14  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:58:14.266369 (MainThread): 11:58:14  Spark adapter: NotImplemented: commit
2023-04-20 11:58:14.266533 (MainThread): 11:58:14  On master: ROLLBACK
2023-04-20 11:58:14.266662 (MainThread): 11:58:14  Databricks adapter: NotImplemented: rollback
2023-04-20 11:58:14.266798 (MainThread): 11:58:14  On master: Close
2023-04-20 11:58:14.428037 (Thread-621): handling ps request
2023-04-20 11:58:14.428530 (Thread-621): 11:58:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bcbb280>]}
2023-04-20 11:58:14.429979 (Thread-621): sending response (<Response 17269 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:14.548007 (MainThread): 11:58:14  Connection 'master' was properly closed.
2023-04-20 11:58:14.548206 (MainThread): 11:58:14  Connection 'model.dbsql_dbt_tpch.DimCustomer' was properly closed.
2023-04-20 11:58:14.548325 (MainThread): 11:58:14  Connection 'model.dbsql_dbt_tpch.Financial' was properly closed.
2023-04-20 11:58:14.548429 (MainThread): 11:58:14  Connection 'model.dbsql_dbt_tpch.Prospect' was properly closed.
2023-04-20 11:58:14.548528 (MainThread): 11:58:14  Connection 'model.dbsql_dbt_tpch.tempDailyMarketHistorical' was properly closed.
2023-04-20 11:58:14.548838 (MainThread): 11:58:14  
2023-04-20 11:58:14.548991 (MainThread): 11:58:14  Finished running 15 table models in 0 hours 0 minutes and 15.92 seconds (15.92s).
2023-04-20 11:58:14.631192 (MainThread): 11:58:14  
2023-04-20 11:58:14.631435 (MainThread): 11:58:14  Completed with 3 errors and 0 warnings:
2023-04-20 11:58:14.631607 (MainThread): 11:58:14  
2023-04-20 11:58:14.631760 (MainThread): 11:58:14  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
2023-04-20 11:58:14.631888 (MainThread): 11:58:14    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:58:14.631998 (MainThread): 11:58:14  
2023-04-20 11:58:14.632120 (MainThread): 11:58:14  Runtime Error in model Financial (models/silver/Financial.sql)
2023-04-20 11:58:14.632232 (MainThread): 11:58:14    Job aborted due to stage failure: Task failed while writing rows.
2023-04-20 11:58:14.632339 (MainThread): 11:58:14  
2023-04-20 11:58:14.632455 (MainThread): 11:58:14  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
2023-04-20 11:58:14.632562 (MainThread): 11:58:14    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:58:14.632696 (MainThread): 11:58:14  
2023-04-20 11:58:14.632825 (MainThread): 11:58:14  Done. PASS=6 WARN=0 ERROR=3 SKIP=6 TOTAL=15
2023-04-20 11:58:14.858903 (Thread-622): handling status request
2023-04-20 11:58:14.859420 (Thread-622): 11:58:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528477400>]}
2023-04-20 11:58:14.860117 (Thread-622): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:15.768231 (Thread-623): handling poll request
2023-04-20 11:58:15.768774 (Thread-623): 11:58:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528477f70>]}
2023-04-20 11:58:15.771416 (Thread-623): sending response (<Response 92871 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:17.026619 (Thread-624): handling ps request
2023-04-20 11:58:17.027111 (Thread-624): 11:58:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528477460>]}
2023-04-20 11:58:17.028563 (Thread-624): sending response (<Response 17293 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:17.642979 (Thread-625): handling status request
2023-04-20 11:58:17.643469 (Thread-625): 11:58:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528477340>]}
2023-04-20 11:58:17.644132 (Thread-625): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:43.395369 (Thread-626): 11:58:43  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:58:43.418473 (Thread-626): 11:58:43  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 11:58:43.418852 (Thread-626): 11:58:43  previous checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, current checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886
2023-04-20 11:58:43.419057 (Thread-626): 11:58:43  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528241640>]}
2023-04-20 11:58:43.818802 (Thread-627): handling status request
2023-04-20 11:58:43.839922 (Thread-627): 11:58:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d165e80>]}
2023-04-20 11:58:43.845516 (Thread-627): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:44.897377 (Thread-628): handling status request
2023-04-20 11:58:44.913209 (Thread-628): 11:58:44  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5582abcd0>]}
2023-04-20 11:58:44.918846 (Thread-628): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:44.951018 (Thread-626): 11:58:44  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 11:58:44.954481 (Thread-626): 11:58:44  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 11:58:44.957691 (Thread-626): 11:58:44  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 11:58:44.960510 (Thread-626): 11:58:44  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 11:58:44.963445 (Thread-626): 11:58:44  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 11:58:44.966160 (Thread-626): 11:58:44  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 11:58:44.969333 (Thread-626): 11:58:44  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 11:58:44.972389 (Thread-626): 11:58:44  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 11:58:44.975510 (Thread-626): 11:58:44  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 11:58:44.978366 (Thread-626): 11:58:44  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 11:58:44.981426 (Thread-626): 11:58:44  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 11:58:44.985076 (Thread-626): 11:58:44  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 11:58:44.987831 (Thread-626): 11:58:44  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 11:58:44.990641 (Thread-626): 11:58:44  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 11:58:44.993576 (Thread-626): 11:58:44  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:58:45.152861 (Thread-626): 11:58:45  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a955e80>]}
2023-04-20 11:58:45.958222 (Thread-629): handling status request
2023-04-20 11:58:45.958742 (Thread-629): 11:58:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a969070>]}
2023-04-20 11:58:45.959359 (Thread-629): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:53.650862 (Thread-630): handling status request
2023-04-20 11:58:53.651381 (Thread-630): 11:58:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a969400>]}
2023-04-20 11:58:53.652029 (Thread-630): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:54.136437 (Thread-631): handling cli_args request
2023-04-20 11:58:54.136935 (Thread-631): 11:58:54  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a969670>]}
2023-04-20 11:58:56.886796 (Thread-631): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:56.944881 (MainThread): 11:58:56  checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
2023-04-20 11:58:56.966592 (MainThread): 11:58:56  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 11:58:56.966767 (MainThread): 11:58:56  previous checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, current checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd
2023-04-20 11:58:56.966920 (MainThread): 11:58:56  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'aa5ea3ce-4125-45b8-82ef-218ceeeb7fce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1447210c10>]}
2023-04-20 11:58:57.609704 (Thread-632): handling ps request
2023-04-20 11:58:57.610409 (Thread-632): 11:58:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd07880>]}
2023-04-20 11:58:57.613013 (Thread-632): sending response (<Response 17675 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:57.650796 (Thread-633): handling poll request
2023-04-20 11:58:57.651267 (Thread-633): 11:58:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd07580>]}
2023-04-20 11:58:57.651854 (Thread-633): sending response (<Response 1863 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:57.880482 (Thread-634): handling status request
2023-04-20 11:58:57.880998 (Thread-634): 11:58:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd074c0>]}
2023-04-20 11:58:57.881717 (Thread-634): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:58.020468 (Thread-635): handling ps request
2023-04-20 11:58:58.020956 (Thread-635): 11:58:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd073a0>]}
2023-04-20 11:58:58.022325 (Thread-635): sending response (<Response 17675 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:58.576291 (MainThread): 11:58:58  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 11:58:58.588735 (MainThread): 11:58:58  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 11:58:58.591811 (MainThread): 11:58:58  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 11:58:58.594743 (MainThread): 11:58:58  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 11:58:58.597737 (MainThread): 11:58:58  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 11:58:58.600551 (MainThread): 11:58:58  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 11:58:58.603724 (MainThread): 11:58:58  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 11:58:58.606883 (MainThread): 11:58:58  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 11:58:58.609918 (MainThread): 11:58:58  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 11:58:58.612738 (MainThread): 11:58:58  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 11:58:58.616573 (MainThread): 11:58:58  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 11:58:58.619430 (MainThread): 11:58:58  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 11:58:58.622213 (MainThread): 11:58:58  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 11:58:58.625026 (MainThread): 11:58:58  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 11:58:58.628360 (MainThread): 11:58:58  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:58:58.793360 (MainThread): 11:58:58  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'aa5ea3ce-4125-45b8-82ef-218ceeeb7fce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13fe2e8c10>]}
2023-04-20 11:58:58.827011 (MainThread): 11:58:58  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'aa5ea3ce-4125-45b8-82ef-218ceeeb7fce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1446ef14f0>]}
2023-04-20 11:58:58.827339 (MainThread): 11:58:58  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:58:58.827483 (MainThread): 11:58:58  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aa5ea3ce-4125-45b8-82ef-218ceeeb7fce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14470b1970>]}
2023-04-20 11:58:58.829335 (MainThread): 11:58:58  
2023-04-20 11:58:58.830565 (MainThread): 11:58:58  Acquiring new databricks connection 'master'
2023-04-20 11:58:58.832293 (ThreadPoolExecutor-0_0): 11:58:58  Acquiring new databricks connection 'list_schemas'
2023-04-20 11:58:58.842855 (ThreadPoolExecutor-0_0): 11:58:58  Using databricks connection "list_schemas"
2023-04-20 11:58:58.843172 (ThreadPoolExecutor-0_0): 11:58:58  On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_schemas"} */

    show databases
  
2023-04-20 11:58:58.843334 (ThreadPoolExecutor-0_0): 11:58:58  Opening a new connection, currently in state init
2023-04-20 11:58:59.764841 (Thread-636): handling poll request
2023-04-20 11:58:59.765333 (Thread-636): 11:58:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a969370>]}
2023-04-20 11:58:59.766052 (Thread-636): sending response (<Response 8689 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:58:59.864935 (ThreadPoolExecutor-0_0): 11:58:59  SQL status: OK in 1.02 seconds
2023-04-20 11:58:59.988202 (ThreadPoolExecutor-0_0): 11:58:59  On list_schemas: Close
2023-04-20 11:59:00.279830 (ThreadPoolExecutor-1_0): 11:59:00  Acquiring new databricks connection 'list_None_dbt_shabbirkdb'
2023-04-20 11:59:00.290393 (ThreadPoolExecutor-1_0): 11:59:00  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:59:00.290602 (ThreadPoolExecutor-1_0): 11:59:00  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 11:59:00.290764 (ThreadPoolExecutor-1_0): 11:59:00  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show tables in `dbt_shabbirkdb`
  
2023-04-20 11:59:00.290916 (ThreadPoolExecutor-1_0): 11:59:00  Opening a new connection, currently in state closed
2023-04-20 11:59:00.785633 (Thread-637): handling ps request
2023-04-20 11:59:00.786133 (Thread-637): 11:59:00  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9dc550>]}
2023-04-20 11:59:00.787501 (Thread-637): sending response (<Response 17675 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:00.995889 (ThreadPoolExecutor-1_0): 11:59:00  SQL status: OK in 0.7 seconds
2023-04-20 11:59:01.005142 (ThreadPoolExecutor-1_0): 11:59:01  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 11:59:01.005367 (ThreadPoolExecutor-1_0): 11:59:01  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show views in `dbt_shabbirkdb`
  
2023-04-20 11:59:01.385197 (Thread-638): handling poll request
2023-04-20 11:59:01.385685 (Thread-638): 11:59:01  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb562269670>]}
2023-04-20 11:59:01.386275 (Thread-638): sending response (<Response 3839 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:01.405270 (ThreadPoolExecutor-1_0): 11:59:01  SQL status: OK in 0.4 seconds
2023-04-20 11:59:01.409042 (ThreadPoolExecutor-1_0): 11:59:01  On list_None_dbt_shabbirkdb: ROLLBACK
2023-04-20 11:59:01.409289 (ThreadPoolExecutor-1_0): 11:59:01  Databricks adapter: NotImplemented: rollback
2023-04-20 11:59:01.409446 (ThreadPoolExecutor-1_0): 11:59:01  On list_None_dbt_shabbirkdb: Close
2023-04-20 11:59:01.687597 (MainThread): 11:59:01  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aa5ea3ce-4125-45b8-82ef-218ceeeb7fce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f144666f280>]}
2023-04-20 11:59:01.688016 (MainThread): 11:59:01  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:59:01.688169 (MainThread): 11:59:01  Spark adapter: NotImplemented: commit
2023-04-20 11:59:01.688673 (MainThread): 11:59:01  Concurrency: 4 threads (target='default')
2023-04-20 11:59:01.688818 (MainThread): 11:59:01  
2023-04-20 11:59:01.691623 (Thread-1): 11:59:01  Began running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:59:01.691995 (Thread-1): 11:59:01  1 of 15 START sql table model dbt_shabbirkdb.DimBroker ......................... [RUN]
2023-04-20 11:59:01.692506 (Thread-1): 11:59:01  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
2023-04-20 11:59:01.692944 (Thread-1): 11:59:01  Began compiling node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:59:01.696668 (Thread-2): 11:59:01  Began running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:59:01.697089 (Thread-2): 11:59:01  2 of 15 START sql table model dbt_shabbirkdb.DimCompany ........................ [RUN]
2023-04-20 11:59:01.697618 (Thread-2): 11:59:01  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCompany'
2023-04-20 11:59:01.697803 (Thread-2): 11:59:01  Began compiling node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:59:01.702714 (Thread-1): 11:59:01  Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:59:01.703350 (Thread-2): 11:59:01  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:59:01.703751 (Thread-3): 11:59:01  Began running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:59:01.704057 (Thread-3): 11:59:01  3 of 15 START sql table model dbt_shabbirkdb.DimCustomerStg .................... [RUN]
2023-04-20 11:59:01.704571 (Thread-3): 11:59:01  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomerStg'
2023-04-20 11:59:01.704756 (Thread-3): 11:59:01  Began compiling node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:59:01.709107 (Thread-4): 11:59:01  Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:59:01.709402 (Thread-4): 11:59:01  4 of 15 START sql table model dbt_shabbirkdb.tempDailyMarketHistorical ......... [RUN]
2023-04-20 11:59:01.709897 (Thread-4): 11:59:01  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempDailyMarketHistorical'
2023-04-20 11:59:01.710075 (Thread-4): 11:59:01  Began compiling node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:59:01.713863 (Thread-4): 11:59:01  Writing injected SQL for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:59:01.714593 (Thread-3): 11:59:01  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:59:01.721720 (Thread-2): 11:59:01  Timing info for model.dbsql_dbt_tpch.DimCompany (compile): 2023-04-20 11:59:01.697858 => 2023-04-20 11:59:01.721548
2023-04-20 11:59:01.721963 (Thread-2): 11:59:01  Began executing node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:59:01.727179 (Thread-1): 11:59:01  Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 2023-04-20 11:59:01.693000 => 2023-04-20 11:59:01.727015
2023-04-20 11:59:01.727396 (Thread-1): 11:59:01  Began executing node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:59:01.748564 (Thread-1): 11:59:01  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:59:01.748781 (Thread-1): 11:59:01  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:59:01.748944 (Thread-1): 11:59:01  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

      describe extended `dbt_shabbirkdb`.`dimbroker`
  
2023-04-20 11:59:01.749080 (Thread-1): 11:59:01  Opening a new connection, currently in state closed
2023-04-20 11:59:01.750968 (Thread-2): 11:59:01  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:59:01.751150 (Thread-2): 11:59:01  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:59:01.751304 (Thread-2): 11:59:01  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

      describe extended `dbt_shabbirkdb`.`dimcompany`
  
2023-04-20 11:59:01.751430 (Thread-2): 11:59:01  Opening a new connection, currently in state init
2023-04-20 11:59:01.757919 (Thread-4): 11:59:01  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (compile): 2023-04-20 11:59:01.710119 => 2023-04-20 11:59:01.757729
2023-04-20 11:59:01.758167 (Thread-4): 11:59:01  Began executing node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:59:01.762338 (Thread-4): 11:59:01  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:59:01.762514 (Thread-4): 11:59:01  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:59:01.762669 (Thread-4): 11:59:01  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

      describe extended `dbt_shabbirkdb`.`tempdailymarkethistorical`
  
2023-04-20 11:59:01.762797 (Thread-4): 11:59:01  Opening a new connection, currently in state init
2023-04-20 11:59:01.773373 (Thread-3): 11:59:01  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (compile): 2023-04-20 11:59:01.704801 => 2023-04-20 11:59:01.773106
2023-04-20 11:59:01.773680 (Thread-3): 11:59:01  Began executing node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:59:01.779549 (Thread-3): 11:59:01  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:59:01.779757 (Thread-3): 11:59:01  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:59:01.779920 (Thread-3): 11:59:01  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

      describe extended `dbt_shabbirkdb`.`dimcustomerstg`
  
2023-04-20 11:59:01.780057 (Thread-3): 11:59:01  Opening a new connection, currently in state init
2023-04-20 11:59:01.996073 (Thread-639): handling status request
2023-04-20 11:59:01.996566 (Thread-639): 11:59:01  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd13a30>]}
2023-04-20 11:59:01.997174 (Thread-639): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:02.493475 (Thread-1): 11:59:02  SQL status: OK in 0.74 seconds
2023-04-20 11:59:02.540987 (Thread-1): 11:59:02  Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:59:02.541571 (Thread-2): 11:59:02  SQL status: OK in 0.79 seconds
2023-04-20 11:59:02.545384 (Thread-3): 11:59:02  SQL status: OK in 0.77 seconds
2023-04-20 11:59:02.549273 (Thread-2): 11:59:02  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:59:02.554645 (Thread-3): 11:59:02  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:59:02.557257 (Thread-4): 11:59:02  SQL status: OK in 0.79 seconds
2023-04-20 11:59:02.561860 (Thread-4): 11:59:02  Writing runtime sql for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:59:02.562244 (Thread-1): 11:59:02  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:59:02.562462 (Thread-1): 11:59:02  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT
  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate`) effectivedate,
  date('9999-12-31') enddate
FROM  `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`HR`
WHERE employeejobcode = 314
  
2023-04-20 11:59:02.568647 (Thread-2): 11:59:02  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:59:02.568905 (Thread-2): 11:59:02  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCompany`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  * FROM (
  SELECT
    md5(cik) sk_companyid,
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
  
2023-04-20 11:59:02.576887 (Thread-4): 11:59:02  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:59:02.577102 (Thread-4): 11:59:02  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempDailyMarketHistorical`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  dmh.*,
  sk_dateid,
  min(dm_low) OVER (
    PARTITION BY dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweeklow,
  max(dm_high) OVER (
    PARTITION by dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweekhigh
FROM (
  SELECT * FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketHistorical`
  UNION ALL
  SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketIncremental`) dmh
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
  ON d.datevalue = dm_date;
  
2023-04-20 11:59:02.580891 (Thread-3): 11:59:02  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:59:02.581230 (Thread-3): 11:59:02  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomerStg`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT * FROM (
  SELECT
    customerid,
    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) taxid,
    status,
    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) lastname,
    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) firstname,
    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) middleinitial,
    coalesce(gender, last_value(gender) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) gender,
    coalesce(tier, last_value(tier) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) tier,
    coalesce(dob, last_value(dob) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) dob,
    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline1,
    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline2,
    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) postalcode,
    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) CITY,
    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) stateprov,
    coalesce(country, last_value(country) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) country,
    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone1,
    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone2,
    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone3,
    coalesce(email1, last_value(email1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email1,
    coalesce(email2, last_value(email2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email2,
    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) LCL_TX_ID,
    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) NAT_TX_ID,
    batchid,
    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,
    date(update_ts) effectivedate,
    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate
  FROM (
    SELECT
      customerid,
      taxid,
      status,
      lastname,
      firstname,
      middleinitial,
      gender,
      tier,
      dob,
      addressline1,
      addressline2,
      postalcode,
      city,
      stateprov,
      country,
      phone1,
      phone2,
      phone3,
      email1,
      email2,
      lcl_tx_id,
      nat_tx_id,
      1 batchid,
      update_ts
    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')
    UNION ALL
    SELECT
      c.customerid,
      nullif(c.taxid, '') taxid,
      nullif(s.st_name, '') as status,
      nullif(c.lastname, '') lastname,
      nullif(c.firstname, '') firstname,
      nullif(c.middleinitial, '') middleinitial,
      gender,
      c.tier,
      c.dob,
      nullif(c.addressline1, '') addressline1,
      nullif(c.addressline2, '') addressline2,
      nullif(c.postalcode, '') postalcode,
      nullif(c.city, '') city,
      nullif(c.stateprov, '') stateprov,
      nullif(c.country, '') country,
      CASE
        WHEN isnull(c_local_1) then c_local_1
        ELSE concat(
          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),
          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),
          c_local_1,
          nvl(c_ext_1, '')) END as phone1,
      CASE
        WHEN isnull(c_local_2) then c_local_2
        ELSE concat(
          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),
          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),
          c_local_2,
          nvl(c_ext_2, '')) END as phone2,
      CASE
        WHEN isnull(c_local_3) then c_local_3
        ELSE concat(
          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),
          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),
          c_local_3,
          nvl(c_ext_3, '')) END as phone3,
      nullif(c.email1, '') email1,
      nullif(c.email2, '') email2,
      c.LCL_TX_ID, 
      c.NAT_TX_ID,
      c.batchid,
      timestamp(bd.batchdate) update_ts
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CustomerIncremental` c
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
      ON c.batchid = bd.batchid
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` s 
      ON c.status = s.st_id
  ) c
  )
  
2023-04-20 11:59:03.326558 (Thread-640): handling poll request
2023-04-20 11:59:03.327087 (Thread-640): 11:59:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be69190>]}
2023-04-20 11:59:03.328435 (Thread-640): sending response (<Response 38348 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:04.114576 (Thread-641): handling ps request
2023-04-20 11:59:04.115070 (Thread-641): 11:59:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be69280>]}
2023-04-20 11:59:04.116472 (Thread-641): sending response (<Response 17675 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:04.575978 (Thread-642): handling status request
2023-04-20 11:59:04.576466 (Thread-642): 11:59:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be69520>]}
2023-04-20 11:59:04.577058 (Thread-642): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:04.957989 (Thread-643): handling poll request
2023-04-20 11:59:04.958514 (Thread-643): 11:59:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be694c0>]}
2023-04-20 11:59:04.959028 (Thread-643): sending response (<Response 291 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:05.463010 (Thread-1): 11:59:05  SQL status: OK in 2.9 seconds
2023-04-20 11:59:05.489868 (Thread-1): 11:59:05  Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 2023-04-20 11:59:01.727445 => 2023-04-20 11:59:05.489680
2023-04-20 11:59:05.490122 (Thread-1): 11:59:05  On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
2023-04-20 11:59:05.490268 (Thread-1): 11:59:05  Databricks adapter: NotImplemented: rollback
2023-04-20 11:59:05.490441 (Thread-1): 11:59:05  On model.dbsql_dbt_tpch.DimBroker: Close
2023-04-20 11:59:05.525584 (Thread-2): 11:59:05  SQL status: OK in 2.96 seconds
2023-04-20 11:59:05.527928 (Thread-2): 11:59:05  Timing info for model.dbsql_dbt_tpch.DimCompany (execute): 2023-04-20 11:59:01.722014 => 2023-04-20 11:59:05.527783
2023-04-20 11:59:05.528125 (Thread-2): 11:59:05  On model.dbsql_dbt_tpch.DimCompany: ROLLBACK
2023-04-20 11:59:05.528301 (Thread-2): 11:59:05  Databricks adapter: NotImplemented: rollback
2023-04-20 11:59:05.528514 (Thread-2): 11:59:05  On model.dbsql_dbt_tpch.DimCompany: Close
2023-04-20 11:59:05.785378 (Thread-1): 11:59:05  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa5ea3ce-4125-45b8-82ef-218ceeeb7fce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f144660e6a0>]}
2023-04-20 11:59:05.785944 (Thread-1): 11:59:05  1 of 15 OK created sql table model dbt_shabbirkdb.DimBroker .................... [OK in 4.09s]
2023-04-20 11:59:05.787128 (Thread-1): 11:59:05  Finished running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:59:05.797433 (Thread-2): 11:59:05  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa5ea3ce-4125-45b8-82ef-218ceeeb7fce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13fce610d0>]}
2023-04-20 11:59:05.797913 (Thread-2): 11:59:05  2 of 15 OK created sql table model dbt_shabbirkdb.DimCompany ................... [OK in 4.10s]
2023-04-20 11:59:05.798147 (Thread-2): 11:59:05  Finished running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:59:05.799043 (Thread-1): 11:59:05  Began running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:59:05.799404 (Thread-1): 11:59:05  5 of 15 START sql table model dbt_shabbirkdb.DimSecurity ....................... [RUN]
2023-04-20 11:59:05.799963 (Thread-1): 11:59:05  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimSecurity'
2023-04-20 11:59:05.800155 (Thread-1): 11:59:05  Began compiling node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:59:05.804212 (Thread-2): 11:59:05  Began running node model.dbsql_dbt_tpch.Financial
2023-04-20 11:59:05.804630 (Thread-2): 11:59:05  6 of 15 START sql table model dbt_shabbirkdb.Financial ......................... [RUN]
2023-04-20 11:59:05.805348 (Thread-2): 11:59:05  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Financial'
2023-04-20 11:59:05.805616 (Thread-2): 11:59:05  Began compiling node model.dbsql_dbt_tpch.Financial
2023-04-20 11:59:05.810498 (Thread-1): 11:59:05  Writing injected SQL for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:59:05.813332 (Thread-2): 11:59:05  Writing injected SQL for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:59:05.827959 (Thread-2): 11:59:05  Timing info for model.dbsql_dbt_tpch.Financial (compile): 2023-04-20 11:59:05.805685 => 2023-04-20 11:59:05.827767
2023-04-20 11:59:05.828246 (Thread-2): 11:59:05  Began executing node model.dbsql_dbt_tpch.Financial
2023-04-20 11:59:05.833373 (Thread-1): 11:59:05  Timing info for model.dbsql_dbt_tpch.DimSecurity (compile): 2023-04-20 11:59:05.800222 => 2023-04-20 11:59:05.833196
2023-04-20 11:59:05.833591 (Thread-1): 11:59:05  Began executing node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:59:05.837976 (Thread-1): 11:59:05  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:59:05.838157 (Thread-1): 11:59:05  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:59:05.839113 (Thread-2): 11:59:05  Writing runtime sql for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:59:05.839336 (Thread-1): 11:59:05  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

      describe extended `dbt_shabbirkdb`.`dimsecurity`
  
2023-04-20 11:59:05.839496 (Thread-1): 11:59:05  Opening a new connection, currently in state closed
2023-04-20 11:59:05.854122 (Thread-2): 11:59:05  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:59:05.854317 (Thread-2): 11:59:05  Using databricks connection "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:59:05.854533 (Thread-2): 11:59:05  On model.dbsql_dbt_tpch.Financial: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 11:59:05.854664 (Thread-2): 11:59:05  Opening a new connection, currently in state closed
2023-04-20 11:59:06.314810 (Thread-3): 11:59:06  SQL status: OK in 3.73 seconds
2023-04-20 11:59:06.317478 (Thread-3): 11:59:06  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (execute): 2023-04-20 11:59:01.773737 => 2023-04-20 11:59:06.317325
2023-04-20 11:59:06.317686 (Thread-3): 11:59:06  On model.dbsql_dbt_tpch.DimCustomerStg: ROLLBACK
2023-04-20 11:59:06.317827 (Thread-3): 11:59:06  Databricks adapter: NotImplemented: rollback
2023-04-20 11:59:06.317963 (Thread-3): 11:59:06  On model.dbsql_dbt_tpch.DimCustomerStg: Close
2023-04-20 11:59:06.576020 (Thread-1): 11:59:06  SQL status: OK in 0.74 seconds
2023-04-20 11:59:06.581468 (Thread-1): 11:59:06  Writing runtime sql for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:59:06.585548 (Thread-644): handling poll request
2023-04-20 11:59:06.586235 (Thread-644): 11:59:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be69400>]}
2023-04-20 11:59:06.587165 (Thread-644): sending response (<Response 20913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:06.596694 (Thread-1): 11:59:06  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:59:06.596991 (Thread-1): 11:59:06  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 11:59:06.610359 (Thread-3): 11:59:06  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa5ea3ce-4125-45b8-82ef-218ceeeb7fce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1446613a00>]}
2023-04-20 11:59:06.610846 (Thread-3): 11:59:06  3 of 15 OK created sql table model dbt_shabbirkdb.DimCustomerStg ............... [OK in 4.91s]
2023-04-20 11:59:06.611083 (Thread-3): 11:59:06  Finished running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:59:06.611895 (Thread-3): 11:59:06  Began running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:59:06.612191 (Thread-3): 11:59:06  7 of 15 START sql table model dbt_shabbirkdb.DimAccount ........................ [RUN]
2023-04-20 11:59:06.612679 (Thread-3): 11:59:06  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimAccount'
2023-04-20 11:59:06.612860 (Thread-3): 11:59:06  Began compiling node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:59:06.617993 (Thread-3): 11:59:06  Writing injected SQL for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:59:06.632273 (Thread-3): 11:59:06  Timing info for model.dbsql_dbt_tpch.DimAccount (compile): 2023-04-20 11:59:06.612905 => 2023-04-20 11:59:06.632106
2023-04-20 11:59:06.632501 (Thread-3): 11:59:06  Began executing node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:59:06.636872 (Thread-3): 11:59:06  Writing runtime sql for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:59:06.650436 (Thread-3): 11:59:06  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:59:06.650628 (Thread-3): 11:59:06  Using databricks connection "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:59:06.650935 (Thread-3): 11:59:06  On model.dbsql_dbt_tpch.DimAccount: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 11:59:06.651078 (Thread-3): 11:59:06  Opening a new connection, currently in state closed
2023-04-20 11:59:06.715468 (Thread-645): handling ps request
2023-04-20 11:59:06.716000 (Thread-645): 11:59:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be77340>]}
2023-04-20 11:59:06.717370 (Thread-645): sending response (<Response 17676 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:07.168002 (Thread-646): handling status request
2023-04-20 11:59:07.168503 (Thread-646): 11:59:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be77700>]}
2023-04-20 11:59:07.201229 (Thread-646): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:07.439465 (Thread-3): 11:59:07  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 11:59:07.439730 (Thread-3): 11:59:07  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:59:07.439870 (Thread-3): 11:59:07  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:59:07.440071 (Thread-3): 11:59:07  Databricks adapter: operation-id: b'\x01\xed\xdfr\xc0\xc3\x1d\xa1\x81\x9f;;\xceBlk'
2023-04-20 11:59:07.440355 (Thread-3): 11:59:07  Timing info for model.dbsql_dbt_tpch.DimAccount (execute): 2023-04-20 11:59:06.632552 => 2023-04-20 11:59:07.440208
2023-04-20 11:59:07.440548 (Thread-3): 11:59:07  On model.dbsql_dbt_tpch.DimAccount: ROLLBACK
2023-04-20 11:59:07.440674 (Thread-3): 11:59:07  Databricks adapter: NotImplemented: rollback
2023-04-20 11:59:07.440819 (Thread-3): 11:59:07  On model.dbsql_dbt_tpch.DimAccount: Close
2023-04-20 11:59:07.711138 (Thread-3): 11:59:07  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:59:07.711723 (Thread-3): 11:59:07  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa5ea3ce-4125-45b8-82ef-218ceeeb7fce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14465eff70>]}
2023-04-20 11:59:07.712169 (Thread-3): 11:59:07  7 of 15 ERROR creating sql table model dbt_shabbirkdb.DimAccount ............... [ERROR in 1.10s]
2023-04-20 11:59:07.712379 (Thread-3): 11:59:07  Finished running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:59:07.712684 (Thread-3): 11:59:07  Began running node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:59:07.712971 (Thread-3): 11:59:07  8 of 15 START sql table model dbt_shabbirkdb.Prospect .......................... [RUN]
2023-04-20 11:59:07.713499 (Thread-3): 11:59:07  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Prospect'
2023-04-20 11:59:07.713678 (Thread-3): 11:59:07  Began compiling node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:59:07.720692 (Thread-3): 11:59:07  Writing injected SQL for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:59:07.736589 (Thread-3): 11:59:07  Timing info for model.dbsql_dbt_tpch.Prospect (compile): 2023-04-20 11:59:07.713722 => 2023-04-20 11:59:07.736425
2023-04-20 11:59:07.736810 (Thread-3): 11:59:07  Began executing node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:59:07.741235 (Thread-3): 11:59:07  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:59:07.741408 (Thread-3): 11:59:07  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:59:07.741566 (Thread-3): 11:59:07  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

      describe extended `dbt_shabbirkdb`.`prospect`
  
2023-04-20 11:59:07.741693 (Thread-3): 11:59:07  Opening a new connection, currently in state closed
2023-04-20 11:59:08.202796 (Thread-647): handling poll request
2023-04-20 11:59:08.203296 (Thread-647): 11:59:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be57a90>]}
2023-04-20 11:59:08.204328 (Thread-647): sending response (<Response 28695 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:08.485553 (Thread-3): 11:59:08  SQL status: OK in 0.74 seconds
2023-04-20 11:59:08.491072 (Thread-3): 11:59:08  Writing runtime sql for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:59:08.506192 (Thread-3): 11:59:08  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:59:08.506533 (Thread-3): 11:59:08  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Prospect`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  agencyid,
  recdate.sk_dateid sk_recorddateid,
  origdate.sk_dateid sk_updatedateid,
  p.batchid,
  nvl2(c.customerid, True, False) iscustomer, 
  p.lastname,
  p.firstname,
  p.middleinitial,
  p.gender,
  p.addressline1,
  p.addressline2,
  p.postalcode,
  city,
  state,
  country,
  phone,
  income,
  numbercars,
  numberchildren,
  maritalstatus,
  age,
  creditrating,
  ownorrentflag,
  employer,
  numbercreditcards,
  networth,
  if(
    isnotnull(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+","")),
    left(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+",""),
      length(
        if(networth > 1000000 or income > 200000,"HighValue+","") || 
        if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
        if(age > 45, "Boomer+", "") ||
        if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
        if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
        if(age < 25 and networth > 1000000, "Inherited+",""))
      -1),
    NULL) marketingnameplate
FROM (
  SELECT 
    * FROM (
    SELECT
      agencyid,
      max(batchid) recordbatchid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth,
      min(batchid) batchid
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`ProspectRaw` p
    GROUP BY
      agencyid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth)
  QUALIFY ROW_NUMBER() OVER (PARTITION BY agencyid ORDER BY batchid DESC) = 1) p
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) recdate
  ON p.recordbatchid = recdate.batchid
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) origdate
  ON p.batchid = origdate.batchid
LEFT JOIN (
  SELECT 
    customerid,
    lastname,
    firstname,
    addressline1,
    addressline2,
    postalcode
  FROM `dbt_shabbirkdb`.`DimCustomerStg`
  WHERE iscurrent) c
  ON 
    upper(p.LastName) = upper(c.lastname)
    and upper(p.FirstName) = upper(c.firstname)
    and upper(p.AddressLine1) = upper(c.addressline1)
    and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
    and upper(p.PostalCode) = upper(c.postalcode)
  
2023-04-20 11:59:09.030596 (Thread-4): 11:59:09  SQL status: OK in 6.45 seconds
2023-04-20 11:59:09.299778 (Thread-648): handling ps request
2023-04-20 11:59:09.300271 (Thread-648): 11:59:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be573a0>]}
2023-04-20 11:59:09.301653 (Thread-648): sending response (<Response 17676 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:09.337798 (Thread-4): 11:59:09  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (execute): 2023-04-20 11:59:01.758219 => 2023-04-20 11:59:09.337617
2023-04-20 11:59:09.338079 (Thread-4): 11:59:09  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: ROLLBACK
2023-04-20 11:59:09.338227 (Thread-4): 11:59:09  Databricks adapter: NotImplemented: rollback
2023-04-20 11:59:09.338355 (Thread-4): 11:59:09  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: Close
2023-04-20 11:59:09.602248 (Thread-4): 11:59:09  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa5ea3ce-4125-45b8-82ef-218ceeeb7fce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13fcd86100>]}
2023-04-20 11:59:09.602769 (Thread-4): 11:59:09  4 of 15 OK created sql table model dbt_shabbirkdb.tempDailyMarketHistorical .... [OK in 7.89s]
2023-04-20 11:59:09.603006 (Thread-4): 11:59:09  Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:59:09.603296 (Thread-4): 11:59:09  Began running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 11:59:09.603543 (Thread-4): 11:59:09  9 of 15 SKIP relation dbt_shabbirkdb.FactCashBalances .......................... [SKIP]
2023-04-20 11:59:09.603738 (Thread-4): 11:59:09  Finished running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 11:59:09.621931 (Thread-1): 11:59:09  SQL status: OK in 3.02 seconds
2023-04-20 11:59:09.624158 (Thread-1): 11:59:09  Timing info for model.dbsql_dbt_tpch.DimSecurity (execute): 2023-04-20 11:59:05.833640 => 2023-04-20 11:59:09.624022
2023-04-20 11:59:09.624345 (Thread-1): 11:59:09  On model.dbsql_dbt_tpch.DimSecurity: ROLLBACK
2023-04-20 11:59:09.624485 (Thread-1): 11:59:09  Databricks adapter: NotImplemented: rollback
2023-04-20 11:59:09.624608 (Thread-1): 11:59:09  On model.dbsql_dbt_tpch.DimSecurity: Close
2023-04-20 11:59:09.753928 (Thread-649): handling status request
2023-04-20 11:59:09.754387 (Thread-649): 11:59:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be57e50>]}
2023-04-20 11:59:09.754983 (Thread-649): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:09.846503 (Thread-650): handling poll request
2023-04-20 11:59:09.847045 (Thread-650): 11:59:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be57790>]}
2023-04-20 11:59:09.847989 (Thread-650): sending response (<Response 12771 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:09.930372 (Thread-1): 11:59:09  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa5ea3ce-4125-45b8-82ef-218ceeeb7fce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13fce656d0>]}
2023-04-20 11:59:09.930894 (Thread-1): 11:59:09  5 of 15 OK created sql table model dbt_shabbirkdb.DimSecurity .................. [OK in 4.13s]
2023-04-20 11:59:09.931125 (Thread-1): 11:59:09  Finished running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:59:09.932049 (Thread-4): 11:59:09  Began running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 11:59:09.932340 (Thread-4): 11:59:09  10 of 15 SKIP relation dbt_shabbirkdb.DimTrade ................................. [SKIP]
2023-04-20 11:59:09.932542 (Thread-4): 11:59:09  Finished running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 11:59:09.933161 (Thread-4): 11:59:09  Began running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 11:59:09.933396 (Thread-4): 11:59:09  11 of 15 SKIP relation dbt_shabbirkdb.FactHoldings ............................. [SKIP]
2023-04-20 11:59:09.933578 (Thread-4): 11:59:09  Finished running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 11:59:11.497569 (Thread-651): handling poll request
2023-04-20 11:59:11.498077 (Thread-651): 11:59:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd6a490>]}
2023-04-20 11:59:11.498675 (Thread-651): sending response (<Response 4877 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:11.968853 (Thread-652): handling ps request
2023-04-20 11:59:11.969343 (Thread-652): 11:59:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd6a580>]}
2023-04-20 11:59:11.970726 (Thread-652): sending response (<Response 17676 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:12.337208 (Thread-3): 11:59:12  SQL status: OK in 3.83 seconds
2023-04-20 11:59:12.339666 (Thread-3): 11:59:12  Timing info for model.dbsql_dbt_tpch.Prospect (execute): 2023-04-20 11:59:07.736860 => 2023-04-20 11:59:12.339474
2023-04-20 11:59:12.339870 (Thread-3): 11:59:12  On model.dbsql_dbt_tpch.Prospect: ROLLBACK
2023-04-20 11:59:12.340039 (Thread-3): 11:59:12  Databricks adapter: NotImplemented: rollback
2023-04-20 11:59:12.340172 (Thread-3): 11:59:12  On model.dbsql_dbt_tpch.Prospect: Close
2023-04-20 11:59:12.415233 (Thread-653): handling status request
2023-04-20 11:59:12.415750 (Thread-653): 11:59:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd6a940>]}
2023-04-20 11:59:12.416353 (Thread-653): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:12.617413 (Thread-3): 11:59:12  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa5ea3ce-4125-45b8-82ef-218ceeeb7fce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13fd0df580>]}
2023-04-20 11:59:12.617954 (Thread-3): 11:59:12  8 of 15 OK created sql table model dbt_shabbirkdb.Prospect ..................... [OK in 4.90s]
2023-04-20 11:59:12.618188 (Thread-3): 11:59:12  Finished running node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:59:12.619073 (Thread-4): 11:59:12  Began running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:59:12.619423 (Thread-4): 11:59:12  12 of 15 START sql table model dbt_shabbirkdb.DimCustomer ...................... [RUN]
2023-04-20 11:59:12.619988 (Thread-4): 11:59:12  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomer'
2023-04-20 11:59:12.620177 (Thread-4): 11:59:12  Began compiling node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:59:12.625218 (Thread-4): 11:59:12  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:59:12.639807 (Thread-4): 11:59:12  Timing info for model.dbsql_dbt_tpch.DimCustomer (compile): 2023-04-20 11:59:12.620222 => 2023-04-20 11:59:12.639642
2023-04-20 11:59:12.640030 (Thread-4): 11:59:12  Began executing node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:59:12.644104 (Thread-4): 11:59:12  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:59:12.658639 (Thread-4): 11:59:12  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:59:12.658824 (Thread-4): 11:59:12  Using databricks connection "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:59:12.659042 (Thread-4): 11:59:12  On model.dbsql_dbt_tpch.DimCustomer: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 11:59:12.659178 (Thread-4): 11:59:12  Opening a new connection, currently in state closed
2023-04-20 11:59:12.827637 (Thread-2): 11:59:12  SQL status: OK in 6.97 seconds
2023-04-20 11:59:13.129271 (Thread-2): 11:59:13  Timing info for model.dbsql_dbt_tpch.Financial (execute): 2023-04-20 11:59:05.828308 => 2023-04-20 11:59:13.129085
2023-04-20 11:59:13.129541 (Thread-2): 11:59:13  On model.dbsql_dbt_tpch.Financial: ROLLBACK
2023-04-20 11:59:13.129688 (Thread-2): 11:59:13  Databricks adapter: NotImplemented: rollback
2023-04-20 11:59:13.129810 (Thread-2): 11:59:13  On model.dbsql_dbt_tpch.Financial: Close
2023-04-20 11:59:13.136829 (Thread-654): handling poll request
2023-04-20 11:59:13.137272 (Thread-654): 11:59:13  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd771c0>]}
2023-04-20 11:59:13.137992 (Thread-654): sending response (<Response 11642 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:13.390640 (Thread-4): 11:59:13  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 11:59:13.390870 (Thread-4): 11:59:13  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:59:13.391001 (Thread-4): 11:59:13  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:59:13.391116 (Thread-4): 11:59:13  Databricks adapter: operation-id: b'\x01\xed\xdfr\xc4Y\x13\x1f\xb2\xa4\x1e\x81n\x81:#'
2023-04-20 11:59:13.391362 (Thread-4): 11:59:13  Timing info for model.dbsql_dbt_tpch.DimCustomer (execute): 2023-04-20 11:59:12.640081 => 2023-04-20 11:59:13.391224
2023-04-20 11:59:13.391584 (Thread-4): 11:59:13  On model.dbsql_dbt_tpch.DimCustomer: ROLLBACK
2023-04-20 11:59:13.391733 (Thread-4): 11:59:13  Databricks adapter: NotImplemented: rollback
2023-04-20 11:59:13.391855 (Thread-4): 11:59:13  On model.dbsql_dbt_tpch.DimCustomer: Close
2023-04-20 11:59:13.406364 (Thread-2): 11:59:13  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa5ea3ce-4125-45b8-82ef-218ceeeb7fce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13fcd6d850>]}
2023-04-20 11:59:13.406861 (Thread-2): 11:59:13  6 of 15 OK created sql table model dbt_shabbirkdb.Financial .................... [OK in 7.60s]
2023-04-20 11:59:13.407093 (Thread-2): 11:59:13  Finished running node model.dbsql_dbt_tpch.Financial
2023-04-20 11:59:13.407882 (Thread-3): 11:59:13  Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 11:59:13.408199 (Thread-3): 11:59:13  13 of 15 START sql table model dbt_shabbirkdb.tempSumpFiBasicEps ............... [RUN]
2023-04-20 11:59:13.408711 (Thread-3): 11:59:13  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempSumpFiBasicEps'
2023-04-20 11:59:13.408897 (Thread-3): 11:59:13  Began compiling node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 11:59:13.414272 (Thread-3): 11:59:13  Writing injected SQL for node "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 11:59:13.424561 (Thread-3): 11:59:13  Timing info for model.dbsql_dbt_tpch.tempSumpFiBasicEps (compile): 2023-04-20 11:59:13.408943 => 2023-04-20 11:59:13.424330
2023-04-20 11:59:13.424891 (Thread-3): 11:59:13  Began executing node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 11:59:13.431207 (Thread-3): 11:59:13  Writing runtime sql for node "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 11:59:13.442238 (Thread-3): 11:59:13  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:59:13.442434 (Thread-3): 11:59:13  Using databricks connection "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 11:59:13.442610 (Thread-3): 11:59:13  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempSumpFiBasicEps"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempSumpFiBasicEps`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  sk_companyid,
  fi_qtr_start_date,
  sum(fi_basic_eps) OVER (PARTITION BY companyid ORDER BY fi_qtr_start_date ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) - fi_basic_eps sum_fi_basic_eps
FROM `dbt_shabbirkdb`.`Financial`
JOIN `dbt_shabbirkdb`.`DimCompany`
  USING (sk_companyid);
  
2023-04-20 11:59:13.442747 (Thread-3): 11:59:13  Opening a new connection, currently in state closed
2023-04-20 11:59:13.670127 (Thread-4): 11:59:13  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:59:13.670534 (Thread-4): 11:59:13  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa5ea3ce-4125-45b8-82ef-218ceeeb7fce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13fe3ddd30>]}
2023-04-20 11:59:13.671138 (Thread-4): 11:59:13  12 of 15 ERROR creating sql table model dbt_shabbirkdb.DimCustomer ............. [ERROR in 1.05s]
2023-04-20 11:59:13.671353 (Thread-4): 11:59:13  Finished running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:59:13.672233 (Thread-2): 11:59:13  Began running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 11:59:13.672528 (Thread-2): 11:59:13  14 of 15 SKIP relation dbt_shabbirkdb.FactWatches .............................. [SKIP]
2023-04-20 11:59:13.672746 (Thread-2): 11:59:13  Finished running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 11:59:14.590452 (Thread-655): handling ps request
2023-04-20 11:59:14.590943 (Thread-655): 11:59:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd77e50>]}
2023-04-20 11:59:14.592602 (Thread-655): sending response (<Response 17676 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:14.764470 (Thread-656): handling poll request
2023-04-20 11:59:14.764898 (Thread-656): 11:59:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd77fd0>]}
2023-04-20 11:59:14.765671 (Thread-656): sending response (<Response 18825 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:15.012437 (Thread-657): handling status request
2023-04-20 11:59:15.012921 (Thread-657): 11:59:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd6afa0>]}
2023-04-20 11:59:15.013522 (Thread-657): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:16.358580 (Thread-658): handling poll request
2023-04-20 11:59:16.359083 (Thread-658): 11:59:16  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd6adc0>]}
2023-04-20 11:59:16.359603 (Thread-658): sending response (<Response 291 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:17.126429 (Thread-3): 11:59:17  SQL status: OK in 3.68 seconds
2023-04-20 11:59:17.129215 (Thread-3): 11:59:17  Timing info for model.dbsql_dbt_tpch.tempSumpFiBasicEps (execute): 2023-04-20 11:59:13.424964 => 2023-04-20 11:59:17.129068
2023-04-20 11:59:17.129418 (Thread-3): 11:59:17  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: ROLLBACK
2023-04-20 11:59:17.129560 (Thread-3): 11:59:17  Databricks adapter: NotImplemented: rollback
2023-04-20 11:59:17.129682 (Thread-3): 11:59:17  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: Close
2023-04-20 11:59:17.201588 (Thread-659): handling ps request
2023-04-20 11:59:17.202064 (Thread-659): 11:59:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be77bb0>]}
2023-04-20 11:59:17.203432 (Thread-659): sending response (<Response 17676 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:17.413513 (Thread-3): 11:59:17  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa5ea3ce-4125-45b8-82ef-218ceeeb7fce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13fcfaef70>]}
2023-04-20 11:59:17.414054 (Thread-3): 11:59:17  13 of 15 OK created sql table model dbt_shabbirkdb.tempSumpFiBasicEps .......... [OK in 4.00s]
2023-04-20 11:59:17.414287 (Thread-3): 11:59:17  Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 11:59:17.415418 (Thread-4): 11:59:17  Began running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 11:59:17.415781 (Thread-4): 11:59:17  15 of 15 START sql table model dbt_shabbirkdb.FactMarketHistory ................ [RUN]
2023-04-20 11:59:17.416297 (Thread-4): 11:59:17  Acquiring new databricks connection 'model.dbsql_dbt_tpch.FactMarketHistory'
2023-04-20 11:59:17.416484 (Thread-4): 11:59:17  Began compiling node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 11:59:17.421223 (Thread-4): 11:59:17  Writing injected SQL for node "model.dbsql_dbt_tpch.FactMarketHistory"
2023-04-20 11:59:17.432376 (Thread-4): 11:59:17  Timing info for model.dbsql_dbt_tpch.FactMarketHistory (compile): 2023-04-20 11:59:17.416531 => 2023-04-20 11:59:17.432181
2023-04-20 11:59:17.432637 (Thread-4): 11:59:17  Began executing node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 11:59:17.436907 (Thread-4): 11:59:17  Writing runtime sql for node "model.dbsql_dbt_tpch.FactMarketHistory"
2023-04-20 11:59:17.448954 (Thread-4): 11:59:17  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:59:17.449151 (Thread-4): 11:59:17  Using databricks connection "model.dbsql_dbt_tpch.FactMarketHistory"
2023-04-20 11:59:17.449365 (Thread-4): 11:59:17  On model.dbsql_dbt_tpch.FactMarketHistory: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date);
  
2023-04-20 11:59:17.449502 (Thread-4): 11:59:17  Opening a new connection, currently in state closed
2023-04-20 11:59:17.641043 (Thread-660): handling status request
2023-04-20 11:59:17.641531 (Thread-660): 11:59:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be77b50>]}
2023-04-20 11:59:17.642146 (Thread-660): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:17.958251 (Thread-661): handling poll request
2023-04-20 11:59:17.958750 (Thread-661): 11:59:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be771f0>]}
2023-04-20 11:59:17.959436 (Thread-661): sending response (<Response 10694 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:18.129142 (Thread-4): 11:59:18  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date);
  
2023-04-20 11:59:18.129369 (Thread-4): 11:59:18  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date)

2023-04-20 11:59:18.129526 (Thread-4): 11:59:18  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date)

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date)

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 11:59:18.129644 (Thread-4): 11:59:18  Databricks adapter: operation-id: b'\x01\xed\xdfr\xc73\x1fB\x91<\x98\xf1\xda\xf1\x98%'
2023-04-20 11:59:18.129892 (Thread-4): 11:59:18  Timing info for model.dbsql_dbt_tpch.FactMarketHistory (execute): 2023-04-20 11:59:17.432693 => 2023-04-20 11:59:18.129755
2023-04-20 11:59:18.130080 (Thread-4): 11:59:18  On model.dbsql_dbt_tpch.FactMarketHistory: ROLLBACK
2023-04-20 11:59:18.130201 (Thread-4): 11:59:18  Databricks adapter: NotImplemented: rollback
2023-04-20 11:59:18.130321 (Thread-4): 11:59:18  On model.dbsql_dbt_tpch.FactMarketHistory: Close
2023-04-20 11:59:18.416242 (Thread-4): 11:59:18  Runtime Error in model FactMarketHistory (models/incremental/FactMarketHistory.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */
  
    
      
          create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
        
        
      using delta
        
        
        
        
        
        
        as
        
  SELECT 
    s.sk_securityid,
    s.sk_companyid,
    sk_dateid,
    fmh.dm_close / sum_fi_basic_eps AS peratio,
    (s.dividend / fmh.dm_close) / 100 yield,
    fiftytwoweekhigh,
    sk_fiftytwoweekhighdate,
    fiftytwoweeklow,
    sk_fiftytwoweeklowdate,
    dm_close closeprice,
    dm_high dayhigh,
    dm_low daylow,
    dm_vol volume,
    fmh.batchid
  FROM (
    SELECT * FROM (
      SELECT 
        a.*,
        b.sk_dateid AS sk_fiftytwoweeklowdate,
        c.sk_dateid AS sk_fiftytwoweekhighdate
      FROM
        `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
      JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
        ON
          a.dm_s_symb = b.dm_s_symb
          AND a.fiftytwoweeklow = b.dm_low
          AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
      JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
        ON 
          a.dm_s_symb = c.dm_s_symb
          AND a.fiftytwoweekhigh = c.dm_high
          AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
    QUALIFY ROW_NUMBER() OVER (
      PARTITION BY dm_s_symb, dm_date 
      ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
  -- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
  ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ^^^
    ON 
      s.symbol = fmh.dm_s_symb
      AND fmh.dm_date >= s.effectivedate 
      AND fmh.dm_date < s.enddate
  LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
    ON 
      f.sk_companyid = s.sk_companyid
      AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
      AND year(fmh.dm_date) = year(fi_qtr_start_date)
  
2023-04-20 11:59:18.416654 (Thread-4): 11:59:18  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa5ea3ce-4125-45b8-82ef-218ceeeb7fce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13fd043310>]}
2023-04-20 11:59:18.417115 (Thread-4): 11:59:18  15 of 15 ERROR creating sql table model dbt_shabbirkdb.FactMarketHistory ....... [ERROR in 1.00s]
2023-04-20 11:59:18.417328 (Thread-4): 11:59:18  Finished running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 11:59:18.419208 (MainThread): 11:59:18  Acquiring new databricks connection 'master'
2023-04-20 11:59:18.419439 (MainThread): 11:59:18  On master: ROLLBACK
2023-04-20 11:59:18.419641 (MainThread): 11:59:18  Opening a new connection, currently in state init
2023-04-20 11:59:18.703623 (MainThread): 11:59:18  Databricks adapter: NotImplemented: rollback
2023-04-20 11:59:18.703864 (MainThread): 11:59:18  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:59:18.704003 (MainThread): 11:59:18  Spark adapter: NotImplemented: commit
2023-04-20 11:59:18.704162 (MainThread): 11:59:18  On master: ROLLBACK
2023-04-20 11:59:18.704294 (MainThread): 11:59:18  Databricks adapter: NotImplemented: rollback
2023-04-20 11:59:18.704430 (MainThread): 11:59:18  On master: Close
2023-04-20 11:59:18.975254 (MainThread): 11:59:18  Connection 'master' was properly closed.
2023-04-20 11:59:18.975448 (MainThread): 11:59:18  Connection 'model.dbsql_dbt_tpch.DimSecurity' was properly closed.
2023-04-20 11:59:18.975590 (MainThread): 11:59:18  Connection 'model.dbsql_dbt_tpch.Financial' was properly closed.
2023-04-20 11:59:18.975698 (MainThread): 11:59:18  Connection 'model.dbsql_dbt_tpch.tempSumpFiBasicEps' was properly closed.
2023-04-20 11:59:18.975798 (MainThread): 11:59:18  Connection 'model.dbsql_dbt_tpch.FactMarketHistory' was properly closed.
2023-04-20 11:59:18.976122 (MainThread): 11:59:18  
2023-04-20 11:59:18.976273 (MainThread): 11:59:18  Finished running 15 table models in 0 hours 0 minutes and 20.15 seconds (20.15s).
2023-04-20 11:59:19.058512 (MainThread): 11:59:19  
2023-04-20 11:59:19.058751 (MainThread): 11:59:19  Completed with 3 errors and 0 warnings:
2023-04-20 11:59:19.058885 (MainThread): 11:59:19  
2023-04-20 11:59:19.059027 (MainThread): 11:59:19  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
2023-04-20 11:59:19.059154 (MainThread): 11:59:19    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:59:19.059266 (MainThread): 11:59:19  
2023-04-20 11:59:19.059387 (MainThread): 11:59:19  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
2023-04-20 11:59:19.059499 (MainThread): 11:59:19    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:59:19.059642 (MainThread): 11:59:19  
2023-04-20 11:59:19.059770 (MainThread): 11:59:19  Runtime Error in model FactMarketHistory (models/incremental/FactMarketHistory.sql)
2023-04-20 11:59:19.059881 (MainThread): 11:59:19    
2023-04-20 11:59:19.059984 (MainThread): 11:59:19    [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)
2023-04-20 11:59:19.060085 (MainThread): 11:59:19    
2023-04-20 11:59:19.060185 (MainThread): 11:59:19    == SQL ==
2023-04-20 11:59:19.060303 (MainThread): 11:59:19    /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */
2023-04-20 11:59:19.060406 (MainThread): 11:59:19    
2023-04-20 11:59:19.060505 (MainThread): 11:59:19      
2023-04-20 11:59:19.060603 (MainThread): 11:59:19        
2023-04-20 11:59:19.060704 (MainThread): 11:59:19            create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
2023-04-20 11:59:19.060804 (MainThread): 11:59:19          
2023-04-20 11:59:19.060903 (MainThread): 11:59:19          
2023-04-20 11:59:19.061002 (MainThread): 11:59:19        using delta
2023-04-20 11:59:19.061099 (MainThread): 11:59:19          
2023-04-20 11:59:19.061197 (MainThread): 11:59:19          
2023-04-20 11:59:19.061294 (MainThread): 11:59:19          
2023-04-20 11:59:19.061389 (MainThread): 11:59:19          
2023-04-20 11:59:19.061486 (MainThread): 11:59:19          
2023-04-20 11:59:19.061585 (MainThread): 11:59:19          
2023-04-20 11:59:19.061682 (MainThread): 11:59:19          as
2023-04-20 11:59:19.061779 (MainThread): 11:59:19          
2023-04-20 11:59:19.061886 (MainThread): 11:59:19    SELECT 
2023-04-20 11:59:19.061985 (MainThread): 11:59:19      s.sk_securityid,
2023-04-20 11:59:19.062080 (MainThread): 11:59:19      s.sk_companyid,
2023-04-20 11:59:19.062178 (MainThread): 11:59:19      sk_dateid,
2023-04-20 11:59:19.062275 (MainThread): 11:59:19      fmh.dm_close / sum_fi_basic_eps AS peratio,
2023-04-20 11:59:19.062374 (MainThread): 11:59:19      (s.dividend / fmh.dm_close) / 100 yield,
2023-04-20 11:59:19.062473 (MainThread): 11:59:19      fiftytwoweekhigh,
2023-04-20 11:59:19.062571 (MainThread): 11:59:19      sk_fiftytwoweekhighdate,
2023-04-20 11:59:19.062668 (MainThread): 11:59:19      fiftytwoweeklow,
2023-04-20 11:59:19.062764 (MainThread): 11:59:19      sk_fiftytwoweeklowdate,
2023-04-20 11:59:19.062862 (MainThread): 11:59:19      dm_close closeprice,
2023-04-20 11:59:19.062959 (MainThread): 11:59:19      dm_high dayhigh,
2023-04-20 11:59:19.063057 (MainThread): 11:59:19      dm_low daylow,
2023-04-20 11:59:19.063156 (MainThread): 11:59:19      dm_vol volume,
2023-04-20 11:59:19.063254 (MainThread): 11:59:19      fmh.batchid
2023-04-20 11:59:19.063353 (MainThread): 11:59:19    FROM (
2023-04-20 11:59:19.063451 (MainThread): 11:59:19      SELECT * FROM (
2023-04-20 11:59:19.063571 (MainThread): 11:59:19        SELECT 
2023-04-20 11:59:19.063792 (MainThread): 11:59:19          a.*,
2023-04-20 11:59:19.063906 (MainThread): 11:59:19          b.sk_dateid AS sk_fiftytwoweeklowdate,
2023-04-20 11:59:19.064009 (MainThread): 11:59:19          c.sk_dateid AS sk_fiftytwoweekhighdate
2023-04-20 11:59:19.064109 (MainThread): 11:59:19        FROM
2023-04-20 11:59:19.064206 (MainThread): 11:59:19          `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
2023-04-20 11:59:19.064303 (MainThread): 11:59:19        JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
2023-04-20 11:59:19.064400 (MainThread): 11:59:19          ON
2023-04-20 11:59:19.064499 (MainThread): 11:59:19            a.dm_s_symb = b.dm_s_symb
2023-04-20 11:59:19.064598 (MainThread): 11:59:19            AND a.fiftytwoweeklow = b.dm_low
2023-04-20 11:59:19.064696 (MainThread): 11:59:19            AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
2023-04-20 11:59:19.064795 (MainThread): 11:59:19        JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
2023-04-20 11:59:19.064894 (MainThread): 11:59:19          ON 
2023-04-20 11:59:19.064992 (MainThread): 11:59:19            a.dm_s_symb = c.dm_s_symb
2023-04-20 11:59:19.065089 (MainThread): 11:59:19            AND a.fiftytwoweekhigh = c.dm_high
2023-04-20 11:59:19.065186 (MainThread): 11:59:19            AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
2023-04-20 11:59:19.065282 (MainThread): 11:59:19      QUALIFY ROW_NUMBER() OVER (
2023-04-20 11:59:19.065376 (MainThread): 11:59:19        PARTITION BY dm_s_symb, dm_date 
2023-04-20 11:59:19.065472 (MainThread): 11:59:19        ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
2023-04-20 11:59:19.065567 (MainThread): 11:59:19    -- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
2023-04-20 11:59:19.065661 (MainThread): 11:59:19    ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
2023-04-20 11:59:19.065757 (MainThread): 11:59:19    ^^^
2023-04-20 11:59:19.065861 (MainThread): 11:59:19      ON 
2023-04-20 11:59:19.065960 (MainThread): 11:59:19        s.symbol = fmh.dm_s_symb
2023-04-20 11:59:19.066057 (MainThread): 11:59:19        AND fmh.dm_date >= s.effectivedate 
2023-04-20 11:59:19.066154 (MainThread): 11:59:19        AND fmh.dm_date < s.enddate
2023-04-20 11:59:19.066251 (MainThread): 11:59:19    LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
2023-04-20 11:59:19.066350 (MainThread): 11:59:19      ON 
2023-04-20 11:59:19.066447 (MainThread): 11:59:19        f.sk_companyid = s.sk_companyid
2023-04-20 11:59:19.066544 (MainThread): 11:59:19        AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
2023-04-20 11:59:19.066641 (MainThread): 11:59:19        AND year(fmh.dm_date) = year(fi_qtr_start_date)
2023-04-20 11:59:19.066739 (MainThread): 11:59:19    
2023-04-20 11:59:19.066871 (MainThread): 11:59:19  
2023-04-20 11:59:19.067002 (MainThread): 11:59:19  Done. PASS=8 WARN=0 ERROR=3 SKIP=4 TOTAL=15
2023-04-20 11:59:19.585760 (Thread-662): handling poll request
2023-04-20 11:59:19.586252 (Thread-662): 11:59:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be0cd30>]}
2023-04-20 11:59:19.589396 (Thread-662): sending response (<Response 148802 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:19.939174 (Thread-663): handling ps request
2023-04-20 11:59:19.939714 (Thread-663): 11:59:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be10910>]}
2023-04-20 11:59:19.941045 (Thread-663): sending response (<Response 17700 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:59:20.448627 (Thread-664): handling status request
2023-04-20 11:59:20.449122 (Thread-664): 11:59:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be29070>]}
2023-04-20 11:59:20.449716 (Thread-664): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:00:21.141004 (Thread-665): 12:00:21  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 12:00:21.162963 (Thread-665): 12:00:21  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 12:00:21.163335 (Thread-665): 12:00:21  previous checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, current checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886
2023-04-20 12:00:21.163569 (Thread-665): 12:00:21  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be77370>]}
2023-04-20 12:00:21.638742 (Thread-666): handling status request
2023-04-20 12:00:21.659777 (Thread-666): 12:00:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528114f70>]}
2023-04-20 12:00:21.665353 (Thread-666): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:00:22.719887 (Thread-667): handling status request
2023-04-20 12:00:22.735886 (Thread-667): 12:00:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52820d5e0>]}
2023-04-20 12:00:22.751628 (Thread-667): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:00:22.822699 (Thread-665): 12:00:22  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 12:00:22.826303 (Thread-665): 12:00:22  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 12:00:22.829789 (Thread-665): 12:00:22  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 12:00:22.832659 (Thread-665): 12:00:22  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 12:00:22.835609 (Thread-665): 12:00:22  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 12:00:22.838371 (Thread-665): 12:00:22  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 12:00:22.841733 (Thread-665): 12:00:22  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 12:00:22.844829 (Thread-665): 12:00:22  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 12:00:22.847852 (Thread-665): 12:00:22  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 12:00:22.850632 (Thread-665): 12:00:22  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 12:00:22.853669 (Thread-665): 12:00:22  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 12:00:22.856823 (Thread-665): 12:00:22  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 12:00:22.859568 (Thread-665): 12:00:22  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 12:00:22.862373 (Thread-665): 12:00:22  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 12:00:22.865264 (Thread-665): 12:00:22  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 12:00:23.018018 (Thread-665): 12:00:23  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5280e7ca0>]}
2023-04-20 12:00:23.996318 (Thread-668): handling status request
2023-04-20 12:00:23.996817 (Thread-668): 12:00:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5280e7bb0>]}
2023-04-20 12:00:23.997434 (Thread-668): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:01:05.942164 (Thread-669): 12:01:05  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 12:01:06.223788 (Thread-669): 12:01:06  Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
2023-04-20 12:01:06.224065 (Thread-669): 12:01:06  Partial parsing enabled, no changes found, skipping parsing
2023-04-20 12:01:06.231563 (Thread-669): 12:01:06  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508566880>]}
2023-04-20 12:01:06.367090 (Thread-670): handling status request
2023-04-20 12:01:06.367642 (Thread-670): 12:01:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508667550>]}
2023-04-20 12:01:06.368265 (Thread-670): sending response (<Response 1593 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:01:15.599851 (Thread-671): 12:01:15  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 12:01:15.886949 (Thread-671): 12:01:15  Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
2023-04-20 12:01:15.887580 (Thread-671): 12:01:15  Partial parsing: added file: dbsql_dbt_tpch://models/silver/AccountIncrementalStg.sql
2023-04-20 12:01:15.895480 (Thread-671): 12:01:15  1699: static parser successfully parsed silver/AccountIncrementalStg.sql
2023-04-20 12:01:15.959960 (Thread-671): 12:01:15  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508395340>]}
2023-04-20 12:01:16.027175 (Thread-672): handling status request
2023-04-20 12:01:16.027707 (Thread-672): 12:01:16  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508555af0>]}
2023-04-20 12:01:16.028221 (Thread-672): sending response (<Response 1933 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:02:46.326799 (Thread-673): handling status request
2023-04-20 12:02:46.328794 (Thread-673): 12:02:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508555880>]}
2023-04-20 12:02:46.329284 (Thread-673): sending response (<Response 1933 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:02:53.206435 (Thread-674): handling run_sql request
2023-04-20 12:02:53.206936 (Thread-674): 12:02:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508555760>]}
2023-04-20 12:02:53.233686 (Thread-675): handling ps request
2023-04-20 12:02:53.235282 (Thread-675): 12:02:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087cd7c0>]}
2023-04-20 12:02:53.267426 (Thread-675): sending response (<Response 18235 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:02:53.843823 (Thread-676): handling poll request
2023-04-20 12:02:53.844316 (Thread-676): 12:02:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087cd580>]}
2023-04-20 12:02:53.844846 (Thread-676): sending response (<Response 444 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:02:54.296258 (Thread-677): handling status request
2023-04-20 12:02:54.296748 (Thread-677): 12:02:54  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087cd8b0>]}
2023-04-20 12:02:54.297285 (Thread-677): sending response (<Response 1933 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:02:54.557989 (Thread-678): handling ps request
2023-04-20 12:02:54.558531 (Thread-678): 12:02:54  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087cd7f0>]}
2023-04-20 12:02:54.559988 (Thread-678): sending response (<Response 18235 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:02:55.949579 (Thread-674): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:02:55.984920 (MainThread): 12:02:55  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '537cb752-b06d-4419-889d-a2ef2e195547', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ab4794160>]}
2023-04-20 12:02:55.985538 (MainThread): 12:02:55  Found 16 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:02:55.986943 (Thread-1): 12:02:55  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 12:02:55.987180 (Thread-1): 12:02:55  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 12:02:55.990358 (Thread-1): 12:02:55  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 12:02:55.987227 => 2023-04-20 12:02:55.990192
2023-04-20 12:02:55.990575 (Thread-1): 12:02:55  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 12:02:55.990919 (Thread-1): 12:02:55  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 12:02:55.991359 (Thread-1): 12:02:55  On rpc.dbsql_dbt_tpch.request: SELECT 
    md5(accountid) AS sk_customerid,
    , *
FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental`
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:02:55.991509 (Thread-1): 12:02:55  Opening a new connection, currently in state init
2023-04-20 12:02:56.448895 (Thread-679): handling poll request
2023-04-20 12:02:56.449372 (Thread-679): 12:02:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087cd850>]}
2023-04-20 12:02:56.449976 (Thread-679): sending response (<Response 3676 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:02:56.577706 (Thread-680): handling ps request
2023-04-20 12:02:56.578209 (Thread-680): 12:02:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087cd9d0>]}
2023-04-20 12:02:56.579672 (Thread-680): sending response (<Response 18229 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:02:56.586733 (Thread-681): handling ps request
2023-04-20 12:02:56.587077 (Thread-681): 12:02:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087c8040>]}
2023-04-20 12:02:56.588344 (Thread-681): sending response (<Response 18230 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:02:56.633823 (Thread-1): 12:02:56  Databricks adapter: Error while running:
SELECT 
    md5(accountid) AS sk_customerid,
    , *
FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental`
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:02:56.634062 (Thread-1): 12:02:56  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 3, pos 4)

== SQL ==
SELECT 
    md5(accountid) AS sk_customerid,
    , *
----^^^
FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental`
limit 500
/* limit added automatically by dbt cloud */

2023-04-20 12:02:56.634187 (Thread-1): 12:02:56  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 3, pos 4)

== SQL ==
SELECT 
    md5(accountid) AS sk_customerid,
    , *
----^^^
FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental`
limit 500
/* limit added automatically by dbt cloud */

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 3, pos 4)

== SQL ==
SELECT 
    md5(accountid) AS sk_customerid,
    , *
----^^^
FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental`
limit 500
/* limit added automatically by dbt cloud */

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 12:02:56.634300 (Thread-1): 12:02:56  Databricks adapter: operation-id: b"\x01\xed\xdfsI{\x1e\xc3\xb2a.@r\xbf\xdb'"
2023-04-20 12:02:56.634497 (Thread-1): 12:02:56  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 12:02:55.990621 => 2023-04-20 12:02:56.634369
2023-04-20 12:02:56.634684 (Thread-1): 12:02:56  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 12:02:56.912869 (Thread-682): handling status request
2023-04-20 12:02:56.913377 (Thread-682): 12:02:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087c8310>]}
2023-04-20 12:02:56.913864 (Thread-682): sending response (<Response 1933 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:02:56.952015 (Thread-1): Got an exception: Runtime Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 3, pos 4)
  
  == SQL ==
  SELECT 
      md5(accountid) AS sk_customerid,
      , *
  ----^^^
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental`
  limit 500
  /* limit added automatically by dbt cloud */
  
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 3, pos 4)

== SQL ==
SELECT 
    md5(accountid) AS sk_customerid,
    , *
----^^^
FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental`
limit 500
/* limit added automatically by dbt cloud */


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 3, pos 4)
  
  == SQL ==
  SELECT 
      md5(accountid) AS sk_customerid,
      , *
  ----^^^
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental`
  limit 500
  /* limit added automatically by dbt cloud */
2023-04-20 12:02:56.953779 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': "Runtime Error in rpc request (from remote system.sql)\n  \n  [PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 3, pos 4)\n  \n  == SQL ==\n  SELECT \n      md5(accountid) AS sk_customerid,\n      , *\n  ----^^^\n  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental`\n  limit 500\n  /* limit added automatically by dbt cloud */\n  ", 'raw_code': "SELECT \n    md5(accountid) AS sk_customerid,\n    , *\nFROM {{source('tpcdi','AccountIncremental')}}\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': 'SELECT \n    md5(accountid) AS sk_customerid,\n    , *\nFROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental`\nlimit 500\n/* limit added automatically by dbt cloud */', 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': "Runtime Error in rpc request (from remote system.sql)\n  \n  [PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 3, pos 4)\n  \n  == SQL ==\n  SELECT \n      md5(accountid) AS sk_customerid,\n      , *\n  ----^^^\n  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental`\n  limit 500\n  /* limit added automatically by dbt cloud */\n  ", 'raw_code': "SELECT \n    md5(accountid) AS sk_customerid,\n    , *\nFROM {{source('tpcdi','AccountIncremental')}}\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': 'SELECT \n    md5(accountid) AS sk_customerid,\n    , *\nFROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental`\nlimit 500\n/* limit added automatically by dbt cloud */', 'tags': None}, None)
2023-04-20 12:02:57.380111 (Thread-683): handling poll request
2023-04-20 12:02:57.380634 (Thread-683): 12:02:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087cd4f0>]}
2023-04-20 12:02:57.381294 (Thread-683): sending response (<Response 19756 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:02:59.088522 (Thread-684): handling poll request
2023-04-20 12:02:59.089163 (Thread-684): 12:02:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087c81c0>]}
2023-04-20 12:02:59.089825 (Thread-684): sending response (<Response 19756 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:02:59.563354 (Thread-685): handling ps request
2023-04-20 12:02:59.563868 (Thread-685): 12:02:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5280e7ac0>]}
2023-04-20 12:02:59.565305 (Thread-685): sending response (<Response 18253 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:03:00.112142 (Thread-686): handling status request
2023-04-20 12:03:00.112645 (Thread-686): 12:03:00  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52811fb20>]}
2023-04-20 12:03:00.113135 (Thread-686): sending response (<Response 1933 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:03:18.780260 (Thread-687): handling ps request
2023-04-20 12:03:18.780763 (Thread-687): 12:03:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5281e86a0>]}
2023-04-20 12:03:18.782183 (Thread-687): sending response (<Response 18253 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:03:18.814406 (Thread-688): handling run_sql request
2023-04-20 12:03:18.814774 (Thread-688): 12:03:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5280b5df0>]}
2023-04-20 12:03:20.531268 (Thread-690): handling ps request
2023-04-20 12:03:20.532224 (Thread-690): 12:03:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087c8280>]}
2023-04-20 12:03:20.535257 (Thread-690): sending response (<Response 18788 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:03:20.568590 (Thread-689): 12:03:20  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 12:03:21.004962 (Thread-691): handling status request
2023-04-20 12:03:21.005661 (Thread-691): 12:03:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5622796d0>]}
2023-04-20 12:03:21.006203 (Thread-691): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:03:21.138654 (Thread-689): 12:03:21  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
2023-04-20 12:03:21.139821 (Thread-689): 12:03:21  Partial parsing: updated file: dbsql_dbt_tpch://models/silver/AccountIncrementalStg.sql
2023-04-20 12:03:21.154715 (Thread-689): 12:03:21  1699: static parser successfully parsed silver/AccountIncrementalStg.sql
2023-04-20 12:03:21.252866 (Thread-689): 12:03:21  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e10d700>]}
2023-04-20 12:03:21.611091 (Thread-688): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:03:21.645293 (MainThread): 12:03:21  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'af153ed9-52d1-4dd3-ab9a-1956e96ceb06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29c9a1c190>]}
2023-04-20 12:03:21.645898 (MainThread): 12:03:21  Found 16 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:03:21.647310 (Thread-1): 12:03:21  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 12:03:21.647572 (Thread-1): 12:03:21  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 12:03:21.650694 (Thread-1): 12:03:21  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 12:03:21.647625 => 2023-04-20 12:03:21.650527
2023-04-20 12:03:21.650912 (Thread-1): 12:03:21  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 12:03:21.651258 (Thread-1): 12:03:21  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 12:03:21.651743 (Thread-1): 12:03:21  On rpc.dbsql_dbt_tpch.request: SELECT 
    md5(accountid) AS sk_customerid,
    *
FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental`
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:03:21.651982 (Thread-1): 12:03:21  Opening a new connection, currently in state init
2023-04-20 12:03:21.798090 (Thread-692): handling poll request
2023-04-20 12:03:21.798579 (Thread-692): 12:03:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd07a90>]}
2023-04-20 12:03:21.799173 (Thread-692): sending response (<Response 3674 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:03:22.221026 (Thread-693): handling status request
2023-04-20 12:03:22.221525 (Thread-693): 12:03:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be77370>]}
2023-04-20 12:03:22.222035 (Thread-693): sending response (<Response 1935 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:03:22.233953 (Thread-694): handling ps request
2023-04-20 12:03:22.234346 (Thread-694): 12:03:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be779d0>]}
2023-04-20 12:03:22.235802 (Thread-694): sending response (<Response 18783 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:03:22.248814 (Thread-695): handling ps request
2023-04-20 12:03:22.249271 (Thread-695): 12:03:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb562270b50>]}
2023-04-20 12:03:22.250700 (Thread-695): sending response (<Response 18783 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:03:22.353379 (Thread-1): 12:03:22  Databricks adapter: Error while running:
SELECT 
    md5(accountid) AS sk_customerid,
    *
FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental`
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:03:22.353614 (Thread-1): 12:03:22  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "md5(accountid)" due to data type mismatch: parameter 1 requires "BINARY" type, however, "accountid" is of "BIGINT" type.; line 2 pos 4
2023-04-20 12:03:22.353772 (Thread-1): 12:03:22  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] org.apache.spark.sql.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "md5(accountid)" due to data type mismatch: parameter 1 requires "BINARY" type, however, "accountid" is of "BIGINT" type.; line 2 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "md5(accountid)" due to data type mismatch: parameter 1 requires "BINARY" type, however, "accountid" is of "BIGINT" type.; line 2 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 12:03:22.353929 (Thread-1): 12:03:22  Databricks adapter: operation-id: b'\x01\xed\xdfsX\xc2\x14\x0e\xa0}i\xe6A\xbfB\xe5'
2023-04-20 12:03:22.354134 (Thread-1): 12:03:22  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 12:03:21.650959 => 2023-04-20 12:03:22.354006
2023-04-20 12:03:22.354348 (Thread-1): 12:03:22  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 12:03:22.643827 (Thread-1): Got an exception: Runtime Error
  [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "md5(accountid)" due to data type mismatch: parameter 1 requires "BINARY" type, however, "accountid" is of "BIGINT" type.; line 2 pos 4
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "md5(accountid)" due to data type mismatch: parameter 1 requires "BINARY" type, however, "accountid" is of "BIGINT" type.; line 2 pos 4

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "md5(accountid)" due to data type mismatch: parameter 1 requires "BINARY" type, however, "accountid" is of "BIGINT" type.; line 2 pos 4
2023-04-20 12:03:22.645624 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "md5(accountid)" due to data type mismatch: parameter 1 requires "BINARY" type, however, "accountid" is of "BIGINT" type.; line 2 pos 4', 'raw_code': "SELECT \n    md5(accountid) AS sk_customerid,\n    *\nFROM {{source('tpcdi','AccountIncremental')}}\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': 'SELECT \n    md5(accountid) AS sk_customerid,\n    *\nFROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental`\nlimit 500\n/* limit added automatically by dbt cloud */', 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "md5(accountid)" due to data type mismatch: parameter 1 requires "BINARY" type, however, "accountid" is of "BIGINT" type.; line 2 pos 4', 'raw_code': "SELECT \n    md5(accountid) AS sk_customerid,\n    *\nFROM {{source('tpcdi','AccountIncremental')}}\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': 'SELECT \n    md5(accountid) AS sk_customerid,\n    *\nFROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental`\nlimit 500\n/* limit added automatically by dbt cloud */', 'tags': None}, None)
2023-04-20 12:03:22.806973 (Thread-696): handling status request
2023-04-20 12:03:22.807476 (Thread-696): 12:03:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52803df10>]}
2023-04-20 12:03:22.807992 (Thread-696): sending response (<Response 1935 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:03:23.059918 (Thread-697): handling poll request
2023-04-20 12:03:23.060428 (Thread-697): 12:03:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52803dca0>]}
2023-04-20 12:03:23.061051 (Thread-697): sending response (<Response 15901 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:03:24.518395 (Thread-698): handling poll request
2023-04-20 12:03:24.518952 (Thread-698): 12:03:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb562270490>]}
2023-04-20 12:03:24.519618 (Thread-698): sending response (<Response 15901 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:03:24.716451 (Thread-699): handling poll request
2023-04-20 12:03:24.716953 (Thread-699): 12:03:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52803d4f0>]}
2023-04-20 12:03:24.717586 (Thread-699): sending response (<Response 15901 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:03:25.836323 (Thread-700): handling ps request
2023-04-20 12:03:25.836834 (Thread-700): 12:03:25  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52803d340>]}
2023-04-20 12:03:25.838334 (Thread-700): sending response (<Response 18804 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:03:26.242470 (Thread-701): handling status request
2023-04-20 12:03:26.242993 (Thread-701): 12:03:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52803d1c0>]}
2023-04-20 12:03:26.243469 (Thread-701): sending response (<Response 1935 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:04:07.170716 (Thread-702): handling status request
2023-04-20 12:04:07.171248 (Thread-702): 12:04:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528125dc0>]}
2023-04-20 12:04:07.171789 (Thread-702): sending response (<Response 1935 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:04:07.318395 (Thread-703): handling ps request
2023-04-20 12:04:07.318909 (Thread-703): 12:04:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5281252b0>]}
2023-04-20 12:04:07.320384 (Thread-703): sending response (<Response 18804 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:04:18.923632 (Thread-704): handling status request
2023-04-20 12:04:18.924149 (Thread-704): 12:04:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be77430>]}
2023-04-20 12:04:18.924618 (Thread-704): sending response (<Response 1935 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:04:38.121842 (Thread-705): handling status request
2023-04-20 12:04:38.122344 (Thread-705): 12:04:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528125a00>]}
2023-04-20 12:04:38.145716 (Thread-705): sending response (<Response 1935 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:05:31.883311 (Thread-706): handling status request
2023-04-20 12:05:31.883855 (Thread-706): 12:05:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528125340>]}
2023-04-20 12:05:31.884356 (Thread-706): sending response (<Response 1935 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:05:47.286915 (Thread-707): handling ps request
2023-04-20 12:05:47.288793 (Thread-707): 12:05:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528125d60>]}
2023-04-20 12:05:47.290240 (Thread-707): sending response (<Response 18804 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:05:47.380737 (Thread-708): handling run_sql request
2023-04-20 12:05:47.381172 (Thread-708): 12:05:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5281251c0>]}
2023-04-20 12:05:50.122278 (Thread-708): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:05:50.158165 (MainThread): 12:05:50  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '59328651-66a5-4ccc-be8f-9219388a88d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71ebb492b0>]}
2023-04-20 12:05:50.158752 (MainThread): 12:05:50  Found 16 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:05:50.160173 (Thread-1): 12:05:50  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 12:05:50.160407 (Thread-1): 12:05:50  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 12:05:50.163742 (Thread-1): 12:05:50  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 12:05:50.160456 => 2023-04-20 12:05:50.163575
2023-04-20 12:05:50.163956 (Thread-1): 12:05:50  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 12:05:50.164461 (Thread-1): 12:05:50  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 12:05:50.164801 (Thread-1): 12:05:50  On rpc.dbsql_dbt_tpch.request: SELECT 
    md5(accountid::string) AS sk_customerid,
    *
FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental`
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:05:50.164964 (Thread-1): 12:05:50  Opening a new connection, currently in state init
2023-04-20 12:05:50.686715 (Thread-709): handling ps request
2023-04-20 12:05:50.687418 (Thread-709): 12:05:50  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528227af0>]}
2023-04-20 12:05:50.690926 (Thread-709): sending response (<Response 19334 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:05:50.774966 (Thread-710): handling ps request
2023-04-20 12:05:50.775404 (Thread-710): 12:05:50  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52803d7f0>]}
2023-04-20 12:05:50.776954 (Thread-710): sending response (<Response 19334 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:05:51.110245 (Thread-711): handling status request
2023-04-20 12:05:51.110756 (Thread-711): 12:05:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be77430>]}
2023-04-20 12:05:51.111301 (Thread-711): sending response (<Response 1935 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:05:51.146017 (Thread-1): 12:05:51  SQL status: OK in 0.98 seconds
2023-04-20 12:05:51.158544 (Thread-1): 12:05:51  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 12:05:50.164003 => 2023-04-20 12:05:51.158348
2023-04-20 12:05:51.158792 (Thread-1): 12:05:51  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 12:05:51.334565 (Thread-712): handling poll request
2023-04-20 12:05:51.335068 (Thread-712): 12:05:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd07430>]}
2023-04-20 12:05:51.335766 (Thread-712): sending response (<Response 4715 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:05:51.485464 (Thread-713): handling poll request
2023-04-20 12:05:51.485997 (Thread-713): 12:05:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e12acd0>]}
2023-04-20 12:05:51.486662 (Thread-713): sending response (<Response 4715 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:05:52.039685 (Thread-714): handling status request
2023-04-20 12:05:52.040181 (Thread-714): 12:05:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e12ab50>]}
2023-04-20 12:05:52.040689 (Thread-714): sending response (<Response 1935 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:05:53.050803 (Thread-715): handling poll request
2023-04-20 12:05:53.051335 (Thread-715): 12:05:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528125a90>]}
2023-04-20 12:05:53.053742 (Thread-715): sending response (<Response 30992 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:05:53.296039 (Thread-716): handling ps request
2023-04-20 12:05:53.296576 (Thread-716): 12:05:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528125a00>]}
2023-04-20 12:05:53.298059 (Thread-716): sending response (<Response 19359 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:05:53.722507 (Thread-717): handling status request
2023-04-20 12:05:53.722991 (Thread-717): 12:05:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e12aeb0>]}
2023-04-20 12:05:53.723462 (Thread-717): sending response (<Response 1935 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:05:54.032337 (Thread-718): handling poll request
2023-04-20 12:05:54.032843 (Thread-718): 12:05:54  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285dc0a0>]}
2023-04-20 12:05:54.035160 (Thread-718): sending response (<Response 35268 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:05:55.070324 (Thread-719): handling status request
2023-04-20 12:05:55.070808 (Thread-719): 12:05:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285dca60>]}
2023-04-20 12:05:55.071294 (Thread-719): sending response (<Response 1935 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:05:55.342664 (Thread-720): handling ps request
2023-04-20 12:05:55.343145 (Thread-720): 12:05:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285dc4c0>]}
2023-04-20 12:05:55.344641 (Thread-720): sending response (<Response 19359 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:05:58.077995 (Thread-721): 12:05:58  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 12:05:58.371976 (Thread-721): 12:05:58  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
2023-04-20 12:05:58.372651 (Thread-721): 12:05:58  Partial parsing: updated file: dbsql_dbt_tpch://models/silver/AccountIncrementalStg.sql
2023-04-20 12:05:58.381118 (Thread-721): 12:05:58  1699: static parser successfully parsed silver/AccountIncrementalStg.sql
2023-04-20 12:05:58.447261 (Thread-721): 12:05:58  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d11de50>]}
2023-04-20 12:05:58.503560 (Thread-722): handling status request
2023-04-20 12:05:58.504023 (Thread-722): 12:05:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54ba92b50>]}
2023-04-20 12:05:58.504501 (Thread-722): sending response (<Response 1935 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:12.372391 (Thread-723): handling status request
2023-04-20 12:06:12.372891 (Thread-723): 12:06:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54ba92ac0>]}
2023-04-20 12:06:12.373394 (Thread-723): sending response (<Response 1935 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:12.567407 (Thread-724): handling ps request
2023-04-20 12:06:12.567949 (Thread-724): 12:06:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54ba92e50>]}
2023-04-20 12:06:12.569417 (Thread-724): sending response (<Response 19359 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:12.747721 (Thread-725): handling ps request
2023-04-20 12:06:12.748184 (Thread-725): 12:06:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be461c0>]}
2023-04-20 12:06:12.749624 (Thread-725): sending response (<Response 19359 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:13.700423 (Thread-726): handling status request
2023-04-20 12:06:13.700917 (Thread-726): 12:06:13  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54ba92d90>]}
2023-04-20 12:06:13.701399 (Thread-726): sending response (<Response 1935 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:14.484640 (Thread-727): handling poll request
2023-04-20 12:06:14.485138 (Thread-727): 12:06:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54ba92820>]}
2023-04-20 12:06:14.491167 (Thread-727): sending response (<Response 289755 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:39.298832 (Thread-728): handling run_sql request
2023-04-20 12:06:39.299335 (Thread-728): 12:06:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54ba92af0>]}
2023-04-20 12:06:39.323334 (Thread-729): handling ps request
2023-04-20 12:06:39.324993 (Thread-729): 12:06:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bcc48e0>]}
2023-04-20 12:06:39.328556 (Thread-729): sending response (<Response 19882 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:40.015093 (Thread-730): handling poll request
2023-04-20 12:06:40.015617 (Thread-730): 12:06:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bcc44f0>]}
2023-04-20 12:06:40.016155 (Thread-730): sending response (<Response 432 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:40.473866 (Thread-731): handling status request
2023-04-20 12:06:40.474367 (Thread-731): 12:06:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bcc4850>]}
2023-04-20 12:06:40.474923 (Thread-731): sending response (<Response 1935 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:40.792236 (Thread-732): handling ps request
2023-04-20 12:06:40.792721 (Thread-732): 12:06:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bcc47c0>]}
2023-04-20 12:06:40.794324 (Thread-732): sending response (<Response 19882 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:42.128243 (Thread-728): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:42.163183 (MainThread): 12:06:42  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '78b1d1ab-594b-4065-9d4c-cbd982b1b492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f725e3e7250>]}
2023-04-20 12:06:42.163785 (MainThread): 12:06:42  Found 16 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:06:42.165106 (Thread-1): 12:06:42  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 12:06:42.165341 (Thread-1): 12:06:42  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 12:06:42.169934 (Thread-1): 12:06:42  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 12:06:42.165396 => 2023-04-20 12:06:42.169757
2023-04-20 12:06:42.170153 (Thread-1): 12:06:42  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 12:06:42.170594 (Thread-1): 12:06:42  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 12:06:42.170966 (Thread-1): 12:06:42  On rpc.dbsql_dbt_tpch.request: 
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `dbt_shabbirkdb`.`AccountIncrementalStg` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:06:42.171109 (Thread-1): 12:06:42  Opening a new connection, currently in state init
2023-04-20 12:06:42.579191 (Thread-733): handling poll request
2023-04-20 12:06:42.579687 (Thread-733): 12:06:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bcc4280>]}
2023-04-20 12:06:42.580303 (Thread-733): sending response (<Response 5930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:42.675176 (Thread-734): handling ps request
2023-04-20 12:06:42.675589 (Thread-734): 12:06:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bcc4d60>]}
2023-04-20 12:06:42.677014 (Thread-734): sending response (<Response 19877 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:42.687706 (Thread-735): handling ps request
2023-04-20 12:06:42.688032 (Thread-735): 12:06:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bcc4fd0>]}
2023-04-20 12:06:42.715738 (Thread-735): sending response (<Response 19877 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:42.945240 (Thread-1): 12:06:42  Databricks adapter: Error while running:

SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `dbt_shabbirkdb`.`AccountIncrementalStg` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:06:42.945546 (Thread-1): 12:06:42  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dbt_shabbirkdb`.`AccountIncrementalStg` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 57 pos 13
2023-04-20 12:06:42.945687 (Thread-1): 12:06:42  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dbt_shabbirkdb`.`AccountIncrementalStg` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 57 pos 13
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dbt_shabbirkdb`.`AccountIncrementalStg` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 57 pos 13
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 12:06:42.945802 (Thread-1): 12:06:42  Databricks adapter: operation-id: b'\x01\xed\xdfs\xd0K\x17U\x99\xe0\xc7\xa6\x92\xb5\x98\xca'
2023-04-20 12:06:42.946028 (Thread-1): 12:06:42  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 12:06:42.170200 => 2023-04-20 12:06:42.945891
2023-04-20 12:06:42.946221 (Thread-1): 12:06:42  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 12:06:43.018316 (Thread-736): handling status request
2023-04-20 12:06:43.018766 (Thread-736): 12:06:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb6c640>]}
2023-04-20 12:06:43.019230 (Thread-736): sending response (<Response 1935 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:43.258502 (Thread-1): Got an exception: Runtime Error
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dbt_shabbirkdb`.`AccountIncrementalStg` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 57 pos 13
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dbt_shabbirkdb`.`AccountIncrementalStg` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 57 pos 13

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dbt_shabbirkdb`.`AccountIncrementalStg` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 57 pos 13
2023-04-20 12:06:43.260471 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dbt_shabbirkdb`.`AccountIncrementalStg` cannot be found. Verify the spelling and correctness of the schema and catalog.\n  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 57 pos 13', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM {{ ref('AccountIncrementalStg') }} a\n        JOIN {{ source('tpcdi', 'BatchDate') }} bd\n          ON a.batchid = bd.batchid\n        JOIN {{ source('tpcdi', 'StatusType') }} st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN {{ ref('DimCustomerStg') }} c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN {{ ref('DimBroker') }} b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM `dbt_shabbirkdb`.`AccountIncrementalStg` a\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd\n          ON a.batchid = bd.batchid\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN `dbt_shabbirkdb`.`DimBroker` b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dbt_shabbirkdb`.`AccountIncrementalStg` cannot be found. Verify the spelling and correctness of the schema and catalog.\n  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 57 pos 13', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM {{ ref('AccountIncrementalStg') }} a\n        JOIN {{ source('tpcdi', 'BatchDate') }} bd\n          ON a.batchid = bd.batchid\n        JOIN {{ source('tpcdi', 'StatusType') }} st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN {{ ref('DimCustomerStg') }} c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN {{ ref('DimBroker') }} b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM `dbt_shabbirkdb`.`AccountIncrementalStg` a\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd\n          ON a.batchid = bd.batchid\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN `dbt_shabbirkdb`.`DimBroker` b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 12:06:43.480711 (Thread-737): handling poll request
2023-04-20 12:06:43.481181 (Thread-737): 12:06:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bcc4430>]}
2023-04-20 12:06:43.481965 (Thread-737): sending response (<Response 36219 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:45.462930 (Thread-738): handling poll request
2023-04-20 12:06:45.463427 (Thread-738): 12:06:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bcc4a30>]}
2023-04-20 12:06:45.464237 (Thread-738): sending response (<Response 36219 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:45.727293 (Thread-739): handling ps request
2023-04-20 12:06:45.727800 (Thread-739): 12:06:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e11e640>]}
2023-04-20 12:06:45.729346 (Thread-739): sending response (<Response 19900 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:46.517314 (Thread-740): handling status request
2023-04-20 12:06:46.517823 (Thread-740): 12:06:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e11ef40>]}
2023-04-20 12:06:46.518337 (Thread-740): sending response (<Response 1935 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:50.682483 (Thread-741): 12:06:50  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 12:06:50.975828 (Thread-741): 12:06:50  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
2023-04-20 12:06:50.976513 (Thread-741): 12:06:50  Partial parsing: updated file: dbsql_dbt_tpch://models/incremental/DimAccount.sql
2023-04-20 12:06:50.986409 (Thread-741): 12:06:50  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 12:06:51.045793 (Thread-741): 12:06:51  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285d0c70>]}
2023-04-20 12:06:51.120549 (Thread-742): handling status request
2023-04-20 12:06:51.121043 (Thread-742): 12:06:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a27850>]}
2023-04-20 12:06:51.121536 (Thread-742): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:53.782366 (Thread-743): handling status request
2023-04-20 12:06:53.782859 (Thread-743): 12:06:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be299d0>]}
2023-04-20 12:06:53.783346 (Thread-743): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:54.256432 (Thread-744): handling cli_args request
2023-04-20 12:06:54.256928 (Thread-744): 12:06:54  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be29340>]}
2023-04-20 12:06:57.025117 (Thread-744): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:57.083032 (MainThread): 12:06:57  checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
2023-04-20 12:06:57.104884 (MainThread): 12:06:57  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 12:06:57.105069 (MainThread): 12:06:57  previous checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, current checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd
2023-04-20 12:06:57.105230 (MainThread): 12:06:57  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '6f1c1edd-0e83-4434-a371-cbca2d07a46b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2693163970>]}
2023-04-20 12:06:57.603803 (Thread-745): handling ps request
2023-04-20 12:06:57.604488 (Thread-745): 12:06:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52bf9c850>]}
2023-04-20 12:06:57.607166 (Thread-745): sending response (<Response 20282 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:57.771816 (Thread-746): handling status request
2023-04-20 12:06:57.772322 (Thread-746): 12:06:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be386d0>]}
2023-04-20 12:06:57.772863 (Thread-746): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:57.780609 (Thread-747): handling poll request
2023-04-20 12:06:57.780959 (Thread-747): 12:06:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54be38a30>]}
2023-04-20 12:06:57.781448 (Thread-747): sending response (<Response 1863 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:58.099927 (Thread-748): handling ps request
2023-04-20 12:06:58.100411 (Thread-748): 12:06:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52bf9cc10>]}
2023-04-20 12:06:58.101958 (Thread-748): sending response (<Response 20282 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:06:58.684313 (MainThread): 12:06:58  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 12:06:58.697180 (MainThread): 12:06:58  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 12:06:58.700452 (MainThread): 12:06:58  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 12:06:58.703664 (MainThread): 12:06:58  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 12:06:58.706972 (MainThread): 12:06:58  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 12:06:58.710229 (MainThread): 12:06:58  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 12:06:58.714262 (MainThread): 12:06:58  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 12:06:58.717503 (MainThread): 12:06:58  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 12:06:58.720733 (MainThread): 12:06:58  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 12:06:58.723702 (MainThread): 12:06:58  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 12:06:58.727009 (MainThread): 12:06:58  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 12:06:58.730091 (MainThread): 12:06:58  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 12:06:58.732850 (MainThread): 12:06:58  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 12:06:58.735810 (MainThread): 12:06:58  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 12:06:58.738654 (MainThread): 12:06:58  1699: static parser successfully parsed silver/AccountIncrementalStg.sql
2023-04-20 12:06:58.741455 (MainThread): 12:06:58  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 12:06:58.902820 (MainThread): 12:06:58  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6f1c1edd-0e83-4434-a371-cbca2d07a46b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f269003b370>]}
2023-04-20 12:06:58.935741 (MainThread): 12:06:58  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6f1c1edd-0e83-4434-a371-cbca2d07a46b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2693159fa0>]}
2023-04-20 12:06:58.936057 (MainThread): 12:06:58  Found 16 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:06:58.936198 (MainThread): 12:06:58  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6f1c1edd-0e83-4434-a371-cbca2d07a46b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26933c69d0>]}
2023-04-20 12:06:58.938054 (MainThread): 12:06:58  
2023-04-20 12:06:58.939259 (MainThread): 12:06:58  Acquiring new databricks connection 'master'
2023-04-20 12:06:58.941029 (ThreadPoolExecutor-0_0): 12:06:58  Acquiring new databricks connection 'list_schemas'
2023-04-20 12:06:58.951634 (ThreadPoolExecutor-0_0): 12:06:58  Using databricks connection "list_schemas"
2023-04-20 12:06:58.951966 (ThreadPoolExecutor-0_0): 12:06:58  On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_schemas"} */

    show databases
  
2023-04-20 12:06:58.952130 (ThreadPoolExecutor-0_0): 12:06:58  Opening a new connection, currently in state init
2023-04-20 12:07:00.005669 (Thread-749): handling poll request
2023-04-20 12:07:00.006225 (Thread-749): 12:07:00  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52bf9c370>]}
2023-04-20 12:07:00.006982 (Thread-749): sending response (<Response 9005 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:00.025474 (ThreadPoolExecutor-0_0): 12:07:00  SQL status: OK in 1.07 seconds
2023-04-20 12:07:00.162248 (ThreadPoolExecutor-0_0): 12:07:00  On list_schemas: Close
2023-04-20 12:07:00.443176 (ThreadPoolExecutor-1_0): 12:07:00  Acquiring new databricks connection 'list_None_dbt_shabbirkdb'
2023-04-20 12:07:00.453820 (ThreadPoolExecutor-1_0): 12:07:00  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:07:00.454050 (ThreadPoolExecutor-1_0): 12:07:00  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 12:07:00.454215 (ThreadPoolExecutor-1_0): 12:07:00  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show tables in `dbt_shabbirkdb`
  
2023-04-20 12:07:00.454367 (ThreadPoolExecutor-1_0): 12:07:00  Opening a new connection, currently in state closed
2023-04-20 12:07:00.893756 (Thread-750): handling ps request
2023-04-20 12:07:00.894255 (Thread-750): 12:07:00  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52bf7c250>]}
2023-04-20 12:07:00.895820 (Thread-750): sending response (<Response 20282 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:01.196543 (ThreadPoolExecutor-1_0): 12:07:01  SQL status: OK in 0.74 seconds
2023-04-20 12:07:01.205810 (ThreadPoolExecutor-1_0): 12:07:01  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 12:07:01.206059 (ThreadPoolExecutor-1_0): 12:07:01  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show views in `dbt_shabbirkdb`
  
2023-04-20 12:07:01.597605 (ThreadPoolExecutor-1_0): 12:07:01  SQL status: OK in 0.39 seconds
2023-04-20 12:07:01.601075 (ThreadPoolExecutor-1_0): 12:07:01  On list_None_dbt_shabbirkdb: ROLLBACK
2023-04-20 12:07:01.601295 (ThreadPoolExecutor-1_0): 12:07:01  Databricks adapter: NotImplemented: rollback
2023-04-20 12:07:01.601456 (ThreadPoolExecutor-1_0): 12:07:01  On list_None_dbt_shabbirkdb: Close
2023-04-20 12:07:01.895178 (MainThread): 12:07:01  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6f1c1edd-0e83-4434-a371-cbca2d07a46b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26927b4bb0>]}
2023-04-20 12:07:01.895637 (MainThread): 12:07:01  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:07:01.895810 (MainThread): 12:07:01  Spark adapter: NotImplemented: commit
2023-04-20 12:07:01.896360 (MainThread): 12:07:01  Concurrency: 4 threads (target='default')
2023-04-20 12:07:01.896511 (MainThread): 12:07:01  
2023-04-20 12:07:01.899438 (Thread-1): 12:07:01  Began running node model.dbsql_dbt_tpch.AccountIncrementalStg
2023-04-20 12:07:01.899838 (Thread-1): 12:07:01  1 of 16 START sql view model dbt_shabbirkdb.AccountIncrementalStg .............. [RUN]
2023-04-20 12:07:01.900621 (Thread-1): 12:07:01  Acquiring new databricks connection 'model.dbsql_dbt_tpch.AccountIncrementalStg'
2023-04-20 12:07:01.900914 (Thread-1): 12:07:01  Began compiling node model.dbsql_dbt_tpch.AccountIncrementalStg
2023-04-20 12:07:01.904708 (Thread-2): 12:07:01  Began running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:07:01.905066 (Thread-2): 12:07:01  2 of 16 START sql table model dbt_shabbirkdb.DimBroker ......................... [RUN]
2023-04-20 12:07:01.905676 (Thread-2): 12:07:01  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
2023-04-20 12:07:01.905862 (Thread-2): 12:07:01  Began compiling node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:07:01.910155 (Thread-1): 12:07:01  Writing injected SQL for node "model.dbsql_dbt_tpch.AccountIncrementalStg"
2023-04-20 12:07:01.910265 (Thread-2): 12:07:01  Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:07:01.910654 (Thread-3): 12:07:01  Began running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:07:01.910950 (Thread-3): 12:07:01  3 of 16 START sql table model dbt_shabbirkdb.DimCompany ........................ [RUN]
2023-04-20 12:07:01.911453 (Thread-3): 12:07:01  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCompany'
2023-04-20 12:07:01.911667 (Thread-3): 12:07:01  Began compiling node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:07:01.915793 (Thread-3): 12:07:01  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:07:01.916144 (Thread-4): 12:07:01  Began running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:07:01.916432 (Thread-4): 12:07:01  4 of 16 START sql table model dbt_shabbirkdb.DimCustomerStg .................... [RUN]
2023-04-20 12:07:01.916925 (Thread-4): 12:07:01  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomerStg'
2023-04-20 12:07:01.917098 (Thread-4): 12:07:01  Began compiling node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:07:01.921420 (Thread-4): 12:07:01  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:07:01.927413 (Thread-1): 12:07:01  Timing info for model.dbsql_dbt_tpch.AccountIncrementalStg (compile): 2023-04-20 12:07:01.900995 => 2023-04-20 12:07:01.927243
2023-04-20 12:07:01.927665 (Thread-1): 12:07:01  Began executing node model.dbsql_dbt_tpch.AccountIncrementalStg
2023-04-20 12:07:01.932870 (Thread-2): 12:07:01  Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 2023-04-20 12:07:01.905918 => 2023-04-20 12:07:01.932703
2023-04-20 12:07:01.933080 (Thread-2): 12:07:01  Began executing node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:07:01.964709 (Thread-2): 12:07:01  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:07:01.964903 (Thread-2): 12:07:01  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:07:01.965068 (Thread-2): 12:07:01  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

      describe extended `dbt_shabbirkdb`.`dimbroker`
  
2023-04-20 12:07:01.965205 (Thread-2): 12:07:01  Opening a new connection, currently in state init
2023-04-20 12:07:01.971243 (Thread-3): 12:07:01  Timing info for model.dbsql_dbt_tpch.DimCompany (compile): 2023-04-20 12:07:01.911715 => 2023-04-20 12:07:01.971051
2023-04-20 12:07:01.971494 (Thread-3): 12:07:01  Began executing node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:07:01.975623 (Thread-3): 12:07:01  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:07:01.975801 (Thread-3): 12:07:01  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:07:01.975959 (Thread-3): 12:07:01  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

      describe extended `dbt_shabbirkdb`.`dimcompany`
  
2023-04-20 12:07:01.976382 (Thread-4): 12:07:01  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (compile): 2023-04-20 12:07:01.917142 => 2023-04-20 12:07:01.976202
2023-04-20 12:07:01.976599 (Thread-4): 12:07:01  Began executing node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:07:01.983878 (Thread-1): 12:07:01  Writing runtime sql for node "model.dbsql_dbt_tpch.AccountIncrementalStg"
2023-04-20 12:07:01.976090 (Thread-3): 12:07:01  Opening a new connection, currently in state init
2023-04-20 12:07:01.984995 (Thread-4): 12:07:01  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:07:01.985172 (Thread-4): 12:07:01  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:07:01.985332 (Thread-4): 12:07:01  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

      describe extended `dbt_shabbirkdb`.`dimcustomerstg`
  
2023-04-20 12:07:01.985461 (Thread-4): 12:07:01  Opening a new connection, currently in state init
2023-04-20 12:07:01.995104 (Thread-1): 12:07:01  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:07:01.995284 (Thread-1): 12:07:01  Using databricks connection "model.dbsql_dbt_tpch.AccountIncrementalStg"
2023-04-20 12:07:01.995447 (Thread-1): 12:07:01  On model.dbsql_dbt_tpch.AccountIncrementalStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.AccountIncrementalStg"} */
create or replace view `dbt_shabbirkdb`.`AccountIncrementalStg`
  
  
  as
    SELECT 
    md5(accountid::string) AS sk_customerid,
    *
FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental`

2023-04-20 12:07:01.995608 (Thread-1): 12:07:01  Opening a new connection, currently in state closed
2023-04-20 12:07:02.124623 (Thread-751): handling status request
2023-04-20 12:07:02.125105 (Thread-751): 12:07:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5083901c0>]}
2023-04-20 12:07:02.125671 (Thread-751): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:02.590650 (Thread-752): handling poll request
2023-04-20 12:07:02.591579 (Thread-752): 12:07:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bda15e0>]}
2023-04-20 12:07:02.592772 (Thread-752): sending response (<Response 26168 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:02.812238 (Thread-3): 12:07:02  SQL status: OK in 0.84 seconds
2023-04-20 12:07:02.845838 (Thread-4): 12:07:02  SQL status: OK in 0.86 seconds
2023-04-20 12:07:02.850908 (Thread-3): 12:07:02  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:07:02.852392 (Thread-2): 12:07:02  SQL status: OK in 0.89 seconds
2023-04-20 12:07:02.858707 (Thread-4): 12:07:02  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:07:02.862847 (Thread-2): 12:07:02  Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:07:02.870292 (Thread-3): 12:07:02  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:07:02.870600 (Thread-3): 12:07:02  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCompany`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  * FROM (
  SELECT
    md5(cik) sk_companyid,
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
  
2023-04-20 12:07:02.878339 (Thread-2): 12:07:02  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:07:02.878572 (Thread-2): 12:07:02  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT
  md5(employeeid) as sk_brokerid,
  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate`) effectivedate,
  date('9999-12-31') enddate
FROM  `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`HR`
WHERE employeejobcode = 314
  
2023-04-20 12:07:02.890741 (Thread-4): 12:07:02  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:07:02.891280 (Thread-4): 12:07:02  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomerStg`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT * FROM (
  SELECT
    customerid,
    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) taxid,
    status,
    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) lastname,
    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) firstname,
    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) middleinitial,
    coalesce(gender, last_value(gender) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) gender,
    coalesce(tier, last_value(tier) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) tier,
    coalesce(dob, last_value(dob) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) dob,
    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline1,
    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline2,
    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) postalcode,
    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) CITY,
    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) stateprov,
    coalesce(country, last_value(country) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) country,
    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone1,
    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone2,
    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone3,
    coalesce(email1, last_value(email1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email1,
    coalesce(email2, last_value(email2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email2,
    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) LCL_TX_ID,
    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) NAT_TX_ID,
    batchid,
    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,
    date(update_ts) effectivedate,
    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate
  FROM (
    SELECT
      customerid,
      taxid,
      status,
      lastname,
      firstname,
      middleinitial,
      gender,
      tier,
      dob,
      addressline1,
      addressline2,
      postalcode,
      city,
      stateprov,
      country,
      phone1,
      phone2,
      phone3,
      email1,
      email2,
      lcl_tx_id,
      nat_tx_id,
      1 batchid,
      update_ts
    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')
    UNION ALL
    SELECT
      c.customerid,
      nullif(c.taxid, '') taxid,
      nullif(s.st_name, '') as status,
      nullif(c.lastname, '') lastname,
      nullif(c.firstname, '') firstname,
      nullif(c.middleinitial, '') middleinitial,
      gender,
      c.tier,
      c.dob,
      nullif(c.addressline1, '') addressline1,
      nullif(c.addressline2, '') addressline2,
      nullif(c.postalcode, '') postalcode,
      nullif(c.city, '') city,
      nullif(c.stateprov, '') stateprov,
      nullif(c.country, '') country,
      CASE
        WHEN isnull(c_local_1) then c_local_1
        ELSE concat(
          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),
          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),
          c_local_1,
          nvl(c_ext_1, '')) END as phone1,
      CASE
        WHEN isnull(c_local_2) then c_local_2
        ELSE concat(
          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),
          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),
          c_local_2,
          nvl(c_ext_2, '')) END as phone2,
      CASE
        WHEN isnull(c_local_3) then c_local_3
        ELSE concat(
          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),
          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),
          c_local_3,
          nvl(c_ext_3, '')) END as phone3,
      nullif(c.email1, '') email1,
      nullif(c.email2, '') email2,
      c.LCL_TX_ID, 
      c.NAT_TX_ID,
      c.batchid,
      timestamp(bd.batchdate) update_ts
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CustomerIncremental` c
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
      ON c.batchid = bd.batchid
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` s 
      ON c.status = s.st_id
  ) c
  )
  
2023-04-20 12:07:03.201931 (Thread-1): 12:07:03  SQL status: OK in 1.21 seconds
2023-04-20 12:07:03.214097 (Thread-1): 12:07:03  Timing info for model.dbsql_dbt_tpch.AccountIncrementalStg (execute): 2023-04-20 12:07:01.927719 => 2023-04-20 12:07:03.213914
2023-04-20 12:07:03.214362 (Thread-1): 12:07:03  On model.dbsql_dbt_tpch.AccountIncrementalStg: ROLLBACK
2023-04-20 12:07:03.214517 (Thread-1): 12:07:03  Databricks adapter: NotImplemented: rollback
2023-04-20 12:07:03.214644 (Thread-1): 12:07:03  On model.dbsql_dbt_tpch.AccountIncrementalStg: Close
2023-04-20 12:07:03.497251 (Thread-1): 12:07:03  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6f1c1edd-0e83-4434-a371-cbca2d07a46b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26929385b0>]}
2023-04-20 12:07:03.497795 (Thread-1): 12:07:03  1 of 16 OK created sql view model dbt_shabbirkdb.AccountIncrementalStg ......... [OK in 1.60s]
2023-04-20 12:07:03.499047 (Thread-1): 12:07:03  Finished running node model.dbsql_dbt_tpch.AccountIncrementalStg
2023-04-20 12:07:03.499464 (Thread-1): 12:07:03  Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:07:03.499835 (Thread-1): 12:07:03  5 of 16 START sql table model dbt_shabbirkdb.tempDailyMarketHistorical ......... [RUN]
2023-04-20 12:07:03.500382 (Thread-1): 12:07:03  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempDailyMarketHistorical'
2023-04-20 12:07:03.500567 (Thread-1): 12:07:03  Began compiling node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:07:03.505221 (Thread-1): 12:07:03  Writing injected SQL for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:07:03.520555 (Thread-1): 12:07:03  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (compile): 2023-04-20 12:07:03.500614 => 2023-04-20 12:07:03.520357
2023-04-20 12:07:03.520806 (Thread-1): 12:07:03  Began executing node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:07:03.526578 (Thread-1): 12:07:03  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:07:03.526755 (Thread-1): 12:07:03  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:07:03.526915 (Thread-1): 12:07:03  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

      describe extended `dbt_shabbirkdb`.`tempdailymarkethistorical`
  
2023-04-20 12:07:03.527051 (Thread-1): 12:07:03  Opening a new connection, currently in state closed
2023-04-20 12:07:04.132340 (Thread-753): handling ps request
2023-04-20 12:07:04.132835 (Thread-753): 12:07:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5083904c0>]}
2023-04-20 12:07:04.134383 (Thread-753): sending response (<Response 20280 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:04.189280 (Thread-754): handling poll request
2023-04-20 12:07:04.189677 (Thread-754): 12:07:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508390d90>]}
2023-04-20 12:07:04.190494 (Thread-754): sending response (<Response 22199 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:04.249676 (Thread-1): 12:07:04  SQL status: OK in 0.72 seconds
2023-04-20 12:07:04.255234 (Thread-1): 12:07:04  Writing runtime sql for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:07:04.269317 (Thread-1): 12:07:04  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:07:04.269561 (Thread-1): 12:07:04  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempDailyMarketHistorical`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  dmh.*,
  sk_dateid,
  min(dm_low) OVER (
    PARTITION BY dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweeklow,
  max(dm_high) OVER (
    PARTITION by dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweekhigh
FROM (
  SELECT * FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketHistorical`
  UNION ALL
  SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketIncremental`) dmh
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
  ON d.datevalue = dm_date;
  
2023-04-20 12:07:04.627695 (Thread-755): handling status request
2023-04-20 12:07:04.628220 (Thread-755): 12:07:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508390550>]}
2023-04-20 12:07:04.628718 (Thread-755): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:05.590942 (Thread-2): 12:07:05  SQL status: OK in 2.71 seconds
2023-04-20 12:07:05.609358 (Thread-2): 12:07:05  Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 2023-04-20 12:07:01.933128 => 2023-04-20 12:07:05.609186
2023-04-20 12:07:05.609627 (Thread-2): 12:07:05  On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
2023-04-20 12:07:05.609795 (Thread-2): 12:07:05  Databricks adapter: NotImplemented: rollback
2023-04-20 12:07:05.609935 (Thread-2): 12:07:05  On model.dbsql_dbt_tpch.DimBroker: Close
2023-04-20 12:07:05.874103 (Thread-756): handling poll request
2023-04-20 12:07:05.874588 (Thread-756): 12:07:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508390040>]}
2023-04-20 12:07:05.875181 (Thread-756): sending response (<Response 4674 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:05.897784 (Thread-2): 12:07:05  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6f1c1edd-0e83-4434-a371-cbca2d07a46b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f269285ee80>]}
2023-04-20 12:07:05.898312 (Thread-2): 12:07:05  2 of 16 OK created sql table model dbt_shabbirkdb.DimBroker .................... [OK in 3.99s]
2023-04-20 12:07:05.898546 (Thread-2): 12:07:05  Finished running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:07:06.099635 (Thread-3): 12:07:06  SQL status: OK in 3.23 seconds
2023-04-20 12:07:06.102004 (Thread-3): 12:07:06  Timing info for model.dbsql_dbt_tpch.DimCompany (execute): 2023-04-20 12:07:01.971577 => 2023-04-20 12:07:06.101836
2023-04-20 12:07:06.102211 (Thread-3): 12:07:06  On model.dbsql_dbt_tpch.DimCompany: ROLLBACK
2023-04-20 12:07:06.102354 (Thread-3): 12:07:06  Databricks adapter: NotImplemented: rollback
2023-04-20 12:07:06.102479 (Thread-3): 12:07:06  On model.dbsql_dbt_tpch.DimCompany: Close
2023-04-20 12:07:06.372482 (Thread-3): 12:07:06  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6f1c1edd-0e83-4434-a371-cbca2d07a46b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f269279d6d0>]}
2023-04-20 12:07:06.372993 (Thread-3): 12:07:06  3 of 16 OK created sql table model dbt_shabbirkdb.DimCompany ................... [OK in 4.46s]
2023-04-20 12:07:06.373224 (Thread-3): 12:07:06  Finished running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:07:06.374145 (Thread-2): 12:07:06  Began running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:07:06.374526 (Thread-2): 12:07:06  6 of 16 START sql table model dbt_shabbirkdb.DimSecurity ....................... [RUN]
2023-04-20 12:07:06.375048 (Thread-2): 12:07:06  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimSecurity'
2023-04-20 12:07:06.375232 (Thread-2): 12:07:06  Began compiling node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:07:06.379231 (Thread-3): 12:07:06  Began running node model.dbsql_dbt_tpch.Financial
2023-04-20 12:07:06.379561 (Thread-3): 12:07:06  7 of 16 START sql table model dbt_shabbirkdb.Financial ......................... [RUN]
2023-04-20 12:07:06.380034 (Thread-3): 12:07:06  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Financial'
2023-04-20 12:07:06.380209 (Thread-3): 12:07:06  Began compiling node model.dbsql_dbt_tpch.Financial
2023-04-20 12:07:06.385083 (Thread-2): 12:07:06  Writing injected SQL for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:07:06.385430 (Thread-3): 12:07:06  Writing injected SQL for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:07:06.401437 (Thread-3): 12:07:06  Timing info for model.dbsql_dbt_tpch.Financial (compile): 2023-04-20 12:07:06.380254 => 2023-04-20 12:07:06.401272
2023-04-20 12:07:06.401661 (Thread-3): 12:07:06  Began executing node model.dbsql_dbt_tpch.Financial
2023-04-20 12:07:06.405939 (Thread-3): 12:07:06  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:07:06.406176 (Thread-3): 12:07:06  Using databricks connection "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:07:06.406564 (Thread-2): 12:07:06  Timing info for model.dbsql_dbt_tpch.DimSecurity (compile): 2023-04-20 12:07:06.375279 => 2023-04-20 12:07:06.406386
2023-04-20 12:07:06.406771 (Thread-2): 12:07:06  Began executing node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:07:06.411547 (Thread-3): 12:07:06  On model.dbsql_dbt_tpch.Financial: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

      describe extended `dbt_shabbirkdb`.`financial`
  
2023-04-20 12:07:06.411722 (Thread-3): 12:07:06  Opening a new connection, currently in state closed
2023-04-20 12:07:06.412250 (Thread-2): 12:07:06  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:07:06.412437 (Thread-2): 12:07:06  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:07:06.412597 (Thread-2): 12:07:06  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

      describe extended `dbt_shabbirkdb`.`dimsecurity`
  
2023-04-20 12:07:06.412722 (Thread-2): 12:07:06  Opening a new connection, currently in state closed
2023-04-20 12:07:06.641765 (Thread-757): handling status request
2023-04-20 12:07:06.642268 (Thread-757): 12:07:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285d0430>]}
2023-04-20 12:07:06.642758 (Thread-757): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:06.758949 (Thread-4): 12:07:06  SQL status: OK in 3.87 seconds
2023-04-20 12:07:06.761581 (Thread-4): 12:07:06  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (execute): 2023-04-20 12:07:01.976647 => 2023-04-20 12:07:06.761415
2023-04-20 12:07:06.761848 (Thread-4): 12:07:06  On model.dbsql_dbt_tpch.DimCustomerStg: ROLLBACK
2023-04-20 12:07:06.762012 (Thread-4): 12:07:06  Databricks adapter: NotImplemented: rollback
2023-04-20 12:07:06.762139 (Thread-4): 12:07:06  On model.dbsql_dbt_tpch.DimCustomerStg: Close
2023-04-20 12:07:06.793986 (Thread-758): handling ps request
2023-04-20 12:07:06.794472 (Thread-758): 12:07:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285d07c0>]}
2023-04-20 12:07:06.796079 (Thread-758): sending response (<Response 20283 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:06.993275 (Thread-759): handling ps request
2023-04-20 12:07:06.993761 (Thread-759): 12:07:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285d0fa0>]}
2023-04-20 12:07:06.995477 (Thread-759): sending response (<Response 20282 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:07.044810 (Thread-4): 12:07:07  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6f1c1edd-0e83-4434-a371-cbca2d07a46b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26900c8460>]}
2023-04-20 12:07:07.045363 (Thread-4): 12:07:07  4 of 16 OK created sql table model dbt_shabbirkdb.DimCustomerStg ............... [OK in 5.13s]
2023-04-20 12:07:07.045670 (Thread-4): 12:07:07  Finished running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:07:07.046575 (Thread-4): 12:07:07  Began running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:07:07.046947 (Thread-4): 12:07:07  8 of 16 START sql table model dbt_shabbirkdb.DimAccount ........................ [RUN]
2023-04-20 12:07:07.047502 (Thread-4): 12:07:07  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimAccount'
2023-04-20 12:07:07.047746 (Thread-4): 12:07:07  Began compiling node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:07:07.053309 (Thread-4): 12:07:07  Writing injected SQL for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 12:07:07.069294 (Thread-4): 12:07:07  Timing info for model.dbsql_dbt_tpch.DimAccount (compile): 2023-04-20 12:07:07.047796 => 2023-04-20 12:07:07.069131
2023-04-20 12:07:07.069517 (Thread-4): 12:07:07  Began executing node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:07:07.073896 (Thread-4): 12:07:07  Writing runtime sql for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 12:07:07.086894 (Thread-4): 12:07:07  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:07:07.087077 (Thread-4): 12:07:07  Using databricks connection "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 12:07:07.087321 (Thread-4): 12:07:07  On model.dbsql_dbt_tpch.DimAccount: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `dbt_shabbirkdb`.`AccountIncrementalStg` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 12:07:07.087461 (Thread-4): 12:07:07  Opening a new connection, currently in state closed
2023-04-20 12:07:07.152030 (Thread-3): 12:07:07  SQL status: OK in 0.74 seconds
2023-04-20 12:07:07.157600 (Thread-3): 12:07:07  Writing runtime sql for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:07:07.170052 (Thread-2): 12:07:07  SQL status: OK in 0.76 seconds
2023-04-20 12:07:07.171545 (Thread-3): 12:07:07  Using databricks connection "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:07:07.171842 (Thread-3): 12:07:07  On model.dbsql_dbt_tpch.Financial: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 12:07:07.176826 (Thread-2): 12:07:07  Writing runtime sql for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:07:07.192734 (Thread-2): 12:07:07  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:07:07.193034 (Thread-2): 12:07:07  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 12:07:07.758058 (Thread-760): handling poll request
2023-04-20 12:07:07.758567 (Thread-760): 12:07:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285d2e80>]}
2023-04-20 12:07:07.759796 (Thread-760): sending response (<Response 32841 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:07.894819 (Thread-4): 12:07:07  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `dbt_shabbirkdb`.`AccountIncrementalStg` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 12:07:07.895058 (Thread-4): 12:07:07  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 12:07:07.895207 (Thread-4): 12:07:07  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 12:07:07.895422 (Thread-4): 12:07:07  Databricks adapter: operation-id: b'\x01\xed\xdfs\xdf\x1d\x187\x98\rT\x19\x8f\xb4`o'
2023-04-20 12:07:07.895758 (Thread-4): 12:07:07  Timing info for model.dbsql_dbt_tpch.DimAccount (execute): 2023-04-20 12:07:07.069568 => 2023-04-20 12:07:07.895593
2023-04-20 12:07:07.895957 (Thread-4): 12:07:07  On model.dbsql_dbt_tpch.DimAccount: ROLLBACK
2023-04-20 12:07:07.896088 (Thread-4): 12:07:07  Databricks adapter: NotImplemented: rollback
2023-04-20 12:07:07.896213 (Thread-4): 12:07:07  On model.dbsql_dbt_tpch.DimAccount: Close
2023-04-20 12:07:07.907573 (Thread-761): handling status request
2023-04-20 12:07:07.908026 (Thread-761): 12:07:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285d2d00>]}
2023-04-20 12:07:07.908504 (Thread-761): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:08.192243 (Thread-4): 12:07:08  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 12:07:08.192822 (Thread-4): 12:07:08  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6f1c1edd-0e83-4434-a371-cbca2d07a46b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26453adcd0>]}
2023-04-20 12:07:08.193298 (Thread-4): 12:07:08  8 of 16 ERROR creating sql table model dbt_shabbirkdb.DimAccount ............... [ERROR in 1.15s]
2023-04-20 12:07:08.193524 (Thread-4): 12:07:08  Finished running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:07:08.193839 (Thread-4): 12:07:08  Began running node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:07:08.194138 (Thread-4): 12:07:08  9 of 16 START sql table model dbt_shabbirkdb.Prospect .......................... [RUN]
2023-04-20 12:07:08.194627 (Thread-4): 12:07:08  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Prospect'
2023-04-20 12:07:08.194808 (Thread-4): 12:07:08  Began compiling node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:07:08.201025 (Thread-4): 12:07:08  Writing injected SQL for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:07:08.215799 (Thread-4): 12:07:08  Timing info for model.dbsql_dbt_tpch.Prospect (compile): 2023-04-20 12:07:08.194854 => 2023-04-20 12:07:08.215623
2023-04-20 12:07:08.216032 (Thread-4): 12:07:08  Began executing node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:07:08.222183 (Thread-4): 12:07:08  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:07:08.222379 (Thread-4): 12:07:08  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:07:08.222547 (Thread-4): 12:07:08  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

      describe extended `dbt_shabbirkdb`.`prospect`
  
2023-04-20 12:07:08.222697 (Thread-4): 12:07:08  Opening a new connection, currently in state closed
2023-04-20 12:07:08.944863 (Thread-4): 12:07:08  SQL status: OK in 0.72 seconds
2023-04-20 12:07:08.950477 (Thread-4): 12:07:08  Writing runtime sql for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:07:08.965947 (Thread-4): 12:07:08  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:07:08.966280 (Thread-4): 12:07:08  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Prospect`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  agencyid,
  recdate.sk_dateid sk_recorddateid,
  origdate.sk_dateid sk_updatedateid,
  p.batchid,
  nvl2(c.customerid, True, False) iscustomer, 
  p.lastname,
  p.firstname,
  p.middleinitial,
  p.gender,
  p.addressline1,
  p.addressline2,
  p.postalcode,
  city,
  state,
  country,
  phone,
  income,
  numbercars,
  numberchildren,
  maritalstatus,
  age,
  creditrating,
  ownorrentflag,
  employer,
  numbercreditcards,
  networth,
  if(
    isnotnull(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+","")),
    left(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+",""),
      length(
        if(networth > 1000000 or income > 200000,"HighValue+","") || 
        if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
        if(age > 45, "Boomer+", "") ||
        if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
        if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
        if(age < 25 and networth > 1000000, "Inherited+",""))
      -1),
    NULL) marketingnameplate
FROM (
  SELECT 
    * FROM (
    SELECT
      agencyid,
      max(batchid) recordbatchid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth,
      min(batchid) batchid
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`ProspectRaw` p
    GROUP BY
      agencyid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth)
  QUALIFY ROW_NUMBER() OVER (PARTITION BY agencyid ORDER BY batchid DESC) = 1) p
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) recdate
  ON p.recordbatchid = recdate.batchid
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) origdate
  ON p.batchid = origdate.batchid
LEFT JOIN (
  SELECT 
    customerid,
    lastname,
    firstname,
    addressline1,
    addressline2,
    postalcode
  FROM `dbt_shabbirkdb`.`DimCustomerStg`
  WHERE iscurrent) c
  ON 
    upper(p.LastName) = upper(c.lastname)
    and upper(p.FirstName) = upper(c.firstname)
    and upper(p.AddressLine1) = upper(c.addressline1)
    and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
    and upper(p.PostalCode) = upper(c.postalcode)
  
2023-04-20 12:07:09.323805 (Thread-762): handling poll request
2023-04-20 12:07:09.324293 (Thread-762): 12:07:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdc4190>]}
2023-04-20 12:07:09.325087 (Thread-762): sending response (<Response 21850 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:10.036186 (Thread-763): handling ps request
2023-04-20 12:07:10.036683 (Thread-763): 12:07:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdc4280>]}
2023-04-20 12:07:10.038365 (Thread-763): sending response (<Response 20283 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:10.060560 (Thread-2): 12:07:10  SQL status: OK in 2.87 seconds
2023-04-20 12:07:10.063315 (Thread-2): 12:07:10  Timing info for model.dbsql_dbt_tpch.DimSecurity (execute): 2023-04-20 12:07:06.406818 => 2023-04-20 12:07:10.063153
2023-04-20 12:07:10.063558 (Thread-2): 12:07:10  On model.dbsql_dbt_tpch.DimSecurity: ROLLBACK
2023-04-20 12:07:10.063749 (Thread-2): 12:07:10  Databricks adapter: NotImplemented: rollback
2023-04-20 12:07:10.063883 (Thread-2): 12:07:10  On model.dbsql_dbt_tpch.DimSecurity: Close
2023-04-20 12:07:10.345397 (Thread-2): 12:07:10  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6f1c1edd-0e83-4434-a371-cbca2d07a46b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26927ad880>]}
2023-04-20 12:07:10.345936 (Thread-2): 12:07:10  6 of 16 OK created sql table model dbt_shabbirkdb.DimSecurity .................. [OK in 3.97s]
2023-04-20 12:07:10.346179 (Thread-2): 12:07:10  Finished running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:07:10.346463 (Thread-2): 12:07:10  Began running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 12:07:10.346710 (Thread-2): 12:07:10  10 of 16 SKIP relation dbt_shabbirkdb.FactCashBalances ......................... [SKIP]
2023-04-20 12:07:10.346898 (Thread-2): 12:07:10  Finished running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 12:07:10.347828 (Thread-2): 12:07:10  Began running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 12:07:10.348054 (Thread-2): 12:07:10  11 of 16 SKIP relation dbt_shabbirkdb.DimTrade ................................. [SKIP]
2023-04-20 12:07:10.348233 (Thread-2): 12:07:10  Finished running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 12:07:10.348857 (Thread-2): 12:07:10  Began running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 12:07:10.349090 (Thread-2): 12:07:10  12 of 16 SKIP relation dbt_shabbirkdb.FactHoldings ............................. [SKIP]
2023-04-20 12:07:10.349270 (Thread-2): 12:07:10  Finished running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 12:07:10.462805 (Thread-764): handling status request
2023-04-20 12:07:10.463277 (Thread-764): 12:07:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5285e6670>]}
2023-04-20 12:07:10.463788 (Thread-764): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:10.754038 (Thread-1): 12:07:10  SQL status: OK in 6.48 seconds
2023-04-20 12:07:10.883827 (Thread-765): handling poll request
2023-04-20 12:07:10.884310 (Thread-765): 12:07:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdc40d0>]}
2023-04-20 12:07:10.911074 (Thread-765): sending response (<Response 8706 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:11.072154 (Thread-1): 12:07:11  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (execute): 2023-04-20 12:07:03.520860 => 2023-04-20 12:07:11.071964
2023-04-20 12:07:11.072419 (Thread-1): 12:07:11  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: ROLLBACK
2023-04-20 12:07:11.072568 (Thread-1): 12:07:11  Databricks adapter: NotImplemented: rollback
2023-04-20 12:07:11.072697 (Thread-1): 12:07:11  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: Close
2023-04-20 12:07:11.353184 (Thread-1): 12:07:11  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6f1c1edd-0e83-4434-a371-cbca2d07a46b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2692938b80>]}
2023-04-20 12:07:11.353708 (Thread-1): 12:07:11  5 of 16 OK created sql table model dbt_shabbirkdb.tempDailyMarketHistorical .... [OK in 7.85s]
2023-04-20 12:07:11.353957 (Thread-1): 12:07:11  Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:07:11.458348 (Thread-766): handling status request
2023-04-20 12:07:11.458836 (Thread-766): 12:07:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdc4a00>]}
2023-04-20 12:07:11.459340 (Thread-766): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:11.548510 (Thread-767): handling ps request
2023-04-20 12:07:11.548983 (Thread-767): 12:07:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdc4b20>]}
2023-04-20 12:07:11.550499 (Thread-767): sending response (<Response 20282 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:12.231015 (Thread-768): handling status request
2023-04-20 12:07:12.231693 (Thread-768): 12:07:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdc48b0>]}
2023-04-20 12:07:12.232369 (Thread-768): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:12.909799 (Thread-4): 12:07:12  SQL status: OK in 3.94 seconds
2023-04-20 12:07:12.912204 (Thread-4): 12:07:12  Timing info for model.dbsql_dbt_tpch.Prospect (execute): 2023-04-20 12:07:08.216084 => 2023-04-20 12:07:12.912048
2023-04-20 12:07:12.912422 (Thread-4): 12:07:12  On model.dbsql_dbt_tpch.Prospect: ROLLBACK
2023-04-20 12:07:12.912566 (Thread-4): 12:07:12  Databricks adapter: NotImplemented: rollback
2023-04-20 12:07:12.912691 (Thread-4): 12:07:12  On model.dbsql_dbt_tpch.Prospect: Close
2023-04-20 12:07:12.987244 (Thread-769): handling poll request
2023-04-20 12:07:12.987758 (Thread-769): 12:07:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdc4d60>]}
2023-04-20 12:07:12.988392 (Thread-769): sending response (<Response 5106 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:13.192902 (Thread-4): 12:07:13  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6f1c1edd-0e83-4434-a371-cbca2d07a46b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2645351f40>]}
2023-04-20 12:07:13.193419 (Thread-4): 12:07:13  9 of 16 OK created sql table model dbt_shabbirkdb.Prospect ..................... [OK in 5.00s]
2023-04-20 12:07:13.193654 (Thread-4): 12:07:13  Finished running node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:07:13.194569 (Thread-2): 12:07:13  Began running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:07:13.194908 (Thread-2): 12:07:13  13 of 16 START sql table model dbt_shabbirkdb.DimCustomer ...................... [RUN]
2023-04-20 12:07:13.195428 (Thread-2): 12:07:13  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomer'
2023-04-20 12:07:13.195666 (Thread-2): 12:07:13  Began compiling node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:07:13.200884 (Thread-2): 12:07:13  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 12:07:13.216136 (Thread-2): 12:07:13  Timing info for model.dbsql_dbt_tpch.DimCustomer (compile): 2023-04-20 12:07:13.195721 => 2023-04-20 12:07:13.215973
2023-04-20 12:07:13.216366 (Thread-2): 12:07:13  Began executing node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:07:13.220525 (Thread-2): 12:07:13  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 12:07:13.239578 (Thread-2): 12:07:13  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:07:13.239781 (Thread-2): 12:07:13  Using databricks connection "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 12:07:13.239982 (Thread-2): 12:07:13  On model.dbsql_dbt_tpch.DimCustomer: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 12:07:13.240118 (Thread-2): 12:07:13  Opening a new connection, currently in state closed
2023-04-20 12:07:13.982166 (Thread-2): 12:07:13  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 12:07:13.982394 (Thread-2): 12:07:13  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 12:07:13.982521 (Thread-2): 12:07:13  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 12:07:13.982634 (Thread-2): 12:07:13  Databricks adapter: operation-id: b'\x01\xed\xdfs\xe2\xcb\x18\xe9\x9a\x05\xf9\xb7\xea\x86e\xfb'
2023-04-20 12:07:13.982873 (Thread-2): 12:07:13  Timing info for model.dbsql_dbt_tpch.DimCustomer (execute): 2023-04-20 12:07:13.216425 => 2023-04-20 12:07:13.982742
2023-04-20 12:07:13.983055 (Thread-2): 12:07:13  On model.dbsql_dbt_tpch.DimCustomer: ROLLBACK
2023-04-20 12:07:13.983204 (Thread-2): 12:07:13  Databricks adapter: NotImplemented: rollback
2023-04-20 12:07:13.983336 (Thread-2): 12:07:13  On model.dbsql_dbt_tpch.DimCustomer: Close
2023-04-20 12:07:14.254387 (Thread-2): 12:07:14  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 12:07:14.254933 (Thread-2): 12:07:14  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6f1c1edd-0e83-4434-a371-cbca2d07a46b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26453ceca0>]}
2023-04-20 12:07:14.255387 (Thread-2): 12:07:14  13 of 16 ERROR creating sql table model dbt_shabbirkdb.DimCustomer ............. [ERROR in 1.06s]
2023-04-20 12:07:14.255640 (Thread-2): 12:07:14  Finished running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:07:14.256752 (Thread-4): 12:07:14  Began running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 12:07:14.257054 (Thread-4): 12:07:14  14 of 16 SKIP relation dbt_shabbirkdb.FactWatches .............................. [SKIP]
2023-04-20 12:07:14.257249 (Thread-4): 12:07:14  Finished running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 12:07:14.501990 (Thread-770): handling ps request
2023-04-20 12:07:14.502478 (Thread-770): 12:07:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdfcd00>]}
2023-04-20 12:07:14.504002 (Thread-770): sending response (<Response 20283 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:14.587912 (Thread-771): handling poll request
2023-04-20 12:07:14.588399 (Thread-771): 12:07:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdfcd90>]}
2023-04-20 12:07:14.589211 (Thread-771): sending response (<Response 19600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:14.673398 (Thread-3): 12:07:14  SQL status: OK in 7.5 seconds
2023-04-20 12:07:14.924510 (Thread-772): handling status request
2023-04-20 12:07:14.925009 (Thread-772): 12:07:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdfc370>]}
2023-04-20 12:07:14.925489 (Thread-772): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:14.992764 (Thread-3): 12:07:14  Timing info for model.dbsql_dbt_tpch.Financial (execute): 2023-04-20 12:07:06.401712 => 2023-04-20 12:07:14.992573
2023-04-20 12:07:14.993037 (Thread-3): 12:07:14  On model.dbsql_dbt_tpch.Financial: ROLLBACK
2023-04-20 12:07:14.993189 (Thread-3): 12:07:14  Databricks adapter: NotImplemented: rollback
2023-04-20 12:07:14.993337 (Thread-3): 12:07:14  On model.dbsql_dbt_tpch.Financial: Close
2023-04-20 12:07:15.275588 (Thread-3): 12:07:15  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6f1c1edd-0e83-4434-a371-cbca2d07a46b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26900c9640>]}
2023-04-20 12:07:15.276104 (Thread-3): 12:07:15  7 of 16 OK created sql table model dbt_shabbirkdb.Financial .................... [OK in 8.90s]
2023-04-20 12:07:15.276335 (Thread-3): 12:07:15  Finished running node model.dbsql_dbt_tpch.Financial
2023-04-20 12:07:15.277615 (Thread-2): 12:07:15  Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:07:15.277968 (Thread-2): 12:07:15  15 of 16 START sql table model dbt_shabbirkdb.tempSumpFiBasicEps ............... [RUN]
2023-04-20 12:07:15.278497 (Thread-2): 12:07:15  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempSumpFiBasicEps'
2023-04-20 12:07:15.278682 (Thread-2): 12:07:15  Began compiling node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:07:15.283250 (Thread-2): 12:07:15  Writing injected SQL for node "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:07:15.299347 (Thread-2): 12:07:15  Timing info for model.dbsql_dbt_tpch.tempSumpFiBasicEps (compile): 2023-04-20 12:07:15.278730 => 2023-04-20 12:07:15.299148
2023-04-20 12:07:15.299644 (Thread-2): 12:07:15  Began executing node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:07:15.305667 (Thread-2): 12:07:15  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:07:15.305845 (Thread-2): 12:07:15  Using databricks connection "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:07:15.306020 (Thread-2): 12:07:15  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempSumpFiBasicEps"} */

      describe extended `dbt_shabbirkdb`.`tempsumpfibasiceps`
  
2023-04-20 12:07:15.306163 (Thread-2): 12:07:15  Opening a new connection, currently in state closed
2023-04-20 12:07:16.026781 (Thread-2): 12:07:16  SQL status: OK in 0.72 seconds
2023-04-20 12:07:16.032136 (Thread-2): 12:07:16  Writing runtime sql for node "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:07:16.046470 (Thread-2): 12:07:16  Using databricks connection "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:07:16.046710 (Thread-2): 12:07:16  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempSumpFiBasicEps"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempSumpFiBasicEps`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  sk_companyid,
  fi_qtr_start_date,
  sum(fi_basic_eps) OVER (PARTITION BY companyid ORDER BY fi_qtr_start_date ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) - fi_basic_eps sum_fi_basic_eps
FROM `dbt_shabbirkdb`.`Financial`
JOIN `dbt_shabbirkdb`.`DimCompany`
  USING (sk_companyid);
  
2023-04-20 12:07:16.287217 (Thread-773): handling poll request
2023-04-20 12:07:16.287749 (Thread-773): 12:07:16  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bda0880>]}
2023-04-20 12:07:16.288485 (Thread-773): sending response (<Response 10406 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:17.100167 (Thread-774): handling ps request
2023-04-20 12:07:17.100658 (Thread-774): 12:07:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bda0970>]}
2023-04-20 12:07:17.102305 (Thread-774): sending response (<Response 20283 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:17.530506 (Thread-775): handling status request
2023-04-20 12:07:17.531004 (Thread-775): 12:07:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bda0c10>]}
2023-04-20 12:07:17.531490 (Thread-775): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:17.851737 (Thread-776): handling poll request
2023-04-20 12:07:17.852217 (Thread-776): 12:07:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bda0ca0>]}
2023-04-20 12:07:17.852688 (Thread-776): sending response (<Response 291 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:19.398918 (Thread-777): handling poll request
2023-04-20 12:07:19.399432 (Thread-777): 12:07:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bda0e50>]}
2023-04-20 12:07:19.400076 (Thread-777): sending response (<Response 290 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:19.439489 (Thread-2): 12:07:19  SQL status: OK in 3.39 seconds
2023-04-20 12:07:19.441998 (Thread-2): 12:07:19  Timing info for model.dbsql_dbt_tpch.tempSumpFiBasicEps (execute): 2023-04-20 12:07:15.299719 => 2023-04-20 12:07:19.441832
2023-04-20 12:07:19.442208 (Thread-2): 12:07:19  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: ROLLBACK
2023-04-20 12:07:19.442351 (Thread-2): 12:07:19  Databricks adapter: NotImplemented: rollback
2023-04-20 12:07:19.442476 (Thread-2): 12:07:19  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: Close
2023-04-20 12:07:19.642656 (Thread-778): handling ps request
2023-04-20 12:07:19.643152 (Thread-778): 12:07:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528221040>]}
2023-04-20 12:07:19.644697 (Thread-778): sending response (<Response 20282 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:19.722196 (Thread-2): 12:07:19  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6f1c1edd-0e83-4434-a371-cbca2d07a46b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26453b4760>]}
2023-04-20 12:07:19.722722 (Thread-2): 12:07:19  15 of 16 OK created sql table model dbt_shabbirkdb.tempSumpFiBasicEps .......... [OK in 4.44s]
2023-04-20 12:07:19.722959 (Thread-2): 12:07:19  Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:07:19.724083 (Thread-1): 12:07:19  Began running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:07:19.724420 (Thread-1): 12:07:19  16 of 16 START sql table model dbt_shabbirkdb.FactMarketHistory ................ [RUN]
2023-04-20 12:07:19.725408 (Thread-1): 12:07:19  Acquiring new databricks connection 'model.dbsql_dbt_tpch.FactMarketHistory'
2023-04-20 12:07:19.725599 (Thread-1): 12:07:19  Began compiling node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:07:19.730063 (Thread-1): 12:07:19  Writing injected SQL for node "model.dbsql_dbt_tpch.FactMarketHistory"
2023-04-20 12:07:19.746050 (Thread-1): 12:07:19  Timing info for model.dbsql_dbt_tpch.FactMarketHistory (compile): 2023-04-20 12:07:19.725646 => 2023-04-20 12:07:19.745854
2023-04-20 12:07:19.746293 (Thread-1): 12:07:19  Began executing node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:07:19.750769 (Thread-1): 12:07:19  Writing runtime sql for node "model.dbsql_dbt_tpch.FactMarketHistory"
2023-04-20 12:07:19.765362 (Thread-1): 12:07:19  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:07:19.765554 (Thread-1): 12:07:19  Using databricks connection "model.dbsql_dbt_tpch.FactMarketHistory"
2023-04-20 12:07:19.765768 (Thread-1): 12:07:19  On model.dbsql_dbt_tpch.FactMarketHistory: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date);
  
2023-04-20 12:07:19.765915 (Thread-1): 12:07:19  Opening a new connection, currently in state closed
2023-04-20 12:07:20.065646 (Thread-779): handling status request
2023-04-20 12:07:20.066120 (Thread-779): 12:07:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528221790>]}
2023-04-20 12:07:20.066594 (Thread-779): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:20.451950 (Thread-1): 12:07:20  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date);
  
2023-04-20 12:07:20.452179 (Thread-1): 12:07:20  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date)

2023-04-20 12:07:20.452308 (Thread-1): 12:07:20  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date)

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date)

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 12:07:20.452426 (Thread-1): 12:07:20  Databricks adapter: operation-id: b'\x01\xed\xdfs\xe6\xaf\x1bw\xadI\x07\x1c\x81,\xe4\xb4'
2023-04-20 12:07:20.452668 (Thread-1): 12:07:20  Timing info for model.dbsql_dbt_tpch.FactMarketHistory (execute): 2023-04-20 12:07:19.746346 => 2023-04-20 12:07:20.452535
2023-04-20 12:07:20.452886 (Thread-1): 12:07:20  On model.dbsql_dbt_tpch.FactMarketHistory: ROLLBACK
2023-04-20 12:07:20.453017 (Thread-1): 12:07:20  Databricks adapter: NotImplemented: rollback
2023-04-20 12:07:20.453145 (Thread-1): 12:07:20  On model.dbsql_dbt_tpch.FactMarketHistory: Close
2023-04-20 12:07:20.734173 (Thread-1): 12:07:20  Runtime Error in model FactMarketHistory (models/incremental/FactMarketHistory.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */
  
    
      
          create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
        
        
      using delta
        
        
        
        
        
        
        as
        
  SELECT 
    s.sk_securityid,
    s.sk_companyid,
    sk_dateid,
    fmh.dm_close / sum_fi_basic_eps AS peratio,
    (s.dividend / fmh.dm_close) / 100 yield,
    fiftytwoweekhigh,
    sk_fiftytwoweekhighdate,
    fiftytwoweeklow,
    sk_fiftytwoweeklowdate,
    dm_close closeprice,
    dm_high dayhigh,
    dm_low daylow,
    dm_vol volume,
    fmh.batchid
  FROM (
    SELECT * FROM (
      SELECT 
        a.*,
        b.sk_dateid AS sk_fiftytwoweeklowdate,
        c.sk_dateid AS sk_fiftytwoweekhighdate
      FROM
        `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
      JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
        ON
          a.dm_s_symb = b.dm_s_symb
          AND a.fiftytwoweeklow = b.dm_low
          AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
      JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
        ON 
          a.dm_s_symb = c.dm_s_symb
          AND a.fiftytwoweekhigh = c.dm_high
          AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
    QUALIFY ROW_NUMBER() OVER (
      PARTITION BY dm_s_symb, dm_date 
      ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
  -- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
  ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ^^^
    ON 
      s.symbol = fmh.dm_s_symb
      AND fmh.dm_date >= s.effectivedate 
      AND fmh.dm_date < s.enddate
  LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
    ON 
      f.sk_companyid = s.sk_companyid
      AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
      AND year(fmh.dm_date) = year(fi_qtr_start_date)
  
2023-04-20 12:07:20.734610 (Thread-1): 12:07:20  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6f1c1edd-0e83-4434-a371-cbca2d07a46b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26451c4e50>]}
2023-04-20 12:07:20.735048 (Thread-1): 12:07:20  16 of 16 ERROR creating sql table model dbt_shabbirkdb.FactMarketHistory ....... [ERROR in 1.01s]
2023-04-20 12:07:20.735259 (Thread-1): 12:07:20  Finished running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:07:20.737313 (MainThread): 12:07:20  Acquiring new databricks connection 'master'
2023-04-20 12:07:20.737538 (MainThread): 12:07:20  On master: ROLLBACK
2023-04-20 12:07:20.737693 (MainThread): 12:07:20  Opening a new connection, currently in state init
2023-04-20 12:07:20.980288 (Thread-780): handling poll request
2023-04-20 12:07:20.980783 (Thread-780): 12:07:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528221dc0>]}
2023-04-20 12:07:20.981976 (Thread-780): sending response (<Response 33131 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:21.028719 (MainThread): 12:07:21  Databricks adapter: NotImplemented: rollback
2023-04-20 12:07:21.028974 (MainThread): 12:07:21  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:07:21.029116 (MainThread): 12:07:21  Spark adapter: NotImplemented: commit
2023-04-20 12:07:21.029283 (MainThread): 12:07:21  On master: ROLLBACK
2023-04-20 12:07:21.029419 (MainThread): 12:07:21  Databricks adapter: NotImplemented: rollback
2023-04-20 12:07:21.029559 (MainThread): 12:07:21  On master: Close
2023-04-20 12:07:21.314539 (MainThread): 12:07:21  Connection 'master' was properly closed.
2023-04-20 12:07:21.314738 (MainThread): 12:07:21  Connection 'model.dbsql_dbt_tpch.FactMarketHistory' was properly closed.
2023-04-20 12:07:21.314848 (MainThread): 12:07:21  Connection 'model.dbsql_dbt_tpch.tempSumpFiBasicEps' was properly closed.
2023-04-20 12:07:21.314950 (MainThread): 12:07:21  Connection 'model.dbsql_dbt_tpch.Financial' was properly closed.
2023-04-20 12:07:21.315049 (MainThread): 12:07:21  Connection 'model.dbsql_dbt_tpch.Prospect' was properly closed.
2023-04-20 12:07:21.315357 (MainThread): 12:07:21  
2023-04-20 12:07:21.315503 (MainThread): 12:07:21  Finished running 1 view model, 15 table models in 0 hours 0 minutes and 22.38 seconds (22.38s).
2023-04-20 12:07:21.403845 (MainThread): 12:07:21  
2023-04-20 12:07:21.404089 (MainThread): 12:07:21  Completed with 3 errors and 0 warnings:
2023-04-20 12:07:21.404221 (MainThread): 12:07:21  
2023-04-20 12:07:21.404363 (MainThread): 12:07:21  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
2023-04-20 12:07:21.404488 (MainThread): 12:07:21    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 12:07:21.404601 (MainThread): 12:07:21  
2023-04-20 12:07:21.404719 (MainThread): 12:07:21  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
2023-04-20 12:07:21.404830 (MainThread): 12:07:21    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 12:07:21.404936 (MainThread): 12:07:21  
2023-04-20 12:07:21.405060 (MainThread): 12:07:21  Runtime Error in model FactMarketHistory (models/incremental/FactMarketHistory.sql)
2023-04-20 12:07:21.405170 (MainThread): 12:07:21    
2023-04-20 12:07:21.405272 (MainThread): 12:07:21    [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)
2023-04-20 12:07:21.405371 (MainThread): 12:07:21    
2023-04-20 12:07:21.405471 (MainThread): 12:07:21    == SQL ==
2023-04-20 12:07:21.405571 (MainThread): 12:07:21    /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */
2023-04-20 12:07:21.405669 (MainThread): 12:07:21    
2023-04-20 12:07:21.405765 (MainThread): 12:07:21      
2023-04-20 12:07:21.405864 (MainThread): 12:07:21        
2023-04-20 12:07:21.405974 (MainThread): 12:07:21            create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
2023-04-20 12:07:21.406075 (MainThread): 12:07:21          
2023-04-20 12:07:21.406171 (MainThread): 12:07:21          
2023-04-20 12:07:21.406268 (MainThread): 12:07:21        using delta
2023-04-20 12:07:21.406364 (MainThread): 12:07:21          
2023-04-20 12:07:21.406460 (MainThread): 12:07:21          
2023-04-20 12:07:21.406557 (MainThread): 12:07:21          
2023-04-20 12:07:21.406655 (MainThread): 12:07:21          
2023-04-20 12:07:21.406753 (MainThread): 12:07:21          
2023-04-20 12:07:21.406852 (MainThread): 12:07:21          
2023-04-20 12:07:21.406950 (MainThread): 12:07:21          as
2023-04-20 12:07:21.407047 (MainThread): 12:07:21          
2023-04-20 12:07:21.407144 (MainThread): 12:07:21    SELECT 
2023-04-20 12:07:21.407242 (MainThread): 12:07:21      s.sk_securityid,
2023-04-20 12:07:21.407341 (MainThread): 12:07:21      s.sk_companyid,
2023-04-20 12:07:21.407441 (MainThread): 12:07:21      sk_dateid,
2023-04-20 12:07:21.407569 (MainThread): 12:07:21      fmh.dm_close / sum_fi_basic_eps AS peratio,
2023-04-20 12:07:21.407675 (MainThread): 12:07:21      (s.dividend / fmh.dm_close) / 100 yield,
2023-04-20 12:07:21.407774 (MainThread): 12:07:21      fiftytwoweekhigh,
2023-04-20 12:07:21.407871 (MainThread): 12:07:21      sk_fiftytwoweekhighdate,
2023-04-20 12:07:21.407966 (MainThread): 12:07:21      fiftytwoweeklow,
2023-04-20 12:07:21.408061 (MainThread): 12:07:21      sk_fiftytwoweeklowdate,
2023-04-20 12:07:21.408156 (MainThread): 12:07:21      dm_close closeprice,
2023-04-20 12:07:21.408252 (MainThread): 12:07:21      dm_high dayhigh,
2023-04-20 12:07:21.408348 (MainThread): 12:07:21      dm_low daylow,
2023-04-20 12:07:21.408442 (MainThread): 12:07:21      dm_vol volume,
2023-04-20 12:07:21.408537 (MainThread): 12:07:21      fmh.batchid
2023-04-20 12:07:21.408632 (MainThread): 12:07:21    FROM (
2023-04-20 12:07:21.408726 (MainThread): 12:07:21      SELECT * FROM (
2023-04-20 12:07:21.408820 (MainThread): 12:07:21        SELECT 
2023-04-20 12:07:21.408914 (MainThread): 12:07:21          a.*,
2023-04-20 12:07:21.409008 (MainThread): 12:07:21          b.sk_dateid AS sk_fiftytwoweeklowdate,
2023-04-20 12:07:21.409209 (MainThread): 12:07:21          c.sk_dateid AS sk_fiftytwoweekhighdate
2023-04-20 12:07:21.409308 (MainThread): 12:07:21        FROM
2023-04-20 12:07:21.409406 (MainThread): 12:07:21          `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
2023-04-20 12:07:21.409501 (MainThread): 12:07:21        JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
2023-04-20 12:07:21.409597 (MainThread): 12:07:21          ON
2023-04-20 12:07:21.409692 (MainThread): 12:07:21            a.dm_s_symb = b.dm_s_symb
2023-04-20 12:07:21.409787 (MainThread): 12:07:21            AND a.fiftytwoweeklow = b.dm_low
2023-04-20 12:07:21.409881 (MainThread): 12:07:21            AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
2023-04-20 12:07:21.409989 (MainThread): 12:07:21        JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
2023-04-20 12:07:21.410084 (MainThread): 12:07:21          ON 
2023-04-20 12:07:21.410179 (MainThread): 12:07:21            a.dm_s_symb = c.dm_s_symb
2023-04-20 12:07:21.410273 (MainThread): 12:07:21            AND a.fiftytwoweekhigh = c.dm_high
2023-04-20 12:07:21.410366 (MainThread): 12:07:21            AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
2023-04-20 12:07:21.410459 (MainThread): 12:07:21      QUALIFY ROW_NUMBER() OVER (
2023-04-20 12:07:21.410553 (MainThread): 12:07:21        PARTITION BY dm_s_symb, dm_date 
2023-04-20 12:07:21.410648 (MainThread): 12:07:21        ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
2023-04-20 12:07:21.410742 (MainThread): 12:07:21    -- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
2023-04-20 12:07:21.410837 (MainThread): 12:07:21    ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
2023-04-20 12:07:21.410933 (MainThread): 12:07:21    ^^^
2023-04-20 12:07:21.411030 (MainThread): 12:07:21      ON 
2023-04-20 12:07:21.411127 (MainThread): 12:07:21        s.symbol = fmh.dm_s_symb
2023-04-20 12:07:21.411223 (MainThread): 12:07:21        AND fmh.dm_date >= s.effectivedate 
2023-04-20 12:07:21.411318 (MainThread): 12:07:21        AND fmh.dm_date < s.enddate
2023-04-20 12:07:21.411412 (MainThread): 12:07:21    LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
2023-04-20 12:07:21.411547 (MainThread): 12:07:21      ON 
2023-04-20 12:07:21.411654 (MainThread): 12:07:21        f.sk_companyid = s.sk_companyid
2023-04-20 12:07:21.411752 (MainThread): 12:07:21        AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
2023-04-20 12:07:21.411851 (MainThread): 12:07:21        AND year(fmh.dm_date) = year(fi_qtr_start_date)
2023-04-20 12:07:21.411949 (MainThread): 12:07:21    
2023-04-20 12:07:21.412082 (MainThread): 12:07:21  
2023-04-20 12:07:21.412221 (MainThread): 12:07:21  Done. PASS=9 WARN=0 ERROR=3 SKIP=4 TOTAL=16
2023-04-20 12:07:22.377423 (Thread-781): handling ps request
2023-04-20 12:07:22.377918 (Thread-781): 12:07:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087f37c0>]}
2023-04-20 12:07:22.379459 (Thread-781): sending response (<Response 20307 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:22.566365 (Thread-782): handling poll request
2023-04-20 12:07:22.566851 (Thread-782): 12:07:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087e1f70>]}
2023-04-20 12:07:22.570052 (Thread-782): sending response (<Response 128725 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:07:22.804915 (Thread-783): handling status request
2023-04-20 12:07:22.805407 (Thread-783): 12:07:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52821d7f0>]}
2023-04-20 12:07:22.805887 (Thread-783): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:08:38.514143 (Thread-784): handling run_sql request
2023-04-20 12:08:38.516107 (Thread-784): 12:08:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52821d8b0>]}
2023-04-20 12:08:38.613041 (Thread-785): handling ps request
2023-04-20 12:08:38.614096 (Thread-785): 12:08:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087d1fa0>]}
2023-04-20 12:08:38.616746 (Thread-785): sending response (<Response 20830 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:08:39.249128 (Thread-786): handling poll request
2023-04-20 12:08:39.249631 (Thread-786): 12:08:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087e8220>]}
2023-04-20 12:08:39.250172 (Thread-786): sending response (<Response 432 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:08:39.684826 (Thread-787): handling status request
2023-04-20 12:08:39.685330 (Thread-787): 12:08:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087e81c0>]}
2023-04-20 12:08:39.685864 (Thread-787): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:08:39.948899 (Thread-788): handling ps request
2023-04-20 12:08:39.949396 (Thread-788): 12:08:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087e82e0>]}
2023-04-20 12:08:39.950924 (Thread-788): sending response (<Response 20830 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:08:41.265580 (Thread-784): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:08:41.302678 (MainThread): 12:08:41  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ac8b3605-469d-43aa-a65a-a349f1fccd82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f54f69eb1f0>]}
2023-04-20 12:08:41.303280 (MainThread): 12:08:41  Found 16 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:08:41.304640 (Thread-1): 12:08:41  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 12:08:41.304874 (Thread-1): 12:08:41  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 12:08:41.309474 (Thread-1): 12:08:41  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 12:08:41.304928 => 2023-04-20 12:08:41.309306
2023-04-20 12:08:41.309690 (Thread-1): 12:08:41  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 12:08:41.310145 (Thread-1): 12:08:41  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 12:08:41.310516 (Thread-1): 12:08:41  On rpc.dbsql_dbt_tpch.request: 
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `dbt_shabbirkdb`.`AccountIncrementalStg` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:08:41.310660 (Thread-1): 12:08:41  Opening a new connection, currently in state init
2023-04-20 12:08:41.856311 (Thread-789): handling poll request
2023-04-20 12:08:41.856810 (Thread-789): 12:08:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087e8dc0>]}
2023-04-20 12:08:41.857432 (Thread-789): sending response (<Response 5908 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:08:41.895089 (Thread-790): handling ps request
2023-04-20 12:08:41.895572 (Thread-790): 12:08:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087e8d90>]}
2023-04-20 12:08:41.897133 (Thread-790): sending response (<Response 20825 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:08:42.082304 (Thread-1): 12:08:42  Databricks adapter: Error while running:

SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `dbt_shabbirkdb`.`AccountIncrementalStg` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:08:42.082537 (Thread-1): 12:08:42  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `a`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`b`.`sk_brokerid`, `a`.`brokerid`, `a`.`accountid`, `b`.`brokerid`, `a`.`accountdesc`].; line 5 pos 2
2023-04-20 12:08:42.082661 (Thread-1): 12:08:42  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `a`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`b`.`sk_brokerid`, `a`.`brokerid`, `a`.`accountid`, `b`.`brokerid`, `a`.`accountdesc`].; line 5 pos 2
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `a`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`b`.`sk_brokerid`, `a`.`brokerid`, `a`.`accountid`, `b`.`brokerid`, `a`.`accountdesc`].; line 5 pos 2
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 12:08:42.082779 (Thread-1): 12:08:42  Databricks adapter: operation-id: b'\x01\xed\xdft\x17I\x14N\x8b\xd3\n0.5)\xad'
2023-04-20 12:08:42.082978 (Thread-1): 12:08:42  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 12:08:41.309737 => 2023-04-20 12:08:42.082851
2023-04-20 12:08:42.083168 (Thread-1): 12:08:42  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 12:08:42.112984 (Thread-791): handling ps request
2023-04-20 12:08:42.113449 (Thread-791): 12:08:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087e89a0>]}
2023-04-20 12:08:42.114936 (Thread-791): sending response (<Response 20825 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:08:42.361905 (Thread-1): Got an exception: Runtime Error
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `a`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`b`.`sk_brokerid`, `a`.`brokerid`, `a`.`accountid`, `b`.`brokerid`, `a`.`accountdesc`].; line 5 pos 2
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `a`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`b`.`sk_brokerid`, `a`.`brokerid`, `a`.`accountid`, `b`.`brokerid`, `a`.`accountdesc`].; line 5 pos 2

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `a`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`b`.`sk_brokerid`, `a`.`brokerid`, `a`.`accountid`, `b`.`brokerid`, `a`.`accountdesc`].; line 5 pos 2
2023-04-20 12:08:42.363718 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `a`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`b`.`sk_brokerid`, `a`.`brokerid`, `a`.`accountid`, `b`.`brokerid`, `a`.`accountdesc`].; line 5 pos 2', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM {{ ref('AccountIncrementalStg') }} a\n        JOIN {{ source('tpcdi', 'BatchDate') }} bd\n          ON a.batchid = bd.batchid\n        JOIN {{ source('tpcdi', 'StatusType') }} st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN {{ ref('DimCustomerStg') }} c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN {{ ref('DimBroker') }} b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM `dbt_shabbirkdb`.`AccountIncrementalStg` a\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd\n          ON a.batchid = bd.batchid\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN `dbt_shabbirkdb`.`DimBroker` b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `a`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`b`.`sk_brokerid`, `a`.`brokerid`, `a`.`accountid`, `b`.`brokerid`, `a`.`accountdesc`].; line 5 pos 2', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM {{ ref('AccountIncrementalStg') }} a\n        JOIN {{ source('tpcdi', 'BatchDate') }} bd\n          ON a.batchid = bd.batchid\n        JOIN {{ source('tpcdi', 'StatusType') }} st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN {{ ref('DimCustomerStg') }} c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN {{ ref('DimBroker') }} b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM `dbt_shabbirkdb`.`AccountIncrementalStg` a\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd\n          ON a.batchid = bd.batchid\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN `dbt_shabbirkdb`.`DimBroker` b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 12:08:42.430709 (Thread-792): handling status request
2023-04-20 12:08:42.431197 (Thread-792): 12:08:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087e8c40>]}
2023-04-20 12:08:42.431711 (Thread-792): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:08:42.828263 (Thread-793): handling poll request
2023-04-20 12:08:42.828740 (Thread-793): 12:08:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087e8490>]}
2023-04-20 12:08:42.829458 (Thread-793): sending response (<Response 34684 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:08:44.464993 (Thread-794): handling poll request
2023-04-20 12:08:44.465492 (Thread-794): 12:08:44  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5087e8580>]}
2023-04-20 12:08:44.466196 (Thread-794): sending response (<Response 34684 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:08:44.810841 (Thread-795): handling ps request
2023-04-20 12:08:44.811326 (Thread-795): 12:08:44  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bec0220>]}
2023-04-20 12:08:44.833965 (Thread-795): sending response (<Response 20847 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:08:45.542359 (Thread-796): handling status request
2023-04-20 12:08:45.542834 (Thread-796): 12:08:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bec04c0>]}
2023-04-20 12:08:45.543312 (Thread-796): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:08:54.462258 (Thread-797): 12:08:54  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 12:08:54.484055 (Thread-797): 12:08:54  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 12:08:54.484463 (Thread-797): 12:08:54  previous checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, current checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886
2023-04-20 12:08:54.484676 (Thread-797): 12:08:54  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52822b310>]}
2023-04-20 12:08:54.962296 (Thread-798): handling status request
2023-04-20 12:08:54.983212 (Thread-798): 12:08:54  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52816be80>]}
2023-04-20 12:08:54.988816 (Thread-798): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:08:56.020625 (Thread-797): 12:08:56  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 12:08:56.024116 (Thread-797): 12:08:56  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 12:08:56.027283 (Thread-797): 12:08:56  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 12:08:56.030046 (Thread-797): 12:08:56  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 12:08:56.033062 (Thread-797): 12:08:56  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 12:08:56.035981 (Thread-797): 12:08:56  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 12:08:56.039184 (Thread-797): 12:08:56  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 12:08:56.042205 (Thread-797): 12:08:56  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 12:08:56.045236 (Thread-797): 12:08:56  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 12:08:56.048016 (Thread-797): 12:08:56  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 12:08:56.051135 (Thread-797): 12:08:56  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 12:08:56.054152 (Thread-797): 12:08:56  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 12:08:56.056854 (Thread-797): 12:08:56  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 12:08:56.059660 (Thread-797): 12:08:56  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 12:08:56.062133 (Thread-797): 12:08:56  1699: static parser successfully parsed silver/AccountIncrementalStg.sql
2023-04-20 12:08:56.065487 (Thread-797): 12:08:56  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 12:08:56.066066 (Thread-799): handling status request
2023-04-20 12:08:56.081185 (Thread-799): 12:08:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5282b1580>]}
2023-04-20 12:08:56.081612 (Thread-799): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:08:56.221091 (Thread-797): 12:08:56  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5085b5ee0>]}
2023-04-20 12:08:57.060834 (Thread-800): handling status request
2023-04-20 12:08:57.061334 (Thread-800): 12:08:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50859e6a0>]}
2023-04-20 12:08:57.061972 (Thread-800): sending response (<Response 7126 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:09:38.270603 (Thread-801): 12:09:38  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 12:09:38.562634 (Thread-801): 12:09:38  Partial parsing enabled: 1 files deleted, 0 files added, 0 files changed.
2023-04-20 12:09:38.563445 (Thread-801): 12:09:38  Partial parsing: deleted file: dbsql_dbt_tpch://models/silver/AccountIncrementalStg.sql
2023-04-20 12:09:38.574282 (Thread-801): 12:09:38  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 12:09:38.578941 (Thread-801): 12:09:38  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 12:09:38.582522 (Thread-801): 12:09:38  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 12:09:38.585679 (Thread-801): 12:09:38  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 12:09:38.743219 (Thread-802): handling status request
2023-04-20 12:09:38.743726 (Thread-802): 12:09:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50817f2b0>]}
2023-04-20 12:09:38.744239 (Thread-802): sending response (<Response 2585 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:10:58.086574 (Thread-803): 12:10:58  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 12:10:58.502609 (Thread-803): 12:10:58  Partial parsing enabled: 1 files deleted, 0 files added, 1 files changed.
2023-04-20 12:10:58.503345 (Thread-803): 12:10:58  Partial parsing: deleted file: dbsql_dbt_tpch://models/silver/AccountIncrementalStg.sql
2023-04-20 12:10:58.503637 (Thread-803): 12:10:58  Partial parsing: updated file: dbsql_dbt_tpch://models/incremental/DimAccount.sql
2023-04-20 12:10:58.511555 (Thread-803): 12:10:58  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 12:10:58.515420 (Thread-803): 12:10:58  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 12:10:58.518443 (Thread-803): 12:10:58  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 12:10:58.521614 (Thread-803): 12:10:58  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 12:10:58.546714 (Thread-804): handling status request
2023-04-20 12:10:58.547091 (Thread-804): 12:10:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508290280>]}
2023-04-20 12:10:58.547608 (Thread-804): sending response (<Response 2924 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:10:59.405058 (Thread-805): handling status request
2023-04-20 12:10:59.405557 (Thread-805): 12:10:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8899ee0>]}
2023-04-20 12:10:59.406083 (Thread-805): sending response (<Response 2924 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:10:59.590733 (Thread-806): handling ps request
2023-04-20 12:10:59.591185 (Thread-806): 12:10:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8899d30>]}
2023-04-20 12:10:59.592708 (Thread-806): sending response (<Response 20847 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:19.721030 (Thread-807): 12:11:19  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 12:11:20.010352 (Thread-807): 12:11:20  Partial parsing enabled: 1 files deleted, 0 files added, 1 files changed.
2023-04-20 12:11:20.011062 (Thread-807): 12:11:20  Partial parsing: deleted file: dbsql_dbt_tpch://models/silver/AccountIncrementalStg.sql
2023-04-20 12:11:20.011301 (Thread-807): 12:11:20  Partial parsing: updated file: dbsql_dbt_tpch://models/incremental/DimAccount.sql
2023-04-20 12:11:20.019662 (Thread-807): 12:11:20  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 12:11:20.023275 (Thread-807): 12:11:20  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 12:11:20.026287 (Thread-807): 12:11:20  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 12:11:20.029205 (Thread-807): 12:11:20  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 12:11:20.085636 (Thread-807): 12:11:20  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8827a60>]}
2023-04-20 12:11:20.208819 (Thread-808): handling status request
2023-04-20 12:11:20.209302 (Thread-808): 12:11:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50808a340>]}
2023-04-20 12:11:20.209814 (Thread-808): sending response (<Response 3182 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:22.603230 (Thread-809): handling ps request
2023-04-20 12:11:22.603798 (Thread-809): 12:11:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f87fca90>]}
2023-04-20 12:11:22.605338 (Thread-809): sending response (<Response 20847 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:22.624490 (Thread-810): handling run_sql request
2023-04-20 12:11:22.624917 (Thread-810): 12:11:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f87fc880>]}
2023-04-20 12:11:23.387938 (Thread-811): handling ps request
2023-04-20 12:11:23.389063 (Thread-811): 12:11:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f87e5220>]}
2023-04-20 12:11:23.391836 (Thread-811): sending response (<Response 21370 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:24.089665 (Thread-812): handling poll request
2023-04-20 12:11:24.090173 (Thread-812): 12:11:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8784eb0>]}
2023-04-20 12:11:24.090710 (Thread-812): sending response (<Response 432 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:24.549083 (Thread-813): handling status request
2023-04-20 12:11:24.549620 (Thread-813): 12:11:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8784b20>]}
2023-04-20 12:11:24.550233 (Thread-813): sending response (<Response 3182 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:25.388921 (Thread-810): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:25.424152 (MainThread): 12:11:25  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dd67c53d-fb5a-48c1-8668-bf4691b124b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81e985df70>]}
2023-04-20 12:11:25.424744 (MainThread): 12:11:25  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:11:25.426094 (Thread-1): 12:11:25  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 12:11:25.426328 (Thread-1): 12:11:25  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 12:11:25.431040 (Thread-1): 12:11:25  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 12:11:25.426381 => 2023-04-20 12:11:25.430874
2023-04-20 12:11:25.431254 (Thread-1): 12:11:25  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 12:11:25.431704 (Thread-1): 12:11:25  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 12:11:25.432114 (Thread-1): 12:11:25  On rpc.dbsql_dbt_tpch.request: 
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          md5(customerid) as sk_customerid
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          md5(ca_c_id) as sk_customerid
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:11:25.432260 (Thread-1): 12:11:25  Opening a new connection, currently in state init
2023-04-20 12:11:26.037303 (Thread-814): handling ps request
2023-04-20 12:11:26.037793 (Thread-814): 12:11:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8784a90>]}
2023-04-20 12:11:26.039482 (Thread-814): sending response (<Response 21365 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:26.055784 (Thread-815): handling ps request
2023-04-20 12:11:26.056115 (Thread-815): 12:11:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f87e5c40>]}
2023-04-20 12:11:26.057415 (Thread-815): sending response (<Response 21365 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:26.205888 (Thread-1): 12:11:26  Databricks adapter: Error while running:

SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          md5(customerid) as sk_customerid
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          md5(ca_c_id) as sk_customerid
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:11:26.206116 (Thread-1): 12:11:26  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'customerid': missing ')'.(line 39, pos 10)

== SQL ==

SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          md5(customerid) as sk_customerid
          customerid,
----------^^^
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          md5(ca_c_id) as sk_customerid
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */

2023-04-20 12:11:26.206243 (Thread-1): 12:11:26  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'customerid': missing ')'.(line 39, pos 10)

== SQL ==

SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          md5(customerid) as sk_customerid
          customerid,
----------^^^
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          md5(ca_c_id) as sk_customerid
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'customerid': missing ')'.(line 39, pos 10)

== SQL ==

SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          md5(customerid) as sk_customerid
          customerid,
----------^^^
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          md5(ca_c_id) as sk_customerid
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 12:11:26.206365 (Thread-1): 12:11:26  Databricks adapter: operation-id: b'\x01\xed\xdfty)\x17)\xa1}h\xca\xc1\x9fUq'
2023-04-20 12:11:26.206565 (Thread-1): 12:11:26  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 12:11:25.431301 => 2023-04-20 12:11:26.206437
2023-04-20 12:11:26.206760 (Thread-1): 12:11:26  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 12:11:26.476990 (Thread-1): Got an exception: Runtime Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'customerid': missing ')'.(line 39, pos 10)
  
  == SQL ==
  
  SELECT
    a.accountid,
    b.sk_brokerid,
    a.sk_customerid,
    a.accountdesc,
    a.TaxStatus,
    a.status,
    a.batchid,
    a.effectivedate,
    a.enddate
  FROM (
    SELECT
      a.* except(effectivedate, enddate, customerid),
      c.sk_customerid,
      if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
      if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
    FROM (
      SELECT *
      FROM (
        SELECT
          accountid,
          customerid,
          coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
            PARTITION BY accountid ORDER BY update_ts)) accountdesc,
          coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
            PARTITION BY accountid ORDER BY update_ts)) taxstatus,
          coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
            PARTITION BY accountid ORDER BY update_ts)) brokerid,
          coalesce(status, last_value(status) IGNORE NULLS OVER (
            PARTITION BY accountid ORDER BY update_ts)) status,
          date(update_ts) effectivedate,
          nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
          batchid
        FROM (
          SELECT
            accountid,
            md5(customerid) as sk_customerid
            customerid,
  ----------^^^
            accountdesc,
            taxstatus,
            brokerid,
            status,
            update_ts,
            1 batchid
          FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
          WHERE ActionType NOT IN ('UPDCUST', 'INACT')
          UNION ALL
          SELECT
            accountid,
            md5(ca_c_id) as sk_customerid
            a.ca_c_id customerid,
            accountDesc,
            TaxStatus,
            a.ca_b_id brokerid,
            st_name as status,
            TIMESTAMP(bd.batchdate) update_ts,
            a.batchid
          FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
            ON a.batchid = bd.batchid
          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
            ON a.CA_ST_ID = st.st_id
        ) a
      ) a
      WHERE a.effectivedate < a.enddate
    ) a
    FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
      ON 
        a.customerid = c.customerid
        AND c.enddate > a.effectivedate
        AND c.effectivedate < a.enddate
  ) a
  LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
    ON a.brokerid = b.brokerid
  limit 500
  /* limit added automatically by dbt cloud */
  
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'customerid': missing ')'.(line 39, pos 10)

== SQL ==

SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          md5(customerid) as sk_customerid
          customerid,
----------^^^
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          md5(ca_c_id) as sk_customerid
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'customerid': missing ')'.(line 39, pos 10)
  
  == SQL ==
  
  SELECT
    a.accountid,
    b.sk_brokerid,
    a.sk_customerid,
    a.accountdesc,
    a.TaxStatus,
    a.status,
    a.batchid,
    a.effectivedate,
    a.enddate
  FROM (
    SELECT
      a.* except(effectivedate, enddate, customerid),
      c.sk_customerid,
      if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
      if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
    FROM (
      SELECT *
      FROM (
        SELECT
          accountid,
          customerid,
          coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
            PARTITION BY accountid ORDER BY update_ts)) accountdesc,
          coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
            PARTITION BY accountid ORDER BY update_ts)) taxstatus,
          coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
            PARTITION BY accountid ORDER BY update_ts)) brokerid,
          coalesce(status, last_value(status) IGNORE NULLS OVER (
            PARTITION BY accountid ORDER BY update_ts)) status,
          date(update_ts) effectivedate,
          nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
          batchid
        FROM (
          SELECT
            accountid,
            md5(customerid) as sk_customerid
            customerid,
  ----------^^^
            accountdesc,
            taxstatus,
            brokerid,
            status,
            update_ts,
            1 batchid
          FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
          WHERE ActionType NOT IN ('UPDCUST', 'INACT')
          UNION ALL
          SELECT
            accountid,
            md5(ca_c_id) as sk_customerid
            a.ca_c_id customerid,
            accountDesc,
            TaxStatus,
            a.ca_b_id brokerid,
            st_name as status,
            TIMESTAMP(bd.batchdate) update_ts,
            a.batchid
          FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
            ON a.batchid = bd.batchid
          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
            ON a.CA_ST_ID = st.st_id
        ) a
      ) a
      WHERE a.effectivedate < a.enddate
    ) a
    FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
      ON 
        a.customerid = c.customerid
        AND c.enddate > a.effectivedate
        AND c.effectivedate < a.enddate
  ) a
  LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
    ON a.brokerid = b.brokerid
  limit 500
  /* limit added automatically by dbt cloud */
2023-04-20 12:11:26.478953 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': "Runtime Error in rpc request (from remote system.sql)\n  \n  [PARSE_SYNTAX_ERROR] Syntax error at or near 'customerid': missing ')'.(line 39, pos 10)\n  \n  == SQL ==\n  \n  SELECT\n    a.accountid,\n    b.sk_brokerid,\n    a.sk_customerid,\n    a.accountdesc,\n    a.TaxStatus,\n    a.status,\n    a.batchid,\n    a.effectivedate,\n    a.enddate\n  FROM (\n    SELECT\n      a.* except(effectivedate, enddate, customerid),\n      c.sk_customerid,\n      if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n      if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n    FROM (\n      SELECT *\n      FROM (\n        SELECT\n          accountid,\n          customerid,\n          coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n            PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n          coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n            PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n          coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n            PARTITION BY accountid ORDER BY update_ts)) brokerid,\n          coalesce(status, last_value(status) IGNORE NULLS OVER (\n            PARTITION BY accountid ORDER BY update_ts)) status,\n          date(update_ts) effectivedate,\n          nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n          batchid\n        FROM (\n          SELECT\n            accountid,\n            md5(customerid) as sk_customerid\n            customerid,\n  ----------^^^\n            accountdesc,\n            taxstatus,\n            brokerid,\n            status,\n            update_ts,\n            1 batchid\n          FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n          WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n          UNION ALL\n          SELECT\n            accountid,\n            md5(ca_c_id) as sk_customerid\n            a.ca_c_id customerid,\n            accountDesc,\n            TaxStatus,\n            a.ca_b_id brokerid,\n            st_name as status,\n            TIMESTAMP(bd.batchdate) update_ts,\n            a.batchid\n          FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a\n          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd\n            ON a.batchid = bd.batchid\n          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st \n            ON a.CA_ST_ID = st.st_id\n        ) a\n      ) a\n      WHERE a.effectivedate < a.enddate\n    ) a\n    FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c \n      ON \n        a.customerid = c.customerid\n        AND c.enddate > a.effectivedate\n        AND c.effectivedate < a.enddate\n  ) a\n  LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b \n    ON a.brokerid = b.brokerid\n  limit 500\n  /* limit added automatically by dbt cloud */\n  ", 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          md5(customerid) as sk_customerid\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          md5(ca_c_id) as sk_customerid\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM {{ source('tpcdi', 'AccountIncremental') }} a\n        JOIN {{ source('tpcdi', 'BatchDate') }} bd\n          ON a.batchid = bd.batchid\n        JOIN {{ source('tpcdi', 'StatusType') }} st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN {{ ref('DimCustomerStg') }} c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN {{ ref('DimBroker') }} b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          md5(customerid) as sk_customerid\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          md5(ca_c_id) as sk_customerid\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd\n          ON a.batchid = bd.batchid\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN `dbt_shabbirkdb`.`DimBroker` b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': "Runtime Error in rpc request (from remote system.sql)\n  \n  [PARSE_SYNTAX_ERROR] Syntax error at or near 'customerid': missing ')'.(line 39, pos 10)\n  \n  == SQL ==\n  \n  SELECT\n    a.accountid,\n    b.sk_brokerid,\n    a.sk_customerid,\n    a.accountdesc,\n    a.TaxStatus,\n    a.status,\n    a.batchid,\n    a.effectivedate,\n    a.enddate\n  FROM (\n    SELECT\n      a.* except(effectivedate, enddate, customerid),\n      c.sk_customerid,\n      if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n      if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n    FROM (\n      SELECT *\n      FROM (\n        SELECT\n          accountid,\n          customerid,\n          coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n            PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n          coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n            PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n          coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n            PARTITION BY accountid ORDER BY update_ts)) brokerid,\n          coalesce(status, last_value(status) IGNORE NULLS OVER (\n            PARTITION BY accountid ORDER BY update_ts)) status,\n          date(update_ts) effectivedate,\n          nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n          batchid\n        FROM (\n          SELECT\n            accountid,\n            md5(customerid) as sk_customerid\n            customerid,\n  ----------^^^\n            accountdesc,\n            taxstatus,\n            brokerid,\n            status,\n            update_ts,\n            1 batchid\n          FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n          WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n          UNION ALL\n          SELECT\n            accountid,\n            md5(ca_c_id) as sk_customerid\n            a.ca_c_id customerid,\n            accountDesc,\n            TaxStatus,\n            a.ca_b_id brokerid,\n            st_name as status,\n            TIMESTAMP(bd.batchdate) update_ts,\n            a.batchid\n          FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a\n          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd\n            ON a.batchid = bd.batchid\n          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st \n            ON a.CA_ST_ID = st.st_id\n        ) a\n      ) a\n      WHERE a.effectivedate < a.enddate\n    ) a\n    FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c \n      ON \n        a.customerid = c.customerid\n        AND c.enddate > a.effectivedate\n        AND c.effectivedate < a.enddate\n  ) a\n  LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b \n    ON a.brokerid = b.brokerid\n  limit 500\n  /* limit added automatically by dbt cloud */\n  ", 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          md5(customerid) as sk_customerid\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          md5(ca_c_id) as sk_customerid\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM {{ source('tpcdi', 'AccountIncremental') }} a\n        JOIN {{ source('tpcdi', 'BatchDate') }} bd\n          ON a.batchid = bd.batchid\n        JOIN {{ source('tpcdi', 'StatusType') }} st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN {{ ref('DimCustomerStg') }} c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN {{ ref('DimBroker') }} b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          md5(customerid) as sk_customerid\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          md5(ca_c_id) as sk_customerid\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd\n          ON a.batchid = bd.batchid\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN `dbt_shabbirkdb`.`DimBroker` b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 12:11:26.559168 (Thread-816): handling status request
2023-04-20 12:11:26.559717 (Thread-816): 12:11:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f88279d0>]}
2023-04-20 12:11:26.560281 (Thread-816): sending response (<Response 3182 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:26.786310 (Thread-817): handling poll request
2023-04-20 12:11:26.786789 (Thread-817): 12:11:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f88279a0>]}
2023-04-20 12:11:26.787646 (Thread-817): sending response (<Response 61392 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:26.847283 (Thread-818): handling poll request
2023-04-20 12:11:26.847679 (Thread-818): 12:11:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f87e5e50>]}
2023-04-20 12:11:26.848431 (Thread-818): sending response (<Response 61392 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:28.120257 (Thread-819): handling status request
2023-04-20 12:11:28.120743 (Thread-819): 12:11:28  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8820250>]}
2023-04-20 12:11:28.121269 (Thread-819): sending response (<Response 3182 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:28.971234 (Thread-820): handling ps request
2023-04-20 12:11:28.971750 (Thread-820): 12:11:28  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8820190>]}
2023-04-20 12:11:28.973294 (Thread-820): sending response (<Response 21388 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:29.419144 (Thread-821): handling status request
2023-04-20 12:11:29.419664 (Thread-821): 12:11:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8820700>]}
2023-04-20 12:11:29.420177 (Thread-821): sending response (<Response 3182 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:39.883084 (Thread-822): handling run_sql request
2023-04-20 12:11:39.883591 (Thread-822): 12:11:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8820970>]}
2023-04-20 12:11:39.918758 (Thread-823): handling ps request
2023-04-20 12:11:39.919720 (Thread-823): 12:11:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f88e3580>]}
2023-04-20 12:11:39.923241 (Thread-823): sending response (<Response 21911 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:40.460768 (Thread-824): handling poll request
2023-04-20 12:11:40.461256 (Thread-824): 12:11:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f87844c0>]}
2023-04-20 12:11:40.461782 (Thread-824): sending response (<Response 432 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:40.884847 (Thread-825): handling status request
2023-04-20 12:11:40.885321 (Thread-825): 12:11:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f87e55e0>]}
2023-04-20 12:11:40.885893 (Thread-825): sending response (<Response 3182 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:41.084053 (Thread-826): handling ps request
2023-04-20 12:11:41.084564 (Thread-826): 12:11:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f87e5250>]}
2023-04-20 12:11:41.110206 (Thread-826): sending response (<Response 21911 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:42.716660 (Thread-822): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:42.753603 (MainThread): 12:11:42  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dbbe39cf-51b2-4597-98ef-7e4a57d0ccb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc6e7ee4dc0>]}
2023-04-20 12:11:42.754247 (MainThread): 12:11:42  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:11:42.755673 (Thread-1): 12:11:42  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 12:11:42.755917 (Thread-1): 12:11:42  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 12:11:42.760967 (Thread-1): 12:11:42  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 12:11:42.755971 => 2023-04-20 12:11:42.760785
2023-04-20 12:11:42.761191 (Thread-1): 12:11:42  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 12:11:42.761659 (Thread-1): 12:11:42  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 12:11:42.762082 (Thread-1): 12:11:42  On rpc.dbsql_dbt_tpch.request: 
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          md5(customerid) as sk_customerid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          md5(ca_c_id) as sk_customerid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:11:42.762255 (Thread-1): 12:11:42  Opening a new connection, currently in state init
2023-04-20 12:11:43.000927 (Thread-827): handling poll request
2023-04-20 12:11:43.001419 (Thread-827): 12:11:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8827cd0>]}
2023-04-20 12:11:43.002058 (Thread-827): sending response (<Response 6040 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:43.275420 (Thread-828): handling ps request
2023-04-20 12:11:43.276046 (Thread-828): 12:11:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8827e80>]}
2023-04-20 12:11:43.277730 (Thread-828): sending response (<Response 21906 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:43.278482 (Thread-829): handling ps request
2023-04-20 12:11:43.279271 (Thread-829): 12:11:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8784b20>]}
2023-04-20 12:11:43.280751 (Thread-829): sending response (<Response 21906 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:43.457653 (Thread-830): handling status request
2023-04-20 12:11:43.458230 (Thread-830): 12:11:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f87ff250>]}
2023-04-20 12:11:43.458752 (Thread-830): sending response (<Response 3182 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:43.503660 (Thread-1): 12:11:43  Databricks adapter: Error while running:

SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          md5(customerid) as sk_customerid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          md5(ca_c_id) as sk_customerid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:11:43.503891 (Thread-1): 12:11:43  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "md5(customerid)" due to data type mismatch: parameter 1 requires "BINARY" type, however, "customerid" is of "BIGINT" type.; line 38 pos 10
2023-04-20 12:11:43.504017 (Thread-1): 12:11:43  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] org.apache.spark.sql.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "md5(customerid)" due to data type mismatch: parameter 1 requires "BINARY" type, however, "customerid" is of "BIGINT" type.; line 38 pos 10
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "md5(customerid)" due to data type mismatch: parameter 1 requires "BINARY" type, however, "customerid" is of "BIGINT" type.; line 38 pos 10
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 12:11:43.504147 (Thread-1): 12:11:43  Databricks adapter: operation-id: b'\x01\xed\xdft\x83t\x1e]\xb0\xd1$\xa0\x14\xec{+'
2023-04-20 12:11:43.504367 (Thread-1): 12:11:43  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 12:11:42.761239 => 2023-04-20 12:11:43.504234
2023-04-20 12:11:43.504566 (Thread-1): 12:11:43  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 12:11:43.806720 (Thread-1): Got an exception: Runtime Error
  [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "md5(customerid)" due to data type mismatch: parameter 1 requires "BINARY" type, however, "customerid" is of "BIGINT" type.; line 38 pos 10
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "md5(customerid)" due to data type mismatch: parameter 1 requires "BINARY" type, however, "customerid" is of "BIGINT" type.; line 38 pos 10

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "md5(customerid)" due to data type mismatch: parameter 1 requires "BINARY" type, however, "customerid" is of "BIGINT" type.; line 38 pos 10
2023-04-20 12:11:43.808686 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "md5(customerid)" due to data type mismatch: parameter 1 requires "BINARY" type, however, "customerid" is of "BIGINT" type.; line 38 pos 10', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          md5(customerid) as sk_customerid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          md5(ca_c_id) as sk_customerid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM {{ source('tpcdi', 'AccountIncremental') }} a\n        JOIN {{ source('tpcdi', 'BatchDate') }} bd\n          ON a.batchid = bd.batchid\n        JOIN {{ source('tpcdi', 'StatusType') }} st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN {{ ref('DimCustomerStg') }} c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN {{ ref('DimBroker') }} b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          md5(customerid) as sk_customerid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          md5(ca_c_id) as sk_customerid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd\n          ON a.batchid = bd.batchid\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN `dbt_shabbirkdb`.`DimBroker` b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "md5(customerid)" due to data type mismatch: parameter 1 requires "BINARY" type, however, "customerid" is of "BIGINT" type.; line 38 pos 10', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          md5(customerid) as sk_customerid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          md5(ca_c_id) as sk_customerid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM {{ source('tpcdi', 'AccountIncremental') }} a\n        JOIN {{ source('tpcdi', 'BatchDate') }} bd\n          ON a.batchid = bd.batchid\n        JOIN {{ source('tpcdi', 'StatusType') }} st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN {{ ref('DimCustomerStg') }} c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN {{ ref('DimBroker') }} b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          md5(customerid) as sk_customerid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          md5(ca_c_id) as sk_customerid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd\n          ON a.batchid = bd.batchid\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN `dbt_shabbirkdb`.`DimBroker` b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 12:11:43.886750 (Thread-831): handling poll request
2023-04-20 12:11:43.887274 (Thread-831): 12:11:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f88e3f10>]}
2023-04-20 12:11:43.888086 (Thread-831): sending response (<Response 30144 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:45.463768 (Thread-832): handling poll request
2023-04-20 12:11:45.464271 (Thread-832): 12:11:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f88e3fa0>]}
2023-04-20 12:11:45.464974 (Thread-832): sending response (<Response 35272 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:45.593583 (Thread-833): handling poll request
2023-04-20 12:11:45.594112 (Thread-833): 12:11:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f87e5250>]}
2023-04-20 12:11:45.594924 (Thread-833): sending response (<Response 35272 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:45.909799 (Thread-834): handling ps request
2023-04-20 12:11:45.910295 (Thread-834): 12:11:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8784bb0>]}
2023-04-20 12:11:45.911908 (Thread-834): sending response (<Response 21929 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:11:46.757331 (Thread-835): handling status request
2023-04-20 12:11:46.757836 (Thread-835): 12:11:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5080a3a60>]}
2023-04-20 12:11:46.758391 (Thread-835): sending response (<Response 3182 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:02.765909 (Thread-836): handling ps request
2023-04-20 12:12:02.766419 (Thread-836): 12:12:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5080a3160>]}
2023-04-20 12:12:02.768071 (Thread-836): sending response (<Response 21929 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:02.857121 (Thread-837): handling run_sql request
2023-04-20 12:12:02.857627 (Thread-837): 12:12:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5080a3460>]}
2023-04-20 12:12:05.821458 (Thread-837): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:05.857721 (MainThread): 12:12:05  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fcab1540-2cce-44e8-bb26-af2bc0f57ae3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f80f18b4f10>]}
2023-04-20 12:12:05.858350 (MainThread): 12:12:05  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:12:05.859784 (Thread-1): 12:12:05  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 12:12:05.860013 (Thread-1): 12:12:05  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 12:12:05.864807 (Thread-1): 12:12:05  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 12:12:05.860062 => 2023-04-20 12:12:05.864638
2023-04-20 12:12:05.865023 (Thread-1): 12:12:05  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 12:12:05.865455 (Thread-1): 12:12:05  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 12:12:05.865852 (Thread-1): 12:12:05  On rpc.dbsql_dbt_tpch.request: 
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          md5(customerid::string) as sk_customerid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          md5(ca_c_id::string) as sk_customerid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:12:05.866003 (Thread-1): 12:12:05  Opening a new connection, currently in state init
2023-04-20 12:12:06.374095 (Thread-838): handling ps request
2023-04-20 12:12:06.374776 (Thread-838): 12:12:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f88e61f0>]}
2023-04-20 12:12:06.377444 (Thread-838): sending response (<Response 22446 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:06.393749 (Thread-839): handling ps request
2023-04-20 12:12:06.394148 (Thread-839): 12:12:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f88e6460>]}
2023-04-20 12:12:06.395664 (Thread-839): sending response (<Response 22447 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:06.632956 (Thread-1): 12:12:06  Databricks adapter: Error while running:

SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          md5(customerid::string) as sk_customerid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          md5(ca_c_id::string) as sk_customerid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:12:06.633182 (Thread-1): 12:12:06  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 15 pos 4
2023-04-20 12:12:06.633307 (Thread-1): 12:12:06  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 15 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 15 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 12:12:06.633422 (Thread-1): 12:12:06  Databricks adapter: operation-id: b'\x01\xed\xdft\x917\x1dZ\xab\x13\x81W\xb6N\xb5W'
2023-04-20 12:12:06.633622 (Thread-1): 12:12:06  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 12:12:05.865070 => 2023-04-20 12:12:06.633494
2023-04-20 12:12:06.633824 (Thread-1): 12:12:06  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 12:12:06.927494 (Thread-1): Got an exception: Runtime Error
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 15 pos 4
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 15 pos 4

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 15 pos 4
2023-04-20 12:12:06.929397 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 15 pos 4', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          md5(customerid::string) as sk_customerid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          md5(ca_c_id::string) as sk_customerid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM {{ source('tpcdi', 'AccountIncremental') }} a\n        JOIN {{ source('tpcdi', 'BatchDate') }} bd\n          ON a.batchid = bd.batchid\n        JOIN {{ source('tpcdi', 'StatusType') }} st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN {{ ref('DimCustomerStg') }} c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN {{ ref('DimBroker') }} b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          md5(customerid::string) as sk_customerid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          md5(ca_c_id::string) as sk_customerid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd\n          ON a.batchid = bd.batchid\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN `dbt_shabbirkdb`.`DimBroker` b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 15 pos 4', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          md5(customerid::string) as sk_customerid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          md5(ca_c_id::string) as sk_customerid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM {{ source('tpcdi', 'AccountIncremental') }} a\n        JOIN {{ source('tpcdi', 'BatchDate') }} bd\n          ON a.batchid = bd.batchid\n        JOIN {{ source('tpcdi', 'StatusType') }} st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN {{ ref('DimCustomerStg') }} c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN {{ ref('DimBroker') }} b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          md5(customerid::string) as sk_customerid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          md5(ca_c_id::string) as sk_customerid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd\n          ON a.batchid = bd.batchid\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN `dbt_shabbirkdb`.`DimBroker` b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 12:12:06.982289 (Thread-840): handling status request
2023-04-20 12:12:06.982807 (Thread-840): 12:12:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f88e68e0>]}
2023-04-20 12:12:06.983401 (Thread-840): sending response (<Response 3182 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:07.101573 (Thread-841): handling poll request
2023-04-20 12:12:07.102067 (Thread-841): 12:12:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f88e69d0>]}
2023-04-20 12:12:07.102873 (Thread-841): sending response (<Response 30618 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:07.211871 (Thread-842): handling poll request
2023-04-20 12:12:07.212350 (Thread-842): 12:12:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f88e6ca0>]}
2023-04-20 12:12:07.213086 (Thread-842): sending response (<Response 35826 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:07.562321 (Thread-843): handling status request
2023-04-20 12:12:07.562822 (Thread-843): 12:12:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f88e6250>]}
2023-04-20 12:12:07.563429 (Thread-843): sending response (<Response 3182 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:09.328982 (Thread-844): handling ps request
2023-04-20 12:12:09.329488 (Thread-844): 12:12:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f88e6550>]}
2023-04-20 12:12:09.331824 (Thread-844): sending response (<Response 22470 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:09.693047 (Thread-845): handling poll request
2023-04-20 12:12:09.693559 (Thread-845): 12:12:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f88e6ee0>]}
2023-04-20 12:12:09.694356 (Thread-845): sending response (<Response 35826 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:09.772961 (Thread-846): handling status request
2023-04-20 12:12:09.773407 (Thread-846): 12:12:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f88e6880>]}
2023-04-20 12:12:09.773941 (Thread-846): sending response (<Response 3182 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:45.385639 (Thread-847): 12:12:45  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 12:12:45.819912 (Thread-848): handling status request
2023-04-20 12:12:45.820441 (Thread-848): 12:12:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5080bd6d0>]}
2023-04-20 12:12:45.820868 (Thread-848): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:46.080012 (Thread-847): 12:12:46  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
2023-04-20 12:12:46.080702 (Thread-847): 12:12:46  Partial parsing: updated file: dbsql_dbt_tpch://models/incremental/DimAccount.sql
2023-04-20 12:12:46.089964 (Thread-847): 12:12:46  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 12:12:46.154246 (Thread-847): 12:12:46  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f86e0700>]}
2023-04-20 12:12:46.962682 (Thread-849): handling status request
2023-04-20 12:12:46.963189 (Thread-849): 12:12:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508084dc0>]}
2023-04-20 12:12:46.963722 (Thread-849): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:51.906017 (Thread-850): handling run_sql request
2023-04-20 12:12:51.906688 (Thread-850): 12:12:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5080845e0>]}
2023-04-20 12:12:51.949179 (Thread-851): handling ps request
2023-04-20 12:12:51.950203 (Thread-851): 12:12:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50808ba00>]}
2023-04-20 12:12:51.953139 (Thread-851): sending response (<Response 22993 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:52.680300 (Thread-852): handling poll request
2023-04-20 12:12:52.680790 (Thread-852): 12:12:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50817fb80>]}
2023-04-20 12:12:52.681336 (Thread-852): sending response (<Response 432 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:53.179365 (Thread-853): handling status request
2023-04-20 12:12:53.179865 (Thread-853): 12:12:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50817fac0>]}
2023-04-20 12:12:53.180393 (Thread-853): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:53.410168 (Thread-854): handling ps request
2023-04-20 12:12:53.410741 (Thread-854): 12:12:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50808ba00>]}
2023-04-20 12:12:53.412533 (Thread-854): sending response (<Response 22993 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:54.692059 (Thread-850): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:54.728072 (MainThread): 12:12:54  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8fffe949-519e-4f0f-9fd5-2aa4b41143c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f021b1e5f70>]}
2023-04-20 12:12:54.728693 (MainThread): 12:12:54  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:12:54.730205 (Thread-1): 12:12:54  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 12:12:54.730452 (Thread-1): 12:12:54  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 12:12:54.735441 (Thread-1): 12:12:54  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 12:12:54.730505 => 2023-04-20 12:12:54.735268
2023-04-20 12:12:54.735690 (Thread-1): 12:12:54  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 12:12:54.736129 (Thread-1): 12:12:54  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 12:12:54.736515 (Thread-1): 12:12:54  On rpc.dbsql_dbt_tpch.request: 
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        sk_customerid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          md5(customerid::string) as sk_customerid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          md5(ca_c_id::string) as sk_customerid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:12:54.736662 (Thread-1): 12:12:54  Opening a new connection, currently in state init
2023-04-20 12:12:55.219436 (Thread-855): handling ps request
2023-04-20 12:12:55.219949 (Thread-855): 12:12:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50817fb50>]}
2023-04-20 12:12:55.221643 (Thread-855): sending response (<Response 22988 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:55.309134 (Thread-856): handling ps request
2023-04-20 12:12:55.309665 (Thread-856): 12:12:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508084550>]}
2023-04-20 12:12:55.333450 (Thread-856): sending response (<Response 22988 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:55.463295 (Thread-857): handling poll request
2023-04-20 12:12:55.463810 (Thread-857): 12:12:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50817faf0>]}
2023-04-20 12:12:55.464393 (Thread-857): sending response (<Response 6080 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:55.510358 (Thread-1): 12:12:55  Databricks adapter: Error while running:

SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        sk_customerid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          md5(customerid::string) as sk_customerid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          md5(ca_c_id::string) as sk_customerid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:12:55.510585 (Thread-1): 12:12:55  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`a`.`sk_customerid`, `c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`].; line 15 pos 4
2023-04-20 12:12:55.510706 (Thread-1): 12:12:55  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`a`.`sk_customerid`, `c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`].; line 15 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`a`.`sk_customerid`, `c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`].; line 15 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 12:12:55.510819 (Thread-1): 12:12:55  Databricks adapter: operation-id: b"\x01\xed\xdft\xae\\\x1a\x9f\xab\xef-\x15\xf2T'd"
2023-04-20 12:12:55.511026 (Thread-1): 12:12:55  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 12:12:54.735739 => 2023-04-20 12:12:55.510895
2023-04-20 12:12:55.511278 (Thread-1): 12:12:55  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 12:12:55.774852 (Thread-858): handling status request
2023-04-20 12:12:55.775330 (Thread-858): 12:12:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50817f220>]}
2023-04-20 12:12:55.775833 (Thread-858): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:55.797578 (Thread-1): Got an exception: Runtime Error
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`a`.`sk_customerid`, `c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`].; line 15 pos 4
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`a`.`sk_customerid`, `c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`].; line 15 pos 4

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`a`.`sk_customerid`, `c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`].; line 15 pos 4
2023-04-20 12:12:55.799439 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`a`.`sk_customerid`, `c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`].; line 15 pos 4', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        sk_customerid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          md5(customerid::string) as sk_customerid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          md5(ca_c_id::string) as sk_customerid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM {{ source('tpcdi', 'AccountIncremental') }} a\n        JOIN {{ source('tpcdi', 'BatchDate') }} bd\n          ON a.batchid = bd.batchid\n        JOIN {{ source('tpcdi', 'StatusType') }} st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN {{ ref('DimCustomerStg') }} c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN {{ ref('DimBroker') }} b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        sk_customerid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          md5(customerid::string) as sk_customerid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          md5(ca_c_id::string) as sk_customerid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd\n          ON a.batchid = bd.batchid\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN `dbt_shabbirkdb`.`DimBroker` b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`a`.`sk_customerid`, `c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`].; line 15 pos 4', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        sk_customerid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          md5(customerid::string) as sk_customerid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          md5(ca_c_id::string) as sk_customerid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM {{ source('tpcdi', 'AccountIncremental') }} a\n        JOIN {{ source('tpcdi', 'BatchDate') }} bd\n          ON a.batchid = bd.batchid\n        JOIN {{ source('tpcdi', 'StatusType') }} st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN {{ ref('DimCustomerStg') }} c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN {{ ref('DimBroker') }} b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        sk_customerid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          md5(customerid::string) as sk_customerid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          md5(ca_c_id::string) as sk_customerid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd\n          ON a.batchid = bd.batchid\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN `dbt_shabbirkdb`.`DimBroker` b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 12:12:56.018365 (Thread-859): handling poll request
2023-04-20 12:12:56.018855 (Thread-859): 12:12:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508138520>]}
2023-04-20 12:12:56.019633 (Thread-859): sending response (<Response 30819 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:57.580434 (Thread-860): handling poll request
2023-04-20 12:12:57.580965 (Thread-860): 12:12:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5081382e0>]}
2023-04-20 12:12:57.581759 (Thread-860): sending response (<Response 36081 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:57.961564 (Thread-861): handling ps request
2023-04-20 12:12:57.962059 (Thread-861): 12:12:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5081382b0>]}
2023-04-20 12:12:57.964311 (Thread-861): sending response (<Response 23011 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:58.090118 (Thread-862): handling poll request
2023-04-20 12:12:58.090611 (Thread-862): 12:12:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50817f5e0>]}
2023-04-20 12:12:58.091327 (Thread-862): sending response (<Response 36081 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:12:58.388189 (Thread-863): handling status request
2023-04-20 12:12:58.388676 (Thread-863): 12:12:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50808bbe0>]}
2023-04-20 12:12:58.389185 (Thread-863): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:13:56.033918 (Thread-864): handling poll request
2023-04-20 12:13:56.034439 (Thread-864): 12:13:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50808ad60>]}
2023-04-20 12:13:56.041844 (Thread-864): sending response (<Response 191859 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:13:56.535443 (Thread-865): handling status request
2023-04-20 12:13:56.535975 (Thread-865): 12:13:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508084ac0>]}
2023-04-20 12:13:56.536460 (Thread-865): sending response (<Response 1923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:13:56.794904 (Thread-866): handling ps request
2023-04-20 12:13:56.795405 (Thread-866): 12:13:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508084a60>]}
2023-04-20 12:13:56.797077 (Thread-866): sending response (<Response 23011 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:14:27.475256 (Thread-867): 12:14:27  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 12:14:27.885593 (Thread-867): 12:14:27  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
2023-04-20 12:14:27.886311 (Thread-867): 12:14:27  Partial parsing: updated file: dbsql_dbt_tpch://models/incremental/DimCustomerStg.sql
2023-04-20 12:14:27.894926 (Thread-867): 12:14:27  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 12:14:27.952288 (Thread-867): 12:14:27  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5082ab430>]}
2023-04-20 12:14:27.982382 (Thread-868): handling status request
2023-04-20 12:14:27.982835 (Thread-868): 12:14:27  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f880e7f0>]}
2023-04-20 12:14:27.983309 (Thread-868): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:05.472260 (Thread-869): 12:15:05  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 12:15:05.791998 (Thread-869): 12:15:05  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
2023-04-20 12:15:05.792627 (Thread-869): 12:15:05  Partial parsing: updated file: dbsql_dbt_tpch://models/incremental/DimCustomerStg.sql
2023-04-20 12:15:05.801223 (Thread-869): 12:15:05  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 12:15:05.862031 (Thread-869): 12:15:05  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5281187f0>]}
2023-04-20 12:15:05.956060 (Thread-870): handling status request
2023-04-20 12:15:05.956563 (Thread-870): 12:15:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f880ea00>]}
2023-04-20 12:15:05.957078 (Thread-870): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:07.758662 (Thread-871): handling ps request
2023-04-20 12:15:07.759155 (Thread-871): 12:15:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f880eca0>]}
2023-04-20 12:15:07.760847 (Thread-871): sending response (<Response 23011 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:07.795288 (Thread-872): handling run_sql request
2023-04-20 12:15:07.795733 (Thread-872): 12:15:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f880ef10>]}
2023-04-20 12:15:10.533177 (Thread-872): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:10.568258 (MainThread): 12:15:10  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '45efac0c-0591-4c91-9574-4b772548bc13', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f613cca1eb0>]}
2023-04-20 12:15:10.568845 (MainThread): 12:15:10  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:15:10.570172 (Thread-1): 12:15:10  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 12:15:10.570406 (Thread-1): 12:15:10  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 12:15:10.574935 (Thread-1): 12:15:10  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 12:15:10.570455 => 2023-04-20 12:15:10.574772
2023-04-20 12:15:10.575146 (Thread-1): 12:15:10  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 12:15:10.575483 (Thread-1): 12:15:10  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 12:15:10.576043 (Thread-1): 12:15:10  On rpc.dbsql_dbt_tpch.request: 
SELECT * FROM (
  SELECT
    sk_customerid,
    customerid,
    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) taxid,
    status,
    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) lastname,
    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) firstname,
    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) middleinitial,
    coalesce(gender, last_value(gender) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) gender,
    coalesce(tier, last_value(tier) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) tier,
    coalesce(dob, last_value(dob) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) dob,
    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline1,
    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline2,
    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) postalcode,
    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) CITY,
    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) stateprov,
    coalesce(country, last_value(country) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) country,
    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone1,
    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone2,
    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone3,
    coalesce(email1, last_value(email1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email1,
    coalesce(email2, last_value(email2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email2,
    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) LCL_TX_ID,
    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) NAT_TX_ID,
    batchid,
    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,
    date(update_ts) effectivedate,
    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate
  FROM (
    SELECT
      md5(customerid::string) as sk_customerid,
      customerid,
      taxid,
      status,
      lastname,
      firstname,
      middleinitial,
      gender,
      tier,
      dob,
      addressline1,
      addressline2,
      postalcode,
      city,
      stateprov,
      country,
      phone1,
      phone2,
      phone3,
      email1,
      email2,
      lcl_tx_id,
      nat_tx_id,
      1 batchid,
      update_ts
    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')
    UNION ALL
    SELECT
      md5(c.customerid::string) as sk_customerid,
      c.customerid,
      nullif(c.taxid, '') taxid,
      nullif(s.st_name, '') as status,
      nullif(c.lastname, '') lastname,
      nullif(c.firstname, '') firstname,
      nullif(c.middleinitial, '') middleinitial,
      gender,
      c.tier,
      c.dob,
      nullif(c.addressline1, '') addressline1,
      nullif(c.addressline2, '') addressline2,
      nullif(c.postalcode, '') postalcode,
      nullif(c.city, '') city,
      nullif(c.stateprov, '') stateprov,
      nullif(c.country, '') country,
      CASE
        WHEN isnull(c_local_1) then c_local_1
        ELSE concat(
          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),
          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),
          c_local_1,
          nvl(c_ext_1, '')) END as phone1,
      CASE
        WHEN isnull(c_local_2) then c_local_2
        ELSE concat(
          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),
          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),
          c_local_2,
          nvl(c_ext_2, '')) END as phone2,
      CASE
        WHEN isnull(c_local_3) then c_local_3
        ELSE concat(
          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),
          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),
          c_local_3,
          nvl(c_ext_3, '')) END as phone3,
      nullif(c.email1, '') email1,
      nullif(c.email2, '') email2,
      c.LCL_TX_ID, 
      c.NAT_TX_ID,
      c.batchid,
      timestamp(bd.batchdate) update_ts
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CustomerIncremental` c
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
      ON c.batchid = bd.batchid
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` s 
      ON c.status = s.st_id
  ) c
  )
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:15:10.576190 (Thread-1): 12:15:10  Opening a new connection, currently in state init
2023-04-20 12:15:11.114420 (Thread-873): handling ps request
2023-04-20 12:15:11.115066 (Thread-873): 12:15:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528238310>]}
2023-04-20 12:15:11.117927 (Thread-873): sending response (<Response 23537 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:11.170053 (Thread-874): handling ps request
2023-04-20 12:15:11.170480 (Thread-874): 12:15:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5282380a0>]}
2023-04-20 12:15:11.172133 (Thread-874): sending response (<Response 23537 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:11.388855 (Thread-875): handling status request
2023-04-20 12:15:11.389337 (Thread-875): 12:15:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528238220>]}
2023-04-20 12:15:11.389895 (Thread-875): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:12.255577 (Thread-876): handling poll request
2023-04-20 12:15:12.256168 (Thread-876): 12:15:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f86dc430>]}
2023-04-20 12:15:12.256908 (Thread-876): sending response (<Response 9039 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:12.295012 (Thread-877): handling poll request
2023-04-20 12:15:12.295439 (Thread-877): 12:15:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f86dcd90>]}
2023-04-20 12:15:12.296072 (Thread-877): sending response (<Response 9039 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:12.621507 (Thread-1): 12:15:12  SQL status: OK in 2.04 seconds
2023-04-20 12:15:12.659418 (Thread-1): 12:15:12  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 12:15:10.575190 => 2023-04-20 12:15:12.659203
2023-04-20 12:15:12.659691 (Thread-1): 12:15:12  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 12:15:12.738167 (Thread-878): handling status request
2023-04-20 12:15:12.738618 (Thread-878): 12:15:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f86dcee0>]}
2023-04-20 12:15:12.739102 (Thread-878): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:13.889209 (Thread-879): handling poll request
2023-04-20 12:15:13.889698 (Thread-879): 12:15:13  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f86e0730>]}
2023-04-20 12:15:13.896228 (Thread-879): sending response (<Response 202596 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:14.192041 (Thread-880): handling ps request
2023-04-20 12:15:14.192548 (Thread-880): 12:15:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f86e09d0>]}
2023-04-20 12:15:14.194235 (Thread-880): sending response (<Response 23562 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:14.647694 (Thread-881): handling status request
2023-04-20 12:15:14.648206 (Thread-881): 12:15:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f86e0d60>]}
2023-04-20 12:15:14.648710 (Thread-881): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:14.981118 (Thread-882): handling poll request
2023-04-20 12:15:14.981609 (Thread-882): 12:15:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f86e0790>]}
2023-04-20 12:15:14.987977 (Thread-882): sending response (<Response 211202 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:16.220625 (Thread-883): handling status request
2023-04-20 12:15:16.221127 (Thread-883): 12:15:16  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f86e0a60>]}
2023-04-20 12:15:16.221625 (Thread-883): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:16.470827 (Thread-884): handling ps request
2023-04-20 12:15:16.471370 (Thread-884): 12:15:16  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508086790>]}
2023-04-20 12:15:16.473040 (Thread-884): sending response (<Response 23562 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:20.133083 (Thread-885): handling run_sql request
2023-04-20 12:15:20.133570 (Thread-885): 12:15:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f88a3850>]}
2023-04-20 12:15:20.152997 (Thread-886): handling ps request
2023-04-20 12:15:20.159728 (Thread-886): 12:15:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5082d6910>]}
2023-04-20 12:15:20.194673 (Thread-886): sending response (<Response 24086 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:20.736620 (Thread-887): handling poll request
2023-04-20 12:15:20.737099 (Thread-887): 12:15:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5080a3cd0>]}
2023-04-20 12:15:20.737613 (Thread-887): sending response (<Response 434 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:21.145307 (Thread-888): handling status request
2023-04-20 12:15:21.145780 (Thread-888): 12:15:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f87366a0>]}
2023-04-20 12:15:21.146329 (Thread-888): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:21.421791 (Thread-889): handling ps request
2023-04-20 12:15:21.422300 (Thread-889): 12:15:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5082d6760>]}
2023-04-20 12:15:21.424018 (Thread-889): sending response (<Response 24087 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:22.851974 (Thread-885): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:22.888585 (MainThread): 12:15:22  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c0cab6ab-aab5-48c4-93d2-c28614090ad8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa1a7e25e80>]}
2023-04-20 12:15:22.889207 (MainThread): 12:15:22  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:15:22.890613 (Thread-1): 12:15:22  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 12:15:22.890836 (Thread-1): 12:15:22  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 12:15:22.895333 (Thread-1): 12:15:22  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 12:15:22.890884 => 2023-04-20 12:15:22.895162
2023-04-20 12:15:22.895590 (Thread-1): 12:15:22  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 12:15:22.895948 (Thread-1): 12:15:22  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 12:15:22.896374 (Thread-1): 12:15:22  On rpc.dbsql_dbt_tpch.request: 

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:15:22.896522 (Thread-1): 12:15:22  Opening a new connection, currently in state init
2023-04-20 12:15:23.303909 (Thread-890): handling poll request
2023-04-20 12:15:23.304408 (Thread-890): 12:15:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f87ff880>]}
2023-04-20 12:15:23.305050 (Thread-890): sending response (<Response 4742 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:23.398458 (Thread-891): handling ps request
2023-04-20 12:15:23.398961 (Thread-891): 12:15:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8730be0>]}
2023-04-20 12:15:23.400730 (Thread-891): sending response (<Response 24082 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:23.410587 (Thread-892): handling ps request
2023-04-20 12:15:23.410978 (Thread-892): 12:15:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5082d6160>]}
2023-04-20 12:15:23.412574 (Thread-892): sending response (<Response 24082 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:23.618985 (Thread-1): 12:15:23  Databricks adapter: Error while running:


SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:15:23.619207 (Thread-1): 12:15:23  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 4 pos 2
2023-04-20 12:15:23.619332 (Thread-1): 12:15:23  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 4 pos 2
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 4 pos 2
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 12:15:23.619448 (Thread-1): 12:15:23  Databricks adapter: operation-id: b'\x01\xed\xdfu\x06\xa8\x13\xdc\x80,<6`j\\W'
2023-04-20 12:15:23.619714 (Thread-1): 12:15:23  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 12:15:22.895642 => 2023-04-20 12:15:23.619547
2023-04-20 12:15:23.619929 (Thread-1): 12:15:23  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 12:15:23.733917 (Thread-893): handling status request
2023-04-20 12:15:23.734407 (Thread-893): 12:15:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8730be0>]}
2023-04-20 12:15:23.734891 (Thread-893): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:23.901489 (Thread-1): Got an exception: Runtime Error
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 4 pos 2
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 4 pos 2

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 4 pos 2
2023-04-20 12:15:23.903311 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 4 pos 2', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\n\nSELECT \n  c.sk_customerid,\n  c.customerid,\n  c.taxid,\n  c.status,\n  c.lastname,\n  c.firstname,\n  c.middleinitial,\n  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,\n  c.tier,\n  c.dob,\n  c.addressline1,\n  c.addressline2,\n  c.postalcode,\n  c.city,\n  c.stateprov,\n  c.country,\n  c.phone1,\n  c.phone2,\n  c.phone3,\n  c.email1,\n  c.email2,\n  r_nat.TX_NAME as nationaltaxratedesc,\n  r_nat.TX_RATE as nationaltaxrate,\n  r_lcl.TX_NAME as localtaxratedesc,\n  r_lcl.TX_RATE as localtaxrate,\n  p.agencyid,\n  p.creditrating,\n  p.networth,\n  p.marketingnameplate,\n  c.iscurrent,\n  c.batchid,\n  c.effectivedate,\n  c.enddate\nFROM {{ ref('DimCustomerStg') }} c\nLEFT JOIN {{ source('tpcdi', 'TaxRate') }} r_lcl \n  ON c.LCL_TX_ID = r_lcl.TX_ID\nLEFT JOIN {{ source('tpcdi', 'TaxRate') }} r_nat \n  ON c.NAT_TX_ID = r_nat.TX_ID\nLEFT JOIN {{ ref('Prospect') }} p \n  on upper(p.lastname) = upper(c.lastname)\n  and upper(p.firstname) = upper(c.firstname)\n  and upper(p.addressline1) = upper(c.addressline1)\n  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))\n  and upper(p.postalcode) = upper(c.postalcode)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\n\nSELECT \n  c.sk_customerid,\n  c.customerid,\n  c.taxid,\n  c.status,\n  c.lastname,\n  c.firstname,\n  c.middleinitial,\n  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,\n  c.tier,\n  c.dob,\n  c.addressline1,\n  c.addressline2,\n  c.postalcode,\n  c.city,\n  c.stateprov,\n  c.country,\n  c.phone1,\n  c.phone2,\n  c.phone3,\n  c.email1,\n  c.email2,\n  r_nat.TX_NAME as nationaltaxratedesc,\n  r_nat.TX_RATE as nationaltaxrate,\n  r_lcl.TX_NAME as localtaxratedesc,\n  r_lcl.TX_RATE as localtaxrate,\n  p.agencyid,\n  p.creditrating,\n  p.networth,\n  p.marketingnameplate,\n  c.iscurrent,\n  c.batchid,\n  c.effectivedate,\n  c.enddate\nFROM `dbt_shabbirkdb`.`DimCustomerStg` c\nLEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl \n  ON c.LCL_TX_ID = r_lcl.TX_ID\nLEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat \n  ON c.NAT_TX_ID = r_nat.TX_ID\nLEFT JOIN `dbt_shabbirkdb`.`Prospect` p \n  on upper(p.lastname) = upper(c.lastname)\n  and upper(p.firstname) = upper(c.firstname)\n  and upper(p.addressline1) = upper(c.addressline1)\n  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))\n  and upper(p.postalcode) = upper(c.postalcode)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 4 pos 2', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\n\nSELECT \n  c.sk_customerid,\n  c.customerid,\n  c.taxid,\n  c.status,\n  c.lastname,\n  c.firstname,\n  c.middleinitial,\n  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,\n  c.tier,\n  c.dob,\n  c.addressline1,\n  c.addressline2,\n  c.postalcode,\n  c.city,\n  c.stateprov,\n  c.country,\n  c.phone1,\n  c.phone2,\n  c.phone3,\n  c.email1,\n  c.email2,\n  r_nat.TX_NAME as nationaltaxratedesc,\n  r_nat.TX_RATE as nationaltaxrate,\n  r_lcl.TX_NAME as localtaxratedesc,\n  r_lcl.TX_RATE as localtaxrate,\n  p.agencyid,\n  p.creditrating,\n  p.networth,\n  p.marketingnameplate,\n  c.iscurrent,\n  c.batchid,\n  c.effectivedate,\n  c.enddate\nFROM {{ ref('DimCustomerStg') }} c\nLEFT JOIN {{ source('tpcdi', 'TaxRate') }} r_lcl \n  ON c.LCL_TX_ID = r_lcl.TX_ID\nLEFT JOIN {{ source('tpcdi', 'TaxRate') }} r_nat \n  ON c.NAT_TX_ID = r_nat.TX_ID\nLEFT JOIN {{ ref('Prospect') }} p \n  on upper(p.lastname) = upper(c.lastname)\n  and upper(p.firstname) = upper(c.firstname)\n  and upper(p.addressline1) = upper(c.addressline1)\n  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))\n  and upper(p.postalcode) = upper(c.postalcode)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\n\nSELECT \n  c.sk_customerid,\n  c.customerid,\n  c.taxid,\n  c.status,\n  c.lastname,\n  c.firstname,\n  c.middleinitial,\n  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,\n  c.tier,\n  c.dob,\n  c.addressline1,\n  c.addressline2,\n  c.postalcode,\n  c.city,\n  c.stateprov,\n  c.country,\n  c.phone1,\n  c.phone2,\n  c.phone3,\n  c.email1,\n  c.email2,\n  r_nat.TX_NAME as nationaltaxratedesc,\n  r_nat.TX_RATE as nationaltaxrate,\n  r_lcl.TX_NAME as localtaxratedesc,\n  r_lcl.TX_RATE as localtaxrate,\n  p.agencyid,\n  p.creditrating,\n  p.networth,\n  p.marketingnameplate,\n  c.iscurrent,\n  c.batchid,\n  c.effectivedate,\n  c.enddate\nFROM `dbt_shabbirkdb`.`DimCustomerStg` c\nLEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl \n  ON c.LCL_TX_ID = r_lcl.TX_ID\nLEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat \n  ON c.NAT_TX_ID = r_nat.TX_ID\nLEFT JOIN `dbt_shabbirkdb`.`Prospect` p \n  on upper(p.lastname) = upper(c.lastname)\n  and upper(p.firstname) = upper(c.firstname)\n  and upper(p.addressline1) = upper(c.addressline1)\n  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))\n  and upper(p.postalcode) = upper(c.postalcode)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 12:15:24.278354 (Thread-894): handling poll request
2023-04-20 12:15:24.278832 (Thread-894): 12:15:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8730ca0>]}
2023-04-20 12:15:24.279538 (Thread-894): sending response (<Response 25225 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:26.341923 (Thread-895): handling poll request
2023-04-20 12:15:26.342432 (Thread-895): 12:15:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5082d6670>]}
2023-04-20 12:15:26.343110 (Thread-895): sending response (<Response 25225 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:26.521163 (Thread-896): handling ps request
2023-04-20 12:15:26.521642 (Thread-896): 12:15:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f872e310>]}
2023-04-20 12:15:26.523818 (Thread-896): sending response (<Response 24105 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:27.479691 (Thread-897): handling status request
2023-04-20 12:15:27.480190 (Thread-897): 12:15:27  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5080e2850>]}
2023-04-20 12:15:27.480688 (Thread-897): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:33.225814 (Thread-898): handling status request
2023-04-20 12:15:33.226324 (Thread-898): 12:15:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5080e25e0>]}
2023-04-20 12:15:33.226815 (Thread-898): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:33.716284 (Thread-899): handling cli_args request
2023-04-20 12:15:33.716772 (Thread-899): 12:15:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5080a3400>]}
2023-04-20 12:15:36.496944 (Thread-899): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:36.555327 (MainThread): 12:15:36  checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
2023-04-20 12:15:36.576900 (MainThread): 12:15:36  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 12:15:36.577077 (MainThread): 12:15:36  previous checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, current checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd
2023-04-20 12:15:36.577233 (MainThread): 12:15:36  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '123769cb-405b-4be6-aff8-286dd3000510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04aa90d910>]}
2023-04-20 12:15:37.105548 (Thread-900): handling ps request
2023-04-20 12:15:37.106209 (Thread-900): 12:15:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f86e05e0>]}
2023-04-20 12:15:37.109169 (Thread-900): sending response (<Response 24487 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:37.238540 (Thread-901): handling poll request
2023-04-20 12:15:37.238998 (Thread-901): 12:15:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f872e190>]}
2023-04-20 12:15:37.239590 (Thread-901): sending response (<Response 1863 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:37.264928 (Thread-902): handling status request
2023-04-20 12:15:37.265299 (Thread-902): 12:15:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f872e250>]}
2023-04-20 12:15:37.265771 (Thread-902): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:37.552417 (Thread-903): handling ps request
2023-04-20 12:15:37.552924 (Thread-903): 12:15:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f872e280>]}
2023-04-20 12:15:37.554708 (Thread-903): sending response (<Response 24487 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:38.172506 (MainThread): 12:15:38  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 12:15:38.185089 (MainThread): 12:15:38  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 12:15:38.188147 (MainThread): 12:15:38  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 12:15:38.191177 (MainThread): 12:15:38  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 12:15:38.194207 (MainThread): 12:15:38  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 12:15:38.197011 (MainThread): 12:15:38  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 12:15:38.200830 (MainThread): 12:15:38  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 12:15:38.203939 (MainThread): 12:15:38  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 12:15:38.206946 (MainThread): 12:15:38  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 12:15:38.209766 (MainThread): 12:15:38  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 12:15:38.213110 (MainThread): 12:15:38  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 12:15:38.215980 (MainThread): 12:15:38  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 12:15:38.218719 (MainThread): 12:15:38  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 12:15:38.221602 (MainThread): 12:15:38  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 12:15:38.224688 (MainThread): 12:15:38  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 12:15:38.394447 (MainThread): 12:15:38  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '123769cb-405b-4be6-aff8-286dd3000510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f047ddaecd0>]}
2023-04-20 12:15:38.439801 (MainThread): 12:15:38  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '123769cb-405b-4be6-aff8-286dd3000510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04aa90c4f0>]}
2023-04-20 12:15:38.440184 (MainThread): 12:15:38  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:15:38.440339 (MainThread): 12:15:38  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '123769cb-405b-4be6-aff8-286dd3000510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04aab749d0>]}
2023-04-20 12:15:38.442286 (MainThread): 12:15:38  
2023-04-20 12:15:38.443603 (MainThread): 12:15:38  Acquiring new databricks connection 'master'
2023-04-20 12:15:38.445365 (ThreadPoolExecutor-0_0): 12:15:38  Acquiring new databricks connection 'list_schemas'
2023-04-20 12:15:38.456454 (ThreadPoolExecutor-0_0): 12:15:38  Using databricks connection "list_schemas"
2023-04-20 12:15:38.456783 (ThreadPoolExecutor-0_0): 12:15:38  On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_schemas"} */

    show databases
  
2023-04-20 12:15:38.456946 (ThreadPoolExecutor-0_0): 12:15:38  Opening a new connection, currently in state init
2023-04-20 12:15:39.541661 (Thread-904): handling poll request
2023-04-20 12:15:39.542192 (Thread-904): 12:15:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f872e7f0>]}
2023-04-20 12:15:39.542954 (Thread-904): sending response (<Response 8689 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:39.594927 (ThreadPoolExecutor-0_0): 12:15:39  SQL status: OK in 1.14 seconds
2023-04-20 12:15:39.721445 (ThreadPoolExecutor-0_0): 12:15:39  On list_schemas: Close
2023-04-20 12:15:39.999691 (ThreadPoolExecutor-1_0): 12:15:39  Acquiring new databricks connection 'list_None_dbt_shabbirkdb'
2023-04-20 12:15:40.010297 (ThreadPoolExecutor-1_0): 12:15:40  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:15:40.010506 (ThreadPoolExecutor-1_0): 12:15:40  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 12:15:40.010670 (ThreadPoolExecutor-1_0): 12:15:40  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show tables in `dbt_shabbirkdb`
  
2023-04-20 12:15:40.010822 (ThreadPoolExecutor-1_0): 12:15:40  Opening a new connection, currently in state closed
2023-04-20 12:15:40.645613 (Thread-905): handling ps request
2023-04-20 12:15:40.646127 (Thread-905): 12:15:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5281a4250>]}
2023-04-20 12:15:40.648033 (Thread-905): sending response (<Response 24487 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:40.732136 (ThreadPoolExecutor-1_0): 12:15:40  SQL status: OK in 0.72 seconds
2023-04-20 12:15:40.741285 (ThreadPoolExecutor-1_0): 12:15:40  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 12:15:40.741503 (ThreadPoolExecutor-1_0): 12:15:40  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show views in `dbt_shabbirkdb`
  
2023-04-20 12:15:41.064943 (Thread-906): handling poll request
2023-04-20 12:15:41.065435 (Thread-906): 12:15:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528192610>]}
2023-04-20 12:15:41.066059 (Thread-906): sending response (<Response 3840 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:41.209915 (ThreadPoolExecutor-1_0): 12:15:41  SQL status: OK in 0.47 seconds
2023-04-20 12:15:41.213683 (ThreadPoolExecutor-1_0): 12:15:41  On list_None_dbt_shabbirkdb: ROLLBACK
2023-04-20 12:15:41.213922 (ThreadPoolExecutor-1_0): 12:15:41  Databricks adapter: NotImplemented: rollback
2023-04-20 12:15:41.214077 (ThreadPoolExecutor-1_0): 12:15:41  On list_None_dbt_shabbirkdb: Close
2023-04-20 12:15:41.490091 (MainThread): 12:15:41  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '123769cb-405b-4be6-aff8-286dd3000510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04a80ac280>]}
2023-04-20 12:15:41.490509 (MainThread): 12:15:41  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:15:41.490671 (MainThread): 12:15:41  Spark adapter: NotImplemented: commit
2023-04-20 12:15:41.491245 (MainThread): 12:15:41  Concurrency: 4 threads (target='default')
2023-04-20 12:15:41.491402 (MainThread): 12:15:41  
2023-04-20 12:15:41.494231 (Thread-1): 12:15:41  Began running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:15:41.495765 (Thread-1): 12:15:41  1 of 15 START sql table model dbt_shabbirkdb.DimBroker ......................... [RUN]
2023-04-20 12:15:41.496269 (Thread-1): 12:15:41  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
2023-04-20 12:15:41.496461 (Thread-1): 12:15:41  Began compiling node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:15:41.499294 (Thread-2): 12:15:41  Began running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:15:41.499650 (Thread-2): 12:15:41  2 of 15 START sql table model dbt_shabbirkdb.DimCompany ........................ [RUN]
2023-04-20 12:15:41.500181 (Thread-2): 12:15:41  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCompany'
2023-04-20 12:15:41.500362 (Thread-2): 12:15:41  Began compiling node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:15:41.505400 (Thread-1): 12:15:41  Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:15:41.505865 (Thread-3): 12:15:41  Began running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:15:41.506159 (Thread-3): 12:15:41  3 of 15 START sql table model dbt_shabbirkdb.DimCustomerStg .................... [RUN]
2023-04-20 12:15:41.506681 (Thread-3): 12:15:41  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomerStg'
2023-04-20 12:15:41.506865 (Thread-3): 12:15:41  Began compiling node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:15:41.510943 (Thread-4): 12:15:41  Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:15:41.511225 (Thread-4): 12:15:41  4 of 15 START sql table model dbt_shabbirkdb.tempDailyMarketHistorical ......... [RUN]
2023-04-20 12:15:41.511749 (Thread-4): 12:15:41  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempDailyMarketHistorical'
2023-04-20 12:15:41.511927 (Thread-4): 12:15:41  Began compiling node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:15:41.515616 (Thread-4): 12:15:41  Writing injected SQL for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:15:41.516371 (Thread-2): 12:15:41  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:15:41.517157 (Thread-3): 12:15:41  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:15:41.532108 (Thread-4): 12:15:41  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (compile): 2023-04-20 12:15:41.511970 => 2023-04-20 12:15:41.531941
2023-04-20 12:15:41.532341 (Thread-4): 12:15:41  Began executing node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:15:41.537205 (Thread-1): 12:15:41  Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 2023-04-20 12:15:41.496507 => 2023-04-20 12:15:41.537036
2023-04-20 12:15:41.537418 (Thread-1): 12:15:41  Began executing node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:15:41.552831 (Thread-3): 12:15:41  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (compile): 2023-04-20 12:15:41.506909 => 2023-04-20 12:15:41.552666
2023-04-20 12:15:41.553051 (Thread-3): 12:15:41  Began executing node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:15:41.558166 (Thread-2): 12:15:41  Timing info for model.dbsql_dbt_tpch.DimCompany (compile): 2023-04-20 12:15:41.500406 => 2023-04-20 12:15:41.558001
2023-04-20 12:15:41.558372 (Thread-2): 12:15:41  Began executing node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:15:41.567764 (Thread-4): 12:15:41  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:15:41.567937 (Thread-4): 12:15:41  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:15:41.568099 (Thread-4): 12:15:41  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

      describe extended `dbt_shabbirkdb`.`tempdailymarkethistorical`
  
2023-04-20 12:15:41.568865 (Thread-2): 12:15:41  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:15:41.569042 (Thread-2): 12:15:41  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:15:41.569197 (Thread-2): 12:15:41  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

      describe extended `dbt_shabbirkdb`.`dimcompany`
  
2023-04-20 12:15:41.569323 (Thread-2): 12:15:41  Opening a new connection, currently in state init
2023-04-20 12:15:41.572499 (Thread-1): 12:15:41  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:15:41.572670 (Thread-1): 12:15:41  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:15:41.572822 (Thread-1): 12:15:41  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

      describe extended `dbt_shabbirkdb`.`dimbroker`
  
2023-04-20 12:15:41.572943 (Thread-1): 12:15:41  Opening a new connection, currently in state closed
2023-04-20 12:15:41.573579 (Thread-3): 12:15:41  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:15:41.573750 (Thread-3): 12:15:41  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:15:41.573919 (Thread-3): 12:15:41  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

      describe extended `dbt_shabbirkdb`.`dimcustomerstg`
  
2023-04-20 12:15:41.574043 (Thread-3): 12:15:41  Opening a new connection, currently in state init
2023-04-20 12:15:41.575234 (Thread-4): 12:15:41  Opening a new connection, currently in state init
2023-04-20 12:15:41.787932 (Thread-907): handling status request
2023-04-20 12:15:41.788472 (Thread-907): 12:15:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5081fee20>]}
2023-04-20 12:15:41.788998 (Thread-907): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:42.346533 (Thread-2): 12:15:42  SQL status: OK in 0.78 seconds
2023-04-20 12:15:42.394223 (Thread-2): 12:15:42  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:15:42.394667 (Thread-4): 12:15:42  SQL status: OK in 0.83 seconds
2023-04-20 12:15:42.397737 (Thread-3): 12:15:42  SQL status: OK in 0.82 seconds
2023-04-20 12:15:42.400696 (Thread-1): 12:15:42  SQL status: OK in 0.83 seconds
2023-04-20 12:15:42.406361 (Thread-3): 12:15:42  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:15:42.410083 (Thread-4): 12:15:42  Writing runtime sql for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:15:42.414279 (Thread-1): 12:15:42  Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:15:42.414678 (Thread-2): 12:15:42  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:15:42.414942 (Thread-2): 12:15:42  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCompany`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  * FROM (
  SELECT
    md5(cik) sk_companyid,
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
  
2023-04-20 12:15:42.424181 (Thread-4): 12:15:42  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:15:42.424397 (Thread-4): 12:15:42  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempDailyMarketHistorical`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  dmh.*,
  sk_dateid,
  min(dm_low) OVER (
    PARTITION BY dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweeklow,
  max(dm_high) OVER (
    PARTITION by dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweekhigh
FROM (
  SELECT * FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketHistorical`
  UNION ALL
  SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketIncremental`) dmh
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
  ON d.datevalue = dm_date;
  
2023-04-20 12:15:42.429762 (Thread-1): 12:15:42  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:15:42.429983 (Thread-1): 12:15:42  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT
  md5(employeeid) as sk_brokerid,
  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate`) effectivedate,
  date('9999-12-31') enddate
FROM  `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`HR`
WHERE employeejobcode = 314
  
2023-04-20 12:15:42.442313 (Thread-3): 12:15:42  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:15:42.442671 (Thread-3): 12:15:42  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomerStg`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT * FROM (
  SELECT
    sk_customerid,
    customerid,
    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) taxid,
    status,
    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) lastname,
    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) firstname,
    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) middleinitial,
    coalesce(gender, last_value(gender) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) gender,
    coalesce(tier, last_value(tier) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) tier,
    coalesce(dob, last_value(dob) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) dob,
    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline1,
    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline2,
    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) postalcode,
    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) CITY,
    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) stateprov,
    coalesce(country, last_value(country) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) country,
    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone1,
    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone2,
    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone3,
    coalesce(email1, last_value(email1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email1,
    coalesce(email2, last_value(email2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email2,
    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) LCL_TX_ID,
    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) NAT_TX_ID,
    batchid,
    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,
    date(update_ts) effectivedate,
    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate
  FROM (
    SELECT
      md5(customerid::string) as sk_customerid,
      customerid,
      taxid,
      status,
      lastname,
      firstname,
      middleinitial,
      gender,
      tier,
      dob,
      addressline1,
      addressline2,
      postalcode,
      city,
      stateprov,
      country,
      phone1,
      phone2,
      phone3,
      email1,
      email2,
      lcl_tx_id,
      nat_tx_id,
      1 batchid,
      update_ts
    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')
    UNION ALL
    SELECT
      md5(c.customerid::string) as sk_customerid,
      c.customerid,
      nullif(c.taxid, '') taxid,
      nullif(s.st_name, '') as status,
      nullif(c.lastname, '') lastname,
      nullif(c.firstname, '') firstname,
      nullif(c.middleinitial, '') middleinitial,
      gender,
      c.tier,
      c.dob,
      nullif(c.addressline1, '') addressline1,
      nullif(c.addressline2, '') addressline2,
      nullif(c.postalcode, '') postalcode,
      nullif(c.city, '') city,
      nullif(c.stateprov, '') stateprov,
      nullif(c.country, '') country,
      CASE
        WHEN isnull(c_local_1) then c_local_1
        ELSE concat(
          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),
          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),
          c_local_1,
          nvl(c_ext_1, '')) END as phone1,
      CASE
        WHEN isnull(c_local_2) then c_local_2
        ELSE concat(
          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),
          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),
          c_local_2,
          nvl(c_ext_2, '')) END as phone2,
      CASE
        WHEN isnull(c_local_3) then c_local_3
        ELSE concat(
          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),
          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),
          c_local_3,
          nvl(c_ext_3, '')) END as phone3,
      nullif(c.email1, '') email1,
      nullif(c.email2, '') email2,
      c.LCL_TX_ID, 
      c.NAT_TX_ID,
      c.batchid,
      timestamp(bd.batchdate) update_ts
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CustomerIncremental` c
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
      ON c.batchid = bd.batchid
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` s 
      ON c.status = s.st_id
  ) c
  )
  
2023-04-20 12:15:43.152121 (Thread-908): handling poll request
2023-04-20 12:15:43.152620 (Thread-908): 12:15:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8793a30>]}
2023-04-20 12:15:43.153977 (Thread-908): sending response (<Response 38504 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:43.915962 (Thread-909): handling ps request
2023-04-20 12:15:43.916460 (Thread-909): 12:15:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528056dc0>]}
2023-04-20 12:15:43.918167 (Thread-909): sending response (<Response 24488 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:44.349077 (Thread-910): handling status request
2023-04-20 12:15:44.349569 (Thread-910): 12:15:44  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528175e80>]}
2023-04-20 12:15:44.350067 (Thread-910): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:44.725199 (Thread-911): handling poll request
2023-04-20 12:15:44.725697 (Thread-911): 12:15:44  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528340940>]}
2023-04-20 12:15:44.726195 (Thread-911): sending response (<Response 291 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:45.585655 (Thread-2): 12:15:45  SQL status: OK in 3.17 seconds
2023-04-20 12:15:45.612509 (Thread-2): 12:15:45  Timing info for model.dbsql_dbt_tpch.DimCompany (execute): 2023-04-20 12:15:41.558419 => 2023-04-20 12:15:45.612347
2023-04-20 12:15:45.612741 (Thread-2): 12:15:45  On model.dbsql_dbt_tpch.DimCompany: ROLLBACK
2023-04-20 12:15:45.612887 (Thread-2): 12:15:45  Databricks adapter: NotImplemented: rollback
2023-04-20 12:15:45.613011 (Thread-2): 12:15:45  On model.dbsql_dbt_tpch.DimCompany: Close
2023-04-20 12:15:45.848542 (Thread-1): 12:15:45  SQL status: OK in 3.42 seconds
2023-04-20 12:15:45.850919 (Thread-1): 12:15:45  Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 2023-04-20 12:15:41.537467 => 2023-04-20 12:15:45.850761
2023-04-20 12:15:45.851126 (Thread-1): 12:15:45  On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
2023-04-20 12:15:45.851270 (Thread-1): 12:15:45  Databricks adapter: NotImplemented: rollback
2023-04-20 12:15:45.851392 (Thread-1): 12:15:45  On model.dbsql_dbt_tpch.DimBroker: Close
2023-04-20 12:15:45.892467 (Thread-2): 12:15:45  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '123769cb-405b-4be6-aff8-286dd3000510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f047ddee190>]}
2023-04-20 12:15:45.892992 (Thread-2): 12:15:45  2 of 15 OK created sql table model dbt_shabbirkdb.DimCompany ................... [OK in 4.39s]
2023-04-20 12:15:45.894584 (Thread-2): 12:15:45  Finished running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:15:45.895459 (Thread-2): 12:15:45  Began running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:15:45.895799 (Thread-2): 12:15:45  5 of 15 START sql table model dbt_shabbirkdb.DimSecurity ....................... [RUN]
2023-04-20 12:15:45.896285 (Thread-2): 12:15:45  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimSecurity'
2023-04-20 12:15:45.896465 (Thread-2): 12:15:45  Began compiling node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:15:45.901102 (Thread-2): 12:15:45  Writing injected SQL for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:15:45.915291 (Thread-2): 12:15:45  Timing info for model.dbsql_dbt_tpch.DimSecurity (compile): 2023-04-20 12:15:45.896509 => 2023-04-20 12:15:45.915124
2023-04-20 12:15:45.915545 (Thread-2): 12:15:45  Began executing node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:15:45.921316 (Thread-2): 12:15:45  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:15:45.921488 (Thread-2): 12:15:45  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:15:45.921649 (Thread-2): 12:15:45  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

      describe extended `dbt_shabbirkdb`.`dimsecurity`
  
2023-04-20 12:15:45.921779 (Thread-2): 12:15:45  Opening a new connection, currently in state closed
2023-04-20 12:15:46.135065 (Thread-1): 12:15:46  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '123769cb-405b-4be6-aff8-286dd3000510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04a80ca910>]}
2023-04-20 12:15:46.135626 (Thread-1): 12:15:46  1 of 15 OK created sql table model dbt_shabbirkdb.DimBroker .................... [OK in 4.64s]
2023-04-20 12:15:46.136097 (Thread-1): 12:15:46  Finished running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:15:46.136429 (Thread-1): 12:15:46  Began running node model.dbsql_dbt_tpch.Financial
2023-04-20 12:15:46.136707 (Thread-1): 12:15:46  6 of 15 START sql table model dbt_shabbirkdb.Financial ......................... [RUN]
2023-04-20 12:15:46.137177 (Thread-1): 12:15:46  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Financial'
2023-04-20 12:15:46.137360 (Thread-1): 12:15:46  Began compiling node model.dbsql_dbt_tpch.Financial
2023-04-20 12:15:46.141973 (Thread-1): 12:15:46  Writing injected SQL for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:15:46.155937 (Thread-1): 12:15:46  Timing info for model.dbsql_dbt_tpch.Financial (compile): 2023-04-20 12:15:46.137404 => 2023-04-20 12:15:46.155774
2023-04-20 12:15:46.156166 (Thread-1): 12:15:46  Began executing node model.dbsql_dbt_tpch.Financial
2023-04-20 12:15:46.160555 (Thread-1): 12:15:46  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:15:46.160742 (Thread-1): 12:15:46  Using databricks connection "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:15:46.160919 (Thread-1): 12:15:46  On model.dbsql_dbt_tpch.Financial: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

      describe extended `dbt_shabbirkdb`.`financial`
  
2023-04-20 12:15:46.161071 (Thread-1): 12:15:46  Opening a new connection, currently in state closed
2023-04-20 12:15:46.407579 (Thread-912): handling poll request
2023-04-20 12:15:46.408079 (Thread-912): 12:15:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50807d7f0>]}
2023-04-20 12:15:46.408949 (Thread-912): sending response (<Response 15973 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:46.583344 (Thread-3): 12:15:46  SQL status: OK in 4.14 seconds
2023-04-20 12:15:46.585779 (Thread-3): 12:15:46  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (execute): 2023-04-20 12:15:41.553101 => 2023-04-20 12:15:46.585615
2023-04-20 12:15:46.586018 (Thread-3): 12:15:46  On model.dbsql_dbt_tpch.DimCustomerStg: ROLLBACK
2023-04-20 12:15:46.586165 (Thread-3): 12:15:46  Databricks adapter: NotImplemented: rollback
2023-04-20 12:15:46.586289 (Thread-3): 12:15:46  On model.dbsql_dbt_tpch.DimCustomerStg: Close
2023-04-20 12:15:46.637855 (Thread-913): handling ps request
2023-04-20 12:15:46.638371 (Thread-913): 12:15:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50807b490>]}
2023-04-20 12:15:46.640119 (Thread-913): sending response (<Response 24488 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:46.646071 (Thread-2): 12:15:46  SQL status: OK in 0.72 seconds
2023-04-20 12:15:46.653843 (Thread-2): 12:15:46  Writing runtime sql for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:15:46.668606 (Thread-2): 12:15:46  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:15:46.668906 (Thread-2): 12:15:46  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 12:15:46.877151 (Thread-3): 12:15:46  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '123769cb-405b-4be6-aff8-286dd3000510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04a80d0340>]}
2023-04-20 12:15:46.877695 (Thread-3): 12:15:46  3 of 15 OK created sql table model dbt_shabbirkdb.DimCustomerStg ............... [OK in 5.37s]
2023-04-20 12:15:46.877947 (Thread-3): 12:15:46  Finished running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:15:46.878762 (Thread-3): 12:15:46  Began running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:15:46.879061 (Thread-3): 12:15:46  7 of 15 START sql table model dbt_shabbirkdb.DimAccount ........................ [RUN]
2023-04-20 12:15:46.879603 (Thread-3): 12:15:46  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimAccount'
2023-04-20 12:15:46.879801 (Thread-3): 12:15:46  Began compiling node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:15:46.886097 (Thread-3): 12:15:46  Writing injected SQL for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 12:15:46.886837 (Thread-1): 12:15:46  SQL status: OK in 0.73 seconds
2023-04-20 12:15:46.891959 (Thread-1): 12:15:46  Writing runtime sql for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:15:46.901597 (Thread-3): 12:15:46  Timing info for model.dbsql_dbt_tpch.DimAccount (compile): 2023-04-20 12:15:46.879846 => 2023-04-20 12:15:46.901431
2023-04-20 12:15:46.901825 (Thread-3): 12:15:46  Began executing node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:15:46.907564 (Thread-3): 12:15:46  Writing runtime sql for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 12:15:46.913930 (Thread-1): 12:15:46  Using databricks connection "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:15:46.914184 (Thread-1): 12:15:46  On model.dbsql_dbt_tpch.Financial: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 12:15:46.920576 (Thread-3): 12:15:46  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:15:46.920754 (Thread-3): 12:15:46  Using databricks connection "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 12:15:46.920984 (Thread-3): 12:15:46  On model.dbsql_dbt_tpch.DimAccount: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        sk_customerid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          md5(customerid::string) as sk_customerid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          md5(ca_c_id::string) as sk_customerid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 12:15:46.921116 (Thread-3): 12:15:46  Opening a new connection, currently in state closed
2023-04-20 12:15:47.119383 (Thread-914): handling status request
2023-04-20 12:15:47.119917 (Thread-914): 12:15:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50807bd30>]}
2023-04-20 12:15:47.120421 (Thread-914): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:47.764818 (Thread-3): 12:15:47  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        sk_customerid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          md5(customerid::string) as sk_customerid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          md5(ca_c_id::string) as sk_customerid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 12:15:47.765054 (Thread-3): 12:15:47  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [AMBIGUOUS_REFERENCE] Reference `a`.`sk_customerid` is ambiguous, could be: [`a`.`sk_customerid`, `a`.`sk_customerid`].; line 20 pos 2
2023-04-20 12:15:47.765184 (Thread-3): 12:15:47  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [AMBIGUOUS_REFERENCE] org.apache.spark.sql.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `a`.`sk_customerid` is ambiguous, could be: [`a`.`sk_customerid`, `a`.`sk_customerid`].; line 20 pos 2
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `a`.`sk_customerid` is ambiguous, could be: [`a`.`sk_customerid`, `a`.`sk_customerid`].; line 20 pos 2
	at org.apache.spark.sql.errors.QueryCompilationErrors$.ambiguousReferenceError(QueryCompilationErrors.scala:2063)
	at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:382)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpressionByPlanChildren$1(ColumnResolutionHelper.scala:384)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$3(ColumnResolutionHelper.scala:164)
	at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:110)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:171)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:139)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpression(ColumnResolutionHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren(ColumnResolutionHelper.scala:391)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren$(ColumnResolutionHelper.scala:377)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.resolveExpressionByPlanChildren(Analyzer.scala:2274)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$17.$anonfun$applyOrElse$122(Analyzer.scala:2404)
	at scala.collection.immutable.List.map(List.scala:297)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$17.applyOrElse(Analyzer.scala:2404)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$17.applyOrElse(Analyzer.scala:2299)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$5(AnalysisHelper.scala:145)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:145)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1335)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1332)
	at org.apache.spark.sql.catalyst.plans.logical.ReplaceTableAsSelect.mapChildren(v2Commands.scala:426)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:2299)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:2274)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:365)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:358)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:265)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:358)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:154)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:338)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:337)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:153)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:372)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:809)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:369)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:147)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:147)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:137)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getRuntimeCategory$2(QueryRuntimePrediction.scala:293)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:292)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 12:15:47.765299 (Thread-3): 12:15:47  Databricks adapter: operation-id: b'\x01\xed\xdfu\x14\xf7\x19A\x83\xf4\x8a\xcd\x81\xf7\x0f\xa9'
2023-04-20 12:15:47.765545 (Thread-3): 12:15:47  Timing info for model.dbsql_dbt_tpch.DimAccount (execute): 2023-04-20 12:15:46.901880 => 2023-04-20 12:15:47.765411
2023-04-20 12:15:47.765731 (Thread-3): 12:15:47  On model.dbsql_dbt_tpch.DimAccount: ROLLBACK
2023-04-20 12:15:47.765858 (Thread-3): 12:15:47  Databricks adapter: NotImplemented: rollback
2023-04-20 12:15:47.765997 (Thread-3): 12:15:47  On model.dbsql_dbt_tpch.DimAccount: Close
2023-04-20 12:15:48.030948 (Thread-915): handling poll request
2023-04-20 12:15:48.031466 (Thread-915): 12:15:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdc18e0>]}
2023-04-20 12:15:48.032474 (Thread-915): sending response (<Response 37864 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:48.052030 (Thread-3): 12:15:48  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
  [AMBIGUOUS_REFERENCE] Reference `a`.`sk_customerid` is ambiguous, could be: [`a`.`sk_customerid`, `a`.`sk_customerid`].; line 20 pos 2
2023-04-20 12:15:48.052582 (Thread-3): 12:15:48  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '123769cb-405b-4be6-aff8-286dd3000510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04aa0fe4f0>]}
2023-04-20 12:15:48.053023 (Thread-3): 12:15:48  7 of 15 ERROR creating sql table model dbt_shabbirkdb.DimAccount ............... [ERROR in 1.17s]
2023-04-20 12:15:48.053235 (Thread-3): 12:15:48  Finished running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:15:48.053527 (Thread-3): 12:15:48  Began running node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:15:48.053793 (Thread-3): 12:15:48  8 of 15 START sql table model dbt_shabbirkdb.Prospect .......................... [RUN]
2023-04-20 12:15:48.054329 (Thread-3): 12:15:48  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Prospect'
2023-04-20 12:15:48.054513 (Thread-3): 12:15:48  Began compiling node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:15:48.060441 (Thread-3): 12:15:48  Writing injected SQL for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:15:48.074350 (Thread-3): 12:15:48  Timing info for model.dbsql_dbt_tpch.Prospect (compile): 2023-04-20 12:15:48.054557 => 2023-04-20 12:15:48.074165
2023-04-20 12:15:48.074596 (Thread-3): 12:15:48  Began executing node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:15:48.079439 (Thread-3): 12:15:48  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:15:48.079685 (Thread-3): 12:15:48  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:15:48.079981 (Thread-3): 12:15:48  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

      describe extended `dbt_shabbirkdb`.`prospect`
  
2023-04-20 12:15:48.080120 (Thread-3): 12:15:48  Opening a new connection, currently in state closed
2023-04-20 12:15:48.792541 (Thread-3): 12:15:48  SQL status: OK in 0.71 seconds
2023-04-20 12:15:48.798259 (Thread-3): 12:15:48  Writing runtime sql for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:15:48.811660 (Thread-3): 12:15:48  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:15:48.811981 (Thread-3): 12:15:48  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Prospect`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  agencyid,
  recdate.sk_dateid sk_recorddateid,
  origdate.sk_dateid sk_updatedateid,
  p.batchid,
  nvl2(c.customerid, True, False) iscustomer, 
  p.lastname,
  p.firstname,
  p.middleinitial,
  p.gender,
  p.addressline1,
  p.addressline2,
  p.postalcode,
  city,
  state,
  country,
  phone,
  income,
  numbercars,
  numberchildren,
  maritalstatus,
  age,
  creditrating,
  ownorrentflag,
  employer,
  numbercreditcards,
  networth,
  if(
    isnotnull(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+","")),
    left(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+",""),
      length(
        if(networth > 1000000 or income > 200000,"HighValue+","") || 
        if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
        if(age > 45, "Boomer+", "") ||
        if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
        if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
        if(age < 25 and networth > 1000000, "Inherited+",""))
      -1),
    NULL) marketingnameplate
FROM (
  SELECT 
    * FROM (
    SELECT
      agencyid,
      max(batchid) recordbatchid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth,
      min(batchid) batchid
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`ProspectRaw` p
    GROUP BY
      agencyid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth)
  QUALIFY ROW_NUMBER() OVER (PARTITION BY agencyid ORDER BY batchid DESC) = 1) p
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) recdate
  ON p.recordbatchid = recdate.batchid
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) origdate
  ON p.batchid = origdate.batchid
LEFT JOIN (
  SELECT 
    customerid,
    lastname,
    firstname,
    addressline1,
    addressline2,
    postalcode
  FROM `dbt_shabbirkdb`.`DimCustomerStg`
  WHERE iscurrent) c
  ON 
    upper(p.LastName) = upper(c.lastname)
    and upper(p.FirstName) = upper(c.firstname)
    and upper(p.AddressLine1) = upper(c.addressline1)
    and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
    and upper(p.PostalCode) = upper(c.postalcode)
  
2023-04-20 12:15:49.009737 (Thread-4): 12:15:49  SQL status: OK in 6.59 seconds
2023-04-20 12:15:49.239595 (Thread-916): handling ps request
2023-04-20 12:15:49.240093 (Thread-916): 12:15:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528063a30>]}
2023-04-20 12:15:49.266075 (Thread-916): sending response (<Response 24488 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:49.328755 (Thread-4): 12:15:49  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (execute): 2023-04-20 12:15:41.532392 => 2023-04-20 12:15:49.328569
2023-04-20 12:15:49.329060 (Thread-4): 12:15:49  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: ROLLBACK
2023-04-20 12:15:49.329219 (Thread-4): 12:15:49  Databricks adapter: NotImplemented: rollback
2023-04-20 12:15:49.329371 (Thread-4): 12:15:49  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: Close
2023-04-20 12:15:49.606274 (Thread-4): 12:15:49  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '123769cb-405b-4be6-aff8-286dd3000510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04aa0fe430>]}
2023-04-20 12:15:49.606969 (Thread-4): 12:15:49  4 of 15 OK created sql table model dbt_shabbirkdb.tempDailyMarketHistorical .... [OK in 8.09s]
2023-04-20 12:15:49.607229 (Thread-4): 12:15:49  Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:15:49.607553 (Thread-4): 12:15:49  Began running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 12:15:49.607798 (Thread-4): 12:15:49  9 of 15 SKIP relation dbt_shabbirkdb.FactCashBalances .......................... [SKIP]
2023-04-20 12:15:49.607982 (Thread-4): 12:15:49  Finished running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 12:15:49.623779 (Thread-917): handling poll request
2023-04-20 12:15:49.624186 (Thread-917): 12:15:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5280632e0>]}
2023-04-20 12:15:49.625019 (Thread-917): sending response (<Response 17397 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:49.729407 (Thread-918): handling status request
2023-04-20 12:15:49.729912 (Thread-918): 12:15:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528063c70>]}
2023-04-20 12:15:49.730407 (Thread-918): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:50.224042 (Thread-2): 12:15:50  SQL status: OK in 3.55 seconds
2023-04-20 12:15:50.226423 (Thread-2): 12:15:50  Timing info for model.dbsql_dbt_tpch.DimSecurity (execute): 2023-04-20 12:15:45.915598 => 2023-04-20 12:15:50.226262
2023-04-20 12:15:50.226638 (Thread-2): 12:15:50  On model.dbsql_dbt_tpch.DimSecurity: ROLLBACK
2023-04-20 12:15:50.226782 (Thread-2): 12:15:50  Databricks adapter: NotImplemented: rollback
2023-04-20 12:15:50.226905 (Thread-2): 12:15:50  On model.dbsql_dbt_tpch.DimSecurity: Close
2023-04-20 12:15:50.504376 (Thread-2): 12:15:50  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '123769cb-405b-4be6-aff8-286dd3000510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04aa0c1670>]}
2023-04-20 12:15:50.505116 (Thread-2): 12:15:50  5 of 15 OK created sql table model dbt_shabbirkdb.DimSecurity .................. [OK in 4.61s]
2023-04-20 12:15:50.505470 (Thread-2): 12:15:50  Finished running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:15:50.506423 (Thread-4): 12:15:50  Began running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 12:15:50.506744 (Thread-4): 12:15:50  10 of 15 SKIP relation dbt_shabbirkdb.DimTrade ................................. [SKIP]
2023-04-20 12:15:50.507025 (Thread-4): 12:15:50  Finished running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 12:15:50.507721 (Thread-4): 12:15:50  Began running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 12:15:50.507976 (Thread-4): 12:15:50  11 of 15 SKIP relation dbt_shabbirkdb.FactHoldings ............................. [SKIP]
2023-04-20 12:15:50.508166 (Thread-4): 12:15:50  Finished running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 12:15:51.275368 (Thread-919): handling poll request
2023-04-20 12:15:51.275885 (Thread-919): 12:15:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508080d90>]}
2023-04-20 12:15:51.276557 (Thread-919): sending response (<Response 6737 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:51.835289 (Thread-920): handling ps request
2023-04-20 12:15:51.835865 (Thread-920): 12:15:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bda6790>]}
2023-04-20 12:15:51.837677 (Thread-920): sending response (<Response 24487 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:52.294956 (Thread-921): handling status request
2023-04-20 12:15:52.295446 (Thread-921): 12:15:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50807b160>]}
2023-04-20 12:15:52.295994 (Thread-921): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:52.727939 (Thread-3): 12:15:52  SQL status: OK in 3.92 seconds
2023-04-20 12:15:52.730309 (Thread-3): 12:15:52  Timing info for model.dbsql_dbt_tpch.Prospect (execute): 2023-04-20 12:15:48.074647 => 2023-04-20 12:15:52.730153
2023-04-20 12:15:52.730547 (Thread-3): 12:15:52  On model.dbsql_dbt_tpch.Prospect: ROLLBACK
2023-04-20 12:15:52.730691 (Thread-3): 12:15:52  Databricks adapter: NotImplemented: rollback
2023-04-20 12:15:52.730812 (Thread-3): 12:15:52  On model.dbsql_dbt_tpch.Prospect: Close
2023-04-20 12:15:52.935682 (Thread-922): handling poll request
2023-04-20 12:15:52.936170 (Thread-922): 12:15:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdc1550>]}
2023-04-20 12:15:52.936716 (Thread-922): sending response (<Response 2125 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:53.001347 (Thread-3): 12:15:53  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '123769cb-405b-4be6-aff8-286dd3000510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f047ca84880>]}
2023-04-20 12:15:53.001882 (Thread-3): 12:15:53  8 of 15 OK created sql table model dbt_shabbirkdb.Prospect ..................... [OK in 4.95s]
2023-04-20 12:15:53.002120 (Thread-3): 12:15:53  Finished running node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:15:53.003015 (Thread-4): 12:15:53  Began running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:15:53.003346 (Thread-4): 12:15:53  12 of 15 START sql table model dbt_shabbirkdb.DimCustomer ...................... [RUN]
2023-04-20 12:15:53.003895 (Thread-4): 12:15:53  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomer'
2023-04-20 12:15:53.004085 (Thread-4): 12:15:53  Began compiling node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:15:53.009136 (Thread-4): 12:15:53  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 12:15:53.023363 (Thread-4): 12:15:53  Timing info for model.dbsql_dbt_tpch.DimCustomer (compile): 2023-04-20 12:15:53.004130 => 2023-04-20 12:15:53.023196
2023-04-20 12:15:53.023627 (Thread-4): 12:15:53  Began executing node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:15:53.029263 (Thread-4): 12:15:53  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 12:15:53.042794 (Thread-4): 12:15:53  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:15:53.042981 (Thread-4): 12:15:53  Using databricks connection "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 12:15:53.043183 (Thread-4): 12:15:53  On model.dbsql_dbt_tpch.DimCustomer: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 12:15:53.043316 (Thread-4): 12:15:53  Opening a new connection, currently in state closed
2023-04-20 12:15:53.596425 (Thread-1): 12:15:53  SQL status: OK in 6.68 seconds
2023-04-20 12:15:53.925789 (Thread-1): 12:15:53  Timing info for model.dbsql_dbt_tpch.Financial (execute): 2023-04-20 12:15:46.156215 => 2023-04-20 12:15:53.925606
2023-04-20 12:15:53.926090 (Thread-1): 12:15:53  On model.dbsql_dbt_tpch.Financial: ROLLBACK
2023-04-20 12:15:53.926247 (Thread-1): 12:15:53  Databricks adapter: NotImplemented: rollback
2023-04-20 12:15:53.926371 (Thread-1): 12:15:53  On model.dbsql_dbt_tpch.Financial: Close
2023-04-20 12:15:54.202645 (Thread-1): 12:15:54  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '123769cb-405b-4be6-aff8-286dd3000510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04aa02f2e0>]}
2023-04-20 12:15:54.203181 (Thread-1): 12:15:54  6 of 15 OK created sql table model dbt_shabbirkdb.Financial .................... [OK in 8.07s]
2023-04-20 12:15:54.203420 (Thread-1): 12:15:54  Finished running node model.dbsql_dbt_tpch.Financial
2023-04-20 12:15:54.204359 (Thread-3): 12:15:54  Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:15:54.204701 (Thread-3): 12:15:54  13 of 15 START sql table model dbt_shabbirkdb.tempSumpFiBasicEps ............... [RUN]
2023-04-20 12:15:54.205221 (Thread-3): 12:15:54  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempSumpFiBasicEps'
2023-04-20 12:15:54.205410 (Thread-3): 12:15:54  Began compiling node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:15:54.209633 (Thread-3): 12:15:54  Writing injected SQL for node "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:15:54.224001 (Thread-3): 12:15:54  Timing info for model.dbsql_dbt_tpch.tempSumpFiBasicEps (compile): 2023-04-20 12:15:54.205455 => 2023-04-20 12:15:54.223741
2023-04-20 12:15:54.224362 (Thread-3): 12:15:54  Began executing node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:15:54.232708 (Thread-3): 12:15:54  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:15:54.232984 (Thread-3): 12:15:54  Using databricks connection "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:15:54.233255 (Thread-3): 12:15:54  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempSumpFiBasicEps"} */

      describe extended `dbt_shabbirkdb`.`tempsumpfibasiceps`
  
2023-04-20 12:15:54.233469 (Thread-3): 12:15:54  Opening a new connection, currently in state closed
2023-04-20 12:15:54.632266 (Thread-923): handling ps request
2023-04-20 12:15:54.632768 (Thread-923): 12:15:54  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8680070>]}
2023-04-20 12:15:54.634461 (Thread-923): sending response (<Response 24486 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:54.685295 (Thread-924): handling poll request
2023-04-20 12:15:54.685732 (Thread-924): 12:15:54  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5081b8250>]}
2023-04-20 12:15:54.686563 (Thread-924): sending response (<Response 15935 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:54.962745 (Thread-3): 12:15:54  SQL status: OK in 0.73 seconds
2023-04-20 12:15:54.968441 (Thread-3): 12:15:54  Writing runtime sql for node "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:15:54.983290 (Thread-3): 12:15:54  Using databricks connection "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:15:54.983710 (Thread-3): 12:15:54  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempSumpFiBasicEps"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempSumpFiBasicEps`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  sk_companyid,
  fi_qtr_start_date,
  sum(fi_basic_eps) OVER (PARTITION BY companyid ORDER BY fi_qtr_start_date ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) - fi_basic_eps sum_fi_basic_eps
FROM `dbt_shabbirkdb`.`Financial`
JOIN `dbt_shabbirkdb`.`DimCompany`
  USING (sk_companyid);
  
2023-04-20 12:15:55.072887 (Thread-925): handling status request
2023-04-20 12:15:55.073374 (Thread-925): 12:15:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5081b83d0>]}
2023-04-20 12:15:55.073852 (Thread-925): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:56.389104 (Thread-926): handling poll request
2023-04-20 12:15:56.389607 (Thread-926): 12:15:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5081b8040>]}
2023-04-20 12:15:56.390163 (Thread-926): sending response (<Response 2434 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:56.962386 (Thread-4): 12:15:56  SQL status: OK in 3.92 seconds
2023-04-20 12:15:56.965056 (Thread-4): 12:15:56  Timing info for model.dbsql_dbt_tpch.DimCustomer (execute): 2023-04-20 12:15:53.023697 => 2023-04-20 12:15:56.964899
2023-04-20 12:15:56.965266 (Thread-4): 12:15:56  On model.dbsql_dbt_tpch.DimCustomer: ROLLBACK
2023-04-20 12:15:56.965408 (Thread-4): 12:15:56  Databricks adapter: NotImplemented: rollback
2023-04-20 12:15:56.965531 (Thread-4): 12:15:56  On model.dbsql_dbt_tpch.DimCustomer: Close
2023-04-20 12:15:57.219491 (Thread-927): handling ps request
2023-04-20 12:15:57.220004 (Thread-927): 12:15:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5081b87c0>]}
2023-04-20 12:15:57.221656 (Thread-927): sending response (<Response 24488 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:57.250253 (Thread-4): 12:15:57  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '123769cb-405b-4be6-aff8-286dd3000510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04a8097100>]}
2023-04-20 12:15:57.250781 (Thread-4): 12:15:57  12 of 15 OK created sql table model dbt_shabbirkdb.DimCustomer ................. [OK in 4.25s]
2023-04-20 12:15:57.251018 (Thread-4): 12:15:57  Finished running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:15:57.251991 (Thread-1): 12:15:57  Began running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 12:15:57.252307 (Thread-1): 12:15:57  14 of 15 START sql table model dbt_shabbirkdb.FactWatches ...................... [RUN]
2023-04-20 12:15:57.252801 (Thread-1): 12:15:57  Acquiring new databricks connection 'model.dbsql_dbt_tpch.FactWatches'
2023-04-20 12:15:57.252983 (Thread-1): 12:15:57  Began compiling node model.dbsql_dbt_tpch.FactWatches
2023-04-20 12:15:57.257990 (Thread-1): 12:15:57  Writing injected SQL for node "model.dbsql_dbt_tpch.FactWatches"
2023-04-20 12:15:57.268858 (Thread-1): 12:15:57  Timing info for model.dbsql_dbt_tpch.FactWatches (compile): 2023-04-20 12:15:57.253028 => 2023-04-20 12:15:57.268699
2023-04-20 12:15:57.269081 (Thread-1): 12:15:57  Began executing node model.dbsql_dbt_tpch.FactWatches
2023-04-20 12:15:57.274345 (Thread-1): 12:15:57  Writing runtime sql for node "model.dbsql_dbt_tpch.FactWatches"
2023-04-20 12:15:57.285184 (Thread-1): 12:15:57  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:15:57.285367 (Thread-1): 12:15:57  Using databricks connection "model.dbsql_dbt_tpch.FactWatches"
2023-04-20 12:15:57.285595 (Thread-1): 12:15:57  On model.dbsql_dbt_tpch.FactWatches: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate;
  
2023-04-20 12:15:57.285729 (Thread-1): 12:15:57  Opening a new connection, currently in state closed
2023-04-20 12:15:57.657954 (Thread-928): handling status request
2023-04-20 12:15:57.658432 (Thread-928): 12:15:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5081b8eb0>]}
2023-04-20 12:15:57.658907 (Thread-928): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:57.964720 (Thread-1): 12:15:57  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate;
  
2023-04-20 12:15:57.964950 (Thread-1): 12:15:57  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate

2023-04-20 12:15:57.965078 (Thread-1): 12:15:57  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 12:15:57.965192 (Thread-1): 12:15:57  Databricks adapter: operation-id: b'\x01\xed\xdfu\x1b%\x1fH\x84Q|\xec\x81\xdeR\x8d'
2023-04-20 12:15:57.965437 (Thread-1): 12:15:57  Timing info for model.dbsql_dbt_tpch.FactWatches (execute): 2023-04-20 12:15:57.269130 => 2023-04-20 12:15:57.965306
2023-04-20 12:15:57.965621 (Thread-1): 12:15:57  On model.dbsql_dbt_tpch.FactWatches: ROLLBACK
2023-04-20 12:15:57.965747 (Thread-1): 12:15:57  Databricks adapter: NotImplemented: rollback
2023-04-20 12:15:57.965878 (Thread-1): 12:15:57  On model.dbsql_dbt_tpch.FactWatches: Close
2023-04-20 12:15:57.982025 (Thread-929): handling poll request
2023-04-20 12:15:57.982453 (Thread-929): 12:15:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50817a1f0>]}
2023-04-20 12:15:57.983289 (Thread-929): sending response (<Response 30283 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:58.239345 (Thread-1): 12:15:58  Runtime Error in model FactWatches (models/incremental/FactWatches.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */
  
    
      
          create or replace table `dbt_shabbirkdb`.`FactWatches`
        
        
      using delta
        
        
        
        
        
        
        as
        
  SELECT
    c.sk_customerid sk_customerid,
    s.sk_securityid sk_securityid,
    sk_dateid_dateplaced,
    sk_dateid_dateremoved,
    wh.batchid
  FROM (
    SELECT * EXCEPT(w_dts)
    FROM (
      SELECT
        customerid,
        symbol,
        coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
          PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
        coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
          PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
        coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
          PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
        w_dts,
        coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
          PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
      FROM ( 
        SELECT 
          wh.w_c_id customerid,
          wh.w_s_symb symbol,
          if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
          if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
          if(w_action = 'ACTV', d.datevalue, null) dateplaced,
          wh.w_dts,
          batchid 
        FROM (
          SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
          UNION ALL
          SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
          ON d.datevalue = date(wh.w_dts)))
    QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
  -- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
  ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ^^^
    ON 
      s.symbol = wh.symbol
      AND wh.dateplaced >= s.effectivedate 
      AND wh.dateplaced < s.enddate
  ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
    ON
      wh.customerid = c.customerid
      AND wh.dateplaced >= c.effectivedate 
      AND wh.dateplaced < c.enddate
  
2023-04-20 12:15:58.239940 (Thread-1): 12:15:58  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '123769cb-405b-4be6-aff8-286dd3000510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04aa02f100>]}
2023-04-20 12:15:58.240469 (Thread-1): 12:15:58  14 of 15 ERROR creating sql table model dbt_shabbirkdb.FactWatches ............. [ERROR in 0.99s]
2023-04-20 12:15:58.240699 (Thread-1): 12:15:58  Finished running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 12:15:58.438381 (Thread-3): 12:15:58  SQL status: OK in 3.45 seconds
2023-04-20 12:15:58.440728 (Thread-3): 12:15:58  Timing info for model.dbsql_dbt_tpch.tempSumpFiBasicEps (execute): 2023-04-20 12:15:54.224441 => 2023-04-20 12:15:58.440579
2023-04-20 12:15:58.440930 (Thread-3): 12:15:58  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: ROLLBACK
2023-04-20 12:15:58.441071 (Thread-3): 12:15:58  Databricks adapter: NotImplemented: rollback
2023-04-20 12:15:58.441191 (Thread-3): 12:15:58  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: Close
2023-04-20 12:15:58.732860 (Thread-3): 12:15:58  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '123769cb-405b-4be6-aff8-286dd3000510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f047cac4b80>]}
2023-04-20 12:15:58.733374 (Thread-3): 12:15:58  13 of 15 OK created sql table model dbt_shabbirkdb.tempSumpFiBasicEps .......... [OK in 4.53s]
2023-04-20 12:15:58.733612 (Thread-3): 12:15:58  Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:15:58.734730 (Thread-4): 12:15:58  Began running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:15:58.735060 (Thread-4): 12:15:58  15 of 15 START sql table model dbt_shabbirkdb.FactMarketHistory ................ [RUN]
2023-04-20 12:15:58.735587 (Thread-4): 12:15:58  Acquiring new databricks connection 'model.dbsql_dbt_tpch.FactMarketHistory'
2023-04-20 12:15:58.735779 (Thread-4): 12:15:58  Began compiling node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:15:58.740765 (Thread-4): 12:15:58  Writing injected SQL for node "model.dbsql_dbt_tpch.FactMarketHistory"
2023-04-20 12:15:58.756696 (Thread-4): 12:15:58  Timing info for model.dbsql_dbt_tpch.FactMarketHistory (compile): 2023-04-20 12:15:58.735825 => 2023-04-20 12:15:58.756528
2023-04-20 12:15:58.756921 (Thread-4): 12:15:58  Began executing node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:15:58.761258 (Thread-4): 12:15:58  Writing runtime sql for node "model.dbsql_dbt_tpch.FactMarketHistory"
2023-04-20 12:15:58.776644 (Thread-4): 12:15:58  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:15:58.776835 (Thread-4): 12:15:58  Using databricks connection "model.dbsql_dbt_tpch.FactMarketHistory"
2023-04-20 12:15:58.777052 (Thread-4): 12:15:58  On model.dbsql_dbt_tpch.FactMarketHistory: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date);
  
2023-04-20 12:15:58.777186 (Thread-4): 12:15:58  Opening a new connection, currently in state closed
2023-04-20 12:15:59.447299 (Thread-4): 12:15:59  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date);
  
2023-04-20 12:15:59.447569 (Thread-4): 12:15:59  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date)

2023-04-20 12:15:59.447712 (Thread-4): 12:15:59  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date)

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date)

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 12:15:59.447827 (Thread-4): 12:15:59  Databricks adapter: operation-id: b'\x01\xed\xdfu\x1c\t\x17\x88\xbc\x85\xef\x15+\x91.\xdb'
2023-04-20 12:15:59.448069 (Thread-4): 12:15:59  Timing info for model.dbsql_dbt_tpch.FactMarketHistory (execute): 2023-04-20 12:15:58.756970 => 2023-04-20 12:15:59.447937
2023-04-20 12:15:59.448255 (Thread-4): 12:15:59  On model.dbsql_dbt_tpch.FactMarketHistory: ROLLBACK
2023-04-20 12:15:59.448380 (Thread-4): 12:15:59  Databricks adapter: NotImplemented: rollback
2023-04-20 12:15:59.448501 (Thread-4): 12:15:59  On model.dbsql_dbt_tpch.FactMarketHistory: Close
2023-04-20 12:15:59.526950 (Thread-930): handling poll request
2023-04-20 12:15:59.527406 (Thread-930): 12:15:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50817aee0>]}
2023-04-20 12:15:59.528311 (Thread-930): sending response (<Response 32724 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:15:59.732822 (Thread-4): 12:15:59  Runtime Error in model FactMarketHistory (models/incremental/FactMarketHistory.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */
  
    
      
          create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
        
        
      using delta
        
        
        
        
        
        
        as
        
  SELECT 
    s.sk_securityid,
    s.sk_companyid,
    sk_dateid,
    fmh.dm_close / sum_fi_basic_eps AS peratio,
    (s.dividend / fmh.dm_close) / 100 yield,
    fiftytwoweekhigh,
    sk_fiftytwoweekhighdate,
    fiftytwoweeklow,
    sk_fiftytwoweeklowdate,
    dm_close closeprice,
    dm_high dayhigh,
    dm_low daylow,
    dm_vol volume,
    fmh.batchid
  FROM (
    SELECT * FROM (
      SELECT 
        a.*,
        b.sk_dateid AS sk_fiftytwoweeklowdate,
        c.sk_dateid AS sk_fiftytwoweekhighdate
      FROM
        `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
      JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
        ON
          a.dm_s_symb = b.dm_s_symb
          AND a.fiftytwoweeklow = b.dm_low
          AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
      JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
        ON 
          a.dm_s_symb = c.dm_s_symb
          AND a.fiftytwoweekhigh = c.dm_high
          AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
    QUALIFY ROW_NUMBER() OVER (
      PARTITION BY dm_s_symb, dm_date 
      ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
  -- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
  ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ^^^
    ON 
      s.symbol = fmh.dm_s_symb
      AND fmh.dm_date >= s.effectivedate 
      AND fmh.dm_date < s.enddate
  LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
    ON 
      f.sk_companyid = s.sk_companyid
      AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
      AND year(fmh.dm_date) = year(fi_qtr_start_date)
  
2023-04-20 12:15:59.733257 (Thread-4): 12:15:59  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '123769cb-405b-4be6-aff8-286dd3000510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f047ca87b50>]}
2023-04-20 12:15:59.733713 (Thread-4): 12:15:59  15 of 15 ERROR creating sql table model dbt_shabbirkdb.FactMarketHistory ....... [ERROR in 1.00s]
2023-04-20 12:15:59.733942 (Thread-4): 12:15:59  Finished running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:15:59.735851 (MainThread): 12:15:59  Acquiring new databricks connection 'master'
2023-04-20 12:15:59.736077 (MainThread): 12:15:59  On master: ROLLBACK
2023-04-20 12:15:59.736232 (MainThread): 12:15:59  Opening a new connection, currently in state init
2023-04-20 12:15:59.768654 (Thread-931): handling ps request
2023-04-20 12:15:59.769072 (Thread-931): 12:15:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5283d92b0>]}
2023-04-20 12:15:59.770959 (Thread-931): sending response (<Response 24488 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:00.018954 (MainThread): 12:16:00  Databricks adapter: NotImplemented: rollback
2023-04-20 12:16:00.019196 (MainThread): 12:16:00  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:16:00.019333 (MainThread): 12:16:00  Spark adapter: NotImplemented: commit
2023-04-20 12:16:00.019497 (MainThread): 12:16:00  On master: ROLLBACK
2023-04-20 12:16:00.019664 (MainThread): 12:16:00  Databricks adapter: NotImplemented: rollback
2023-04-20 12:16:00.019805 (MainThread): 12:16:00  On master: Close
2023-04-20 12:16:00.296030 (Thread-932): handling status request
2023-04-20 12:16:00.296522 (Thread-932): 12:16:00  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528063130>]}
2023-04-20 12:16:00.297011 (Thread-932): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:00.300873 (MainThread): 12:16:00  Connection 'master' was properly closed.
2023-04-20 12:16:00.301071 (MainThread): 12:16:00  Connection 'model.dbsql_dbt_tpch.FactWatches' was properly closed.
2023-04-20 12:16:00.301183 (MainThread): 12:16:00  Connection 'model.dbsql_dbt_tpch.DimSecurity' was properly closed.
2023-04-20 12:16:00.301287 (MainThread): 12:16:00  Connection 'model.dbsql_dbt_tpch.tempSumpFiBasicEps' was properly closed.
2023-04-20 12:16:00.301387 (MainThread): 12:16:00  Connection 'model.dbsql_dbt_tpch.FactMarketHistory' was properly closed.
2023-04-20 12:16:00.301697 (MainThread): 12:16:00  
2023-04-20 12:16:00.301848 (MainThread): 12:16:00  Finished running 15 table models in 0 hours 0 minutes and 21.86 seconds (21.86s).
2023-04-20 12:16:00.406134 (MainThread): 12:16:00  
2023-04-20 12:16:00.406403 (MainThread): 12:16:00  Completed with 3 errors and 0 warnings:
2023-04-20 12:16:00.406534 (MainThread): 12:16:00  
2023-04-20 12:16:00.406674 (MainThread): 12:16:00  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
2023-04-20 12:16:00.406799 (MainThread): 12:16:00    [AMBIGUOUS_REFERENCE] Reference `a`.`sk_customerid` is ambiguous, could be: [`a`.`sk_customerid`, `a`.`sk_customerid`].; line 20 pos 2
2023-04-20 12:16:00.406909 (MainThread): 12:16:00  
2023-04-20 12:16:00.407037 (MainThread): 12:16:00  Runtime Error in model FactWatches (models/incremental/FactWatches.sql)
2023-04-20 12:16:00.407147 (MainThread): 12:16:00    
2023-04-20 12:16:00.407251 (MainThread): 12:16:00    [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)
2023-04-20 12:16:00.407353 (MainThread): 12:16:00    
2023-04-20 12:16:00.407455 (MainThread): 12:16:00    == SQL ==
2023-04-20 12:16:00.407584 (MainThread): 12:16:00    /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */
2023-04-20 12:16:00.407693 (MainThread): 12:16:00    
2023-04-20 12:16:00.407793 (MainThread): 12:16:00      
2023-04-20 12:16:00.407894 (MainThread): 12:16:00        
2023-04-20 12:16:00.407994 (MainThread): 12:16:00            create or replace table `dbt_shabbirkdb`.`FactWatches`
2023-04-20 12:16:00.408096 (MainThread): 12:16:00          
2023-04-20 12:16:00.408197 (MainThread): 12:16:00          
2023-04-20 12:16:00.408296 (MainThread): 12:16:00        using delta
2023-04-20 12:16:00.408395 (MainThread): 12:16:00          
2023-04-20 12:16:00.408494 (MainThread): 12:16:00          
2023-04-20 12:16:00.408593 (MainThread): 12:16:00          
2023-04-20 12:16:00.408692 (MainThread): 12:16:00          
2023-04-20 12:16:00.408791 (MainThread): 12:16:00          
2023-04-20 12:16:00.408892 (MainThread): 12:16:00          
2023-04-20 12:16:00.408990 (MainThread): 12:16:00          as
2023-04-20 12:16:00.409087 (MainThread): 12:16:00          
2023-04-20 12:16:00.409183 (MainThread): 12:16:00    SELECT
2023-04-20 12:16:00.409278 (MainThread): 12:16:00      c.sk_customerid sk_customerid,
2023-04-20 12:16:00.409372 (MainThread): 12:16:00      s.sk_securityid sk_securityid,
2023-04-20 12:16:00.409467 (MainThread): 12:16:00      sk_dateid_dateplaced,
2023-04-20 12:16:00.409563 (MainThread): 12:16:00      sk_dateid_dateremoved,
2023-04-20 12:16:00.409685 (MainThread): 12:16:00      wh.batchid
2023-04-20 12:16:00.409806 (MainThread): 12:16:00    FROM (
2023-04-20 12:16:00.409938 (MainThread): 12:16:00      SELECT * EXCEPT(w_dts)
2023-04-20 12:16:00.410073 (MainThread): 12:16:00      FROM (
2023-04-20 12:16:00.410198 (MainThread): 12:16:00        SELECT
2023-04-20 12:16:00.410312 (MainThread): 12:16:00          customerid,
2023-04-20 12:16:00.410423 (MainThread): 12:16:00          symbol,
2023-04-20 12:16:00.410538 (MainThread): 12:16:00          coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
2023-04-20 12:16:00.410648 (MainThread): 12:16:00            PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
2023-04-20 12:16:00.410765 (MainThread): 12:16:00          coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
2023-04-20 12:16:00.410876 (MainThread): 12:16:00            PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
2023-04-20 12:16:00.410983 (MainThread): 12:16:00          coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
2023-04-20 12:16:00.411099 (MainThread): 12:16:00            PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
2023-04-20 12:16:00.411212 (MainThread): 12:16:00          w_dts,
2023-04-20 12:16:00.411464 (MainThread): 12:16:00          coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
2023-04-20 12:16:00.411658 (MainThread): 12:16:00            PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
2023-04-20 12:16:00.411784 (MainThread): 12:16:00        FROM ( 
2023-04-20 12:16:00.411902 (MainThread): 12:16:00          SELECT 
2023-04-20 12:16:00.412015 (MainThread): 12:16:00            wh.w_c_id customerid,
2023-04-20 12:16:00.412134 (MainThread): 12:16:00            wh.w_s_symb symbol,
2023-04-20 12:16:00.412245 (MainThread): 12:16:00            if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
2023-04-20 12:16:00.412362 (MainThread): 12:16:00            if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
2023-04-20 12:16:00.412473 (MainThread): 12:16:00            if(w_action = 'ACTV', d.datevalue, null) dateplaced,
2023-04-20 12:16:00.412584 (MainThread): 12:16:00            wh.w_dts,
2023-04-20 12:16:00.412702 (MainThread): 12:16:00            batchid 
2023-04-20 12:16:00.412812 (MainThread): 12:16:00          FROM (
2023-04-20 12:16:00.412922 (MainThread): 12:16:00            SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
2023-04-20 12:16:00.413035 (MainThread): 12:16:00            UNION ALL
2023-04-20 12:16:00.413145 (MainThread): 12:16:00            SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
2023-04-20 12:16:00.413260 (MainThread): 12:16:00          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
2023-04-20 12:16:00.413371 (MainThread): 12:16:00            ON d.datevalue = date(wh.w_dts)))
2023-04-20 12:16:00.413482 (MainThread): 12:16:00      QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
2023-04-20 12:16:00.413602 (MainThread): 12:16:00    -- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
2023-04-20 12:16:00.413712 (MainThread): 12:16:00    ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
2023-04-20 12:16:00.413816 (MainThread): 12:16:00    ^^^
2023-04-20 12:16:00.413936 (MainThread): 12:16:00      ON 
2023-04-20 12:16:00.414040 (MainThread): 12:16:00        s.symbol = wh.symbol
2023-04-20 12:16:00.414151 (MainThread): 12:16:00        AND wh.dateplaced >= s.effectivedate 
2023-04-20 12:16:00.414253 (MainThread): 12:16:00        AND wh.dateplaced < s.enddate
2023-04-20 12:16:00.414352 (MainThread): 12:16:00    ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
2023-04-20 12:16:00.414463 (MainThread): 12:16:00      ON
2023-04-20 12:16:00.414567 (MainThread): 12:16:00        wh.customerid = c.customerid
2023-04-20 12:16:00.414667 (MainThread): 12:16:00        AND wh.dateplaced >= c.effectivedate 
2023-04-20 12:16:00.414764 (MainThread): 12:16:00        AND wh.dateplaced < c.enddate
2023-04-20 12:16:00.414861 (MainThread): 12:16:00    
2023-04-20 12:16:00.414972 (MainThread): 12:16:00  
2023-04-20 12:16:00.415099 (MainThread): 12:16:00  Runtime Error in model FactMarketHistory (models/incremental/FactMarketHistory.sql)
2023-04-20 12:16:00.415218 (MainThread): 12:16:00    
2023-04-20 12:16:00.415321 (MainThread): 12:16:00    [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)
2023-04-20 12:16:00.415421 (MainThread): 12:16:00    
2023-04-20 12:16:00.415540 (MainThread): 12:16:00    == SQL ==
2023-04-20 12:16:00.415651 (MainThread): 12:16:00    /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */
2023-04-20 12:16:00.415750 (MainThread): 12:16:00    
2023-04-20 12:16:00.415848 (MainThread): 12:16:00      
2023-04-20 12:16:00.415945 (MainThread): 12:16:00        
2023-04-20 12:16:00.416042 (MainThread): 12:16:00            create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
2023-04-20 12:16:00.416138 (MainThread): 12:16:00          
2023-04-20 12:16:00.416235 (MainThread): 12:16:00          
2023-04-20 12:16:00.416334 (MainThread): 12:16:00        using delta
2023-04-20 12:16:00.416432 (MainThread): 12:16:00          
2023-04-20 12:16:00.416651 (MainThread): 12:16:00          
2023-04-20 12:16:00.416766 (MainThread): 12:16:00          
2023-04-20 12:16:00.416868 (MainThread): 12:16:00          
2023-04-20 12:16:00.416968 (MainThread): 12:16:00          
2023-04-20 12:16:00.417150 (MainThread): 12:16:00          
2023-04-20 12:16:00.417258 (MainThread): 12:16:00          as
2023-04-20 12:16:00.417359 (MainThread): 12:16:00          
2023-04-20 12:16:00.417458 (MainThread): 12:16:00    SELECT 
2023-04-20 12:16:00.417558 (MainThread): 12:16:00      s.sk_securityid,
2023-04-20 12:16:00.417656 (MainThread): 12:16:00      s.sk_companyid,
2023-04-20 12:16:00.417754 (MainThread): 12:16:00      sk_dateid,
2023-04-20 12:16:00.417850 (MainThread): 12:16:00      fmh.dm_close / sum_fi_basic_eps AS peratio,
2023-04-20 12:16:00.417958 (MainThread): 12:16:00      (s.dividend / fmh.dm_close) / 100 yield,
2023-04-20 12:16:00.418053 (MainThread): 12:16:00      fiftytwoweekhigh,
2023-04-20 12:16:00.418148 (MainThread): 12:16:00      sk_fiftytwoweekhighdate,
2023-04-20 12:16:00.418245 (MainThread): 12:16:00      fiftytwoweeklow,
2023-04-20 12:16:00.418342 (MainThread): 12:16:00      sk_fiftytwoweeklowdate,
2023-04-20 12:16:00.418438 (MainThread): 12:16:00      dm_close closeprice,
2023-04-20 12:16:00.418536 (MainThread): 12:16:00      dm_high dayhigh,
2023-04-20 12:16:00.418633 (MainThread): 12:16:00      dm_low daylow,
2023-04-20 12:16:00.418728 (MainThread): 12:16:00      dm_vol volume,
2023-04-20 12:16:00.418823 (MainThread): 12:16:00      fmh.batchid
2023-04-20 12:16:00.418918 (MainThread): 12:16:00    FROM (
2023-04-20 12:16:00.419012 (MainThread): 12:16:00      SELECT * FROM (
2023-04-20 12:16:00.419108 (MainThread): 12:16:00        SELECT 
2023-04-20 12:16:00.419223 (MainThread): 12:16:00          a.*,
2023-04-20 12:16:00.419322 (MainThread): 12:16:00          b.sk_dateid AS sk_fiftytwoweeklowdate,
2023-04-20 12:16:00.419419 (MainThread): 12:16:00          c.sk_dateid AS sk_fiftytwoweekhighdate
2023-04-20 12:16:00.419539 (MainThread): 12:16:00        FROM
2023-04-20 12:16:00.419649 (MainThread): 12:16:00          `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
2023-04-20 12:16:00.419746 (MainThread): 12:16:00        JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
2023-04-20 12:16:00.419843 (MainThread): 12:16:00          ON
2023-04-20 12:16:00.419939 (MainThread): 12:16:00            a.dm_s_symb = b.dm_s_symb
2023-04-20 12:16:00.420036 (MainThread): 12:16:00            AND a.fiftytwoweeklow = b.dm_low
2023-04-20 12:16:00.420132 (MainThread): 12:16:00            AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
2023-04-20 12:16:00.420228 (MainThread): 12:16:00        JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
2023-04-20 12:16:00.420324 (MainThread): 12:16:00          ON 
2023-04-20 12:16:00.420420 (MainThread): 12:16:00            a.dm_s_symb = c.dm_s_symb
2023-04-20 12:16:00.420515 (MainThread): 12:16:00            AND a.fiftytwoweekhigh = c.dm_high
2023-04-20 12:16:00.420611 (MainThread): 12:16:00            AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
2023-04-20 12:16:00.420708 (MainThread): 12:16:00      QUALIFY ROW_NUMBER() OVER (
2023-04-20 12:16:00.420805 (MainThread): 12:16:00        PARTITION BY dm_s_symb, dm_date 
2023-04-20 12:16:00.420901 (MainThread): 12:16:00        ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
2023-04-20 12:16:00.420995 (MainThread): 12:16:00    -- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
2023-04-20 12:16:00.421089 (MainThread): 12:16:00    ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
2023-04-20 12:16:00.421183 (MainThread): 12:16:00    ^^^
2023-04-20 12:16:00.421277 (MainThread): 12:16:00      ON 
2023-04-20 12:16:00.421370 (MainThread): 12:16:00        s.symbol = fmh.dm_s_symb
2023-04-20 12:16:00.421464 (MainThread): 12:16:00        AND fmh.dm_date >= s.effectivedate 
2023-04-20 12:16:00.421557 (MainThread): 12:16:00        AND fmh.dm_date < s.enddate
2023-04-20 12:16:00.421652 (MainThread): 12:16:00    LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
2023-04-20 12:16:00.421875 (MainThread): 12:16:00      ON 
2023-04-20 12:16:00.421998 (MainThread): 12:16:00        f.sk_companyid = s.sk_companyid
2023-04-20 12:16:00.422103 (MainThread): 12:16:00        AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
2023-04-20 12:16:00.422206 (MainThread): 12:16:00        AND year(fmh.dm_date) = year(fi_qtr_start_date)
2023-04-20 12:16:00.422305 (MainThread): 12:16:00    
2023-04-20 12:16:00.422437 (MainThread): 12:16:00  
2023-04-20 12:16:00.422569 (MainThread): 12:16:00  Done. PASS=9 WARN=0 ERROR=3 SKIP=3 TOTAL=15
2023-04-20 12:16:01.251011 (Thread-933): handling poll request
2023-04-20 12:16:01.251548 (Thread-933): 12:16:01  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528252dc0>]}
2023-04-20 12:16:01.255086 (Thread-933): sending response (<Response 165739 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:02.784867 (Thread-934): handling ps request
2023-04-20 12:16:02.785366 (Thread-934): 12:16:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85d3a90>]}
2023-04-20 12:16:02.787090 (Thread-934): sending response (<Response 24512 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:03.376563 (Thread-935): handling status request
2023-04-20 12:16:03.377069 (Thread-935): 12:16:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85d3af0>]}
2023-04-20 12:16:03.377576 (Thread-935): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:07.192991 (Thread-936): handling status request
2023-04-20 12:16:07.193486 (Thread-936): 12:16:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85de340>]}
2023-04-20 12:16:07.193985 (Thread-936): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:07.344087 (Thread-937): handling ps request
2023-04-20 12:16:07.344608 (Thread-937): 12:16:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85de490>]}
2023-04-20 12:16:07.346284 (Thread-937): sending response (<Response 24512 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:07.357840 (Thread-938): handling ps request
2023-04-20 12:16:07.358223 (Thread-938): 12:16:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85de610>]}
2023-04-20 12:16:07.359784 (Thread-938): sending response (<Response 24512 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:08.607062 (Thread-939): handling status request
2023-04-20 12:16:08.607601 (Thread-939): 12:16:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85de7f0>]}
2023-04-20 12:16:08.608119 (Thread-939): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:09.764516 (Thread-940): handling status request
2023-04-20 12:16:09.765025 (Thread-940): 12:16:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85de3a0>]}
2023-04-20 12:16:09.765521 (Thread-940): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:09.925373 (Thread-941): handling ps request
2023-04-20 12:16:09.925925 (Thread-941): 12:16:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52825aa00>]}
2023-04-20 12:16:09.927550 (Thread-941): sending response (<Response 24512 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:10.712936 (Thread-942): handling status request
2023-04-20 12:16:10.713468 (Thread-942): 12:16:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528252850>]}
2023-04-20 12:16:10.713972 (Thread-942): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:26.658443 (Thread-943): handling status request
2023-04-20 12:16:26.658940 (Thread-943): 12:16:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85de8e0>]}
2023-04-20 12:16:26.659423 (Thread-943): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:26.779245 (Thread-944): handling ps request
2023-04-20 12:16:26.779721 (Thread-944): 12:16:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85de970>]}
2023-04-20 12:16:26.781393 (Thread-944): sending response (<Response 24512 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:32.848263 (Thread-945): handling status request
2023-04-20 12:16:32.848785 (Thread-945): 12:16:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85def40>]}
2023-04-20 12:16:32.849256 (Thread-945): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:33.012178 (Thread-946): handling ps request
2023-04-20 12:16:33.012662 (Thread-946): 12:16:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85deca0>]}
2023-04-20 12:16:33.042905 (Thread-946): sending response (<Response 24512 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:45.594937 (Thread-947): handling status request
2023-04-20 12:16:45.595465 (Thread-947): 12:16:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85defd0>]}
2023-04-20 12:16:45.595993 (Thread-947): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:45.727128 (Thread-948): handling ps request
2023-04-20 12:16:45.727655 (Thread-948): 12:16:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85def70>]}
2023-04-20 12:16:45.729309 (Thread-948): sending response (<Response 24512 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:45.969046 (Thread-949): handling ps request
2023-04-20 12:16:45.969557 (Thread-949): 12:16:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85de6a0>]}
2023-04-20 12:16:45.971212 (Thread-949): sending response (<Response 24512 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:46.951389 (Thread-950): handling status request
2023-04-20 12:16:46.951904 (Thread-950): 12:16:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85de2b0>]}
2023-04-20 12:16:46.952411 (Thread-950): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:48.077200 (Thread-951): handling status request
2023-04-20 12:16:48.077711 (Thread-951): 12:16:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85de580>]}
2023-04-20 12:16:48.078215 (Thread-951): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:48.174468 (Thread-952): handling ps request
2023-04-20 12:16:48.174953 (Thread-952): 12:16:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85e96a0>]}
2023-04-20 12:16:48.176614 (Thread-952): sending response (<Response 24512 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:16:48.844714 (Thread-953): handling status request
2023-04-20 12:16:48.845235 (Thread-953): 12:16:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85e9d30>]}
2023-04-20 12:16:48.845734 (Thread-953): sending response (<Response 1931 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:18.769491 (Thread-954): 12:17:18  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 12:17:18.790970 (Thread-954): 12:17:18  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 12:17:18.791343 (Thread-954): 12:17:18  previous checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, current checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886
2023-04-20 12:17:18.791562 (Thread-954): 12:17:18  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f872e880>]}
2023-04-20 12:17:19.248737 (Thread-955): handling status request
2023-04-20 12:17:19.259454 (Thread-955): 12:17:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5080d1af0>]}
2023-04-20 12:17:19.265064 (Thread-955): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:20.303440 (Thread-954): 12:17:20  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 12:17:20.307070 (Thread-954): 12:17:20  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 12:17:20.310434 (Thread-954): 12:17:20  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 12:17:20.313435 (Thread-954): 12:17:20  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 12:17:20.316459 (Thread-954): 12:17:20  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 12:17:20.319427 (Thread-954): 12:17:20  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 12:17:20.322740 (Thread-954): 12:17:20  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 12:17:20.326002 (Thread-954): 12:17:20  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 12:17:20.329040 (Thread-954): 12:17:20  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 12:17:20.331789 (Thread-954): 12:17:20  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 12:17:20.335017 (Thread-954): 12:17:20  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 12:17:20.337911 (Thread-954): 12:17:20  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 12:17:20.340587 (Thread-954): 12:17:20  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 12:17:20.343382 (Thread-954): 12:17:20  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 12:17:20.346322 (Thread-954): 12:17:20  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 12:17:20.500595 (Thread-954): 12:17:20  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f843cf10>]}
2023-04-20 12:17:20.747892 (Thread-956): handling status request
2023-04-20 12:17:20.748384 (Thread-956): 12:17:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f84423a0>]}
2023-04-20 12:17:20.748997 (Thread-956): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:44.616761 (Thread-957): handling status request
2023-04-20 12:17:44.617261 (Thread-957): 12:17:44  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f84426a0>]}
2023-04-20 12:17:44.617878 (Thread-957): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:45.171106 (Thread-958): handling cli_args request
2023-04-20 12:17:45.171631 (Thread-958): 12:17:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8442550>]}
2023-04-20 12:17:47.919885 (Thread-958): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:47.979303 (MainThread): 12:17:47  checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
2023-04-20 12:17:48.001736 (MainThread): 12:17:48  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 12:17:48.001945 (MainThread): 12:17:48  previous checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, current checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd
2023-04-20 12:17:48.002105 (MainThread): 12:17:48  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '55fa0d41-c972-441b-b239-baf0d4f35f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc6c7e6fa60>]}
2023-04-20 12:17:48.527951 (Thread-959): handling ps request
2023-04-20 12:17:48.528682 (Thread-959): 12:17:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f86dc610>]}
2023-04-20 12:17:48.531460 (Thread-959): sending response (<Response 24893 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:48.708024 (Thread-960): handling poll request
2023-04-20 12:17:48.708518 (Thread-960): 12:17:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85425e0>]}
2023-04-20 12:17:48.709118 (Thread-960): sending response (<Response 1863 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:48.754985 (Thread-961): handling status request
2023-04-20 12:17:48.755471 (Thread-961): 12:17:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8542c40>]}
2023-04-20 12:17:48.756177 (Thread-961): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:49.160399 (Thread-962): handling ps request
2023-04-20 12:17:49.160882 (Thread-962): 12:17:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8542e50>]}
2023-04-20 12:17:49.162574 (Thread-962): sending response (<Response 24894 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:49.601749 (MainThread): 12:17:49  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 12:17:49.614439 (MainThread): 12:17:49  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 12:17:49.617679 (MainThread): 12:17:49  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 12:17:49.620735 (MainThread): 12:17:49  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 12:17:49.623742 (MainThread): 12:17:49  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 12:17:49.626637 (MainThread): 12:17:49  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 12:17:49.629849 (MainThread): 12:17:49  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 12:17:49.633080 (MainThread): 12:17:49  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 12:17:49.636225 (MainThread): 12:17:49  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 12:17:49.639010 (MainThread): 12:17:49  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 12:17:49.642975 (MainThread): 12:17:49  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 12:17:49.645877 (MainThread): 12:17:49  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 12:17:49.648788 (MainThread): 12:17:49  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 12:17:49.651790 (MainThread): 12:17:49  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 12:17:49.654979 (MainThread): 12:17:49  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 12:17:49.820259 (MainThread): 12:17:49  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '55fa0d41-c972-441b-b239-baf0d4f35f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc6c409cd30>]}
2023-04-20 12:17:49.851983 (MainThread): 12:17:49  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '55fa0d41-c972-441b-b239-baf0d4f35f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc6c7e734c0>]}
2023-04-20 12:17:49.852361 (MainThread): 12:17:49  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:17:49.852518 (MainThread): 12:17:49  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '55fa0d41-c972-441b-b239-baf0d4f35f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc6cc034a90>]}
2023-04-20 12:17:49.854475 (MainThread): 12:17:49  
2023-04-20 12:17:49.855806 (MainThread): 12:17:49  Acquiring new databricks connection 'master'
2023-04-20 12:17:49.857485 (ThreadPoolExecutor-0_0): 12:17:49  Acquiring new databricks connection 'list_schemas'
2023-04-20 12:17:49.868811 (ThreadPoolExecutor-0_0): 12:17:49  Using databricks connection "list_schemas"
2023-04-20 12:17:49.869134 (ThreadPoolExecutor-0_0): 12:17:49  On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_schemas"} */

    show databases
  
2023-04-20 12:17:49.869300 (ThreadPoolExecutor-0_0): 12:17:49  Opening a new connection, currently in state init
2023-04-20 12:17:50.970763 (ThreadPoolExecutor-0_0): 12:17:50  SQL status: OK in 1.1 seconds
2023-04-20 12:17:50.974174 (Thread-963): handling poll request
2023-04-20 12:17:50.974641 (Thread-963): 12:17:50  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f841c550>]}
2023-04-20 12:17:50.975318 (Thread-963): sending response (<Response 8975 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:51.099619 (ThreadPoolExecutor-0_0): 12:17:51  On list_schemas: Close
2023-04-20 12:17:51.401816 (ThreadPoolExecutor-1_0): 12:17:51  Acquiring new databricks connection 'list_None_dbt_shabbirkdb'
2023-04-20 12:17:51.412331 (ThreadPoolExecutor-1_0): 12:17:51  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:17:51.412541 (ThreadPoolExecutor-1_0): 12:17:51  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 12:17:51.412701 (ThreadPoolExecutor-1_0): 12:17:51  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show tables in `dbt_shabbirkdb`
  
2023-04-20 12:17:51.412853 (ThreadPoolExecutor-1_0): 12:17:51  Opening a new connection, currently in state closed
2023-04-20 12:17:51.922697 (Thread-964): handling ps request
2023-04-20 12:17:51.923182 (Thread-964): 12:17:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f841c130>]}
2023-04-20 12:17:51.924918 (Thread-964): sending response (<Response 24894 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:52.148292 (ThreadPoolExecutor-1_0): 12:17:52  SQL status: OK in 0.74 seconds
2023-04-20 12:17:52.162994 (ThreadPoolExecutor-1_0): 12:17:52  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 12:17:52.163388 (ThreadPoolExecutor-1_0): 12:17:52  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show views in `dbt_shabbirkdb`
  
2023-04-20 12:17:52.637325 (ThreadPoolExecutor-1_0): 12:17:52  SQL status: OK in 0.47 seconds
2023-04-20 12:17:52.643463 (ThreadPoolExecutor-1_0): 12:17:52  On list_None_dbt_shabbirkdb: ROLLBACK
2023-04-20 12:17:52.643870 (ThreadPoolExecutor-1_0): 12:17:52  Databricks adapter: NotImplemented: rollback
2023-04-20 12:17:52.644145 (ThreadPoolExecutor-1_0): 12:17:52  On list_None_dbt_shabbirkdb: Close
2023-04-20 12:17:52.922195 (MainThread): 12:17:52  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '55fa0d41-c972-441b-b239-baf0d4f35f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc6c409af10>]}
2023-04-20 12:17:52.922637 (MainThread): 12:17:52  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:17:52.922799 (MainThread): 12:17:52  Spark adapter: NotImplemented: commit
2023-04-20 12:17:52.923333 (MainThread): 12:17:52  Concurrency: 4 threads (target='default')
2023-04-20 12:17:52.923486 (MainThread): 12:17:52  
2023-04-20 12:17:52.926830 (Thread-1): 12:17:52  Began running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:17:52.927238 (Thread-1): 12:17:52  1 of 15 START sql table model dbt_shabbirkdb.DimBroker ......................... [RUN]
2023-04-20 12:17:52.927772 (Thread-1): 12:17:52  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
2023-04-20 12:17:52.927967 (Thread-1): 12:17:52  Began compiling node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:17:52.931721 (Thread-2): 12:17:52  Began running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:17:52.932039 (Thread-2): 12:17:52  2 of 15 START sql table model dbt_shabbirkdb.DimCompany ........................ [RUN]
2023-04-20 12:17:52.932566 (Thread-2): 12:17:52  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCompany'
2023-04-20 12:17:52.932758 (Thread-2): 12:17:52  Began compiling node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:17:52.938033 (Thread-1): 12:17:52  Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:17:52.938414 (Thread-3): 12:17:52  Began running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:17:52.938725 (Thread-3): 12:17:52  3 of 15 START sql table model dbt_shabbirkdb.DimCustomerStg .................... [RUN]
2023-04-20 12:17:52.939238 (Thread-3): 12:17:52  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomerStg'
2023-04-20 12:17:52.939415 (Thread-3): 12:17:52  Began compiling node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:17:52.943611 (Thread-4): 12:17:52  Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:17:52.943920 (Thread-4): 12:17:52  4 of 15 START sql table model dbt_shabbirkdb.tempDailyMarketHistorical ......... [RUN]
2023-04-20 12:17:52.944429 (Thread-4): 12:17:52  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempDailyMarketHistorical'
2023-04-20 12:17:52.944629 (Thread-4): 12:17:52  Began compiling node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:17:52.949072 (Thread-3): 12:17:52  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:17:52.949546 (Thread-2): 12:17:52  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:17:52.949641 (Thread-4): 12:17:52  Writing injected SQL for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:17:52.958291 (Thread-1): 12:17:52  Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 2023-04-20 12:17:52.928015 => 2023-04-20 12:17:52.958121
2023-04-20 12:17:52.958524 (Thread-1): 12:17:52  Began executing node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:17:52.968943 (Thread-2): 12:17:52  Timing info for model.dbsql_dbt_tpch.DimCompany (compile): 2023-04-20 12:17:52.932803 => 2023-04-20 12:17:52.968774
2023-04-20 12:17:52.969159 (Thread-2): 12:17:52  Began executing node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:17:52.978528 (Thread-1): 12:17:52  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:17:52.978701 (Thread-1): 12:17:52  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:17:52.978984 (Thread-1): 12:17:52  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

      describe extended `dbt_shabbirkdb`.`dimbroker`
  
2023-04-20 12:17:52.979324 (Thread-1): 12:17:52  Opening a new connection, currently in state closed
2023-04-20 12:17:52.981245 (Thread-2): 12:17:52  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:17:52.981416 (Thread-2): 12:17:52  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:17:52.981564 (Thread-2): 12:17:52  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

      describe extended `dbt_shabbirkdb`.`dimcompany`
  
2023-04-20 12:17:52.981689 (Thread-2): 12:17:52  Opening a new connection, currently in state init
2023-04-20 12:17:52.988823 (Thread-4): 12:17:52  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (compile): 2023-04-20 12:17:52.944674 => 2023-04-20 12:17:52.988661
2023-04-20 12:17:52.989044 (Thread-4): 12:17:52  Began executing node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:17:52.993076 (Thread-4): 12:17:52  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:17:52.993246 (Thread-4): 12:17:52  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:17:52.993396 (Thread-4): 12:17:52  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

      describe extended `dbt_shabbirkdb`.`tempdailymarkethistorical`
  
2023-04-20 12:17:52.993518 (Thread-4): 12:17:52  Opening a new connection, currently in state init
2023-04-20 12:17:52.997717 (Thread-3): 12:17:52  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (compile): 2023-04-20 12:17:52.939459 => 2023-04-20 12:17:52.997564
2023-04-20 12:17:52.997948 (Thread-3): 12:17:52  Began executing node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:17:53.002723 (Thread-3): 12:17:53  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:17:53.002993 (Thread-3): 12:17:53  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:17:53.003154 (Thread-3): 12:17:53  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

      describe extended `dbt_shabbirkdb`.`dimcustomerstg`
  
2023-04-20 12:17:53.003278 (Thread-3): 12:17:53  Opening a new connection, currently in state init
2023-04-20 12:17:53.133688 (Thread-965): handling status request
2023-04-20 12:17:53.134168 (Thread-965): 12:17:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f83e6340>]}
2023-04-20 12:17:53.135017 (Thread-965): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:53.535248 (Thread-966): handling poll request
2023-04-20 12:17:53.535813 (Thread-966): 12:17:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f843e940>]}
2023-04-20 12:17:53.536850 (Thread-966): sending response (<Response 25558 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:53.757186 (Thread-1): 12:17:53  SQL status: OK in 0.78 seconds
2023-04-20 12:17:53.800637 (Thread-4): 12:17:53  SQL status: OK in 0.81 seconds
2023-04-20 12:17:53.805108 (Thread-1): 12:17:53  Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:17:53.808015 (Thread-2): 12:17:53  SQL status: OK in 0.83 seconds
2023-04-20 12:17:53.813181 (Thread-4): 12:17:53  Writing runtime sql for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:17:53.817182 (Thread-2): 12:17:53  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:17:53.818865 (Thread-3): 12:17:53  SQL status: OK in 0.82 seconds
2023-04-20 12:17:53.824127 (Thread-3): 12:17:53  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:17:53.824471 (Thread-1): 12:17:53  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:17:53.824678 (Thread-1): 12:17:53  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT
  md5(employeeid) as sk_brokerid,
  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate`) effectivedate,
  date('9999-12-31') enddate
FROM  `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`HR`
WHERE employeejobcode = 314
  
2023-04-20 12:17:53.831360 (Thread-4): 12:17:53  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:17:53.831597 (Thread-4): 12:17:53  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempDailyMarketHistorical`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  dmh.*,
  sk_dateid,
  min(dm_low) OVER (
    PARTITION BY dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweeklow,
  max(dm_high) OVER (
    PARTITION by dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweekhigh
FROM (
  SELECT * FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketHistorical`
  UNION ALL
  SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketIncremental`) dmh
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
  ON d.datevalue = dm_date;
  
2023-04-20 12:17:53.832649 (Thread-2): 12:17:53  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:17:53.832921 (Thread-2): 12:17:53  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCompany`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  * FROM (
  SELECT
    md5(cik) sk_companyid,
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
  
2023-04-20 12:17:53.849612 (Thread-3): 12:17:53  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:17:53.849960 (Thread-3): 12:17:53  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomerStg`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT * FROM (
  SELECT
    sk_customerid,
    customerid,
    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) taxid,
    status,
    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) lastname,
    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) firstname,
    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) middleinitial,
    coalesce(gender, last_value(gender) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) gender,
    coalesce(tier, last_value(tier) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) tier,
    coalesce(dob, last_value(dob) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) dob,
    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline1,
    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline2,
    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) postalcode,
    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) CITY,
    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) stateprov,
    coalesce(country, last_value(country) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) country,
    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone1,
    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone2,
    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone3,
    coalesce(email1, last_value(email1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email1,
    coalesce(email2, last_value(email2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email2,
    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) LCL_TX_ID,
    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) NAT_TX_ID,
    batchid,
    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,
    date(update_ts) effectivedate,
    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate
  FROM (
    SELECT
      md5(customerid::string) as sk_customerid,
      customerid,
      taxid,
      status,
      lastname,
      firstname,
      middleinitial,
      gender,
      tier,
      dob,
      addressline1,
      addressline2,
      postalcode,
      city,
      stateprov,
      country,
      phone1,
      phone2,
      phone3,
      email1,
      email2,
      lcl_tx_id,
      nat_tx_id,
      1 batchid,
      update_ts
    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')
    UNION ALL
    SELECT
      md5(c.customerid::string) as sk_customerid,
      c.customerid,
      nullif(c.taxid, '') taxid,
      nullif(s.st_name, '') as status,
      nullif(c.lastname, '') lastname,
      nullif(c.firstname, '') firstname,
      nullif(c.middleinitial, '') middleinitial,
      gender,
      c.tier,
      c.dob,
      nullif(c.addressline1, '') addressline1,
      nullif(c.addressline2, '') addressline2,
      nullif(c.postalcode, '') postalcode,
      nullif(c.city, '') city,
      nullif(c.stateprov, '') stateprov,
      nullif(c.country, '') country,
      CASE
        WHEN isnull(c_local_1) then c_local_1
        ELSE concat(
          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),
          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),
          c_local_1,
          nvl(c_ext_1, '')) END as phone1,
      CASE
        WHEN isnull(c_local_2) then c_local_2
        ELSE concat(
          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),
          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),
          c_local_2,
          nvl(c_ext_2, '')) END as phone2,
      CASE
        WHEN isnull(c_local_3) then c_local_3
        ELSE concat(
          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),
          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),
          c_local_3,
          nvl(c_ext_3, '')) END as phone3,
      nullif(c.email1, '') email1,
      nullif(c.email2, '') email2,
      c.LCL_TX_ID, 
      c.NAT_TX_ID,
      c.batchid,
      timestamp(bd.batchdate) update_ts
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CustomerIncremental` c
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
      ON c.batchid = bd.batchid
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` s 
      ON c.status = s.st_id
  ) c
  )
  
2023-04-20 12:17:55.110745 (Thread-967): handling poll request
2023-04-20 12:17:55.111249 (Thread-967): 12:17:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8532eb0>]}
2023-04-20 12:17:55.111967 (Thread-967): sending response (<Response 16499 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:55.294077 (Thread-968): handling ps request
2023-04-20 12:17:55.294534 (Thread-968): 12:17:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8425310>]}
2023-04-20 12:17:55.296202 (Thread-968): sending response (<Response 24895 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:55.725818 (Thread-969): handling status request
2023-04-20 12:17:55.726302 (Thread-969): 12:17:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8425670>]}
2023-04-20 12:17:55.726918 (Thread-969): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:56.709902 (Thread-970): handling poll request
2023-04-20 12:17:56.710393 (Thread-970): 12:17:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8425100>]}
2023-04-20 12:17:56.710876 (Thread-970): sending response (<Response 291 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:56.832239 (Thread-2): 12:17:56  SQL status: OK in 3.0 seconds
2023-04-20 12:17:56.858772 (Thread-2): 12:17:56  Timing info for model.dbsql_dbt_tpch.DimCompany (execute): 2023-04-20 12:17:52.969208 => 2023-04-20 12:17:56.858613
2023-04-20 12:17:56.859010 (Thread-2): 12:17:56  On model.dbsql_dbt_tpch.DimCompany: ROLLBACK
2023-04-20 12:17:56.859159 (Thread-2): 12:17:56  Databricks adapter: NotImplemented: rollback
2023-04-20 12:17:56.859283 (Thread-2): 12:17:56  On model.dbsql_dbt_tpch.DimCompany: Close
2023-04-20 12:17:56.917973 (Thread-1): 12:17:56  SQL status: OK in 3.09 seconds
2023-04-20 12:17:56.920138 (Thread-1): 12:17:56  Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 2023-04-20 12:17:52.958574 => 2023-04-20 12:17:56.920004
2023-04-20 12:17:56.920354 (Thread-1): 12:17:56  On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
2023-04-20 12:17:56.920498 (Thread-1): 12:17:56  Databricks adapter: NotImplemented: rollback
2023-04-20 12:17:56.920619 (Thread-1): 12:17:56  On model.dbsql_dbt_tpch.DimBroker: Close
2023-04-20 12:17:57.148759 (Thread-2): 12:17:57  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55fa0d41-c972-441b-b239-baf0d4f35f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc65e0c1a60>]}
2023-04-20 12:17:57.149274 (Thread-2): 12:17:57  2 of 15 OK created sql table model dbt_shabbirkdb.DimCompany ................... [OK in 4.22s]
2023-04-20 12:17:57.150830 (Thread-2): 12:17:57  Finished running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:17:57.151776 (Thread-2): 12:17:57  Began running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:17:57.152112 (Thread-2): 12:17:57  5 of 15 START sql table model dbt_shabbirkdb.DimSecurity ....................... [RUN]
2023-04-20 12:17:57.152590 (Thread-2): 12:17:57  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimSecurity'
2023-04-20 12:17:57.152772 (Thread-2): 12:17:57  Began compiling node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:17:57.157357 (Thread-2): 12:17:57  Writing injected SQL for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:17:57.172349 (Thread-2): 12:17:57  Timing info for model.dbsql_dbt_tpch.DimSecurity (compile): 2023-04-20 12:17:57.152818 => 2023-04-20 12:17:57.172193
2023-04-20 12:17:57.172571 (Thread-2): 12:17:57  Began executing node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:17:57.177781 (Thread-2): 12:17:57  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:17:57.177954 (Thread-2): 12:17:57  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:17:57.178112 (Thread-2): 12:17:57  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

      describe extended `dbt_shabbirkdb`.`dimsecurity`
  
2023-04-20 12:17:57.178242 (Thread-2): 12:17:57  Opening a new connection, currently in state closed
2023-04-20 12:17:57.193408 (Thread-1): 12:17:57  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55fa0d41-c972-441b-b239-baf0d4f35f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc65f1d3460>]}
2023-04-20 12:17:57.193806 (Thread-1): 12:17:57  1 of 15 OK created sql table model dbt_shabbirkdb.DimBroker .................... [OK in 4.27s]
2023-04-20 12:17:57.194024 (Thread-1): 12:17:57  Finished running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:17:57.194287 (Thread-1): 12:17:57  Began running node model.dbsql_dbt_tpch.Financial
2023-04-20 12:17:57.194546 (Thread-1): 12:17:57  6 of 15 START sql table model dbt_shabbirkdb.Financial ......................... [RUN]
2023-04-20 12:17:57.195184 (Thread-1): 12:17:57  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Financial'
2023-04-20 12:17:57.195365 (Thread-1): 12:17:57  Began compiling node model.dbsql_dbt_tpch.Financial
2023-04-20 12:17:57.199215 (Thread-1): 12:17:57  Writing injected SQL for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:17:57.214184 (Thread-1): 12:17:57  Timing info for model.dbsql_dbt_tpch.Financial (compile): 2023-04-20 12:17:57.195409 => 2023-04-20 12:17:57.214020
2023-04-20 12:17:57.214409 (Thread-1): 12:17:57  Began executing node model.dbsql_dbt_tpch.Financial
2023-04-20 12:17:57.218497 (Thread-1): 12:17:57  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:17:57.218669 (Thread-1): 12:17:57  Using databricks connection "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:17:57.218927 (Thread-1): 12:17:57  On model.dbsql_dbt_tpch.Financial: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

      describe extended `dbt_shabbirkdb`.`financial`
  
2023-04-20 12:17:57.219079 (Thread-1): 12:17:57  Opening a new connection, currently in state closed
2023-04-20 12:17:57.884428 (Thread-971): handling ps request
2023-04-20 12:17:57.884912 (Thread-971): 12:17:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f83e6eb0>]}
2023-04-20 12:17:57.886572 (Thread-971): sending response (<Response 24895 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:57.901708 (Thread-2): 12:17:57  SQL status: OK in 0.72 seconds
2023-04-20 12:17:57.907215 (Thread-2): 12:17:57  Writing runtime sql for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:17:57.922440 (Thread-2): 12:17:57  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:17:57.922739 (Thread-2): 12:17:57  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 12:17:57.950814 (Thread-1): 12:17:57  SQL status: OK in 0.73 seconds
2023-04-20 12:17:57.956221 (Thread-1): 12:17:57  Writing runtime sql for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:17:57.970068 (Thread-1): 12:17:57  Using databricks connection "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:17:57.970314 (Thread-1): 12:17:57  On model.dbsql_dbt_tpch.Financial: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 12:17:58.201161 (Thread-3): 12:17:58  SQL status: OK in 4.35 seconds
2023-04-20 12:17:58.203432 (Thread-3): 12:17:58  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (execute): 2023-04-20 12:17:52.997998 => 2023-04-20 12:17:58.203281
2023-04-20 12:17:58.203673 (Thread-3): 12:17:58  On model.dbsql_dbt_tpch.DimCustomerStg: ROLLBACK
2023-04-20 12:17:58.203824 (Thread-3): 12:17:58  Databricks adapter: NotImplemented: rollback
2023-04-20 12:17:58.203948 (Thread-3): 12:17:58  On model.dbsql_dbt_tpch.DimCustomerStg: Close
2023-04-20 12:17:58.278064 (Thread-972): handling poll request
2023-04-20 12:17:58.278539 (Thread-972): 12:17:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f83d14c0>]}
2023-04-20 12:17:58.279464 (Thread-972): sending response (<Response 25733 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:58.355562 (Thread-973): handling status request
2023-04-20 12:17:58.355954 (Thread-973): 12:17:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f83d15b0>]}
2023-04-20 12:17:58.356530 (Thread-973): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:17:58.482894 (Thread-3): 12:17:58  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55fa0d41-c972-441b-b239-baf0d4f35f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc6c4027e80>]}
2023-04-20 12:17:58.483414 (Thread-3): 12:17:58  3 of 15 OK created sql table model dbt_shabbirkdb.DimCustomerStg ............... [OK in 5.54s]
2023-04-20 12:17:58.483690 (Thread-3): 12:17:58  Finished running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:17:58.484531 (Thread-3): 12:17:58  Began running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:17:58.484847 (Thread-3): 12:17:58  7 of 15 START sql table model dbt_shabbirkdb.DimAccount ........................ [RUN]
2023-04-20 12:17:58.485343 (Thread-3): 12:17:58  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimAccount'
2023-04-20 12:17:58.485533 (Thread-3): 12:17:58  Began compiling node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:17:58.491048 (Thread-3): 12:17:58  Writing injected SQL for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 12:17:58.504794 (Thread-3): 12:17:58  Timing info for model.dbsql_dbt_tpch.DimAccount (compile): 2023-04-20 12:17:58.485584 => 2023-04-20 12:17:58.504628
2023-04-20 12:17:58.505097 (Thread-3): 12:17:58  Began executing node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:17:58.510613 (Thread-3): 12:17:58  Writing runtime sql for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 12:17:58.525348 (Thread-3): 12:17:58  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:17:58.525529 (Thread-3): 12:17:58  Using databricks connection "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 12:17:58.525757 (Thread-3): 12:17:58  On model.dbsql_dbt_tpch.DimAccount: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid,sk_customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 12:17:58.525902 (Thread-3): 12:17:58  Opening a new connection, currently in state closed
2023-04-20 12:17:59.332489 (Thread-3): 12:17:59  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid,sk_customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 12:17:59.332713 (Thread-3): 12:17:59  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_customerid` cannot be resolved. Did you mean one of the following? [`a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `a`.`batchid`, `a`.`accountdesc`].; line 29 pos 4
2023-04-20 12:17:59.332840 (Thread-3): 12:17:59  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_customerid` cannot be resolved. Did you mean one of the following? [`a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `a`.`batchid`, `a`.`accountdesc`].; line 29 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_customerid` cannot be resolved. Did you mean one of the following? [`a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `a`.`batchid`, `a`.`accountdesc`].; line 29 pos 4
	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedColumnError(QueryCompilationErrors.scala:239)
	at org.apache.spark.sql.catalyst.analysis.UnresolvedStarExcept.$anonfun$expand$11(unresolved.scala:591)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.catalyst.analysis.UnresolvedStarExcept.$anonfun$expand$9(unresolved.scala:578)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.catalyst.analysis.UnresolvedStarExcept.expand(unresolved.scala:577)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.$anonfun$expand$1(Analyzer.scala:2645)
	at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:110)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$expand(Analyzer.scala:2644)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.$anonfun$buildExpandedProjectList$1(Analyzer.scala:2671)
	at scala.collection.immutable.List.flatMap(List.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.buildExpandedProjectList(Analyzer.scala:2669)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$17.applyOrElse(Analyzer.scala:2308)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$17.applyOrElse(Analyzer.scala:2299)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$5(AnalysisHelper.scala:145)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:145)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1306)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1305)
	at org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1333)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1332)
	at org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:582)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1306)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1305)
	at org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1335)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1332)
	at org.apache.spark.sql.catalyst.plans.logical.ReplaceTableAsSelect.mapChildren(v2Commands.scala:426)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:2299)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:2274)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:365)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:358)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:265)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:358)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:154)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:338)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:337)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:153)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:372)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:809)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:369)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:147)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:147)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:137)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getRuntimeCategory$2(QueryRuntimePrediction.scala:293)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:292)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 12:17:59.332954 (Thread-3): 12:17:59  Databricks adapter: operation-id: b'\x01\xed\xdfuci\x16r\x86I\x03c\xc62q\xa0'
2023-04-20 12:17:59.333193 (Thread-3): 12:17:59  Timing info for model.dbsql_dbt_tpch.DimAccount (execute): 2023-04-20 12:17:58.505173 => 2023-04-20 12:17:59.333061
2023-04-20 12:17:59.333374 (Thread-3): 12:17:59  On model.dbsql_dbt_tpch.DimAccount: ROLLBACK
2023-04-20 12:17:59.333498 (Thread-3): 12:17:59  Databricks adapter: NotImplemented: rollback
2023-04-20 12:17:59.333619 (Thread-3): 12:17:59  On model.dbsql_dbt_tpch.DimAccount: Close
2023-04-20 12:17:59.611616 (Thread-3): 12:17:59  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_customerid` cannot be resolved. Did you mean one of the following? [`a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `a`.`batchid`, `a`.`accountdesc`].; line 29 pos 4
2023-04-20 12:17:59.612156 (Thread-3): 12:17:59  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55fa0d41-c972-441b-b239-baf0d4f35f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc65e070df0>]}
2023-04-20 12:17:59.612594 (Thread-3): 12:17:59  7 of 15 ERROR creating sql table model dbt_shabbirkdb.DimAccount ............... [ERROR in 1.13s]
2023-04-20 12:17:59.612810 (Thread-3): 12:17:59  Finished running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:17:59.613093 (Thread-3): 12:17:59  Began running node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:17:59.613365 (Thread-3): 12:17:59  8 of 15 START sql table model dbt_shabbirkdb.Prospect .......................... [RUN]
2023-04-20 12:17:59.613889 (Thread-3): 12:17:59  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Prospect'
2023-04-20 12:17:59.614077 (Thread-3): 12:17:59  Began compiling node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:17:59.620356 (Thread-3): 12:17:59  Writing injected SQL for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:17:59.633609 (Thread-3): 12:17:59  Timing info for model.dbsql_dbt_tpch.Prospect (compile): 2023-04-20 12:17:59.614123 => 2023-04-20 12:17:59.633449
2023-04-20 12:17:59.633843 (Thread-3): 12:17:59  Began executing node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:17:59.638216 (Thread-3): 12:17:59  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:17:59.638386 (Thread-3): 12:17:59  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:17:59.638548 (Thread-3): 12:17:59  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

      describe extended `dbt_shabbirkdb`.`prospect`
  
2023-04-20 12:17:59.638678 (Thread-3): 12:17:59  Opening a new connection, currently in state closed
2023-04-20 12:17:59.816746 (Thread-974): handling poll request
2023-04-20 12:17:59.817241 (Thread-974): 12:17:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8390520>]}
2023-04-20 12:17:59.818158 (Thread-974): sending response (<Response 37838 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:00.349237 (Thread-3): 12:18:00  SQL status: OK in 0.71 seconds
2023-04-20 12:18:00.355368 (Thread-3): 12:18:00  Writing runtime sql for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:18:00.368582 (Thread-3): 12:18:00  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:18:00.368910 (Thread-3): 12:18:00  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Prospect`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  agencyid,
  recdate.sk_dateid sk_recorddateid,
  origdate.sk_dateid sk_updatedateid,
  p.batchid,
  nvl2(c.customerid, True, False) iscustomer, 
  p.lastname,
  p.firstname,
  p.middleinitial,
  p.gender,
  p.addressline1,
  p.addressline2,
  p.postalcode,
  city,
  state,
  country,
  phone,
  income,
  numbercars,
  numberchildren,
  maritalstatus,
  age,
  creditrating,
  ownorrentflag,
  employer,
  numbercreditcards,
  networth,
  if(
    isnotnull(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+","")),
    left(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+",""),
      length(
        if(networth > 1000000 or income > 200000,"HighValue+","") || 
        if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
        if(age > 45, "Boomer+", "") ||
        if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
        if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
        if(age < 25 and networth > 1000000, "Inherited+",""))
      -1),
    NULL) marketingnameplate
FROM (
  SELECT 
    * FROM (
    SELECT
      agencyid,
      max(batchid) recordbatchid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth,
      min(batchid) batchid
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`ProspectRaw` p
    GROUP BY
      agencyid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth)
  QUALIFY ROW_NUMBER() OVER (PARTITION BY agencyid ORDER BY batchid DESC) = 1) p
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) recdate
  ON p.recordbatchid = recdate.batchid
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) origdate
  ON p.batchid = origdate.batchid
LEFT JOIN (
  SELECT 
    customerid,
    lastname,
    firstname,
    addressline1,
    addressline2,
    postalcode
  FROM `dbt_shabbirkdb`.`DimCustomerStg`
  WHERE iscurrent) c
  ON 
    upper(p.LastName) = upper(c.lastname)
    and upper(p.FirstName) = upper(c.firstname)
    and upper(p.AddressLine1) = upper(c.addressline1)
    and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
    and upper(p.PostalCode) = upper(c.postalcode)
  
2023-04-20 12:18:00.488926 (Thread-975): handling ps request
2023-04-20 12:18:00.489426 (Thread-975): 12:18:00  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8390760>]}
2023-04-20 12:18:00.515699 (Thread-975): sending response (<Response 24895 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:00.814409 (Thread-2): 12:18:00  SQL status: OK in 2.89 seconds
2023-04-20 12:18:00.817663 (Thread-2): 12:18:00  Timing info for model.dbsql_dbt_tpch.DimSecurity (execute): 2023-04-20 12:17:57.172621 => 2023-04-20 12:18:00.817497
2023-04-20 12:18:00.817895 (Thread-2): 12:18:00  On model.dbsql_dbt_tpch.DimSecurity: ROLLBACK
2023-04-20 12:18:00.818042 (Thread-2): 12:18:00  Databricks adapter: NotImplemented: rollback
2023-04-20 12:18:00.818166 (Thread-2): 12:18:00  On model.dbsql_dbt_tpch.DimSecurity: Close
2023-04-20 12:18:00.964850 (Thread-4): 12:18:00  SQL status: OK in 7.13 seconds
2023-04-20 12:18:00.973031 (Thread-976): handling status request
2023-04-20 12:18:00.973502 (Thread-976): 12:18:00  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f83907f0>]}
2023-04-20 12:18:00.974135 (Thread-976): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:01.095499 (Thread-2): 12:18:01  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55fa0d41-c972-441b-b239-baf0d4f35f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc65e0cac10>]}
2023-04-20 12:18:01.096063 (Thread-2): 12:18:01  5 of 15 OK created sql table model dbt_shabbirkdb.DimSecurity .................. [OK in 3.94s]
2023-04-20 12:18:01.096303 (Thread-2): 12:18:01  Finished running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:18:01.096595 (Thread-2): 12:18:01  Began running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 12:18:01.096824 (Thread-2): 12:18:01  9 of 15 SKIP relation dbt_shabbirkdb.FactCashBalances .......................... [SKIP]
2023-04-20 12:18:01.097006 (Thread-2): 12:18:01  Finished running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 12:18:01.097992 (Thread-2): 12:18:01  Began running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 12:18:01.098258 (Thread-2): 12:18:01  10 of 15 SKIP relation dbt_shabbirkdb.DimTrade ................................. [SKIP]
2023-04-20 12:18:01.098459 (Thread-2): 12:18:01  Finished running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 12:18:01.099062 (Thread-2): 12:18:01  Began running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 12:18:01.099296 (Thread-2): 12:18:01  11 of 15 SKIP relation dbt_shabbirkdb.FactHoldings ............................. [SKIP]
2023-04-20 12:18:01.099479 (Thread-2): 12:18:01  Finished running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 12:18:01.272426 (Thread-4): 12:18:01  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (execute): 2023-04-20 12:17:52.989095 => 2023-04-20 12:18:01.272246
2023-04-20 12:18:01.272707 (Thread-4): 12:18:01  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: ROLLBACK
2023-04-20 12:18:01.272860 (Thread-4): 12:18:01  Databricks adapter: NotImplemented: rollback
2023-04-20 12:18:01.273003 (Thread-4): 12:18:01  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: Close
2023-04-20 12:18:01.561054 (Thread-4): 12:18:01  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55fa0d41-c972-441b-b239-baf0d4f35f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc65e0d3670>]}
2023-04-20 12:18:01.561579 (Thread-4): 12:18:01  4 of 15 OK created sql table model dbt_shabbirkdb.tempDailyMarketHistorical .... [OK in 8.62s]
2023-04-20 12:18:01.561828 (Thread-4): 12:18:01  Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:18:01.832265 (Thread-977): handling poll request
2023-04-20 12:18:01.832808 (Thread-977): 12:18:01  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f85422b0>]}
2023-04-20 12:18:01.833674 (Thread-977): sending response (<Response 17359 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:03.217784 (Thread-978): handling ps request
2023-04-20 12:18:03.218366 (Thread-978): 12:18:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f83e6d30>]}
2023-04-20 12:18:03.220233 (Thread-978): sending response (<Response 24895 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:03.497607 (Thread-979): handling poll request
2023-04-20 12:18:03.498117 (Thread-979): 12:18:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8389160>]}
2023-04-20 12:18:03.498611 (Thread-979): sending response (<Response 291 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:03.743467 (Thread-980): handling status request
2023-04-20 12:18:03.743996 (Thread-980): 12:18:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f83892e0>]}
2023-04-20 12:18:03.744628 (Thread-980): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:04.365624 (Thread-3): 12:18:04  SQL status: OK in 4.0 seconds
2023-04-20 12:18:04.368030 (Thread-3): 12:18:04  Timing info for model.dbsql_dbt_tpch.Prospect (execute): 2023-04-20 12:17:59.633894 => 2023-04-20 12:18:04.367872
2023-04-20 12:18:04.368250 (Thread-3): 12:18:04  On model.dbsql_dbt_tpch.Prospect: ROLLBACK
2023-04-20 12:18:04.368394 (Thread-3): 12:18:04  Databricks adapter: NotImplemented: rollback
2023-04-20 12:18:04.368561 (Thread-3): 12:18:04  On model.dbsql_dbt_tpch.Prospect: Close
2023-04-20 12:18:04.395646 (Thread-1): 12:18:04  SQL status: OK in 6.43 seconds
2023-04-20 12:18:04.640327 (Thread-3): 12:18:04  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55fa0d41-c972-441b-b239-baf0d4f35f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc6c40ef2b0>]}
2023-04-20 12:18:04.640878 (Thread-3): 12:18:04  8 of 15 OK created sql table model dbt_shabbirkdb.Prospect ..................... [OK in 5.03s]
2023-04-20 12:18:04.641125 (Thread-3): 12:18:04  Finished running node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:18:04.642042 (Thread-2): 12:18:04  Began running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:18:04.642400 (Thread-2): 12:18:04  12 of 15 START sql table model dbt_shabbirkdb.DimCustomer ...................... [RUN]
2023-04-20 12:18:04.642915 (Thread-2): 12:18:04  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomer'
2023-04-20 12:18:04.643101 (Thread-2): 12:18:04  Began compiling node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:18:04.648372 (Thread-2): 12:18:04  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 12:18:04.661506 (Thread-2): 12:18:04  Timing info for model.dbsql_dbt_tpch.DimCustomer (compile): 2023-04-20 12:18:04.643149 => 2023-04-20 12:18:04.661332
2023-04-20 12:18:04.661741 (Thread-2): 12:18:04  Began executing node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:18:04.667678 (Thread-2): 12:18:04  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:18:04.667851 (Thread-2): 12:18:04  Using databricks connection "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 12:18:04.668010 (Thread-2): 12:18:04  On model.dbsql_dbt_tpch.DimCustomer: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

      describe extended `dbt_shabbirkdb`.`dimcustomer`
  
2023-04-20 12:18:04.668141 (Thread-2): 12:18:04  Opening a new connection, currently in state closed
2023-04-20 12:18:04.724463 (Thread-1): 12:18:04  Timing info for model.dbsql_dbt_tpch.Financial (execute): 2023-04-20 12:17:57.214458 => 2023-04-20 12:18:04.724319
2023-04-20 12:18:04.724695 (Thread-1): 12:18:04  On model.dbsql_dbt_tpch.Financial: ROLLBACK
2023-04-20 12:18:04.724844 (Thread-1): 12:18:04  Databricks adapter: NotImplemented: rollback
2023-04-20 12:18:04.724968 (Thread-1): 12:18:04  On model.dbsql_dbt_tpch.Financial: Close
2023-04-20 12:18:05.015004 (Thread-1): 12:18:05  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55fa0d41-c972-441b-b239-baf0d4f35f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc65ddde3a0>]}
2023-04-20 12:18:05.015607 (Thread-1): 12:18:05  6 of 15 OK created sql table model dbt_shabbirkdb.Financial .................... [OK in 7.82s]
2023-04-20 12:18:05.015866 (Thread-1): 12:18:05  Finished running node model.dbsql_dbt_tpch.Financial
2023-04-20 12:18:05.016767 (Thread-3): 12:18:05  Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:18:05.017091 (Thread-3): 12:18:05  13 of 15 START sql table model dbt_shabbirkdb.tempSumpFiBasicEps ............... [RUN]
2023-04-20 12:18:05.017604 (Thread-3): 12:18:05  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempSumpFiBasicEps'
2023-04-20 12:18:05.017815 (Thread-3): 12:18:05  Began compiling node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:18:05.022147 (Thread-3): 12:18:05  Writing injected SQL for node "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:18:05.035932 (Thread-3): 12:18:05  Timing info for model.dbsql_dbt_tpch.tempSumpFiBasicEps (compile): 2023-04-20 12:18:05.017863 => 2023-04-20 12:18:05.035754
2023-04-20 12:18:05.036171 (Thread-3): 12:18:05  Began executing node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:18:05.040673 (Thread-3): 12:18:05  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:18:05.040845 (Thread-3): 12:18:05  Using databricks connection "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:18:05.041001 (Thread-3): 12:18:05  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempSumpFiBasicEps"} */

      describe extended `dbt_shabbirkdb`.`tempsumpfibasiceps`
  
2023-04-20 12:18:05.041289 (Thread-3): 12:18:05  Opening a new connection, currently in state closed
2023-04-20 12:18:05.123817 (Thread-981): handling poll request
2023-04-20 12:18:05.124292 (Thread-981): 12:18:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8381100>]}
2023-04-20 12:18:05.125106 (Thread-981): sending response (<Response 16095 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:05.404695 (Thread-2): 12:18:05  SQL status: OK in 0.74 seconds
2023-04-20 12:18:05.410599 (Thread-2): 12:18:05  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 12:18:05.425080 (Thread-2): 12:18:05  Using databricks connection "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 12:18:05.425420 (Thread-2): 12:18:05  On model.dbsql_dbt_tpch.DimCustomer: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 12:18:05.778568 (Thread-3): 12:18:05  SQL status: OK in 0.74 seconds
2023-04-20 12:18:05.784282 (Thread-3): 12:18:05  Writing runtime sql for node "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:18:05.800486 (Thread-3): 12:18:05  Using databricks connection "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:18:05.800746 (Thread-3): 12:18:05  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempSumpFiBasicEps"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempSumpFiBasicEps`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  sk_companyid,
  fi_qtr_start_date,
  sum(fi_basic_eps) OVER (PARTITION BY companyid ORDER BY fi_qtr_start_date ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) - fi_basic_eps sum_fi_basic_eps
FROM `dbt_shabbirkdb`.`Financial`
JOIN `dbt_shabbirkdb`.`DimCompany`
  USING (sk_companyid);
  
2023-04-20 12:18:05.821785 (Thread-982): handling ps request
2023-04-20 12:18:05.822309 (Thread-982): 12:18:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f83814f0>]}
2023-04-20 12:18:05.824155 (Thread-982): sending response (<Response 24895 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:06.354850 (Thread-983): handling status request
2023-04-20 12:18:06.355359 (Thread-983): 12:18:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8381730>]}
2023-04-20 12:18:06.356024 (Thread-983): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:06.757371 (Thread-984): handling poll request
2023-04-20 12:18:06.757871 (Thread-984): 12:18:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8381130>]}
2023-04-20 12:18:06.758448 (Thread-984): sending response (<Response 5423 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:08.397407 (Thread-985): handling poll request
2023-04-20 12:18:08.397901 (Thread-985): 12:18:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8381520>]}
2023-04-20 12:18:08.398386 (Thread-985): sending response (<Response 291 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:08.430855 (Thread-986): handling ps request
2023-04-20 12:18:08.431313 (Thread-986): 12:18:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8381760>]}
2023-04-20 12:18:08.432999 (Thread-986): sending response (<Response 24895 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:08.764970 (Thread-2): 12:18:08  SQL status: OK in 3.34 seconds
2023-04-20 12:18:08.767307 (Thread-2): 12:18:08  Timing info for model.dbsql_dbt_tpch.DimCustomer (execute): 2023-04-20 12:18:04.661802 => 2023-04-20 12:18:08.767149
2023-04-20 12:18:08.767540 (Thread-2): 12:18:08  On model.dbsql_dbt_tpch.DimCustomer: ROLLBACK
2023-04-20 12:18:08.767698 (Thread-2): 12:18:08  Databricks adapter: NotImplemented: rollback
2023-04-20 12:18:08.767823 (Thread-2): 12:18:08  On model.dbsql_dbt_tpch.DimCustomer: Close
2023-04-20 12:18:08.922108 (Thread-987): handling status request
2023-04-20 12:18:08.922589 (Thread-987): 12:18:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8381d60>]}
2023-04-20 12:18:08.923184 (Thread-987): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:09.024991 (Thread-3): 12:18:09  SQL status: OK in 3.22 seconds
2023-04-20 12:18:09.027434 (Thread-3): 12:18:09  Timing info for model.dbsql_dbt_tpch.tempSumpFiBasicEps (execute): 2023-04-20 12:18:05.036221 => 2023-04-20 12:18:09.027285
2023-04-20 12:18:09.027671 (Thread-3): 12:18:09  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: ROLLBACK
2023-04-20 12:18:09.027817 (Thread-3): 12:18:09  Databricks adapter: NotImplemented: rollback
2023-04-20 12:18:09.027938 (Thread-3): 12:18:09  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: Close
2023-04-20 12:18:09.052324 (Thread-2): 12:18:09  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55fa0d41-c972-441b-b239-baf0d4f35f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc65df71df0>]}
2023-04-20 12:18:09.052836 (Thread-2): 12:18:09  12 of 15 OK created sql table model dbt_shabbirkdb.DimCustomer ................. [OK in 4.41s]
2023-04-20 12:18:09.053072 (Thread-2): 12:18:09  Finished running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:18:09.053939 (Thread-1): 12:18:09  Began running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 12:18:09.054266 (Thread-1): 12:18:09  14 of 15 START sql table model dbt_shabbirkdb.FactWatches ...................... [RUN]
2023-04-20 12:18:09.054774 (Thread-1): 12:18:09  Acquiring new databricks connection 'model.dbsql_dbt_tpch.FactWatches'
2023-04-20 12:18:09.054957 (Thread-1): 12:18:09  Began compiling node model.dbsql_dbt_tpch.FactWatches
2023-04-20 12:18:09.061455 (Thread-1): 12:18:09  Writing injected SQL for node "model.dbsql_dbt_tpch.FactWatches"
2023-04-20 12:18:09.075390 (Thread-1): 12:18:09  Timing info for model.dbsql_dbt_tpch.FactWatches (compile): 2023-04-20 12:18:09.055004 => 2023-04-20 12:18:09.075227
2023-04-20 12:18:09.075646 (Thread-1): 12:18:09  Began executing node model.dbsql_dbt_tpch.FactWatches
2023-04-20 12:18:09.079883 (Thread-1): 12:18:09  Writing runtime sql for node "model.dbsql_dbt_tpch.FactWatches"
2023-04-20 12:18:09.093179 (Thread-1): 12:18:09  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:18:09.093373 (Thread-1): 12:18:09  Using databricks connection "model.dbsql_dbt_tpch.FactWatches"
2023-04-20 12:18:09.093600 (Thread-1): 12:18:09  On model.dbsql_dbt_tpch.FactWatches: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate;
  
2023-04-20 12:18:09.093736 (Thread-1): 12:18:09  Opening a new connection, currently in state closed
2023-04-20 12:18:09.308112 (Thread-3): 12:18:09  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55fa0d41-c972-441b-b239-baf0d4f35f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc65dfc9fd0>]}
2023-04-20 12:18:09.308641 (Thread-3): 12:18:09  13 of 15 OK created sql table model dbt_shabbirkdb.tempSumpFiBasicEps .......... [OK in 4.29s]
2023-04-20 12:18:09.308876 (Thread-3): 12:18:09  Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:18:09.309777 (Thread-2): 12:18:09  Began running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:18:09.310130 (Thread-2): 12:18:09  15 of 15 START sql table model dbt_shabbirkdb.FactMarketHistory ................ [RUN]
2023-04-20 12:18:09.310688 (Thread-2): 12:18:09  Acquiring new databricks connection 'model.dbsql_dbt_tpch.FactMarketHistory'
2023-04-20 12:18:09.310877 (Thread-2): 12:18:09  Began compiling node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:18:09.315490 (Thread-2): 12:18:09  Writing injected SQL for node "model.dbsql_dbt_tpch.FactMarketHistory"
2023-04-20 12:18:09.330688 (Thread-2): 12:18:09  Timing info for model.dbsql_dbt_tpch.FactMarketHistory (compile): 2023-04-20 12:18:09.310924 => 2023-04-20 12:18:09.330525
2023-04-20 12:18:09.330913 (Thread-2): 12:18:09  Began executing node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:18:09.334926 (Thread-2): 12:18:09  Writing runtime sql for node "model.dbsql_dbt_tpch.FactMarketHistory"
2023-04-20 12:18:09.350903 (Thread-2): 12:18:09  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:18:09.351086 (Thread-2): 12:18:09  Using databricks connection "model.dbsql_dbt_tpch.FactMarketHistory"
2023-04-20 12:18:09.351292 (Thread-2): 12:18:09  On model.dbsql_dbt_tpch.FactMarketHistory: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date);
  
2023-04-20 12:18:09.351423 (Thread-2): 12:18:09  Opening a new connection, currently in state closed
2023-04-20 12:18:09.763340 (Thread-1): 12:18:09  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate;
  
2023-04-20 12:18:09.763611 (Thread-1): 12:18:09  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate

2023-04-20 12:18:09.763750 (Thread-1): 12:18:09  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 12:18:09.763866 (Thread-1): 12:18:09  Databricks adapter: operation-id: b'\x01\xed\xdfui\xb6\x17\x83\x9c\xb8@TO\x10j\xa6'
2023-04-20 12:18:09.764102 (Thread-1): 12:18:09  Timing info for model.dbsql_dbt_tpch.FactWatches (execute): 2023-04-20 12:18:09.075698 => 2023-04-20 12:18:09.763975
2023-04-20 12:18:09.764284 (Thread-1): 12:18:09  On model.dbsql_dbt_tpch.FactWatches: ROLLBACK
2023-04-20 12:18:09.764408 (Thread-1): 12:18:09  Databricks adapter: NotImplemented: rollback
2023-04-20 12:18:09.764539 (Thread-1): 12:18:09  On model.dbsql_dbt_tpch.FactWatches: Close
2023-04-20 12:18:10.028774 (Thread-2): 12:18:10  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date);
  
2023-04-20 12:18:10.029005 (Thread-2): 12:18:10  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date)

2023-04-20 12:18:10.029134 (Thread-2): 12:18:10  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date)

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date)

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 12:18:10.029265 (Thread-2): 12:18:10  Databricks adapter: operation-id: b'\x01\xed\xdfui\xdd\x1c\x0f\xb3\xfc\xe2\xd2\x93\xc5\xb2\xa7'
2023-04-20 12:18:10.029510 (Thread-2): 12:18:10  Timing info for model.dbsql_dbt_tpch.FactMarketHistory (execute): 2023-04-20 12:18:09.330964 => 2023-04-20 12:18:10.029382
2023-04-20 12:18:10.029692 (Thread-2): 12:18:10  On model.dbsql_dbt_tpch.FactMarketHistory: ROLLBACK
2023-04-20 12:18:10.029816 (Thread-2): 12:18:10  Databricks adapter: NotImplemented: rollback
2023-04-20 12:18:10.029949 (Thread-2): 12:18:10  On model.dbsql_dbt_tpch.FactMarketHistory: Close
2023-04-20 12:18:10.051080 (Thread-1): 12:18:10  Runtime Error in model FactWatches (models/incremental/FactWatches.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */
  
    
      
          create or replace table `dbt_shabbirkdb`.`FactWatches`
        
        
      using delta
        
        
        
        
        
        
        as
        
  SELECT
    c.sk_customerid sk_customerid,
    s.sk_securityid sk_securityid,
    sk_dateid_dateplaced,
    sk_dateid_dateremoved,
    wh.batchid
  FROM (
    SELECT * EXCEPT(w_dts)
    FROM (
      SELECT
        customerid,
        symbol,
        coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
          PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
        coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
          PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
        coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
          PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
        w_dts,
        coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
          PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
      FROM ( 
        SELECT 
          wh.w_c_id customerid,
          wh.w_s_symb symbol,
          if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
          if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
          if(w_action = 'ACTV', d.datevalue, null) dateplaced,
          wh.w_dts,
          batchid 
        FROM (
          SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
          UNION ALL
          SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
          ON d.datevalue = date(wh.w_dts)))
    QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
  -- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
  ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ^^^
    ON 
      s.symbol = wh.symbol
      AND wh.dateplaced >= s.effectivedate 
      AND wh.dateplaced < s.enddate
  ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
    ON
      wh.customerid = c.customerid
      AND wh.dateplaced >= c.effectivedate 
      AND wh.dateplaced < c.enddate
  
2023-04-20 12:18:10.051796 (Thread-1): 12:18:10  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55fa0d41-c972-441b-b239-baf0d4f35f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc65dfafb50>]}
2023-04-20 12:18:10.052429 (Thread-1): 12:18:10  14 of 15 ERROR creating sql table model dbt_shabbirkdb.FactWatches ............. [ERROR in 1.00s]
2023-04-20 12:18:10.052781 (Thread-1): 12:18:10  Finished running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 12:18:10.323104 (Thread-2): 12:18:10  Runtime Error in model FactMarketHistory (models/incremental/FactMarketHistory.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */
  
    
      
          create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
        
        
      using delta
        
        
        
        
        
        
        as
        
  SELECT 
    s.sk_securityid,
    s.sk_companyid,
    sk_dateid,
    fmh.dm_close / sum_fi_basic_eps AS peratio,
    (s.dividend / fmh.dm_close) / 100 yield,
    fiftytwoweekhigh,
    sk_fiftytwoweekhighdate,
    fiftytwoweeklow,
    sk_fiftytwoweeklowdate,
    dm_close closeprice,
    dm_high dayhigh,
    dm_low daylow,
    dm_vol volume,
    fmh.batchid
  FROM (
    SELECT * FROM (
      SELECT 
        a.*,
        b.sk_dateid AS sk_fiftytwoweeklowdate,
        c.sk_dateid AS sk_fiftytwoweekhighdate
      FROM
        `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
      JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
        ON
          a.dm_s_symb = b.dm_s_symb
          AND a.fiftytwoweeklow = b.dm_low
          AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
      JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
        ON 
          a.dm_s_symb = c.dm_s_symb
          AND a.fiftytwoweekhigh = c.dm_high
          AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
    QUALIFY ROW_NUMBER() OVER (
      PARTITION BY dm_s_symb, dm_date 
      ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
  -- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
  ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ^^^
    ON 
      s.symbol = fmh.dm_s_symb
      AND fmh.dm_date >= s.effectivedate 
      AND fmh.dm_date < s.enddate
  LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
    ON 
      f.sk_companyid = s.sk_companyid
      AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
      AND year(fmh.dm_date) = year(fi_qtr_start_date)
  
2023-04-20 12:18:10.323568 (Thread-2): 12:18:10  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55fa0d41-c972-441b-b239-baf0d4f35f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc65e0676a0>]}
2023-04-20 12:18:10.324020 (Thread-2): 12:18:10  15 of 15 ERROR creating sql table model dbt_shabbirkdb.FactMarketHistory ....... [ERROR in 1.01s]
2023-04-20 12:18:10.324231 (Thread-2): 12:18:10  Finished running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:18:10.326356 (MainThread): 12:18:10  Acquiring new databricks connection 'master'
2023-04-20 12:18:10.326592 (MainThread): 12:18:10  On master: ROLLBACK
2023-04-20 12:18:10.326769 (MainThread): 12:18:10  Opening a new connection, currently in state init
2023-04-20 12:18:10.613595 (MainThread): 12:18:10  Databricks adapter: NotImplemented: rollback
2023-04-20 12:18:10.614004 (MainThread): 12:18:10  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:18:10.614205 (MainThread): 12:18:10  Spark adapter: NotImplemented: commit
2023-04-20 12:18:10.614376 (MainThread): 12:18:10  On master: ROLLBACK
2023-04-20 12:18:10.614513 (MainThread): 12:18:10  Databricks adapter: NotImplemented: rollback
2023-04-20 12:18:10.614651 (MainThread): 12:18:10  On master: Close
2023-04-20 12:18:10.697093 (Thread-988): handling poll request
2023-04-20 12:18:10.697576 (Thread-988): 12:18:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f83a1670>]}
2023-04-20 12:18:10.700066 (Thread-988): sending response (<Response 69310 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:10.895744 (MainThread): 12:18:10  Connection 'master' was properly closed.
2023-04-20 12:18:10.895943 (MainThread): 12:18:10  Connection 'model.dbsql_dbt_tpch.FactWatches' was properly closed.
2023-04-20 12:18:10.896053 (MainThread): 12:18:10  Connection 'model.dbsql_dbt_tpch.FactMarketHistory' was properly closed.
2023-04-20 12:18:10.896158 (MainThread): 12:18:10  Connection 'model.dbsql_dbt_tpch.tempSumpFiBasicEps' was properly closed.
2023-04-20 12:18:10.896259 (MainThread): 12:18:10  Connection 'model.dbsql_dbt_tpch.tempDailyMarketHistorical' was properly closed.
2023-04-20 12:18:10.896564 (MainThread): 12:18:10  
2023-04-20 12:18:10.896713 (MainThread): 12:18:10  Finished running 15 table models in 0 hours 0 minutes and 21.04 seconds (21.04s).
2023-04-20 12:18:10.985278 (MainThread): 12:18:10  
2023-04-20 12:18:10.985589 (MainThread): 12:18:10  Completed with 3 errors and 0 warnings:
2023-04-20 12:18:10.985746 (MainThread): 12:18:10  
2023-04-20 12:18:10.985923 (MainThread): 12:18:10  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
2023-04-20 12:18:10.986069 (MainThread): 12:18:10    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_customerid` cannot be resolved. Did you mean one of the following? [`a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `a`.`batchid`, `a`.`accountdesc`].; line 29 pos 4
2023-04-20 12:18:10.986189 (MainThread): 12:18:10  
2023-04-20 12:18:10.986322 (MainThread): 12:18:10  Runtime Error in model FactWatches (models/incremental/FactWatches.sql)
2023-04-20 12:18:10.986437 (MainThread): 12:18:10    
2023-04-20 12:18:10.986544 (MainThread): 12:18:10    [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)
2023-04-20 12:18:10.986648 (MainThread): 12:18:10    
2023-04-20 12:18:10.986751 (MainThread): 12:18:10    == SQL ==
2023-04-20 12:18:10.986853 (MainThread): 12:18:10    /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */
2023-04-20 12:18:10.986953 (MainThread): 12:18:10    
2023-04-20 12:18:10.987053 (MainThread): 12:18:10      
2023-04-20 12:18:10.987153 (MainThread): 12:18:10        
2023-04-20 12:18:10.987255 (MainThread): 12:18:10            create or replace table `dbt_shabbirkdb`.`FactWatches`
2023-04-20 12:18:10.987354 (MainThread): 12:18:10          
2023-04-20 12:18:10.987455 (MainThread): 12:18:10          
2023-04-20 12:18:10.987589 (MainThread): 12:18:10        using delta
2023-04-20 12:18:10.987700 (MainThread): 12:18:10          
2023-04-20 12:18:10.987802 (MainThread): 12:18:10          
2023-04-20 12:18:10.987903 (MainThread): 12:18:10          
2023-04-20 12:18:10.988002 (MainThread): 12:18:10          
2023-04-20 12:18:10.988101 (MainThread): 12:18:10          
2023-04-20 12:18:10.988199 (MainThread): 12:18:10          
2023-04-20 12:18:10.988297 (MainThread): 12:18:10          as
2023-04-20 12:18:10.988394 (MainThread): 12:18:10          
2023-04-20 12:18:10.988492 (MainThread): 12:18:10    SELECT
2023-04-20 12:18:10.988590 (MainThread): 12:18:10      c.sk_customerid sk_customerid,
2023-04-20 12:18:10.988687 (MainThread): 12:18:10      s.sk_securityid sk_securityid,
2023-04-20 12:18:10.988785 (MainThread): 12:18:10      sk_dateid_dateplaced,
2023-04-20 12:18:10.988883 (MainThread): 12:18:10      sk_dateid_dateremoved,
2023-04-20 12:18:10.988981 (MainThread): 12:18:10      wh.batchid
2023-04-20 12:18:10.989081 (MainThread): 12:18:10    FROM (
2023-04-20 12:18:10.989179 (MainThread): 12:18:10      SELECT * EXCEPT(w_dts)
2023-04-20 12:18:10.989277 (MainThread): 12:18:10      FROM (
2023-04-20 12:18:10.989373 (MainThread): 12:18:10        SELECT
2023-04-20 12:18:10.989469 (MainThread): 12:18:10          customerid,
2023-04-20 12:18:10.989566 (MainThread): 12:18:10          symbol,
2023-04-20 12:18:10.989661 (MainThread): 12:18:10          coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
2023-04-20 12:18:10.989757 (MainThread): 12:18:10            PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
2023-04-20 12:18:10.989853 (MainThread): 12:18:10          coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
2023-04-20 12:18:10.989962 (MainThread): 12:18:10            PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
2023-04-20 12:18:10.990058 (MainThread): 12:18:10          coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
2023-04-20 12:18:10.990155 (MainThread): 12:18:10            PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
2023-04-20 12:18:10.990253 (MainThread): 12:18:10          w_dts,
2023-04-20 12:18:10.990351 (MainThread): 12:18:10          coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
2023-04-20 12:18:10.990449 (MainThread): 12:18:10            PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
2023-04-20 12:18:10.990591 (MainThread): 12:18:10        FROM ( 
2023-04-20 12:18:10.990837 (MainThread): 12:18:10          SELECT 
2023-04-20 12:18:10.990944 (MainThread): 12:18:10            wh.w_c_id customerid,
2023-04-20 12:18:10.991046 (MainThread): 12:18:10            wh.w_s_symb symbol,
2023-04-20 12:18:10.991147 (MainThread): 12:18:10            if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
2023-04-20 12:18:10.991246 (MainThread): 12:18:10            if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
2023-04-20 12:18:10.991344 (MainThread): 12:18:10            if(w_action = 'ACTV', d.datevalue, null) dateplaced,
2023-04-20 12:18:10.991443 (MainThread): 12:18:10            wh.w_dts,
2023-04-20 12:18:10.991562 (MainThread): 12:18:10            batchid 
2023-04-20 12:18:10.991665 (MainThread): 12:18:10          FROM (
2023-04-20 12:18:10.991764 (MainThread): 12:18:10            SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
2023-04-20 12:18:10.991865 (MainThread): 12:18:10            UNION ALL
2023-04-20 12:18:10.991965 (MainThread): 12:18:10            SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
2023-04-20 12:18:10.992074 (MainThread): 12:18:10          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
2023-04-20 12:18:10.992181 (MainThread): 12:18:10            ON d.datevalue = date(wh.w_dts)))
2023-04-20 12:18:10.992279 (MainThread): 12:18:10      QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
2023-04-20 12:18:10.992378 (MainThread): 12:18:10    -- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
2023-04-20 12:18:10.992476 (MainThread): 12:18:10    ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
2023-04-20 12:18:10.992574 (MainThread): 12:18:10    ^^^
2023-04-20 12:18:10.992671 (MainThread): 12:18:10      ON 
2023-04-20 12:18:10.992771 (MainThread): 12:18:10        s.symbol = wh.symbol
2023-04-20 12:18:10.992869 (MainThread): 12:18:10        AND wh.dateplaced >= s.effectivedate 
2023-04-20 12:18:10.992967 (MainThread): 12:18:10        AND wh.dateplaced < s.enddate
2023-04-20 12:18:10.993065 (MainThread): 12:18:10    ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
2023-04-20 12:18:10.993163 (MainThread): 12:18:10      ON
2023-04-20 12:18:10.993261 (MainThread): 12:18:10        wh.customerid = c.customerid
2023-04-20 12:18:10.993359 (MainThread): 12:18:10        AND wh.dateplaced >= c.effectivedate 
2023-04-20 12:18:10.993455 (MainThread): 12:18:10        AND wh.dateplaced < c.enddate
2023-04-20 12:18:10.993551 (MainThread): 12:18:10    
2023-04-20 12:18:10.993662 (MainThread): 12:18:10  
2023-04-20 12:18:10.993791 (MainThread): 12:18:10  Runtime Error in model FactMarketHistory (models/incremental/FactMarketHistory.sql)
2023-04-20 12:18:10.993910 (MainThread): 12:18:10    
2023-04-20 12:18:10.994012 (MainThread): 12:18:10    [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)
2023-04-20 12:18:10.994112 (MainThread): 12:18:10    
2023-04-20 12:18:10.994211 (MainThread): 12:18:10    == SQL ==
2023-04-20 12:18:10.994308 (MainThread): 12:18:10    /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */
2023-04-20 12:18:10.994407 (MainThread): 12:18:10    
2023-04-20 12:18:10.994506 (MainThread): 12:18:10      
2023-04-20 12:18:10.994607 (MainThread): 12:18:10        
2023-04-20 12:18:10.994705 (MainThread): 12:18:10            create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
2023-04-20 12:18:10.994805 (MainThread): 12:18:10          
2023-04-20 12:18:10.994902 (MainThread): 12:18:10          
2023-04-20 12:18:10.994999 (MainThread): 12:18:10        using delta
2023-04-20 12:18:10.995095 (MainThread): 12:18:10          
2023-04-20 12:18:10.995192 (MainThread): 12:18:10          
2023-04-20 12:18:10.995287 (MainThread): 12:18:10          
2023-04-20 12:18:10.995383 (MainThread): 12:18:10          
2023-04-20 12:18:10.995587 (MainThread): 12:18:10          
2023-04-20 12:18:10.995698 (MainThread): 12:18:10          
2023-04-20 12:18:10.995798 (MainThread): 12:18:10          as
2023-04-20 12:18:10.996031 (MainThread): 12:18:10          
2023-04-20 12:18:10.996139 (MainThread): 12:18:10    SELECT 
2023-04-20 12:18:10.996239 (MainThread): 12:18:10      s.sk_securityid,
2023-04-20 12:18:10.996342 (MainThread): 12:18:10      s.sk_companyid,
2023-04-20 12:18:10.996441 (MainThread): 12:18:10      sk_dateid,
2023-04-20 12:18:10.996538 (MainThread): 12:18:10      fmh.dm_close / sum_fi_basic_eps AS peratio,
2023-04-20 12:18:10.996634 (MainThread): 12:18:10      (s.dividend / fmh.dm_close) / 100 yield,
2023-04-20 12:18:10.996730 (MainThread): 12:18:10      fiftytwoweekhigh,
2023-04-20 12:18:10.996826 (MainThread): 12:18:10      sk_fiftytwoweekhighdate,
2023-04-20 12:18:10.996922 (MainThread): 12:18:10      fiftytwoweeklow,
2023-04-20 12:18:10.997018 (MainThread): 12:18:10      sk_fiftytwoweeklowdate,
2023-04-20 12:18:10.997133 (MainThread): 12:18:10      dm_close closeprice,
2023-04-20 12:18:10.997244 (MainThread): 12:18:10      dm_high dayhigh,
2023-04-20 12:18:10.997348 (MainThread): 12:18:10      dm_low daylow,
2023-04-20 12:18:10.997450 (MainThread): 12:18:10      dm_vol volume,
2023-04-20 12:18:10.997549 (MainThread): 12:18:10      fmh.batchid
2023-04-20 12:18:10.997649 (MainThread): 12:18:10    FROM (
2023-04-20 12:18:10.997747 (MainThread): 12:18:10      SELECT * FROM (
2023-04-20 12:18:10.997844 (MainThread): 12:18:10        SELECT 
2023-04-20 12:18:10.997953 (MainThread): 12:18:10          a.*,
2023-04-20 12:18:10.998050 (MainThread): 12:18:10          b.sk_dateid AS sk_fiftytwoweeklowdate,
2023-04-20 12:18:10.998147 (MainThread): 12:18:10          c.sk_dateid AS sk_fiftytwoweekhighdate
2023-04-20 12:18:10.998242 (MainThread): 12:18:10        FROM
2023-04-20 12:18:10.998338 (MainThread): 12:18:10          `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
2023-04-20 12:18:10.998432 (MainThread): 12:18:10        JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
2023-04-20 12:18:10.998524 (MainThread): 12:18:10          ON
2023-04-20 12:18:10.998619 (MainThread): 12:18:10            a.dm_s_symb = b.dm_s_symb
2023-04-20 12:18:10.998713 (MainThread): 12:18:10            AND a.fiftytwoweeklow = b.dm_low
2023-04-20 12:18:10.998809 (MainThread): 12:18:10            AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
2023-04-20 12:18:10.998907 (MainThread): 12:18:10        JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
2023-04-20 12:18:10.999006 (MainThread): 12:18:10          ON 
2023-04-20 12:18:10.999104 (MainThread): 12:18:10            a.dm_s_symb = c.dm_s_symb
2023-04-20 12:18:10.999201 (MainThread): 12:18:10            AND a.fiftytwoweekhigh = c.dm_high
2023-04-20 12:18:10.999299 (MainThread): 12:18:10            AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
2023-04-20 12:18:10.999395 (MainThread): 12:18:10      QUALIFY ROW_NUMBER() OVER (
2023-04-20 12:18:10.999492 (MainThread): 12:18:10        PARTITION BY dm_s_symb, dm_date 
2023-04-20 12:18:10.999621 (MainThread): 12:18:10        ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
2023-04-20 12:18:10.999722 (MainThread): 12:18:10    -- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
2023-04-20 12:18:10.999819 (MainThread): 12:18:10    ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
2023-04-20 12:18:10.999914 (MainThread): 12:18:10    ^^^
2023-04-20 12:18:11.000008 (MainThread): 12:18:10      ON 
2023-04-20 12:18:11.000104 (MainThread): 12:18:11        s.symbol = fmh.dm_s_symb
2023-04-20 12:18:11.000199 (MainThread): 12:18:11        AND fmh.dm_date >= s.effectivedate 
2023-04-20 12:18:11.000295 (MainThread): 12:18:11        AND fmh.dm_date < s.enddate
2023-04-20 12:18:11.000390 (MainThread): 12:18:11    LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
2023-04-20 12:18:11.000486 (MainThread): 12:18:11      ON 
2023-04-20 12:18:11.000581 (MainThread): 12:18:11        f.sk_companyid = s.sk_companyid
2023-04-20 12:18:11.000676 (MainThread): 12:18:11        AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
2023-04-20 12:18:11.000772 (MainThread): 12:18:11        AND year(fmh.dm_date) = year(fi_qtr_start_date)
2023-04-20 12:18:11.000868 (MainThread): 12:18:11    
2023-04-20 12:18:11.001003 (MainThread): 12:18:11  
2023-04-20 12:18:11.001268 (MainThread): 12:18:11  Done. PASS=9 WARN=0 ERROR=3 SKIP=3 TOTAL=15
2023-04-20 12:18:11.156010 (Thread-989): handling ps request
2023-04-20 12:18:11.156494 (Thread-989): 12:18:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8339100>]}
2023-04-20 12:18:11.158230 (Thread-989): sending response (<Response 24894 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:11.614742 (Thread-990): handling status request
2023-04-20 12:18:11.615218 (Thread-990): 12:18:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8335ca0>]}
2023-04-20 12:18:11.615870 (Thread-990): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:12.336128 (Thread-991): handling poll request
2023-04-20 12:18:12.336635 (Thread-991): 12:18:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8339d30>]}
2023-04-20 12:18:12.340525 (Thread-991): sending response (<Response 159167 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:14.229397 (Thread-992): handling ps request
2023-04-20 12:18:14.229892 (Thread-992): 12:18:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f83a1430>]}
2023-04-20 12:18:14.231764 (Thread-992): sending response (<Response 24918 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:14.651600 (Thread-993): handling status request
2023-04-20 12:18:14.652056 (Thread-993): 12:18:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8339cd0>]}
2023-04-20 12:18:14.652649 (Thread-993): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:34.496326 (Thread-994): handling status request
2023-04-20 12:18:34.496825 (Thread-994): 12:18:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8339c70>]}
2023-04-20 12:18:34.497454 (Thread-994): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:34.726763 (Thread-995): handling ps request
2023-04-20 12:18:34.727243 (Thread-995): 12:18:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8339fd0>]}
2023-04-20 12:18:34.728938 (Thread-995): sending response (<Response 24918 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:36.436607 (Thread-996): handling compile_sql request
2023-04-20 12:18:36.437097 (Thread-996): 12:18:36  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8339f10>]}
2023-04-20 12:18:36.485358 (Thread-997): handling ps request
2023-04-20 12:18:36.486321 (Thread-997): 12:18:36  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f835adc0>]}
2023-04-20 12:18:36.489189 (Thread-997): sending response (<Response 25449 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:37.074107 (Thread-998): handling poll request
2023-04-20 12:18:37.074601 (Thread-998): 12:18:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f835af70>]}
2023-04-20 12:18:37.075141 (Thread-998): sending response (<Response 436 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:37.510697 (Thread-999): handling status request
2023-04-20 12:18:37.511181 (Thread-999): 12:18:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8349130>]}
2023-04-20 12:18:37.511895 (Thread-999): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:39.229358 (Thread-996): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:39.265925 (MainThread): 12:18:39  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5419885c-0f84-4411-aa64-6439f023ed7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f77f99baee0>]}
2023-04-20 12:18:39.266501 (MainThread): 12:18:39  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:18:39.267891 (Thread-1): 12:18:39  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 12:18:39.268116 (Thread-1): 12:18:39  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 12:18:39.272734 (Thread-1): 12:18:39  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 12:18:39.268166 => 2023-04-20 12:18:39.272567
2023-04-20 12:18:39.272948 (Thread-1): 12:18:39  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 12:18:39.273109 (Thread-1): 12:18:39  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 12:18:39.272995 => 2023-04-20 12:18:39.273010
2023-04-20 12:18:39.623429 (Thread-1000): handling poll request
2023-04-20 12:18:39.623959 (Thread-1000): 12:18:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8349370>]}
2023-04-20 12:18:39.624940 (Thread-1000): sending response (<Response 14917 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:39.763936 (Thread-1001): handling ps request
2023-04-20 12:18:39.764448 (Thread-1001): 12:18:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8349460>]}
2023-04-20 12:18:39.766146 (Thread-1001): sending response (<Response 25468 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:39.779877 (Thread-1002): handling ps request
2023-04-20 12:18:39.780215 (Thread-1002): 12:18:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f83496d0>]}
2023-04-20 12:18:39.781615 (Thread-1002): sending response (<Response 25468 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:40.808258 (Thread-1003): handling status request
2023-04-20 12:18:40.808743 (Thread-1003): 12:18:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f83498b0>]}
2023-04-20 12:18:40.809481 (Thread-1003): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:18:41.318959 (Thread-1004): handling poll request
2023-04-20 12:18:41.319454 (Thread-1004): 12:18:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8349490>]}
2023-04-20 12:18:41.320277 (Thread-1004): sending response (<Response 14917 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:07.807569 (Thread-1005): handling status request
2023-04-20 12:19:07.808086 (Thread-1005): 12:19:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8349700>]}
2023-04-20 12:19:07.840783 (Thread-1005): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:14.187987 (Thread-1006): handling status request
2023-04-20 12:19:14.188508 (Thread-1006): 12:19:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8349d60>]}
2023-04-20 12:19:14.189122 (Thread-1006): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:14.302259 (Thread-1007): handling ps request
2023-04-20 12:19:14.302949 (Thread-1007): 12:19:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f835a730>]}
2023-04-20 12:19:14.304700 (Thread-1007): sending response (<Response 25468 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:15.461524 (Thread-1008): handling ps request
2023-04-20 12:19:15.462070 (Thread-1008): 12:19:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8349eb0>]}
2023-04-20 12:19:15.463831 (Thread-1008): sending response (<Response 25468 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:15.559727 (Thread-1009): handling run_sql request
2023-04-20 12:19:15.560217 (Thread-1009): 12:19:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8349b80>]}
2023-04-20 12:19:18.292003 (Thread-1009): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:18.327751 (MainThread): 12:19:18  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ed58a47f-3636-4e74-9450-66bb724eaa2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe246c20ee0>]}
2023-04-20 12:19:18.328329 (MainThread): 12:19:18  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:19:18.329676 (Thread-1): 12:19:18  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 12:19:18.329917 (Thread-1): 12:19:18  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 12:19:18.334563 (Thread-1): 12:19:18  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 12:19:18.329967 => 2023-04-20 12:19:18.334398
2023-04-20 12:19:18.334809 (Thread-1): 12:19:18  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 12:19:18.335284 (Thread-1): 12:19:18  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 12:19:18.335717 (Thread-1): 12:19:18  On rpc.dbsql_dbt_tpch.request: 
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:19:18.335866 (Thread-1): 12:19:18  Opening a new connection, currently in state init
2023-04-20 12:19:18.804191 (Thread-1010): handling ps request
2023-04-20 12:19:18.804819 (Thread-1010): 12:19:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8354ee0>]}
2023-04-20 12:19:18.808224 (Thread-1010): sending response (<Response 25986 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:18.886645 (Thread-1011): handling ps request
2023-04-20 12:19:18.887034 (Thread-1011): 12:19:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8349430>]}
2023-04-20 12:19:18.888685 (Thread-1011): sending response (<Response 25986 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:19.350307 (Thread-1012): handling status request
2023-04-20 12:19:19.350792 (Thread-1012): 12:19:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f843cdc0>]}
2023-04-20 12:19:19.351477 (Thread-1012): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:19.446140 (Thread-1013): handling poll request
2023-04-20 12:19:19.446528 (Thread-1013): 12:19:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f83356a0>]}
2023-04-20 12:19:19.447112 (Thread-1013): sending response (<Response 5953 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:19.629594 (Thread-1014): handling poll request
2023-04-20 12:19:19.630048 (Thread-1014): 12:19:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8349b80>]}
2023-04-20 12:19:19.630617 (Thread-1014): sending response (<Response 5952 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:19.901262 (Thread-1015): handling status request
2023-04-20 12:19:19.901765 (Thread-1015): 12:19:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f83497f0>]}
2023-04-20 12:19:19.902396 (Thread-1015): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:20.011627 (Thread-1): 12:19:20  SQL status: OK in 1.68 seconds
2023-04-20 12:19:20.032754 (Thread-1): 12:19:20  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 12:19:18.334865 => 2023-04-20 12:19:20.032547
2023-04-20 12:19:20.032999 (Thread-1): 12:19:20  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 12:19:21.232478 (Thread-1016): handling poll request
2023-04-20 12:19:21.233007 (Thread-1016): 12:19:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f87304c0>]}
2023-04-20 12:19:21.237653 (Thread-1016): sending response (<Response 99690 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:21.464365 (Thread-1017): handling ps request
2023-04-20 12:19:21.464866 (Thread-1017): 12:19:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8730160>]}
2023-04-20 12:19:21.466832 (Thread-1017): sending response (<Response 26011 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:21.972973 (Thread-1018): handling status request
2023-04-20 12:19:21.973469 (Thread-1018): 12:19:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8730c40>]}
2023-04-20 12:19:21.974089 (Thread-1018): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:22.221096 (Thread-1019): handling poll request
2023-04-20 12:19:22.221537 (Thread-1019): 12:19:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f87304f0>]}
2023-04-20 12:19:22.225827 (Thread-1019): sending response (<Response 105218 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:23.404106 (Thread-1020): handling status request
2023-04-20 12:19:23.404645 (Thread-1020): 12:19:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528152520>]}
2023-04-20 12:19:23.405262 (Thread-1020): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:23.662776 (Thread-1021): handling ps request
2023-04-20 12:19:23.663263 (Thread-1021): 12:19:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8730ac0>]}
2023-04-20 12:19:23.665007 (Thread-1021): sending response (<Response 26011 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:28.215437 (Thread-1022): 12:19:28  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 12:19:28.235078 (Thread-1022): 12:19:28  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 12:19:28.235448 (Thread-1022): 12:19:28  previous checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, current checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886
2023-04-20 12:19:28.235669 (Thread-1022): 12:19:28  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f843e070>]}
2023-04-20 12:19:28.748979 (Thread-1023): handling status request
2023-04-20 12:19:28.759787 (Thread-1023): 12:19:28  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f80c86a0>]}
2023-04-20 12:19:28.770481 (Thread-1023): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:29.704096 (Thread-1022): 12:19:29  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 12:19:29.707663 (Thread-1022): 12:19:29  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 12:19:29.710886 (Thread-1022): 12:19:29  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 12:19:29.713826 (Thread-1022): 12:19:29  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 12:19:29.716852 (Thread-1022): 12:19:29  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 12:19:29.719618 (Thread-1022): 12:19:29  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 12:19:29.722836 (Thread-1022): 12:19:29  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 12:19:29.725951 (Thread-1022): 12:19:29  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 12:19:29.728975 (Thread-1022): 12:19:29  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 12:19:29.731777 (Thread-1022): 12:19:29  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 12:19:29.735099 (Thread-1022): 12:19:29  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 12:19:29.738019 (Thread-1022): 12:19:29  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 12:19:29.740783 (Thread-1022): 12:19:29  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 12:19:29.743678 (Thread-1022): 12:19:29  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 12:19:29.746444 (Thread-1022): 12:19:29  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 12:19:29.890968 (Thread-1022): 12:19:29  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f803ee50>]}
2023-04-20 12:19:30.343139 (Thread-1024): handling status request
2023-04-20 12:19:30.343658 (Thread-1024): 12:19:30  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f80307f0>]}
2023-04-20 12:19:30.344276 (Thread-1024): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:34.130490 (Thread-1025): handling status request
2023-04-20 12:19:34.130981 (Thread-1025): 12:19:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f804b1c0>]}
2023-04-20 12:19:34.131600 (Thread-1025): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:34.686258 (Thread-1026): handling cli_args request
2023-04-20 12:19:34.686776 (Thread-1026): 12:19:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f804b370>]}
2023-04-20 12:19:37.449792 (Thread-1026): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:37.506528 (MainThread): 12:19:37  checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
2023-04-20 12:19:37.526231 (MainThread): 12:19:37  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 12:19:37.526407 (MainThread): 12:19:37  previous checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, current checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd
2023-04-20 12:19:37.526558 (MainThread): 12:19:37  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe9bc5cc880>]}
2023-04-20 12:19:38.068968 (Thread-1027): handling ps request
2023-04-20 12:19:38.069617 (Thread-1027): 12:19:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f804b910>]}
2023-04-20 12:19:38.072600 (Thread-1027): sending response (<Response 26393 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:38.187367 (Thread-1028): handling poll request
2023-04-20 12:19:38.187884 (Thread-1028): 12:19:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437f81400>]}
2023-04-20 12:19:38.188451 (Thread-1028): sending response (<Response 1863 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:38.303272 (Thread-1029): handling status request
2023-04-20 12:19:38.303928 (Thread-1029): 12:19:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437f817c0>]}
2023-04-20 12:19:38.304678 (Thread-1029): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:38.510282 (Thread-1030): handling ps request
2023-04-20 12:19:38.510776 (Thread-1030): 12:19:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437f81760>]}
2023-04-20 12:19:38.512593 (Thread-1030): sending response (<Response 26393 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:39.039836 (MainThread): 12:19:39  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 12:19:39.052141 (MainThread): 12:19:39  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 12:19:39.055201 (MainThread): 12:19:39  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 12:19:39.058335 (MainThread): 12:19:39  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 12:19:39.061372 (MainThread): 12:19:39  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 12:19:39.064192 (MainThread): 12:19:39  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 12:19:39.067262 (MainThread): 12:19:39  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 12:19:39.070361 (MainThread): 12:19:39  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 12:19:39.073436 (MainThread): 12:19:39  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 12:19:39.076248 (MainThread): 12:19:39  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 12:19:39.080065 (MainThread): 12:19:39  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 12:19:39.082849 (MainThread): 12:19:39  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 12:19:39.085585 (MainThread): 12:19:39  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 12:19:39.088451 (MainThread): 12:19:39  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 12:19:39.091466 (MainThread): 12:19:39  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 12:19:39.250938 (MainThread): 12:19:39  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe9b41d9cd0>]}
2023-04-20 12:19:39.282895 (MainThread): 12:19:39  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe9bc5d8670>]}
2023-04-20 12:19:39.283204 (MainThread): 12:19:39  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:19:39.283345 (MainThread): 12:19:39  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe9bc793a30>]}
2023-04-20 12:19:39.285146 (MainThread): 12:19:39  
2023-04-20 12:19:39.286339 (MainThread): 12:19:39  Acquiring new databricks connection 'master'
2023-04-20 12:19:39.288036 (ThreadPoolExecutor-0_0): 12:19:39  Acquiring new databricks connection 'list_schemas'
2023-04-20 12:19:39.298599 (ThreadPoolExecutor-0_0): 12:19:39  Using databricks connection "list_schemas"
2023-04-20 12:19:39.298917 (ThreadPoolExecutor-0_0): 12:19:39  On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_schemas"} */

    show databases
  
2023-04-20 12:19:39.299076 (ThreadPoolExecutor-0_0): 12:19:39  Opening a new connection, currently in state init
2023-04-20 12:19:40.317285 (ThreadPoolExecutor-0_0): 12:19:40  SQL status: OK in 1.02 seconds
2023-04-20 12:19:40.453657 (ThreadPoolExecutor-0_0): 12:19:40  On list_schemas: Close
2023-04-20 12:19:40.669277 (Thread-1031): handling poll request
2023-04-20 12:19:40.669777 (Thread-1031): 12:19:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437f9c4f0>]}
2023-04-20 12:19:40.670515 (Thread-1031): sending response (<Response 9253 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:40.742857 (ThreadPoolExecutor-1_0): 12:19:40  Acquiring new databricks connection 'list_None_dbt_shabbirkdb'
2023-04-20 12:19:40.753325 (ThreadPoolExecutor-1_0): 12:19:40  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:19:40.753515 (ThreadPoolExecutor-1_0): 12:19:40  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 12:19:40.753671 (ThreadPoolExecutor-1_0): 12:19:40  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show tables in `dbt_shabbirkdb`
  
2023-04-20 12:19:40.753820 (ThreadPoolExecutor-1_0): 12:19:40  Opening a new connection, currently in state closed
2023-04-20 12:19:41.458547 (ThreadPoolExecutor-1_0): 12:19:41  SQL status: OK in 0.7 seconds
2023-04-20 12:19:41.467993 (ThreadPoolExecutor-1_0): 12:19:41  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 12:19:41.468217 (ThreadPoolExecutor-1_0): 12:19:41  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show views in `dbt_shabbirkdb`
  
2023-04-20 12:19:41.560035 (Thread-1032): handling ps request
2023-04-20 12:19:41.560501 (Thread-1032): 12:19:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437f9cb50>]}
2023-04-20 12:19:41.562511 (Thread-1032): sending response (<Response 26393 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:41.938995 (ThreadPoolExecutor-1_0): 12:19:41  SQL status: OK in 0.47 seconds
2023-04-20 12:19:41.942676 (ThreadPoolExecutor-1_0): 12:19:41  On list_None_dbt_shabbirkdb: ROLLBACK
2023-04-20 12:19:41.942884 (ThreadPoolExecutor-1_0): 12:19:41  Databricks adapter: NotImplemented: rollback
2023-04-20 12:19:41.943030 (ThreadPoolExecutor-1_0): 12:19:41  On list_None_dbt_shabbirkdb: Close
2023-04-20 12:19:42.228739 (MainThread): 12:19:42  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe9bc3b7eb0>]}
2023-04-20 12:19:42.229133 (MainThread): 12:19:42  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:19:42.229286 (MainThread): 12:19:42  Spark adapter: NotImplemented: commit
2023-04-20 12:19:42.229798 (MainThread): 12:19:42  Concurrency: 4 threads (target='default')
2023-04-20 12:19:42.229957 (MainThread): 12:19:42  
2023-04-20 12:19:42.232939 (Thread-1): 12:19:42  Began running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:19:42.233329 (Thread-1): 12:19:42  1 of 15 START sql table model dbt_shabbirkdb.DimBroker ......................... [RUN]
2023-04-20 12:19:42.233815 (Thread-1): 12:19:42  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
2023-04-20 12:19:42.234017 (Thread-1): 12:19:42  Began compiling node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:19:42.237801 (Thread-2): 12:19:42  Began running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:19:42.238129 (Thread-2): 12:19:42  2 of 15 START sql table model dbt_shabbirkdb.DimCompany ........................ [RUN]
2023-04-20 12:19:42.238645 (Thread-2): 12:19:42  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCompany'
2023-04-20 12:19:42.238824 (Thread-2): 12:19:42  Began compiling node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:19:42.242890 (Thread-3): 12:19:42  Began running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:19:42.243188 (Thread-3): 12:19:42  3 of 15 START sql table model dbt_shabbirkdb.DimCustomerStg .................... [RUN]
2023-04-20 12:19:42.243728 (Thread-3): 12:19:42  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomerStg'
2023-04-20 12:19:42.243905 (Thread-3): 12:19:42  Began compiling node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:19:42.247973 (Thread-4): 12:19:42  Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:19:42.248266 (Thread-4): 12:19:42  4 of 15 START sql table model dbt_shabbirkdb.tempDailyMarketHistorical ......... [RUN]
2023-04-20 12:19:42.248793 (Thread-4): 12:19:42  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempDailyMarketHistorical'
2023-04-20 12:19:42.248967 (Thread-4): 12:19:42  Began compiling node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:19:42.252676 (Thread-4): 12:19:42  Writing injected SQL for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:19:42.254070 (Thread-1): 12:19:42  Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:19:42.254496 (Thread-2): 12:19:42  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:19:42.255108 (Thread-3): 12:19:42  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:19:42.268380 (Thread-4): 12:19:42  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (compile): 2023-04-20 12:19:42.249012 => 2023-04-20 12:19:42.268210
2023-04-20 12:19:42.268607 (Thread-4): 12:19:42  Began executing node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:19:42.273715 (Thread-2): 12:19:42  Timing info for model.dbsql_dbt_tpch.DimCompany (compile): 2023-04-20 12:19:42.238869 => 2023-04-20 12:19:42.273555
2023-04-20 12:19:42.273932 (Thread-2): 12:19:42  Began executing node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:19:42.289226 (Thread-1): 12:19:42  Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 2023-04-20 12:19:42.234067 => 2023-04-20 12:19:42.289064
2023-04-20 12:19:42.289433 (Thread-1): 12:19:42  Began executing node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:19:42.294500 (Thread-3): 12:19:42  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (compile): 2023-04-20 12:19:42.243949 => 2023-04-20 12:19:42.294349
2023-04-20 12:19:42.294701 (Thread-3): 12:19:42  Began executing node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:19:42.302562 (Thread-4): 12:19:42  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:19:42.302734 (Thread-4): 12:19:42  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:19:42.302889 (Thread-4): 12:19:42  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

      describe extended `dbt_shabbirkdb`.`tempdailymarkethistorical`
  
2023-04-20 12:19:42.303020 (Thread-4): 12:19:42  Opening a new connection, currently in state init
2023-04-20 12:19:42.306366 (Thread-2): 12:19:42  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:19:42.306535 (Thread-2): 12:19:42  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:19:42.306686 (Thread-2): 12:19:42  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

      describe extended `dbt_shabbirkdb`.`dimcompany`
  
2023-04-20 12:19:42.306807 (Thread-2): 12:19:42  Opening a new connection, currently in state init
2023-04-20 12:19:42.307838 (Thread-1): 12:19:42  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:19:42.308013 (Thread-1): 12:19:42  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:19:42.308161 (Thread-1): 12:19:42  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

      describe extended `dbt_shabbirkdb`.`dimbroker`
  
2023-04-20 12:19:42.308280 (Thread-1): 12:19:42  Opening a new connection, currently in state closed
2023-04-20 12:19:42.308833 (Thread-3): 12:19:42  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:19:42.309000 (Thread-3): 12:19:42  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:19:42.309151 (Thread-3): 12:19:42  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

      describe extended `dbt_shabbirkdb`.`dimcustomerstg`
  
2023-04-20 12:19:42.309271 (Thread-3): 12:19:42  Opening a new connection, currently in state init
2023-04-20 12:19:42.824202 (Thread-1033): handling status request
2023-04-20 12:19:42.824689 (Thread-1033): 12:19:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437ec9310>]}
2023-04-20 12:19:42.825287 (Thread-1033): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:43.087850 (Thread-3): 12:19:43  SQL status: OK in 0.78 seconds
2023-04-20 12:19:43.133707 (Thread-2): 12:19:43  SQL status: OK in 0.83 seconds
2023-04-20 12:19:43.144522 (Thread-4): 12:19:43  SQL status: OK in 0.84 seconds
2023-04-20 12:19:43.144929 (Thread-1): 12:19:43  SQL status: OK in 0.84 seconds
2023-04-20 12:19:43.153764 (Thread-3): 12:19:43  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:19:43.161279 (Thread-1): 12:19:43  Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:19:43.165117 (Thread-2): 12:19:43  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:19:43.169261 (Thread-4): 12:19:43  Writing runtime sql for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:19:43.175292 (Thread-1): 12:19:43  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:19:43.175540 (Thread-1): 12:19:43  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT
  md5(employeeid) as sk_brokerid,
  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate`) effectivedate,
  date('9999-12-31') enddate
FROM  `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`HR`
WHERE employeejobcode = 314
  
2023-04-20 12:19:43.177856 (Thread-2): 12:19:43  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:19:43.178116 (Thread-2): 12:19:43  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCompany`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  * FROM (
  SELECT
    md5(cik) sk_companyid,
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
  
2023-04-20 12:19:43.182890 (Thread-4): 12:19:43  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:19:43.183102 (Thread-4): 12:19:43  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempDailyMarketHistorical`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  dmh.*,
  sk_dateid,
  min(dm_low) OVER (
    PARTITION BY dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweeklow,
  max(dm_high) OVER (
    PARTITION by dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweekhigh
FROM (
  SELECT * FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketHistorical`
  UNION ALL
  SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketIncremental`) dmh
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
  ON d.datevalue = dm_date;
  
2023-04-20 12:19:43.184601 (Thread-3): 12:19:43  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:19:43.184931 (Thread-3): 12:19:43  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomerStg`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT * FROM (
  SELECT
    sk_customerid,
    customerid,
    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) taxid,
    status,
    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) lastname,
    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) firstname,
    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) middleinitial,
    coalesce(gender, last_value(gender) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) gender,
    coalesce(tier, last_value(tier) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) tier,
    coalesce(dob, last_value(dob) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) dob,
    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline1,
    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline2,
    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) postalcode,
    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) CITY,
    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) stateprov,
    coalesce(country, last_value(country) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) country,
    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone1,
    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone2,
    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone3,
    coalesce(email1, last_value(email1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email1,
    coalesce(email2, last_value(email2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email2,
    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) LCL_TX_ID,
    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) NAT_TX_ID,
    batchid,
    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,
    date(update_ts) effectivedate,
    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate
  FROM (
    SELECT
      md5(customerid::string) as sk_customerid,
      customerid,
      taxid,
      status,
      lastname,
      firstname,
      middleinitial,
      gender,
      tier,
      dob,
      addressline1,
      addressline2,
      postalcode,
      city,
      stateprov,
      country,
      phone1,
      phone2,
      phone3,
      email1,
      email2,
      lcl_tx_id,
      nat_tx_id,
      1 batchid,
      update_ts
    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')
    UNION ALL
    SELECT
      md5(c.customerid::string) as sk_customerid,
      c.customerid,
      nullif(c.taxid, '') taxid,
      nullif(s.st_name, '') as status,
      nullif(c.lastname, '') lastname,
      nullif(c.firstname, '') firstname,
      nullif(c.middleinitial, '') middleinitial,
      gender,
      c.tier,
      c.dob,
      nullif(c.addressline1, '') addressline1,
      nullif(c.addressline2, '') addressline2,
      nullif(c.postalcode, '') postalcode,
      nullif(c.city, '') city,
      nullif(c.stateprov, '') stateprov,
      nullif(c.country, '') country,
      CASE
        WHEN isnull(c_local_1) then c_local_1
        ELSE concat(
          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),
          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),
          c_local_1,
          nvl(c_ext_1, '')) END as phone1,
      CASE
        WHEN isnull(c_local_2) then c_local_2
        ELSE concat(
          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),
          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),
          c_local_2,
          nvl(c_ext_2, '')) END as phone2,
      CASE
        WHEN isnull(c_local_3) then c_local_3
        ELSE concat(
          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),
          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),
          c_local_3,
          nvl(c_ext_3, '')) END as phone3,
      nullif(c.email1, '') email1,
      nullif(c.email2, '') email2,
      c.LCL_TX_ID, 
      c.NAT_TX_ID,
      c.batchid,
      timestamp(bd.batchdate) update_ts
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CustomerIncremental` c
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
      ON c.batchid = bd.batchid
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` s 
      ON c.status = s.st_id
  ) c
  )
  
2023-04-20 12:19:43.236162 (Thread-1034): handling poll request
2023-04-20 12:19:43.236640 (Thread-1034): 12:19:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437ec9a00>]}
2023-04-20 12:19:43.263929 (Thread-1034): sending response (<Response 41489 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:44.882424 (Thread-1035): handling poll request
2023-04-20 12:19:44.882924 (Thread-1035): 12:19:44  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437ec9ca0>]}
2023-04-20 12:19:44.883423 (Thread-1035): sending response (<Response 291 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:45.019768 (Thread-1036): handling ps request
2023-04-20 12:19:45.020185 (Thread-1036): 12:19:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437ec9340>]}
2023-04-20 12:19:45.021942 (Thread-1036): sending response (<Response 26394 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:45.500286 (Thread-1037): handling status request
2023-04-20 12:19:45.500783 (Thread-1037): 12:19:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437ec9ee0>]}
2023-04-20 12:19:45.501382 (Thread-1037): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:45.921375 (Thread-1): 12:19:45  SQL status: OK in 2.75 seconds
2023-04-20 12:19:45.948171 (Thread-1): 12:19:45  Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 2023-04-20 12:19:42.289482 => 2023-04-20 12:19:45.948000
2023-04-20 12:19:45.948423 (Thread-1): 12:19:45  On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
2023-04-20 12:19:45.948572 (Thread-1): 12:19:45  Databricks adapter: NotImplemented: rollback
2023-04-20 12:19:45.948695 (Thread-1): 12:19:45  On model.dbsql_dbt_tpch.DimBroker: Close
2023-04-20 12:19:46.158916 (Thread-2): 12:19:46  SQL status: OK in 2.98 seconds
2023-04-20 12:19:46.161282 (Thread-2): 12:19:46  Timing info for model.dbsql_dbt_tpch.DimCompany (execute): 2023-04-20 12:19:42.273982 => 2023-04-20 12:19:46.161128
2023-04-20 12:19:46.161498 (Thread-2): 12:19:46  On model.dbsql_dbt_tpch.DimCompany: ROLLBACK
2023-04-20 12:19:46.161638 (Thread-2): 12:19:46  Databricks adapter: NotImplemented: rollback
2023-04-20 12:19:46.161760 (Thread-2): 12:19:46  On model.dbsql_dbt_tpch.DimCompany: Close
2023-04-20 12:19:46.236119 (Thread-1): 12:19:46  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe9b40c8640>]}
2023-04-20 12:19:46.236640 (Thread-1): 12:19:46  1 of 15 OK created sql table model dbt_shabbirkdb.DimBroker .................... [OK in 4.00s]
2023-04-20 12:19:46.238238 (Thread-1): 12:19:46  Finished running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:19:46.437933 (Thread-2): 12:19:46  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe94654bb20>]}
2023-04-20 12:19:46.438479 (Thread-2): 12:19:46  2 of 15 OK created sql table model dbt_shabbirkdb.DimCompany ................... [OK in 4.20s]
2023-04-20 12:19:46.438715 (Thread-2): 12:19:46  Finished running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:19:46.439756 (Thread-1): 12:19:46  Began running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:19:46.440096 (Thread-1): 12:19:46  5 of 15 START sql table model dbt_shabbirkdb.DimSecurity ....................... [RUN]
2023-04-20 12:19:46.440624 (Thread-1): 12:19:46  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimSecurity'
2023-04-20 12:19:46.440813 (Thread-1): 12:19:46  Began compiling node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:19:46.444923 (Thread-2): 12:19:46  Began running node model.dbsql_dbt_tpch.Financial
2023-04-20 12:19:46.445225 (Thread-2): 12:19:46  6 of 15 START sql table model dbt_shabbirkdb.Financial ......................... [RUN]
2023-04-20 12:19:46.445741 (Thread-2): 12:19:46  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Financial'
2023-04-20 12:19:46.445934 (Thread-2): 12:19:46  Began compiling node model.dbsql_dbt_tpch.Financial
2023-04-20 12:19:46.451038 (Thread-1): 12:19:46  Writing injected SQL for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:19:46.451893 (Thread-2): 12:19:46  Writing injected SQL for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:19:46.465911 (Thread-1): 12:19:46  Timing info for model.dbsql_dbt_tpch.DimSecurity (compile): 2023-04-20 12:19:46.440883 => 2023-04-20 12:19:46.465709
2023-04-20 12:19:46.466163 (Thread-1): 12:19:46  Began executing node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:19:46.470983 (Thread-2): 12:19:46  Timing info for model.dbsql_dbt_tpch.Financial (compile): 2023-04-20 12:19:46.445981 => 2023-04-20 12:19:46.470809
2023-04-20 12:19:46.471200 (Thread-2): 12:19:46  Began executing node model.dbsql_dbt_tpch.Financial
2023-04-20 12:19:46.475398 (Thread-2): 12:19:46  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:19:46.475609 (Thread-2): 12:19:46  Using databricks connection "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:19:46.475778 (Thread-2): 12:19:46  On model.dbsql_dbt_tpch.Financial: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

      describe extended `dbt_shabbirkdb`.`financial`
  
2023-04-20 12:19:46.477260 (Thread-1): 12:19:46  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:19:46.477444 (Thread-1): 12:19:46  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:19:46.477601 (Thread-1): 12:19:46  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

      describe extended `dbt_shabbirkdb`.`dimsecurity`
  
2023-04-20 12:19:46.477727 (Thread-1): 12:19:46  Opening a new connection, currently in state closed
2023-04-20 12:19:46.477993 (Thread-2): 12:19:46  Opening a new connection, currently in state closed
2023-04-20 12:19:46.630675 (Thread-1038): handling poll request
2023-04-20 12:19:46.631179 (Thread-1038): 12:19:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437eef070>]}
2023-04-20 12:19:46.632066 (Thread-1038): sending response (<Response 15973 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:47.185091 (Thread-3): 12:19:47  SQL status: OK in 4.0 seconds
2023-04-20 12:19:47.187484 (Thread-3): 12:19:47  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (execute): 2023-04-20 12:19:42.294749 => 2023-04-20 12:19:47.187328
2023-04-20 12:19:47.187730 (Thread-3): 12:19:47  On model.dbsql_dbt_tpch.DimCustomerStg: ROLLBACK
2023-04-20 12:19:47.187873 (Thread-3): 12:19:47  Databricks adapter: NotImplemented: rollback
2023-04-20 12:19:47.187997 (Thread-3): 12:19:47  On model.dbsql_dbt_tpch.DimCustomerStg: Close
2023-04-20 12:19:47.208791 (Thread-2): 12:19:47  SQL status: OK in 0.73 seconds
2023-04-20 12:19:47.214241 (Thread-2): 12:19:47  Writing runtime sql for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:19:47.229088 (Thread-2): 12:19:47  Using databricks connection "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:19:47.229341 (Thread-2): 12:19:47  On model.dbsql_dbt_tpch.Financial: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 12:19:47.245956 (Thread-1): 12:19:47  SQL status: OK in 0.77 seconds
2023-04-20 12:19:47.250610 (Thread-1): 12:19:47  Writing runtime sql for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:19:47.266424 (Thread-1): 12:19:47  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:19:47.266744 (Thread-1): 12:19:47  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 12:19:47.469240 (Thread-3): 12:19:47  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe9bc11ffd0>]}
2023-04-20 12:19:47.469741 (Thread-3): 12:19:47  3 of 15 OK created sql table model dbt_shabbirkdb.DimCustomerStg ............... [OK in 5.23s]
2023-04-20 12:19:47.469991 (Thread-3): 12:19:47  Finished running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:19:47.470811 (Thread-3): 12:19:47  Began running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:19:47.471095 (Thread-3): 12:19:47  7 of 15 START sql table model dbt_shabbirkdb.DimAccount ........................ [RUN]
2023-04-20 12:19:47.471599 (Thread-3): 12:19:47  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimAccount'
2023-04-20 12:19:47.471782 (Thread-3): 12:19:47  Began compiling node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:19:47.477044 (Thread-3): 12:19:47  Writing injected SQL for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 12:19:47.491500 (Thread-3): 12:19:47  Timing info for model.dbsql_dbt_tpch.DimAccount (compile): 2023-04-20 12:19:47.471828 => 2023-04-20 12:19:47.491340
2023-04-20 12:19:47.491738 (Thread-3): 12:19:47  Began executing node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:19:47.497022 (Thread-3): 12:19:47  Writing runtime sql for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 12:19:47.510430 (Thread-3): 12:19:47  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:19:47.510608 (Thread-3): 12:19:47  Using databricks connection "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 12:19:47.510833 (Thread-3): 12:19:47  On model.dbsql_dbt_tpch.DimAccount: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 12:19:47.510963 (Thread-3): 12:19:47  Opening a new connection, currently in state closed
2023-04-20 12:19:47.621464 (Thread-1039): handling ps request
2023-04-20 12:19:47.621914 (Thread-1039): 12:19:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437eefbe0>]}
2023-04-20 12:19:47.623915 (Thread-1039): sending response (<Response 26394 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:48.039465 (Thread-1040): handling status request
2023-04-20 12:19:48.039990 (Thread-1040): 12:19:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437eefe20>]}
2023-04-20 12:19:48.040595 (Thread-1040): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:48.434116 (Thread-1041): handling poll request
2023-04-20 12:19:48.434600 (Thread-1041): 12:19:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437eef0a0>]}
2023-04-20 12:19:48.435377 (Thread-1041): sending response (<Response 19027 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:49.694543 (Thread-4): 12:19:49  SQL status: OK in 6.51 seconds
2023-04-20 12:19:50.008705 (Thread-4): 12:19:50  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (execute): 2023-04-20 12:19:42.268657 => 2023-04-20 12:19:50.008515
2023-04-20 12:19:50.008981 (Thread-4): 12:19:50  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: ROLLBACK
2023-04-20 12:19:50.009132 (Thread-4): 12:19:50  Databricks adapter: NotImplemented: rollback
2023-04-20 12:19:50.009259 (Thread-4): 12:19:50  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: Close
2023-04-20 12:19:50.096215 (Thread-1): 12:19:50  SQL status: OK in 2.83 seconds
2023-04-20 12:19:50.098526 (Thread-1): 12:19:50  Timing info for model.dbsql_dbt_tpch.DimSecurity (execute): 2023-04-20 12:19:46.466226 => 2023-04-20 12:19:50.098379
2023-04-20 12:19:50.098728 (Thread-1): 12:19:50  On model.dbsql_dbt_tpch.DimSecurity: ROLLBACK
2023-04-20 12:19:50.098869 (Thread-1): 12:19:50  Databricks adapter: NotImplemented: rollback
2023-04-20 12:19:50.098990 (Thread-1): 12:19:50  On model.dbsql_dbt_tpch.DimSecurity: Close
2023-04-20 12:19:50.171993 (Thread-1042): handling poll request
2023-04-20 12:19:50.172467 (Thread-1042): 12:19:50  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437ec9040>]}
2023-04-20 12:19:50.173047 (Thread-1042): sending response (<Response 4121 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:50.289813 (Thread-4): 12:19:50  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe9bc500a90>]}
2023-04-20 12:19:50.290343 (Thread-4): 12:19:50  4 of 15 OK created sql table model dbt_shabbirkdb.tempDailyMarketHistorical .... [OK in 8.04s]
2023-04-20 12:19:50.290582 (Thread-4): 12:19:50  Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:19:50.290868 (Thread-4): 12:19:50  Began running node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:19:50.291142 (Thread-4): 12:19:50  8 of 15 START sql table model dbt_shabbirkdb.Prospect .......................... [RUN]
2023-04-20 12:19:50.291687 (Thread-4): 12:19:50  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Prospect'
2023-04-20 12:19:50.291868 (Thread-4): 12:19:50  Began compiling node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:19:50.297731 (Thread-4): 12:19:50  Writing injected SQL for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:19:50.311393 (Thread-4): 12:19:50  Timing info for model.dbsql_dbt_tpch.Prospect (compile): 2023-04-20 12:19:50.291914 => 2023-04-20 12:19:50.311230
2023-04-20 12:19:50.311641 (Thread-4): 12:19:50  Began executing node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:19:50.315903 (Thread-4): 12:19:50  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:19:50.316072 (Thread-4): 12:19:50  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:19:50.316227 (Thread-4): 12:19:50  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

      describe extended `dbt_shabbirkdb`.`prospect`
  
2023-04-20 12:19:50.316355 (Thread-4): 12:19:50  Opening a new connection, currently in state closed
2023-04-20 12:19:50.339243 (Thread-1043): handling ps request
2023-04-20 12:19:50.339652 (Thread-1043): 12:19:50  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437ee25e0>]}
2023-04-20 12:19:50.341347 (Thread-1043): sending response (<Response 26394 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:50.366227 (Thread-1): 12:19:50  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe9bc4002b0>]}
2023-04-20 12:19:50.366665 (Thread-1): 12:19:50  5 of 15 OK created sql table model dbt_shabbirkdb.DimSecurity .................. [OK in 3.93s]
2023-04-20 12:19:50.366890 (Thread-1): 12:19:50  Finished running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:19:50.797728 (Thread-1044): handling status request
2023-04-20 12:19:50.798215 (Thread-1044): 12:19:50  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437efe0d0>]}
2023-04-20 12:19:50.798797 (Thread-1044): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:51.040642 (Thread-4): 12:19:51  SQL status: OK in 0.72 seconds
2023-04-20 12:19:51.046579 (Thread-4): 12:19:51  Writing runtime sql for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:19:51.060469 (Thread-4): 12:19:51  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:19:51.060796 (Thread-4): 12:19:51  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Prospect`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  agencyid,
  recdate.sk_dateid sk_recorddateid,
  origdate.sk_dateid sk_updatedateid,
  p.batchid,
  nvl2(c.customerid, True, False) iscustomer, 
  p.lastname,
  p.firstname,
  p.middleinitial,
  p.gender,
  p.addressline1,
  p.addressline2,
  p.postalcode,
  city,
  state,
  country,
  phone,
  income,
  numbercars,
  numberchildren,
  maritalstatus,
  age,
  creditrating,
  ownorrentflag,
  employer,
  numbercreditcards,
  networth,
  if(
    isnotnull(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+","")),
    left(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+",""),
      length(
        if(networth > 1000000 or income > 200000,"HighValue+","") || 
        if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
        if(age > 45, "Boomer+", "") ||
        if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
        if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
        if(age < 25 and networth > 1000000, "Inherited+",""))
      -1),
    NULL) marketingnameplate
FROM (
  SELECT 
    * FROM (
    SELECT
      agencyid,
      max(batchid) recordbatchid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth,
      min(batchid) batchid
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`ProspectRaw` p
    GROUP BY
      agencyid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth)
  QUALIFY ROW_NUMBER() OVER (PARTITION BY agencyid ORDER BY batchid DESC) = 1) p
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) recdate
  ON p.recordbatchid = recdate.batchid
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) origdate
  ON p.batchid = origdate.batchid
LEFT JOIN (
  SELECT 
    customerid,
    lastname,
    firstname,
    addressline1,
    addressline2,
    postalcode
  FROM `dbt_shabbirkdb`.`DimCustomerStg`
  WHERE iscurrent) c
  ON 
    upper(p.LastName) = upper(c.lastname)
    and upper(p.FirstName) = upper(c.firstname)
    and upper(p.AddressLine1) = upper(c.addressline1)
    and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
    and upper(p.PostalCode) = upper(c.postalcode)
  
2023-04-20 12:19:51.333178 (Thread-3): 12:19:51  SQL status: OK in 3.82 seconds
2023-04-20 12:19:51.335365 (Thread-3): 12:19:51  Timing info for model.dbsql_dbt_tpch.DimAccount (execute): 2023-04-20 12:19:47.491790 => 2023-04-20 12:19:51.335211
2023-04-20 12:19:51.335603 (Thread-3): 12:19:51  On model.dbsql_dbt_tpch.DimAccount: ROLLBACK
2023-04-20 12:19:51.335858 (Thread-3): 12:19:51  Databricks adapter: NotImplemented: rollback
2023-04-20 12:19:51.336030 (Thread-3): 12:19:51  On model.dbsql_dbt_tpch.DimAccount: Close
2023-04-20 12:19:51.636540 (Thread-3): 12:19:51  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe9bc4cf910>]}
2023-04-20 12:19:51.637078 (Thread-3): 12:19:51  7 of 15 OK created sql table model dbt_shabbirkdb.DimAccount ................... [OK in 4.17s]
2023-04-20 12:19:51.637320 (Thread-3): 12:19:51  Finished running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:19:51.638390 (Thread-1): 12:19:51  Began running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 12:19:51.638894 (Thread-1): 12:19:51  9 of 15 START sql table model dbt_shabbirkdb.DimTrade .......................... [RUN]
2023-04-20 12:19:51.639709 (Thread-1): 12:19:51  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimTrade'
2023-04-20 12:19:51.640005 (Thread-1): 12:19:51  Began compiling node model.dbsql_dbt_tpch.DimTrade
2023-04-20 12:19:51.643295 (Thread-3): 12:19:51  Began running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 12:19:51.643591 (Thread-3): 12:19:51  10 of 15 START sql table model dbt_shabbirkdb.FactCashBalances ................. [RUN]
2023-04-20 12:19:51.644065 (Thread-3): 12:19:51  Acquiring new databricks connection 'model.dbsql_dbt_tpch.FactCashBalances'
2023-04-20 12:19:51.644239 (Thread-3): 12:19:51  Began compiling node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 12:19:51.655094 (Thread-3): 12:19:51  Writing injected SQL for node "model.dbsql_dbt_tpch.FactCashBalances"
2023-04-20 12:19:51.656502 (Thread-1): 12:19:51  Writing injected SQL for node "model.dbsql_dbt_tpch.DimTrade"
2023-04-20 12:19:51.667493 (Thread-3): 12:19:51  Timing info for model.dbsql_dbt_tpch.FactCashBalances (compile): 2023-04-20 12:19:51.644284 => 2023-04-20 12:19:51.667277
2023-04-20 12:19:51.667803 (Thread-3): 12:19:51  Began executing node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 12:19:51.672241 (Thread-3): 12:19:51  Writing runtime sql for node "model.dbsql_dbt_tpch.FactCashBalances"
2023-04-20 12:19:51.678841 (Thread-1): 12:19:51  Timing info for model.dbsql_dbt_tpch.DimTrade (compile): 2023-04-20 12:19:51.640086 => 2023-04-20 12:19:51.678648
2023-04-20 12:19:51.679093 (Thread-1): 12:19:51  Began executing node model.dbsql_dbt_tpch.DimTrade
2023-04-20 12:19:51.683559 (Thread-1): 12:19:51  Writing runtime sql for node "model.dbsql_dbt_tpch.DimTrade"
2023-04-20 12:19:51.683974 (Thread-3): 12:19:51  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:19:51.684159 (Thread-3): 12:19:51  Using databricks connection "model.dbsql_dbt_tpch.FactCashBalances"
2023-04-20 12:19:51.684354 (Thread-3): 12:19:51  On model.dbsql_dbt_tpch.FactCashBalances: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactCashBalances"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactCashBalances`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT
  a.sk_customerid, 
  a.sk_accountid, 
  d.sk_dateid, 
  sum(account_daily_total) OVER (partition by c.accountid order by c.datevalue) cash,
  c.batchid
FROM (
  SELECT 
    ct_ca_id accountid,
    to_date(ct_dts) datevalue,
    sum(ct_amt) account_daily_total,
    batchid
  FROM (
    SELECT * , 1 batchid
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CashTransactionHistory`
    UNION ALL
    SELECT * except(cdc_flag, cdc_dsn)
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CashTransactionIncremental`
  )
  GROUP BY
    accountid,
    datevalue,
    batchid) c 
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
  ON c.datevalue = d.datevalue
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Account IDs are missing from DimAccount, causing audit check failures. 
 JOIN `dbt_shabbirkdb`.`DimAccount` a 
  ON 
    c.accountid = a.accountid
    AND c.datevalue >= a.effectivedate 
    AND c.datevalue < a.enddate
  
2023-04-20 12:19:51.684488 (Thread-3): 12:19:51  Opening a new connection, currently in state closed
2023-04-20 12:19:51.711058 (Thread-1): 12:19:51  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:19:51.711287 (Thread-1): 12:19:51  Using databricks connection "model.dbsql_dbt_tpch.DimTrade"
2023-04-20 12:19:51.711617 (Thread-1): 12:19:51  On model.dbsql_dbt_tpch.DimTrade: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimTrade"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimTrade`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  trade.tradeid,
  sk_brokerid,
  trade.sk_createdateid,
  trade.sk_createtimeid,
  trade.sk_closedateid,
  trade.sk_closetimeid,
  st_name status,
  tt_name type,
  trade.cashflag,
  sk_securityid,
  sk_companyid,
  trade.quantity,
  trade.bidprice,
  sk_customerid,
  sk_accountid,
  trade.executedby,
  trade.tradeprice,
  trade.fee,
  trade.commission,
  trade.tax,
  trade.batchid
FROM (
  SELECT * EXCEPT(t_dts)
  FROM (
    SELECT
      tradeid,
      min(date(t_dts)) OVER (PARTITION BY tradeid) createdate,
      t_dts,
      coalesce(sk_createdateid, last_value(sk_createdateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createdateid,
      coalesce(sk_createtimeid, last_value(sk_createtimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createtimeid,
      coalesce(sk_closedateid, last_value(sk_closedateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closedateid,
      coalesce(sk_closetimeid, last_value(sk_closetimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closetimeid,
      cashflag,
      t_st_id,
      t_tt_id,
      t_s_symb,
      quantity,
      bidprice,
      t_ca_id,
      executedby,
      tradeprice,
      fee,
      commission,
      tax,
      batchid
    FROM (
      SELECT
        tradeid,
        t_dts,
        if(create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_createdateid,
        if(create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_createtimeid,
        if(!create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_closedateid,
        if(!create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_closetimeid,
        CASE 
          WHEN t_is_cash = 1 then TRUE
          WHEN t_is_cash = 0 then FALSE
          ELSE cast(null as BOOLEAN) END AS cashflag,
        t_st_id,
        t_tt_id,
        t_s_symb,
        quantity,
        bidprice,
        t_ca_id,
        executedby,
        tradeprice,
        fee,
        commission,
        tax,
        t.batchid
      FROM (
        SELECT
          t_id tradeid,
          th_dts t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          1 batchid,
          CASE 
            WHEN (th_st_id == "SBMT" AND t_tt_id IN ("TMB", "TMS")) OR th_st_id = "PNDG" THEN TRUE 
            WHEN th_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistory` t
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistoryRaw` th
          ON th_t_id = t_id
        UNION ALL
        SELECT
          t_id tradeid,
          t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          t.batchid,
          CASE 
            WHEN cdc_flag = 'I' THEN TRUE 
            WHEN t_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeIncremental` t
      ) t
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` dd
        ON date(t.t_dts) = dd.datevalue
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimTime` dt
        ON date_format(t.t_dts, 'HH:mm:ss') = dt.timevalue
    )
  )
  QUALIFY ROW_NUMBER() OVER (PARTITION BY tradeid ORDER BY t_dts desc) = 1
) trade
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` status
  ON status.st_id = trade.t_st_id
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeType` tt
  ON tt.tt_id == trade.t_tt_id
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Account IDs are missing from DimSecurity/DimAccount, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` ds
  ON 
    ds.symbol = trade.t_s_symb
    AND createdate >= ds.effectivedate 
    AND createdate < ds.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimAccount` da
  ON 
    trade.t_ca_id = da.accountid 
    AND createdate >= da.effectivedate 
    AND createdate < da.enddate
  
2023-04-20 12:19:51.711764 (Thread-1): 12:19:51  Opening a new connection, currently in state closed
2023-04-20 12:19:51.844302 (Thread-1045): handling poll request
2023-04-20 12:19:51.844800 (Thread-1045): 12:19:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437efef40>]}
2023-04-20 12:19:51.845809 (Thread-1045): sending response (<Response 32964 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:52.416467 (Thread-1): 12:19:52  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimTrade"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimTrade`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  trade.tradeid,
  sk_brokerid,
  trade.sk_createdateid,
  trade.sk_createtimeid,
  trade.sk_closedateid,
  trade.sk_closetimeid,
  st_name status,
  tt_name type,
  trade.cashflag,
  sk_securityid,
  sk_companyid,
  trade.quantity,
  trade.bidprice,
  sk_customerid,
  sk_accountid,
  trade.executedby,
  trade.tradeprice,
  trade.fee,
  trade.commission,
  trade.tax,
  trade.batchid
FROM (
  SELECT * EXCEPT(t_dts)
  FROM (
    SELECT
      tradeid,
      min(date(t_dts)) OVER (PARTITION BY tradeid) createdate,
      t_dts,
      coalesce(sk_createdateid, last_value(sk_createdateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createdateid,
      coalesce(sk_createtimeid, last_value(sk_createtimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createtimeid,
      coalesce(sk_closedateid, last_value(sk_closedateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closedateid,
      coalesce(sk_closetimeid, last_value(sk_closetimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closetimeid,
      cashflag,
      t_st_id,
      t_tt_id,
      t_s_symb,
      quantity,
      bidprice,
      t_ca_id,
      executedby,
      tradeprice,
      fee,
      commission,
      tax,
      batchid
    FROM (
      SELECT
        tradeid,
        t_dts,
        if(create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_createdateid,
        if(create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_createtimeid,
        if(!create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_closedateid,
        if(!create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_closetimeid,
        CASE 
          WHEN t_is_cash = 1 then TRUE
          WHEN t_is_cash = 0 then FALSE
          ELSE cast(null as BOOLEAN) END AS cashflag,
        t_st_id,
        t_tt_id,
        t_s_symb,
        quantity,
        bidprice,
        t_ca_id,
        executedby,
        tradeprice,
        fee,
        commission,
        tax,
        t.batchid
      FROM (
        SELECT
          t_id tradeid,
          th_dts t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          1 batchid,
          CASE 
            WHEN (th_st_id == "SBMT" AND t_tt_id IN ("TMB", "TMS")) OR th_st_id = "PNDG" THEN TRUE 
            WHEN th_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistory` t
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistoryRaw` th
          ON th_t_id = t_id
        UNION ALL
        SELECT
          t_id tradeid,
          t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          t.batchid,
          CASE 
            WHEN cdc_flag = 'I' THEN TRUE 
            WHEN t_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeIncremental` t
      ) t
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` dd
        ON date(t.t_dts) = dd.datevalue
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimTime` dt
        ON date_format(t.t_dts, 'HH:mm:ss') = dt.timevalue
    )
  )
  QUALIFY ROW_NUMBER() OVER (PARTITION BY tradeid ORDER BY t_dts desc) = 1
) trade
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` status
  ON status.st_id = trade.t_st_id
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeType` tt
  ON tt.tt_id == trade.t_tt_id
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Account IDs are missing from DimSecurity/DimAccount, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` ds
  ON 
    ds.symbol = trade.t_s_symb
    AND createdate >= ds.effectivedate 
    AND createdate < ds.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimAccount` da
  ON 
    trade.t_ca_id = da.accountid 
    AND createdate >= da.effectivedate 
    AND createdate < da.enddate
  
2023-04-20 12:19:52.416701 (Thread-1): 12:19:52  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 151, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimTrade"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimTrade`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  trade.tradeid,
  sk_brokerid,
  trade.sk_createdateid,
  trade.sk_createtimeid,
  trade.sk_closedateid,
  trade.sk_closetimeid,
  st_name status,
  tt_name type,
  trade.cashflag,
  sk_securityid,
  sk_companyid,
  trade.quantity,
  trade.bidprice,
  sk_customerid,
  sk_accountid,
  trade.executedby,
  trade.tradeprice,
  trade.fee,
  trade.commission,
  trade.tax,
  trade.batchid
FROM (
  SELECT * EXCEPT(t_dts)
  FROM (
    SELECT
      tradeid,
      min(date(t_dts)) OVER (PARTITION BY tradeid) createdate,
      t_dts,
      coalesce(sk_createdateid, last_value(sk_createdateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createdateid,
      coalesce(sk_createtimeid, last_value(sk_createtimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createtimeid,
      coalesce(sk_closedateid, last_value(sk_closedateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closedateid,
      coalesce(sk_closetimeid, last_value(sk_closetimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closetimeid,
      cashflag,
      t_st_id,
      t_tt_id,
      t_s_symb,
      quantity,
      bidprice,
      t_ca_id,
      executedby,
      tradeprice,
      fee,
      commission,
      tax,
      batchid
    FROM (
      SELECT
        tradeid,
        t_dts,
        if(create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_createdateid,
        if(create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_createtimeid,
        if(!create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_closedateid,
        if(!create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_closetimeid,
        CASE 
          WHEN t_is_cash = 1 then TRUE
          WHEN t_is_cash = 0 then FALSE
          ELSE cast(null as BOOLEAN) END AS cashflag,
        t_st_id,
        t_tt_id,
        t_s_symb,
        quantity,
        bidprice,
        t_ca_id,
        executedby,
        tradeprice,
        fee,
        commission,
        tax,
        t.batchid
      FROM (
        SELECT
          t_id tradeid,
          th_dts t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          1 batchid,
          CASE 
            WHEN (th_st_id == "SBMT" AND t_tt_id IN ("TMB", "TMS")) OR th_st_id = "PNDG" THEN TRUE 
            WHEN th_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistory` t
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistoryRaw` th
          ON th_t_id = t_id
        UNION ALL
        SELECT
          t_id tradeid,
          t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          t.batchid,
          CASE 
            WHEN cdc_flag = 'I' THEN TRUE 
            WHEN t_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeIncremental` t
      ) t
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` dd
        ON date(t.t_dts) = dd.datevalue
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimTime` dt
        ON date_format(t.t_dts, 'HH:mm:ss') = dt.timevalue
    )
  )
  QUALIFY ROW_NUMBER() OVER (PARTITION BY tradeid ORDER BY t_dts desc) = 1
) trade
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` status
  ON status.st_id = trade.t_st_id
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeType` tt
  ON tt.tt_id == trade.t_tt_id
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Account IDs are missing from DimSecurity/DimAccount, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` ds
^^^
  ON 
    ds.symbol = trade.t_s_symb
    AND createdate >= ds.effectivedate 
    AND createdate < ds.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimAccount` da
  ON 
    trade.t_ca_id = da.accountid 
    AND createdate >= da.effectivedate 
    AND createdate < da.enddate
  

2023-04-20 12:19:52.416834 (Thread-1): 12:19:52  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 151, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimTrade"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimTrade`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  trade.tradeid,
  sk_brokerid,
  trade.sk_createdateid,
  trade.sk_createtimeid,
  trade.sk_closedateid,
  trade.sk_closetimeid,
  st_name status,
  tt_name type,
  trade.cashflag,
  sk_securityid,
  sk_companyid,
  trade.quantity,
  trade.bidprice,
  sk_customerid,
  sk_accountid,
  trade.executedby,
  trade.tradeprice,
  trade.fee,
  trade.commission,
  trade.tax,
  trade.batchid
FROM (
  SELECT * EXCEPT(t_dts)
  FROM (
    SELECT
      tradeid,
      min(date(t_dts)) OVER (PARTITION BY tradeid) createdate,
      t_dts,
      coalesce(sk_createdateid, last_value(sk_createdateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createdateid,
      coalesce(sk_createtimeid, last_value(sk_createtimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createtimeid,
      coalesce(sk_closedateid, last_value(sk_closedateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closedateid,
      coalesce(sk_closetimeid, last_value(sk_closetimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closetimeid,
      cashflag,
      t_st_id,
      t_tt_id,
      t_s_symb,
      quantity,
      bidprice,
      t_ca_id,
      executedby,
      tradeprice,
      fee,
      commission,
      tax,
      batchid
    FROM (
      SELECT
        tradeid,
        t_dts,
        if(create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_createdateid,
        if(create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_createtimeid,
        if(!create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_closedateid,
        if(!create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_closetimeid,
        CASE 
          WHEN t_is_cash = 1 then TRUE
          WHEN t_is_cash = 0 then FALSE
          ELSE cast(null as BOOLEAN) END AS cashflag,
        t_st_id,
        t_tt_id,
        t_s_symb,
        quantity,
        bidprice,
        t_ca_id,
        executedby,
        tradeprice,
        fee,
        commission,
        tax,
        t.batchid
      FROM (
        SELECT
          t_id tradeid,
          th_dts t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          1 batchid,
          CASE 
            WHEN (th_st_id == "SBMT" AND t_tt_id IN ("TMB", "TMS")) OR th_st_id = "PNDG" THEN TRUE 
            WHEN th_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistory` t
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistoryRaw` th
          ON th_t_id = t_id
        UNION ALL
        SELECT
          t_id tradeid,
          t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          t.batchid,
          CASE 
            WHEN cdc_flag = 'I' THEN TRUE 
            WHEN t_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeIncremental` t
      ) t
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` dd
        ON date(t.t_dts) = dd.datevalue
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimTime` dt
        ON date_format(t.t_dts, 'HH:mm:ss') = dt.timevalue
    )
  )
  QUALIFY ROW_NUMBER() OVER (PARTITION BY tradeid ORDER BY t_dts desc) = 1
) trade
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` status
  ON status.st_id = trade.t_st_id
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeType` tt
  ON tt.tt_id == trade.t_tt_id
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Account IDs are missing from DimSecurity/DimAccount, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` ds
^^^
  ON 
    ds.symbol = trade.t_s_symb
    AND createdate >= ds.effectivedate 
    AND createdate < ds.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimAccount` da
  ON 
    trade.t_ca_id = da.accountid 
    AND createdate >= da.effectivedate 
    AND createdate < da.enddate
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 151, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimTrade"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimTrade`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  trade.tradeid,
  sk_brokerid,
  trade.sk_createdateid,
  trade.sk_createtimeid,
  trade.sk_closedateid,
  trade.sk_closetimeid,
  st_name status,
  tt_name type,
  trade.cashflag,
  sk_securityid,
  sk_companyid,
  trade.quantity,
  trade.bidprice,
  sk_customerid,
  sk_accountid,
  trade.executedby,
  trade.tradeprice,
  trade.fee,
  trade.commission,
  trade.tax,
  trade.batchid
FROM (
  SELECT * EXCEPT(t_dts)
  FROM (
    SELECT
      tradeid,
      min(date(t_dts)) OVER (PARTITION BY tradeid) createdate,
      t_dts,
      coalesce(sk_createdateid, last_value(sk_createdateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createdateid,
      coalesce(sk_createtimeid, last_value(sk_createtimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createtimeid,
      coalesce(sk_closedateid, last_value(sk_closedateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closedateid,
      coalesce(sk_closetimeid, last_value(sk_closetimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closetimeid,
      cashflag,
      t_st_id,
      t_tt_id,
      t_s_symb,
      quantity,
      bidprice,
      t_ca_id,
      executedby,
      tradeprice,
      fee,
      commission,
      tax,
      batchid
    FROM (
      SELECT
        tradeid,
        t_dts,
        if(create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_createdateid,
        if(create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_createtimeid,
        if(!create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_closedateid,
        if(!create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_closetimeid,
        CASE 
          WHEN t_is_cash = 1 then TRUE
          WHEN t_is_cash = 0 then FALSE
          ELSE cast(null as BOOLEAN) END AS cashflag,
        t_st_id,
        t_tt_id,
        t_s_symb,
        quantity,
        bidprice,
        t_ca_id,
        executedby,
        tradeprice,
        fee,
        commission,
        tax,
        t.batchid
      FROM (
        SELECT
          t_id tradeid,
          th_dts t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          1 batchid,
          CASE 
            WHEN (th_st_id == "SBMT" AND t_tt_id IN ("TMB", "TMS")) OR th_st_id = "PNDG" THEN TRUE 
            WHEN th_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistory` t
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistoryRaw` th
          ON th_t_id = t_id
        UNION ALL
        SELECT
          t_id tradeid,
          t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          t.batchid,
          CASE 
            WHEN cdc_flag = 'I' THEN TRUE 
            WHEN t_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeIncremental` t
      ) t
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` dd
        ON date(t.t_dts) = dd.datevalue
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimTime` dt
        ON date_format(t.t_dts, 'HH:mm:ss') = dt.timevalue
    )
  )
  QUALIFY ROW_NUMBER() OVER (PARTITION BY tradeid ORDER BY t_dts desc) = 1
) trade
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` status
  ON status.st_id = trade.t_st_id
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeType` tt
  ON tt.tt_id == trade.t_tt_id
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Account IDs are missing from DimSecurity/DimAccount, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` ds
^^^
  ON 
    ds.symbol = trade.t_s_symb
    AND createdate >= ds.effectivedate 
    AND createdate < ds.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimAccount` da
  ON 
    trade.t_ca_id = da.accountid 
    AND createdate >= da.effectivedate 
    AND createdate < da.enddate
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 12:19:52.416955 (Thread-1): 12:19:52  Databricks adapter: operation-id: b'\x01\xed\xdfu\xa6\xde\x14\x8d\xbb\x8c\xf6vG\xb66r'
2023-04-20 12:19:52.417212 (Thread-1): 12:19:52  Timing info for model.dbsql_dbt_tpch.DimTrade (execute): 2023-04-20 12:19:51.679147 => 2023-04-20 12:19:52.417067
2023-04-20 12:19:52.417403 (Thread-1): 12:19:52  On model.dbsql_dbt_tpch.DimTrade: ROLLBACK
2023-04-20 12:19:52.417530 (Thread-1): 12:19:52  Databricks adapter: NotImplemented: rollback
2023-04-20 12:19:52.417653 (Thread-1): 12:19:52  On model.dbsql_dbt_tpch.DimTrade: Close
2023-04-20 12:19:52.477289 (Thread-3): 12:19:52  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactCashBalances"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactCashBalances`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT
  a.sk_customerid, 
  a.sk_accountid, 
  d.sk_dateid, 
  sum(account_daily_total) OVER (partition by c.accountid order by c.datevalue) cash,
  c.batchid
FROM (
  SELECT 
    ct_ca_id accountid,
    to_date(ct_dts) datevalue,
    sum(ct_amt) account_daily_total,
    batchid
  FROM (
    SELECT * , 1 batchid
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CashTransactionHistory`
    UNION ALL
    SELECT * except(cdc_flag, cdc_dsn)
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CashTransactionIncremental`
  )
  GROUP BY
    accountid,
    datevalue,
    batchid) c 
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
  ON c.datevalue = d.datevalue
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Account IDs are missing from DimAccount, causing audit check failures. 
 JOIN `dbt_shabbirkdb`.`DimAccount` a 
  ON 
    c.accountid = a.accountid
    AND c.datevalue >= a.effectivedate 
    AND c.datevalue < a.enddate
  
2023-04-20 12:19:52.477472 (Thread-3): 12:19:52  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `a`.`sk_accountid` cannot be resolved. Did you mean one of the following? [`a`.`accountid`, `c`.`accountid`, `a`.`sk_brokerid`, `a`.`accountdesc`, `a`.`sk_customerid`].; line 20 pos 2
2023-04-20 12:19:52.477596 (Thread-3): 12:19:52  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `a`.`sk_accountid` cannot be resolved. Did you mean one of the following? [`a`.`accountid`, `c`.`accountid`, `a`.`sk_brokerid`, `a`.`accountdesc`, `a`.`sk_customerid`].; line 20 pos 2
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `a`.`sk_accountid` cannot be resolved. Did you mean one of the following? [`a`.`accountid`, `c`.`accountid`, `a`.`sk_brokerid`, `a`.`accountdesc`, `a`.`sk_customerid`].; line 20 pos 2
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 12:19:52.477709 (Thread-3): 12:19:52  Databricks adapter: operation-id: b'\x01\xed\xdfu\xa6\xdb\x1d\xe0\xa7\x8b?\x98\xb8\x91|\x1c'
2023-04-20 12:19:52.477944 (Thread-3): 12:19:52  Timing info for model.dbsql_dbt_tpch.FactCashBalances (execute): 2023-04-20 12:19:51.667870 => 2023-04-20 12:19:52.477806
2023-04-20 12:19:52.478121 (Thread-3): 12:19:52  On model.dbsql_dbt_tpch.FactCashBalances: ROLLBACK
2023-04-20 12:19:52.478244 (Thread-3): 12:19:52  Databricks adapter: NotImplemented: rollback
2023-04-20 12:19:52.478357 (Thread-3): 12:19:52  On model.dbsql_dbt_tpch.FactCashBalances: Close
2023-04-20 12:19:52.698636 (Thread-1): 12:19:52  Runtime Error in model DimTrade (models/incremental/DimTrade.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 151, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimTrade"} */
  
    
      
          create or replace table `dbt_shabbirkdb`.`DimTrade`
        
        
      using delta
        
        
        
        
        
        
        as
        
  SELECT
    trade.tradeid,
    sk_brokerid,
    trade.sk_createdateid,
    trade.sk_createtimeid,
    trade.sk_closedateid,
    trade.sk_closetimeid,
    st_name status,
    tt_name type,
    trade.cashflag,
    sk_securityid,
    sk_companyid,
    trade.quantity,
    trade.bidprice,
    sk_customerid,
    sk_accountid,
    trade.executedby,
    trade.tradeprice,
    trade.fee,
    trade.commission,
    trade.tax,
    trade.batchid
  FROM (
    SELECT * EXCEPT(t_dts)
    FROM (
      SELECT
        tradeid,
        min(date(t_dts)) OVER (PARTITION BY tradeid) createdate,
        t_dts,
        coalesce(sk_createdateid, last_value(sk_createdateid) IGNORE NULLS OVER (
          PARTITION BY tradeid ORDER BY t_dts)) sk_createdateid,
        coalesce(sk_createtimeid, last_value(sk_createtimeid) IGNORE NULLS OVER (
          PARTITION BY tradeid ORDER BY t_dts)) sk_createtimeid,
        coalesce(sk_closedateid, last_value(sk_closedateid) IGNORE NULLS OVER (
          PARTITION BY tradeid ORDER BY t_dts)) sk_closedateid,
        coalesce(sk_closetimeid, last_value(sk_closetimeid) IGNORE NULLS OVER (
          PARTITION BY tradeid ORDER BY t_dts)) sk_closetimeid,
        cashflag,
        t_st_id,
        t_tt_id,
        t_s_symb,
        quantity,
        bidprice,
        t_ca_id,
        executedby,
        tradeprice,
        fee,
        commission,
        tax,
        batchid
      FROM (
        SELECT
          tradeid,
          t_dts,
          if(create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_createdateid,
          if(create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_createtimeid,
          if(!create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_closedateid,
          if(!create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_closetimeid,
          CASE 
            WHEN t_is_cash = 1 then TRUE
            WHEN t_is_cash = 0 then FALSE
            ELSE cast(null as BOOLEAN) END AS cashflag,
          t_st_id,
          t_tt_id,
          t_s_symb,
          quantity,
          bidprice,
          t_ca_id,
          executedby,
          tradeprice,
          fee,
          commission,
          tax,
          t.batchid
        FROM (
          SELECT
            t_id tradeid,
            th_dts t_dts,
            t_st_id,
            t_tt_id,
            t_is_cash,
            t_s_symb,
            t_qty AS quantity,
            t_bid_price AS bidprice,
            t_ca_id,
            t_exec_name AS executedby,
            t_trade_price AS tradeprice,
            t_chrg AS fee,
            t_comm AS commission,
            t_tax AS tax,
            1 batchid,
            CASE 
              WHEN (th_st_id == "SBMT" AND t_tt_id IN ("TMB", "TMS")) OR th_st_id = "PNDG" THEN TRUE 
              WHEN th_st_id IN ("CMPT", "CNCL") THEN FALSE 
              ELSE cast(null as boolean) END AS create_flg
          FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistory` t
          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistoryRaw` th
            ON th_t_id = t_id
          UNION ALL
          SELECT
            t_id tradeid,
            t_dts,
            t_st_id,
            t_tt_id,
            t_is_cash,
            t_s_symb,
            t_qty AS quantity,
            t_bid_price AS bidprice,
            t_ca_id,
            t_exec_name AS executedby,
            t_trade_price AS tradeprice,
            t_chrg AS fee,
            t_comm AS commission,
            t_tax AS tax,
            t.batchid,
            CASE 
              WHEN cdc_flag = 'I' THEN TRUE 
              WHEN t_st_id IN ("CMPT", "CNCL") THEN FALSE 
              ELSE cast(null as boolean) END AS create_flg
          FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeIncremental` t
        ) t
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` dd
          ON date(t.t_dts) = dd.datevalue
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimTime` dt
          ON date_format(t.t_dts, 'HH:mm:ss') = dt.timevalue
      )
    )
    QUALIFY ROW_NUMBER() OVER (PARTITION BY tradeid ORDER BY t_dts desc) = 1
  ) trade
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` status
    ON status.st_id = trade.t_st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeType` tt
    ON tt.tt_id == trade.t_tt_id
  -- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Account IDs are missing from DimSecurity/DimAccount, causing audit check failures. 
  ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` ds
  ^^^
    ON 
      ds.symbol = trade.t_s_symb
      AND createdate >= ds.effectivedate 
      AND createdate < ds.enddate
  ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimAccount` da
    ON 
      trade.t_ca_id = da.accountid 
      AND createdate >= da.effectivedate 
      AND createdate < da.enddate
    
  
2023-04-20 12:19:52.699242 (Thread-1): 12:19:52  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe9467f1370>]}
2023-04-20 12:19:52.699714 (Thread-1): 12:19:52  9 of 15 ERROR creating sql table model dbt_shabbirkdb.DimTrade ................. [ERROR in 1.06s]
2023-04-20 12:19:52.699927 (Thread-1): 12:19:52  Finished running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 12:19:52.700661 (Thread-1): 12:19:52  Began running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 12:19:52.700912 (Thread-1): 12:19:52  11 of 15 SKIP relation dbt_shabbirkdb.FactHoldings ............................. [SKIP]
2023-04-20 12:19:52.701140 (Thread-1): 12:19:52  Finished running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 12:19:52.782217 (Thread-3): 12:19:52  Runtime Error in model FactCashBalances (models/incremental/FactCashBalances.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `a`.`sk_accountid` cannot be resolved. Did you mean one of the following? [`a`.`accountid`, `c`.`accountid`, `a`.`sk_brokerid`, `a`.`accountdesc`, `a`.`sk_customerid`].; line 20 pos 2
2023-04-20 12:19:52.782651 (Thread-3): 12:19:52  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe9bc400cd0>]}
2023-04-20 12:19:52.783098 (Thread-3): 12:19:52  10 of 15 ERROR creating sql table model dbt_shabbirkdb.FactCashBalances ........ [ERROR in 1.14s]
2023-04-20 12:19:52.783323 (Thread-3): 12:19:52  Finished running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 12:19:52.901576 (Thread-1046): handling ps request
2023-04-20 12:19:52.902096 (Thread-1046): 12:19:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437f36ac0>]}
2023-04-20 12:19:52.903903 (Thread-1046): sending response (<Response 26394 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:53.369784 (Thread-1047): handling status request
2023-04-20 12:19:53.370295 (Thread-1047): 12:19:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437f36d00>]}
2023-04-20 12:19:53.370876 (Thread-1047): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:53.419594 (Thread-1048): handling poll request
2023-04-20 12:19:53.419978 (Thread-1048): 12:19:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437f36040>]}
2023-04-20 12:19:53.420792 (Thread-1048): sending response (<Response 48470 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:53.602411 (Thread-2): 12:19:53  SQL status: OK in 6.37 seconds
2023-04-20 12:19:53.910642 (Thread-2): 12:19:53  Timing info for model.dbsql_dbt_tpch.Financial (execute): 2023-04-20 12:19:46.471252 => 2023-04-20 12:19:53.910457
2023-04-20 12:19:53.910912 (Thread-2): 12:19:53  On model.dbsql_dbt_tpch.Financial: ROLLBACK
2023-04-20 12:19:53.911063 (Thread-2): 12:19:53  Databricks adapter: NotImplemented: rollback
2023-04-20 12:19:53.911184 (Thread-2): 12:19:53  On model.dbsql_dbt_tpch.Financial: Close
2023-04-20 12:19:54.179979 (Thread-2): 12:19:54  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe9bc4cfd00>]}
2023-04-20 12:19:54.180499 (Thread-2): 12:19:54  6 of 15 OK created sql table model dbt_shabbirkdb.Financial .................... [OK in 7.73s]
2023-04-20 12:19:54.180734 (Thread-2): 12:19:54  Finished running node model.dbsql_dbt_tpch.Financial
2023-04-20 12:19:54.181615 (Thread-1): 12:19:54  Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:19:54.181958 (Thread-1): 12:19:54  12 of 15 START sql table model dbt_shabbirkdb.tempSumpFiBasicEps ............... [RUN]
2023-04-20 12:19:54.182470 (Thread-1): 12:19:54  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempSumpFiBasicEps'
2023-04-20 12:19:54.182651 (Thread-1): 12:19:54  Began compiling node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:19:54.187295 (Thread-1): 12:19:54  Writing injected SQL for node "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:19:54.203988 (Thread-1): 12:19:54  Timing info for model.dbsql_dbt_tpch.tempSumpFiBasicEps (compile): 2023-04-20 12:19:54.182699 => 2023-04-20 12:19:54.203828
2023-04-20 12:19:54.204205 (Thread-1): 12:19:54  Began executing node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:19:54.208482 (Thread-1): 12:19:54  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:19:54.208658 (Thread-1): 12:19:54  Using databricks connection "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:19:54.208817 (Thread-1): 12:19:54  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempSumpFiBasicEps"} */

      describe extended `dbt_shabbirkdb`.`tempsumpfibasiceps`
  
2023-04-20 12:19:54.208946 (Thread-1): 12:19:54  Opening a new connection, currently in state closed
2023-04-20 12:19:54.937173 (Thread-1): 12:19:54  SQL status: OK in 0.73 seconds
2023-04-20 12:19:54.946302 (Thread-1): 12:19:54  Writing runtime sql for node "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:19:54.962406 (Thread-1): 12:19:54  Using databricks connection "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:19:54.962787 (Thread-1): 12:19:54  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempSumpFiBasicEps"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempSumpFiBasicEps`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  sk_companyid,
  fi_qtr_start_date,
  sum(fi_basic_eps) OVER (PARTITION BY companyid ORDER BY fi_qtr_start_date ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) - fi_basic_eps sum_fi_basic_eps
FROM `dbt_shabbirkdb`.`Financial`
JOIN `dbt_shabbirkdb`.`DimCompany`
  USING (sk_companyid);
  
2023-04-20 12:19:54.979152 (Thread-4): 12:19:54  SQL status: OK in 3.92 seconds
2023-04-20 12:19:54.981502 (Thread-4): 12:19:54  Timing info for model.dbsql_dbt_tpch.Prospect (execute): 2023-04-20 12:19:50.311695 => 2023-04-20 12:19:54.981354
2023-04-20 12:19:54.981701 (Thread-4): 12:19:54  On model.dbsql_dbt_tpch.Prospect: ROLLBACK
2023-04-20 12:19:54.981842 (Thread-4): 12:19:54  Databricks adapter: NotImplemented: rollback
2023-04-20 12:19:54.981979 (Thread-4): 12:19:54  On model.dbsql_dbt_tpch.Prospect: Close
2023-04-20 12:19:55.102457 (Thread-1049): handling poll request
2023-04-20 12:19:55.102949 (Thread-1049): 12:19:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437f05880>]}
2023-04-20 12:19:55.103745 (Thread-1049): sending response (<Response 12243 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:55.263612 (Thread-4): 12:19:55  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe94666a9a0>]}
2023-04-20 12:19:55.264176 (Thread-4): 12:19:55  8 of 15 OK created sql table model dbt_shabbirkdb.Prospect ..................... [OK in 4.97s]
2023-04-20 12:19:55.264416 (Thread-4): 12:19:55  Finished running node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:19:55.265360 (Thread-2): 12:19:55  Began running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:19:55.265698 (Thread-2): 12:19:55  13 of 15 START sql table model dbt_shabbirkdb.DimCustomer ...................... [RUN]
2023-04-20 12:19:55.266220 (Thread-2): 12:19:55  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomer'
2023-04-20 12:19:55.266404 (Thread-2): 12:19:55  Began compiling node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:19:55.271244 (Thread-2): 12:19:55  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 12:19:55.284753 (Thread-2): 12:19:55  Timing info for model.dbsql_dbt_tpch.DimCustomer (compile): 2023-04-20 12:19:55.266451 => 2023-04-20 12:19:55.284575
2023-04-20 12:19:55.284983 (Thread-2): 12:19:55  Began executing node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:19:55.289406 (Thread-2): 12:19:55  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:19:55.289573 (Thread-2): 12:19:55  Using databricks connection "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 12:19:55.289729 (Thread-2): 12:19:55  On model.dbsql_dbt_tpch.DimCustomer: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

      describe extended `dbt_shabbirkdb`.`dimcustomer`
  
2023-04-20 12:19:55.289871 (Thread-2): 12:19:55  Opening a new connection, currently in state closed
2023-04-20 12:19:55.531200 (Thread-1050): handling ps request
2023-04-20 12:19:55.531704 (Thread-1050): 12:19:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437f05ee0>]}
2023-04-20 12:19:55.533653 (Thread-1050): sending response (<Response 26394 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:55.988857 (Thread-1051): handling status request
2023-04-20 12:19:55.989357 (Thread-1051): 12:19:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437f05f40>]}
2023-04-20 12:19:55.990017 (Thread-1051): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:56.029303 (Thread-2): 12:19:56  SQL status: OK in 0.74 seconds
2023-04-20 12:19:56.035045 (Thread-2): 12:19:56  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 12:19:56.050278 (Thread-2): 12:19:56  Using databricks connection "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 12:19:56.050517 (Thread-2): 12:19:56  On model.dbsql_dbt_tpch.DimCustomer: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 12:19:56.737540 (Thread-1052): handling poll request
2023-04-20 12:19:56.738042 (Thread-1052): 12:19:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437f36880>]}
2023-04-20 12:19:56.738723 (Thread-1052): sending response (<Response 9277 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:58.253120 (Thread-1053): handling ps request
2023-04-20 12:19:58.253610 (Thread-1053): 12:19:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437f368b0>]}
2023-04-20 12:19:58.255410 (Thread-1053): sending response (<Response 26394 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:58.272734 (Thread-1): 12:19:58  SQL status: OK in 3.31 seconds
2023-04-20 12:19:58.276346 (Thread-1): 12:19:58  Timing info for model.dbsql_dbt_tpch.tempSumpFiBasicEps (execute): 2023-04-20 12:19:54.204256 => 2023-04-20 12:19:58.276122
2023-04-20 12:19:58.276670 (Thread-1): 12:19:58  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: ROLLBACK
2023-04-20 12:19:58.276907 (Thread-1): 12:19:58  Databricks adapter: NotImplemented: rollback
2023-04-20 12:19:58.277117 (Thread-1): 12:19:58  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: Close
2023-04-20 12:19:58.366244 (Thread-1054): handling poll request
2023-04-20 12:19:58.366717 (Thread-1054): 12:19:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437ee2670>]}
2023-04-20 12:19:58.367254 (Thread-1054): sending response (<Response 2205 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:58.573961 (Thread-1): 12:19:58  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe9467a5c70>]}
2023-04-20 12:19:58.574498 (Thread-1): 12:19:58  12 of 15 OK created sql table model dbt_shabbirkdb.tempSumpFiBasicEps .......... [OK in 4.39s]
2023-04-20 12:19:58.574738 (Thread-1): 12:19:58  Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:19:58.575676 (Thread-4): 12:19:58  Began running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:19:58.576017 (Thread-4): 12:19:58  14 of 15 START sql table model dbt_shabbirkdb.FactMarketHistory ................ [RUN]
2023-04-20 12:19:58.576554 (Thread-4): 12:19:58  Acquiring new databricks connection 'model.dbsql_dbt_tpch.FactMarketHistory'
2023-04-20 12:19:58.576742 (Thread-4): 12:19:58  Began compiling node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:19:58.581626 (Thread-4): 12:19:58  Writing injected SQL for node "model.dbsql_dbt_tpch.FactMarketHistory"
2023-04-20 12:19:58.596722 (Thread-4): 12:19:58  Timing info for model.dbsql_dbt_tpch.FactMarketHistory (compile): 2023-04-20 12:19:58.576790 => 2023-04-20 12:19:58.596458
2023-04-20 12:19:58.597109 (Thread-4): 12:19:58  Began executing node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:19:58.602315 (Thread-4): 12:19:58  Writing runtime sql for node "model.dbsql_dbt_tpch.FactMarketHistory"
2023-04-20 12:19:58.616881 (Thread-4): 12:19:58  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:19:58.617075 (Thread-4): 12:19:58  Using databricks connection "model.dbsql_dbt_tpch.FactMarketHistory"
2023-04-20 12:19:58.617396 (Thread-4): 12:19:58  On model.dbsql_dbt_tpch.FactMarketHistory: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date);
  
2023-04-20 12:19:58.617644 (Thread-4): 12:19:58  Opening a new connection, currently in state closed
2023-04-20 12:19:58.758529 (Thread-1055): handling status request
2023-04-20 12:19:58.759022 (Thread-1055): 12:19:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437f16370>]}
2023-04-20 12:19:58.759686 (Thread-1055): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:19:59.289487 (Thread-4): 12:19:59  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date);
  
2023-04-20 12:19:59.289872 (Thread-4): 12:19:59  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date)

2023-04-20 12:19:59.290120 (Thread-4): 12:19:59  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date)

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date)

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 12:19:59.290325 (Thread-4): 12:19:59  Databricks adapter: operation-id: b'\x01\xed\xdfu\xaa\xfe\x18\x87\xbdeCp\x1cS\x85\xe8'
2023-04-20 12:19:59.290701 (Thread-4): 12:19:59  Timing info for model.dbsql_dbt_tpch.FactMarketHistory (execute): 2023-04-20 12:19:58.597203 => 2023-04-20 12:19:59.290497
2023-04-20 12:19:59.291015 (Thread-4): 12:19:59  On model.dbsql_dbt_tpch.FactMarketHistory: ROLLBACK
2023-04-20 12:19:59.291243 (Thread-4): 12:19:59  Databricks adapter: NotImplemented: rollback
2023-04-20 12:19:59.291463 (Thread-4): 12:19:59  On model.dbsql_dbt_tpch.FactMarketHistory: Close
2023-04-20 12:19:59.528337 (Thread-2): 12:19:59  SQL status: OK in 3.48 seconds
2023-04-20 12:19:59.532525 (Thread-2): 12:19:59  Timing info for model.dbsql_dbt_tpch.DimCustomer (execute): 2023-04-20 12:19:55.285037 => 2023-04-20 12:19:59.532363
2023-04-20 12:19:59.532739 (Thread-2): 12:19:59  On model.dbsql_dbt_tpch.DimCustomer: ROLLBACK
2023-04-20 12:19:59.532882 (Thread-2): 12:19:59  Databricks adapter: NotImplemented: rollback
2023-04-20 12:19:59.533001 (Thread-2): 12:19:59  On model.dbsql_dbt_tpch.DimCustomer: Close
2023-04-20 12:19:59.575373 (Thread-4): 12:19:59  Runtime Error in model FactMarketHistory (models/incremental/FactMarketHistory.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */
  
    
      
          create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
        
        
      using delta
        
        
        
        
        
        
        as
        
  SELECT 
    s.sk_securityid,
    s.sk_companyid,
    sk_dateid,
    fmh.dm_close / sum_fi_basic_eps AS peratio,
    (s.dividend / fmh.dm_close) / 100 yield,
    fiftytwoweekhigh,
    sk_fiftytwoweekhighdate,
    fiftytwoweeklow,
    sk_fiftytwoweeklowdate,
    dm_close closeprice,
    dm_high dayhigh,
    dm_low daylow,
    dm_vol volume,
    fmh.batchid
  FROM (
    SELECT * FROM (
      SELECT 
        a.*,
        b.sk_dateid AS sk_fiftytwoweeklowdate,
        c.sk_dateid AS sk_fiftytwoweekhighdate
      FROM
        `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
      JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
        ON
          a.dm_s_symb = b.dm_s_symb
          AND a.fiftytwoweeklow = b.dm_low
          AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
      JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
        ON 
          a.dm_s_symb = c.dm_s_symb
          AND a.fiftytwoweekhigh = c.dm_high
          AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
    QUALIFY ROW_NUMBER() OVER (
      PARTITION BY dm_s_symb, dm_date 
      ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
  -- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
  ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ^^^
    ON 
      s.symbol = fmh.dm_s_symb
      AND fmh.dm_date >= s.effectivedate 
      AND fmh.dm_date < s.enddate
  LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
    ON 
      f.sk_companyid = s.sk_companyid
      AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
      AND year(fmh.dm_date) = year(fi_qtr_start_date)
  
2023-04-20 12:19:59.575845 (Thread-4): 12:19:59  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe946611370>]}
2023-04-20 12:19:59.576316 (Thread-4): 12:19:59  14 of 15 ERROR creating sql table model dbt_shabbirkdb.FactMarketHistory ....... [ERROR in 1.00s]
2023-04-20 12:19:59.576535 (Thread-4): 12:19:59  Finished running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:19:59.813549 (Thread-2): 12:19:59  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe946611490>]}
2023-04-20 12:19:59.814092 (Thread-2): 12:19:59  13 of 15 OK created sql table model dbt_shabbirkdb.DimCustomer ................. [OK in 4.55s]
2023-04-20 12:19:59.814332 (Thread-2): 12:19:59  Finished running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:19:59.815507 (Thread-1): 12:19:59  Began running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 12:19:59.815880 (Thread-1): 12:19:59  15 of 15 START sql table model dbt_shabbirkdb.FactWatches ...................... [RUN]
2023-04-20 12:19:59.816384 (Thread-1): 12:19:59  Acquiring new databricks connection 'model.dbsql_dbt_tpch.FactWatches'
2023-04-20 12:19:59.816585 (Thread-1): 12:19:59  Began compiling node model.dbsql_dbt_tpch.FactWatches
2023-04-20 12:19:59.821782 (Thread-1): 12:19:59  Writing injected SQL for node "model.dbsql_dbt_tpch.FactWatches"
2023-04-20 12:19:59.836171 (Thread-1): 12:19:59  Timing info for model.dbsql_dbt_tpch.FactWatches (compile): 2023-04-20 12:19:59.816637 => 2023-04-20 12:19:59.836003
2023-04-20 12:19:59.836396 (Thread-1): 12:19:59  Began executing node model.dbsql_dbt_tpch.FactWatches
2023-04-20 12:19:59.840493 (Thread-1): 12:19:59  Writing runtime sql for node "model.dbsql_dbt_tpch.FactWatches"
2023-04-20 12:19:59.854908 (Thread-1): 12:19:59  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:19:59.855089 (Thread-1): 12:19:59  Using databricks connection "model.dbsql_dbt_tpch.FactWatches"
2023-04-20 12:19:59.855310 (Thread-1): 12:19:59  On model.dbsql_dbt_tpch.FactWatches: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate;
  
2023-04-20 12:19:59.855444 (Thread-1): 12:19:59  Opening a new connection, currently in state closed
2023-04-20 12:20:00.369808 (Thread-1056): handling poll request
2023-04-20 12:20:00.370302 (Thread-1056): 12:20:00  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437f0e0a0>]}
2023-04-20 12:20:00.371374 (Thread-1056): sending response (<Response 40993 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:20:00.501998 (Thread-1): 12:20:00  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate;
  
2023-04-20 12:20:00.502226 (Thread-1): 12:20:00  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate

2023-04-20 12:20:00.502354 (Thread-1): 12:20:00  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 12:20:00.502468 (Thread-1): 12:20:00  Databricks adapter: operation-id: b'\x01\xed\xdfu\xab\xb8\x14\x19\x95\xda\xb9\xe0\x9a\x1f#\x05'
2023-04-20 12:20:00.502710 (Thread-1): 12:20:00  Timing info for model.dbsql_dbt_tpch.FactWatches (execute): 2023-04-20 12:19:59.836448 => 2023-04-20 12:20:00.502579
2023-04-20 12:20:00.502895 (Thread-1): 12:20:00  On model.dbsql_dbt_tpch.FactWatches: ROLLBACK
2023-04-20 12:20:00.503020 (Thread-1): 12:20:00  Databricks adapter: NotImplemented: rollback
2023-04-20 12:20:00.503139 (Thread-1): 12:20:00  On model.dbsql_dbt_tpch.FactWatches: Close
2023-04-20 12:20:00.784456 (Thread-1): 12:20:00  Runtime Error in model FactWatches (models/incremental/FactWatches.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */
  
    
      
          create or replace table `dbt_shabbirkdb`.`FactWatches`
        
        
      using delta
        
        
        
        
        
        
        as
        
  SELECT
    c.sk_customerid sk_customerid,
    s.sk_securityid sk_securityid,
    sk_dateid_dateplaced,
    sk_dateid_dateremoved,
    wh.batchid
  FROM (
    SELECT * EXCEPT(w_dts)
    FROM (
      SELECT
        customerid,
        symbol,
        coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
          PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
        coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
          PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
        coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
          PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
        w_dts,
        coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
          PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
      FROM ( 
        SELECT 
          wh.w_c_id customerid,
          wh.w_s_symb symbol,
          if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
          if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
          if(w_action = 'ACTV', d.datevalue, null) dateplaced,
          wh.w_dts,
          batchid 
        FROM (
          SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
          UNION ALL
          SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
          ON d.datevalue = date(wh.w_dts)))
    QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
  -- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
  ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ^^^
    ON 
      s.symbol = wh.symbol
      AND wh.dateplaced >= s.effectivedate 
      AND wh.dateplaced < s.enddate
  ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
    ON
      wh.customerid = c.customerid
      AND wh.dateplaced >= c.effectivedate 
      AND wh.dateplaced < c.enddate
  
2023-04-20 12:20:00.785212 (Thread-1): 12:20:00  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb097b1-a1f2-4df1-a1e2-ae9f328d8723', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe9467bb430>]}
2023-04-20 12:20:00.785939 (Thread-1): 12:20:00  15 of 15 ERROR creating sql table model dbt_shabbirkdb.FactWatches ............. [ERROR in 0.97s]
2023-04-20 12:20:00.786267 (Thread-1): 12:20:00  Finished running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 12:20:00.788167 (MainThread): 12:20:00  Acquiring new databricks connection 'master'
2023-04-20 12:20:00.788398 (MainThread): 12:20:00  On master: ROLLBACK
2023-04-20 12:20:00.788552 (MainThread): 12:20:00  Opening a new connection, currently in state init
2023-04-20 12:20:00.862523 (Thread-1057): handling ps request
2023-04-20 12:20:00.863013 (Thread-1057): 12:20:00  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437f0e730>]}
2023-04-20 12:20:00.864856 (Thread-1057): sending response (<Response 26393 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:20:01.081956 (MainThread): 12:20:01  Databricks adapter: NotImplemented: rollback
2023-04-20 12:20:01.082211 (MainThread): 12:20:01  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:20:01.082347 (MainThread): 12:20:01  Spark adapter: NotImplemented: commit
2023-04-20 12:20:01.082512 (MainThread): 12:20:01  On master: ROLLBACK
2023-04-20 12:20:01.082646 (MainThread): 12:20:01  Databricks adapter: NotImplemented: rollback
2023-04-20 12:20:01.082782 (MainThread): 12:20:01  On master: Close
2023-04-20 12:20:01.332783 (Thread-1058): handling status request
2023-04-20 12:20:01.333285 (Thread-1058): 12:20:01  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437f0eb50>]}
2023-04-20 12:20:01.333887 (Thread-1058): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:20:01.364467 (MainThread): 12:20:01  Connection 'master' was properly closed.
2023-04-20 12:20:01.364673 (MainThread): 12:20:01  Connection 'model.dbsql_dbt_tpch.FactWatches' was properly closed.
2023-04-20 12:20:01.364780 (MainThread): 12:20:01  Connection 'model.dbsql_dbt_tpch.DimCustomer' was properly closed.
2023-04-20 12:20:01.364879 (MainThread): 12:20:01  Connection 'model.dbsql_dbt_tpch.FactCashBalances' was properly closed.
2023-04-20 12:20:01.364978 (MainThread): 12:20:01  Connection 'model.dbsql_dbt_tpch.FactMarketHistory' was properly closed.
2023-04-20 12:20:01.365299 (MainThread): 12:20:01  
2023-04-20 12:20:01.365450 (MainThread): 12:20:01  Finished running 15 table models in 0 hours 0 minutes and 22.08 seconds (22.08s).
2023-04-20 12:20:01.460635 (MainThread): 12:20:01  
2023-04-20 12:20:01.460962 (MainThread): 12:20:01  Completed with 4 errors and 0 warnings:
2023-04-20 12:20:01.461114 (MainThread): 12:20:01  
2023-04-20 12:20:01.461285 (MainThread): 12:20:01  Runtime Error in model DimTrade (models/incremental/DimTrade.sql)
2023-04-20 12:20:01.461433 (MainThread): 12:20:01    
2023-04-20 12:20:01.461549 (MainThread): 12:20:01    [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 151, pos 0)
2023-04-20 12:20:01.461656 (MainThread): 12:20:01    
2023-04-20 12:20:01.461761 (MainThread): 12:20:01    == SQL ==
2023-04-20 12:20:01.461872 (MainThread): 12:20:01    /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimTrade"} */
2023-04-20 12:20:01.461979 (MainThread): 12:20:01    
2023-04-20 12:20:01.462081 (MainThread): 12:20:01      
2023-04-20 12:20:01.462182 (MainThread): 12:20:01        
2023-04-20 12:20:01.462283 (MainThread): 12:20:01            create or replace table `dbt_shabbirkdb`.`DimTrade`
2023-04-20 12:20:01.462388 (MainThread): 12:20:01          
2023-04-20 12:20:01.462509 (MainThread): 12:20:01          
2023-04-20 12:20:01.462610 (MainThread): 12:20:01        using delta
2023-04-20 12:20:01.462709 (MainThread): 12:20:01          
2023-04-20 12:20:01.462808 (MainThread): 12:20:01          
2023-04-20 12:20:01.462907 (MainThread): 12:20:01          
2023-04-20 12:20:01.463005 (MainThread): 12:20:01          
2023-04-20 12:20:01.463101 (MainThread): 12:20:01          
2023-04-20 12:20:01.463197 (MainThread): 12:20:01          
2023-04-20 12:20:01.463298 (MainThread): 12:20:01          as
2023-04-20 12:20:01.463396 (MainThread): 12:20:01          
2023-04-20 12:20:01.463494 (MainThread): 12:20:01    SELECT
2023-04-20 12:20:01.463649 (MainThread): 12:20:01      trade.tradeid,
2023-04-20 12:20:01.463772 (MainThread): 12:20:01      sk_brokerid,
2023-04-20 12:20:01.463898 (MainThread): 12:20:01      trade.sk_createdateid,
2023-04-20 12:20:01.464014 (MainThread): 12:20:01      trade.sk_createtimeid,
2023-04-20 12:20:01.464125 (MainThread): 12:20:01      trade.sk_closedateid,
2023-04-20 12:20:01.464225 (MainThread): 12:20:01      trade.sk_closetimeid,
2023-04-20 12:20:01.464323 (MainThread): 12:20:01      st_name status,
2023-04-20 12:20:01.464420 (MainThread): 12:20:01      tt_name type,
2023-04-20 12:20:01.464518 (MainThread): 12:20:01      trade.cashflag,
2023-04-20 12:20:01.464615 (MainThread): 12:20:01      sk_securityid,
2023-04-20 12:20:01.464711 (MainThread): 12:20:01      sk_companyid,
2023-04-20 12:20:01.464807 (MainThread): 12:20:01      trade.quantity,
2023-04-20 12:20:01.464903 (MainThread): 12:20:01      trade.bidprice,
2023-04-20 12:20:01.464998 (MainThread): 12:20:01      sk_customerid,
2023-04-20 12:20:01.465094 (MainThread): 12:20:01      sk_accountid,
2023-04-20 12:20:01.465190 (MainThread): 12:20:01      trade.executedby,
2023-04-20 12:20:01.465289 (MainThread): 12:20:01      trade.tradeprice,
2023-04-20 12:20:01.465385 (MainThread): 12:20:01      trade.fee,
2023-04-20 12:20:01.465480 (MainThread): 12:20:01      trade.commission,
2023-04-20 12:20:01.465574 (MainThread): 12:20:01      trade.tax,
2023-04-20 12:20:01.465668 (MainThread): 12:20:01      trade.batchid
2023-04-20 12:20:01.465762 (MainThread): 12:20:01    FROM (
2023-04-20 12:20:01.466032 (MainThread): 12:20:01      SELECT * EXCEPT(t_dts)
2023-04-20 12:20:01.466153 (MainThread): 12:20:01      FROM (
2023-04-20 12:20:01.466253 (MainThread): 12:20:01        SELECT
2023-04-20 12:20:01.466350 (MainThread): 12:20:01          tradeid,
2023-04-20 12:20:01.466449 (MainThread): 12:20:01          min(date(t_dts)) OVER (PARTITION BY tradeid) createdate,
2023-04-20 12:20:01.466546 (MainThread): 12:20:01          t_dts,
2023-04-20 12:20:01.466642 (MainThread): 12:20:01          coalesce(sk_createdateid, last_value(sk_createdateid) IGNORE NULLS OVER (
2023-04-20 12:20:01.466738 (MainThread): 12:20:01            PARTITION BY tradeid ORDER BY t_dts)) sk_createdateid,
2023-04-20 12:20:01.466834 (MainThread): 12:20:01          coalesce(sk_createtimeid, last_value(sk_createtimeid) IGNORE NULLS OVER (
2023-04-20 12:20:01.466928 (MainThread): 12:20:01            PARTITION BY tradeid ORDER BY t_dts)) sk_createtimeid,
2023-04-20 12:20:01.467024 (MainThread): 12:20:01          coalesce(sk_closedateid, last_value(sk_closedateid) IGNORE NULLS OVER (
2023-04-20 12:20:01.467118 (MainThread): 12:20:01            PARTITION BY tradeid ORDER BY t_dts)) sk_closedateid,
2023-04-20 12:20:01.467213 (MainThread): 12:20:01          coalesce(sk_closetimeid, last_value(sk_closetimeid) IGNORE NULLS OVER (
2023-04-20 12:20:01.467307 (MainThread): 12:20:01            PARTITION BY tradeid ORDER BY t_dts)) sk_closetimeid,
2023-04-20 12:20:01.467402 (MainThread): 12:20:01          cashflag,
2023-04-20 12:20:01.467496 (MainThread): 12:20:01          t_st_id,
2023-04-20 12:20:01.467660 (MainThread): 12:20:01          t_tt_id,
2023-04-20 12:20:01.467766 (MainThread): 12:20:01          t_s_symb,
2023-04-20 12:20:01.467863 (MainThread): 12:20:01          quantity,
2023-04-20 12:20:01.467960 (MainThread): 12:20:01          bidprice,
2023-04-20 12:20:01.468055 (MainThread): 12:20:01          t_ca_id,
2023-04-20 12:20:01.468156 (MainThread): 12:20:01          executedby,
2023-04-20 12:20:01.468254 (MainThread): 12:20:01          tradeprice,
2023-04-20 12:20:01.468349 (MainThread): 12:20:01          fee,
2023-04-20 12:20:01.468445 (MainThread): 12:20:01          commission,
2023-04-20 12:20:01.468541 (MainThread): 12:20:01          tax,
2023-04-20 12:20:01.468636 (MainThread): 12:20:01          batchid
2023-04-20 12:20:01.468731 (MainThread): 12:20:01        FROM (
2023-04-20 12:20:01.468824 (MainThread): 12:20:01          SELECT
2023-04-20 12:20:01.468917 (MainThread): 12:20:01            tradeid,
2023-04-20 12:20:01.469011 (MainThread): 12:20:01            t_dts,
2023-04-20 12:20:01.469104 (MainThread): 12:20:01            if(create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_createdateid,
2023-04-20 12:20:01.469197 (MainThread): 12:20:01            if(create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_createtimeid,
2023-04-20 12:20:01.469290 (MainThread): 12:20:01            if(!create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_closedateid,
2023-04-20 12:20:01.469384 (MainThread): 12:20:01            if(!create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_closetimeid,
2023-04-20 12:20:01.469479 (MainThread): 12:20:01            CASE 
2023-04-20 12:20:01.469578 (MainThread): 12:20:01              WHEN t_is_cash = 1 then TRUE
2023-04-20 12:20:01.469676 (MainThread): 12:20:01              WHEN t_is_cash = 0 then FALSE
2023-04-20 12:20:01.469773 (MainThread): 12:20:01              ELSE cast(null as BOOLEAN) END AS cashflag,
2023-04-20 12:20:01.469880 (MainThread): 12:20:01            t_st_id,
2023-04-20 12:20:01.469980 (MainThread): 12:20:01            t_tt_id,
2023-04-20 12:20:01.470075 (MainThread): 12:20:01            t_s_symb,
2023-04-20 12:20:01.470170 (MainThread): 12:20:01            quantity,
2023-04-20 12:20:01.470265 (MainThread): 12:20:01            bidprice,
2023-04-20 12:20:01.470361 (MainThread): 12:20:01            t_ca_id,
2023-04-20 12:20:01.470455 (MainThread): 12:20:01            executedby,
2023-04-20 12:20:01.470550 (MainThread): 12:20:01            tradeprice,
2023-04-20 12:20:01.470645 (MainThread): 12:20:01            fee,
2023-04-20 12:20:01.470847 (MainThread): 12:20:01            commission,
2023-04-20 12:20:01.470952 (MainThread): 12:20:01            tax,
2023-04-20 12:20:01.471062 (MainThread): 12:20:01            t.batchid
2023-04-20 12:20:01.471289 (MainThread): 12:20:01          FROM (
2023-04-20 12:20:01.471396 (MainThread): 12:20:01            SELECT
2023-04-20 12:20:01.471511 (MainThread): 12:20:01              t_id tradeid,
2023-04-20 12:20:01.471652 (MainThread): 12:20:01              th_dts t_dts,
2023-04-20 12:20:01.471758 (MainThread): 12:20:01              t_st_id,
2023-04-20 12:20:01.471859 (MainThread): 12:20:01              t_tt_id,
2023-04-20 12:20:01.471960 (MainThread): 12:20:01              t_is_cash,
2023-04-20 12:20:01.472061 (MainThread): 12:20:01              t_s_symb,
2023-04-20 12:20:01.472161 (MainThread): 12:20:01              t_qty AS quantity,
2023-04-20 12:20:01.472259 (MainThread): 12:20:01              t_bid_price AS bidprice,
2023-04-20 12:20:01.472355 (MainThread): 12:20:01              t_ca_id,
2023-04-20 12:20:01.472451 (MainThread): 12:20:01              t_exec_name AS executedby,
2023-04-20 12:20:01.472551 (MainThread): 12:20:01              t_trade_price AS tradeprice,
2023-04-20 12:20:01.472674 (MainThread): 12:20:01              t_chrg AS fee,
2023-04-20 12:20:01.472776 (MainThread): 12:20:01              t_comm AS commission,
2023-04-20 12:20:01.472873 (MainThread): 12:20:01              t_tax AS tax,
2023-04-20 12:20:01.472968 (MainThread): 12:20:01              1 batchid,
2023-04-20 12:20:01.473062 (MainThread): 12:20:01              CASE 
2023-04-20 12:20:01.473158 (MainThread): 12:20:01                WHEN (th_st_id == "SBMT" AND t_tt_id IN ("TMB", "TMS")) OR th_st_id = "PNDG" THEN TRUE 
2023-04-20 12:20:01.473254 (MainThread): 12:20:01                WHEN th_st_id IN ("CMPT", "CNCL") THEN FALSE 
2023-04-20 12:20:01.473350 (MainThread): 12:20:01                ELSE cast(null as boolean) END AS create_flg
2023-04-20 12:20:01.473447 (MainThread): 12:20:01            FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistory` t
2023-04-20 12:20:01.473543 (MainThread): 12:20:01            JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistoryRaw` th
2023-04-20 12:20:01.473639 (MainThread): 12:20:01              ON th_t_id = t_id
2023-04-20 12:20:01.473735 (MainThread): 12:20:01            UNION ALL
2023-04-20 12:20:01.473832 (MainThread): 12:20:01            SELECT
2023-04-20 12:20:01.473944 (MainThread): 12:20:01              t_id tradeid,
2023-04-20 12:20:01.474044 (MainThread): 12:20:01              t_dts,
2023-04-20 12:20:01.474141 (MainThread): 12:20:01              t_st_id,
2023-04-20 12:20:01.474237 (MainThread): 12:20:01              t_tt_id,
2023-04-20 12:20:01.474332 (MainThread): 12:20:01              t_is_cash,
2023-04-20 12:20:01.474426 (MainThread): 12:20:01              t_s_symb,
2023-04-20 12:20:01.474519 (MainThread): 12:20:01              t_qty AS quantity,
2023-04-20 12:20:01.474614 (MainThread): 12:20:01              t_bid_price AS bidprice,
2023-04-20 12:20:01.474711 (MainThread): 12:20:01              t_ca_id,
2023-04-20 12:20:01.474806 (MainThread): 12:20:01              t_exec_name AS executedby,
2023-04-20 12:20:01.474902 (MainThread): 12:20:01              t_trade_price AS tradeprice,
2023-04-20 12:20:01.474999 (MainThread): 12:20:01              t_chrg AS fee,
2023-04-20 12:20:01.475100 (MainThread): 12:20:01              t_comm AS commission,
2023-04-20 12:20:01.475199 (MainThread): 12:20:01              t_tax AS tax,
2023-04-20 12:20:01.475297 (MainThread): 12:20:01              t.batchid,
2023-04-20 12:20:01.475397 (MainThread): 12:20:01              CASE 
2023-04-20 12:20:01.475495 (MainThread): 12:20:01                WHEN cdc_flag = 'I' THEN TRUE 
2023-04-20 12:20:01.475616 (MainThread): 12:20:01                WHEN t_st_id IN ("CMPT", "CNCL") THEN FALSE 
2023-04-20 12:20:01.475715 (MainThread): 12:20:01                ELSE cast(null as boolean) END AS create_flg
2023-04-20 12:20:01.475811 (MainThread): 12:20:01            FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeIncremental` t
2023-04-20 12:20:01.475907 (MainThread): 12:20:01          ) t
2023-04-20 12:20:01.476003 (MainThread): 12:20:01          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` dd
2023-04-20 12:20:01.476099 (MainThread): 12:20:01            ON date(t.t_dts) = dd.datevalue
2023-04-20 12:20:01.476196 (MainThread): 12:20:01          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimTime` dt
2023-04-20 12:20:01.476405 (MainThread): 12:20:01            ON date_format(t.t_dts, 'HH:mm:ss') = dt.timevalue
2023-04-20 12:20:01.476514 (MainThread): 12:20:01        )
2023-04-20 12:20:01.476615 (MainThread): 12:20:01      )
2023-04-20 12:20:01.476715 (MainThread): 12:20:01      QUALIFY ROW_NUMBER() OVER (PARTITION BY tradeid ORDER BY t_dts desc) = 1
2023-04-20 12:20:01.476813 (MainThread): 12:20:01    ) trade
2023-04-20 12:20:01.476911 (MainThread): 12:20:01    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` status
2023-04-20 12:20:01.477009 (MainThread): 12:20:01      ON status.st_id = trade.t_st_id
2023-04-20 12:20:01.477107 (MainThread): 12:20:01    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeType` tt
2023-04-20 12:20:01.477205 (MainThread): 12:20:01      ON tt.tt_id == trade.t_tt_id
2023-04-20 12:20:01.477302 (MainThread): 12:20:01    -- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Account IDs are missing from DimSecurity/DimAccount, causing audit check failures. 
2023-04-20 12:20:01.477399 (MainThread): 12:20:01    ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` ds
2023-04-20 12:20:01.477495 (MainThread): 12:20:01    ^^^
2023-04-20 12:20:01.477591 (MainThread): 12:20:01      ON 
2023-04-20 12:20:01.477686 (MainThread): 12:20:01        ds.symbol = trade.t_s_symb
2023-04-20 12:20:01.477780 (MainThread): 12:20:01        AND createdate >= ds.effectivedate 
2023-04-20 12:20:01.477885 (MainThread): 12:20:01        AND createdate < ds.enddate
2023-04-20 12:20:01.477983 (MainThread): 12:20:01    ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimAccount` da
2023-04-20 12:20:01.478079 (MainThread): 12:20:01      ON 
2023-04-20 12:20:01.478177 (MainThread): 12:20:01        trade.t_ca_id = da.accountid 
2023-04-20 12:20:01.478272 (MainThread): 12:20:01        AND createdate >= da.effectivedate 
2023-04-20 12:20:01.478368 (MainThread): 12:20:01        AND createdate < da.enddate
2023-04-20 12:20:01.478465 (MainThread): 12:20:01      
2023-04-20 12:20:01.478563 (MainThread): 12:20:01    
2023-04-20 12:20:01.478689 (MainThread): 12:20:01  
2023-04-20 12:20:01.478824 (MainThread): 12:20:01  Runtime Error in model FactCashBalances (models/incremental/FactCashBalances.sql)
2023-04-20 12:20:01.478934 (MainThread): 12:20:01    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `a`.`sk_accountid` cannot be resolved. Did you mean one of the following? [`a`.`accountid`, `c`.`accountid`, `a`.`sk_brokerid`, `a`.`accountdesc`, `a`.`sk_customerid`].; line 20 pos 2
2023-04-20 12:20:01.479042 (MainThread): 12:20:01  
2023-04-20 12:20:01.479166 (MainThread): 12:20:01  Runtime Error in model FactMarketHistory (models/incremental/FactMarketHistory.sql)
2023-04-20 12:20:01.479274 (MainThread): 12:20:01    
2023-04-20 12:20:01.479377 (MainThread): 12:20:01    [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)
2023-04-20 12:20:01.479478 (MainThread): 12:20:01    
2023-04-20 12:20:01.479604 (MainThread): 12:20:01    == SQL ==
2023-04-20 12:20:01.479709 (MainThread): 12:20:01    /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */
2023-04-20 12:20:01.479811 (MainThread): 12:20:01    
2023-04-20 12:20:01.479911 (MainThread): 12:20:01      
2023-04-20 12:20:01.480816 (MainThread): 12:20:01        
2023-04-20 12:20:01.481006 (MainThread): 12:20:01            create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
2023-04-20 12:20:01.481123 (MainThread): 12:20:01          
2023-04-20 12:20:01.481243 (MainThread): 12:20:01          
2023-04-20 12:20:01.481350 (MainThread): 12:20:01        using delta
2023-04-20 12:20:01.481549 (MainThread): 12:20:01          
2023-04-20 12:20:01.481673 (MainThread): 12:20:01          
2023-04-20 12:20:01.481785 (MainThread): 12:20:01          
2023-04-20 12:20:01.481899 (MainThread): 12:20:01          
2023-04-20 12:20:01.482002 (MainThread): 12:20:01          
2023-04-20 12:20:01.482102 (MainThread): 12:20:01          
2023-04-20 12:20:01.482202 (MainThread): 12:20:01          as
2023-04-20 12:20:01.482301 (MainThread): 12:20:01          
2023-04-20 12:20:01.482398 (MainThread): 12:20:01    SELECT 
2023-04-20 12:20:01.482496 (MainThread): 12:20:01      s.sk_securityid,
2023-04-20 12:20:01.482594 (MainThread): 12:20:01      s.sk_companyid,
2023-04-20 12:20:01.482692 (MainThread): 12:20:01      sk_dateid,
2023-04-20 12:20:01.482824 (MainThread): 12:20:01      fmh.dm_close / sum_fi_basic_eps AS peratio,
2023-04-20 12:20:01.482933 (MainThread): 12:20:01      (s.dividend / fmh.dm_close) / 100 yield,
2023-04-20 12:20:01.483033 (MainThread): 12:20:01      fiftytwoweekhigh,
2023-04-20 12:20:01.483133 (MainThread): 12:20:01      sk_fiftytwoweekhighdate,
2023-04-20 12:20:01.483232 (MainThread): 12:20:01      fiftytwoweeklow,
2023-04-20 12:20:01.483330 (MainThread): 12:20:01      sk_fiftytwoweeklowdate,
2023-04-20 12:20:01.483428 (MainThread): 12:20:01      dm_close closeprice,
2023-04-20 12:20:01.483563 (MainThread): 12:20:01      dm_high dayhigh,
2023-04-20 12:20:01.483680 (MainThread): 12:20:01      dm_low daylow,
2023-04-20 12:20:01.483782 (MainThread): 12:20:01      dm_vol volume,
2023-04-20 12:20:01.483905 (MainThread): 12:20:01      fmh.batchid
2023-04-20 12:20:01.484014 (MainThread): 12:20:01    FROM (
2023-04-20 12:20:01.484115 (MainThread): 12:20:01      SELECT * FROM (
2023-04-20 12:20:01.484215 (MainThread): 12:20:01        SELECT 
2023-04-20 12:20:01.484315 (MainThread): 12:20:01          a.*,
2023-04-20 12:20:01.484414 (MainThread): 12:20:01          b.sk_dateid AS sk_fiftytwoweeklowdate,
2023-04-20 12:20:01.484512 (MainThread): 12:20:01          c.sk_dateid AS sk_fiftytwoweekhighdate
2023-04-20 12:20:01.484610 (MainThread): 12:20:01        FROM
2023-04-20 12:20:01.484706 (MainThread): 12:20:01          `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
2023-04-20 12:20:01.484802 (MainThread): 12:20:01        JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
2023-04-20 12:20:01.484897 (MainThread): 12:20:01          ON
2023-04-20 12:20:01.484993 (MainThread): 12:20:01            a.dm_s_symb = b.dm_s_symb
2023-04-20 12:20:01.485090 (MainThread): 12:20:01            AND a.fiftytwoweeklow = b.dm_low
2023-04-20 12:20:01.485187 (MainThread): 12:20:01            AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
2023-04-20 12:20:01.485285 (MainThread): 12:20:01        JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
2023-04-20 12:20:01.485384 (MainThread): 12:20:01          ON 
2023-04-20 12:20:01.485482 (MainThread): 12:20:01            a.dm_s_symb = c.dm_s_symb
2023-04-20 12:20:01.485580 (MainThread): 12:20:01            AND a.fiftytwoweekhigh = c.dm_high
2023-04-20 12:20:01.485676 (MainThread): 12:20:01            AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
2023-04-20 12:20:01.485771 (MainThread): 12:20:01      QUALIFY ROW_NUMBER() OVER (
2023-04-20 12:20:01.485875 (MainThread): 12:20:01        PARTITION BY dm_s_symb, dm_date 
2023-04-20 12:20:01.485976 (MainThread): 12:20:01        ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
2023-04-20 12:20:01.486071 (MainThread): 12:20:01    -- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
2023-04-20 12:20:01.486165 (MainThread): 12:20:01    ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
2023-04-20 12:20:01.486261 (MainThread): 12:20:01    ^^^
2023-04-20 12:20:01.486357 (MainThread): 12:20:01      ON 
2023-04-20 12:20:01.486452 (MainThread): 12:20:01        s.symbol = fmh.dm_s_symb
2023-04-20 12:20:01.486637 (MainThread): 12:20:01        AND fmh.dm_date >= s.effectivedate 
2023-04-20 12:20:01.486752 (MainThread): 12:20:01        AND fmh.dm_date < s.enddate
2023-04-20 12:20:01.486855 (MainThread): 12:20:01    LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
2023-04-20 12:20:01.486955 (MainThread): 12:20:01      ON 
2023-04-20 12:20:01.487052 (MainThread): 12:20:01        f.sk_companyid = s.sk_companyid
2023-04-20 12:20:01.487149 (MainThread): 12:20:01        AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
2023-04-20 12:20:01.487246 (MainThread): 12:20:01        AND year(fmh.dm_date) = year(fi_qtr_start_date)
2023-04-20 12:20:01.487343 (MainThread): 12:20:01    
2023-04-20 12:20:01.487462 (MainThread): 12:20:01  
2023-04-20 12:20:01.487627 (MainThread): 12:20:01  Runtime Error in model FactWatches (models/incremental/FactWatches.sql)
2023-04-20 12:20:01.487741 (MainThread): 12:20:01    
2023-04-20 12:20:01.487844 (MainThread): 12:20:01    [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)
2023-04-20 12:20:01.487945 (MainThread): 12:20:01    
2023-04-20 12:20:01.488060 (MainThread): 12:20:01    == SQL ==
2023-04-20 12:20:01.488167 (MainThread): 12:20:01    /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */
2023-04-20 12:20:01.488264 (MainThread): 12:20:01    
2023-04-20 12:20:01.488360 (MainThread): 12:20:01      
2023-04-20 12:20:01.488456 (MainThread): 12:20:01        
2023-04-20 12:20:01.488553 (MainThread): 12:20:01            create or replace table `dbt_shabbirkdb`.`FactWatches`
2023-04-20 12:20:01.488650 (MainThread): 12:20:01          
2023-04-20 12:20:01.488746 (MainThread): 12:20:01          
2023-04-20 12:20:01.488840 (MainThread): 12:20:01        using delta
2023-04-20 12:20:01.488936 (MainThread): 12:20:01          
2023-04-20 12:20:01.489033 (MainThread): 12:20:01          
2023-04-20 12:20:01.489130 (MainThread): 12:20:01          
2023-04-20 12:20:01.489229 (MainThread): 12:20:01          
2023-04-20 12:20:01.489331 (MainThread): 12:20:01          
2023-04-20 12:20:01.489432 (MainThread): 12:20:01          
2023-04-20 12:20:01.489533 (MainThread): 12:20:01          as
2023-04-20 12:20:01.489632 (MainThread): 12:20:01          
2023-04-20 12:20:01.489732 (MainThread): 12:20:01    SELECT
2023-04-20 12:20:01.489829 (MainThread): 12:20:01      c.sk_customerid sk_customerid,
2023-04-20 12:20:01.489939 (MainThread): 12:20:01      s.sk_securityid sk_securityid,
2023-04-20 12:20:01.490036 (MainThread): 12:20:01      sk_dateid_dateplaced,
2023-04-20 12:20:01.490131 (MainThread): 12:20:01      sk_dateid_dateremoved,
2023-04-20 12:20:01.490226 (MainThread): 12:20:01      wh.batchid
2023-04-20 12:20:01.490321 (MainThread): 12:20:01    FROM (
2023-04-20 12:20:01.490416 (MainThread): 12:20:01      SELECT * EXCEPT(w_dts)
2023-04-20 12:20:01.490511 (MainThread): 12:20:01      FROM (
2023-04-20 12:20:01.490605 (MainThread): 12:20:01        SELECT
2023-04-20 12:20:01.490698 (MainThread): 12:20:01          customerid,
2023-04-20 12:20:01.490793 (MainThread): 12:20:01          symbol,
2023-04-20 12:20:01.490890 (MainThread): 12:20:01          coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
2023-04-20 12:20:01.490987 (MainThread): 12:20:01            PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
2023-04-20 12:20:01.491159 (MainThread): 12:20:01          coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
2023-04-20 12:20:01.491273 (MainThread): 12:20:01            PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
2023-04-20 12:20:01.491389 (MainThread): 12:20:01          coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
2023-04-20 12:20:01.491489 (MainThread): 12:20:01            PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
2023-04-20 12:20:01.491610 (MainThread): 12:20:01          w_dts,
2023-04-20 12:20:01.491809 (MainThread): 12:20:01          coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
2023-04-20 12:20:01.491920 (MainThread): 12:20:01            PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
2023-04-20 12:20:01.492019 (MainThread): 12:20:01        FROM ( 
2023-04-20 12:20:01.492116 (MainThread): 12:20:01          SELECT 
2023-04-20 12:20:01.492212 (MainThread): 12:20:01            wh.w_c_id customerid,
2023-04-20 12:20:01.492309 (MainThread): 12:20:01            wh.w_s_symb symbol,
2023-04-20 12:20:01.492405 (MainThread): 12:20:01            if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
2023-04-20 12:20:01.492500 (MainThread): 12:20:01            if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
2023-04-20 12:20:01.492596 (MainThread): 12:20:01            if(w_action = 'ACTV', d.datevalue, null) dateplaced,
2023-04-20 12:20:01.492694 (MainThread): 12:20:01            wh.w_dts,
2023-04-20 12:20:01.492790 (MainThread): 12:20:01            batchid 
2023-04-20 12:20:01.492886 (MainThread): 12:20:01          FROM (
2023-04-20 12:20:01.492984 (MainThread): 12:20:01            SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
2023-04-20 12:20:01.493079 (MainThread): 12:20:01            UNION ALL
2023-04-20 12:20:01.493182 (MainThread): 12:20:01            SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
2023-04-20 12:20:01.493289 (MainThread): 12:20:01          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
2023-04-20 12:20:01.493387 (MainThread): 12:20:01            ON d.datevalue = date(wh.w_dts)))
2023-04-20 12:20:01.493486 (MainThread): 12:20:01      QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
2023-04-20 12:20:01.493584 (MainThread): 12:20:01    -- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
2023-04-20 12:20:01.493682 (MainThread): 12:20:01    ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
2023-04-20 12:20:01.493779 (MainThread): 12:20:01    ^^^
2023-04-20 12:20:01.493885 (MainThread): 12:20:01      ON 
2023-04-20 12:20:01.493983 (MainThread): 12:20:01        s.symbol = wh.symbol
2023-04-20 12:20:01.494079 (MainThread): 12:20:01        AND wh.dateplaced >= s.effectivedate 
2023-04-20 12:20:01.494175 (MainThread): 12:20:01        AND wh.dateplaced < s.enddate
2023-04-20 12:20:01.494273 (MainThread): 12:20:01    ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
2023-04-20 12:20:01.494371 (MainThread): 12:20:01      ON
2023-04-20 12:20:01.494467 (MainThread): 12:20:01        wh.customerid = c.customerid
2023-04-20 12:20:01.494563 (MainThread): 12:20:01        AND wh.dateplaced >= c.effectivedate 
2023-04-20 12:20:01.494660 (MainThread): 12:20:01        AND wh.dateplaced < c.enddate
2023-04-20 12:20:01.494756 (MainThread): 12:20:01    
2023-04-20 12:20:01.494899 (MainThread): 12:20:01  
2023-04-20 12:20:01.495033 (MainThread): 12:20:01  Done. PASS=10 WARN=0 ERROR=4 SKIP=1 TOTAL=15
2023-04-20 12:20:02.767270 (Thread-1059): handling poll request
2023-04-20 12:20:02.767827 (Thread-1059): 12:20:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437e635b0>]}
2023-04-20 12:20:02.775693 (Thread-1059): sending response (<Response 267709 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:20:03.767773 (Thread-1060): handling ps request
2023-04-20 12:20:03.768275 (Thread-1060): 12:20:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437e63bb0>]}
2023-04-20 12:20:03.770000 (Thread-1060): sending response (<Response 26418 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:20:04.703227 (Thread-1061): handling status request
2023-04-20 12:20:04.703741 (Thread-1061): 12:20:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437e6b0a0>]}
2023-04-20 12:20:04.704349 (Thread-1061): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:21:00.634265 (Thread-1062): handling status request
2023-04-20 12:21:00.634805 (Thread-1062): 12:21:00  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437e500d0>]}
2023-04-20 12:21:00.635397 (Thread-1062): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:21:00.774713 (Thread-1063): handling ps request
2023-04-20 12:21:00.775204 (Thread-1063): 12:21:00  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437f0e760>]}
2023-04-20 12:21:00.776884 (Thread-1063): sending response (<Response 26418 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:21:53.775252 (Thread-1064): 12:21:53  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 12:21:53.795257 (Thread-1064): 12:21:53  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 12:21:53.795734 (Thread-1064): 12:21:53  previous checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, current checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886
2023-04-20 12:21:53.795962 (Thread-1064): 12:21:53  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437e76610>]}
2023-04-20 12:21:54.266426 (Thread-1065): handling status request
2023-04-20 12:21:54.282227 (Thread-1065): 12:21:54  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437d01070>]}
2023-04-20 12:21:54.298029 (Thread-1065): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:21:55.591298 (Thread-1064): 12:21:55  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 12:21:55.594903 (Thread-1064): 12:21:55  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 12:21:55.598099 (Thread-1064): 12:21:55  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 12:21:55.600882 (Thread-1064): 12:21:55  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 12:21:55.603814 (Thread-1064): 12:21:55  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 12:21:55.606566 (Thread-1064): 12:21:55  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 12:21:55.609750 (Thread-1064): 12:21:55  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 12:21:55.613051 (Thread-1064): 12:21:55  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 12:21:55.616014 (Thread-1064): 12:21:55  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 12:21:55.618807 (Thread-1064): 12:21:55  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 12:21:55.622125 (Thread-1064): 12:21:55  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 12:21:55.624994 (Thread-1064): 12:21:55  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 12:21:55.627692 (Thread-1064): 12:21:55  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 12:21:55.630553 (Thread-1064): 12:21:55  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 12:21:55.633636 (Thread-1064): 12:21:55  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 12:21:55.785758 (Thread-1064): 12:21:55  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5085a6d30>]}
2023-04-20 12:21:56.043243 (Thread-1066): handling status request
2023-04-20 12:21:56.043782 (Thread-1066): 12:21:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5085a6670>]}
2023-04-20 12:21:56.044420 (Thread-1066): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:21:58.350380 (Thread-1067): handling run_sql request
2023-04-20 12:21:58.350924 (Thread-1067): 12:21:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a068b0>]}
2023-04-20 12:21:58.352578 (Thread-1068): handling ps request
2023-04-20 12:21:58.353057 (Thread-1068): 12:21:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528161a90>]}
2023-04-20 12:21:58.354981 (Thread-1068): sending response (<Response 26941 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:21:58.966645 (Thread-1069): handling poll request
2023-04-20 12:21:58.967600 (Thread-1069): 12:21:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5281933d0>]}
2023-04-20 12:21:58.968486 (Thread-1069): sending response (<Response 432 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:21:59.393816 (Thread-1070): handling status request
2023-04-20 12:21:59.394297 (Thread-1070): 12:21:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528193d60>]}
2023-04-20 12:21:59.395103 (Thread-1070): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:01.072070 (Thread-1067): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:01.107995 (MainThread): 12:22:01  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3f0ea073-7e53-420e-bea9-548ef71b84fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feea2e0ceb0>]}
2023-04-20 12:22:01.108585 (MainThread): 12:22:01  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:22:01.110015 (Thread-1): 12:22:01  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 12:22:01.110247 (Thread-1): 12:22:01  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 12:22:01.114976 (Thread-1): 12:22:01  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 12:22:01.110295 => 2023-04-20 12:22:01.114812
2023-04-20 12:22:01.115190 (Thread-1): 12:22:01  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 12:22:01.115577 (Thread-1): 12:22:01  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 12:22:01.115952 (Thread-1): 12:22:01  On rpc.dbsql_dbt_tpch.request: 
SELECT
  md5(a.accountid::string) as sk_accountid
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:22:01.116095 (Thread-1): 12:22:01  Opening a new connection, currently in state init
2023-04-20 12:22:01.727362 (Thread-1071): handling ps request
2023-04-20 12:22:01.727913 (Thread-1071): 12:22:01  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528193af0>]}
2023-04-20 12:22:01.730731 (Thread-1071): sending response (<Response 26936 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:01.762741 (Thread-1072): handling poll request
2023-04-20 12:22:01.763085 (Thread-1072): 12:22:01  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528193f10>]}
2023-04-20 12:22:01.763670 (Thread-1072): sending response (<Response 5997 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:01.764749 (Thread-1073): handling ps request
2023-04-20 12:22:01.765191 (Thread-1073): 12:22:01  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a06460>]}
2023-04-20 12:22:01.766984 (Thread-1073): sending response (<Response 26936 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:01.866210 (Thread-1): 12:22:01  Databricks adapter: Error while running:

SELECT
  md5(a.accountid::string) as sk_accountid
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:22:01.866442 (Thread-1): 12:22:01  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'a'.(line 4, pos 2)

== SQL ==

SELECT
  md5(a.accountid::string) as sk_accountid
  a.accountid,
--^^^
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */

2023-04-20 12:22:01.866570 (Thread-1): 12:22:01  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'a'.(line 4, pos 2)

== SQL ==

SELECT
  md5(a.accountid::string) as sk_accountid
  a.accountid,
--^^^
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'a'.(line 4, pos 2)

== SQL ==

SELECT
  md5(a.accountid::string) as sk_accountid
  a.accountid,
--^^^
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 12:22:01.866685 (Thread-1): 12:22:01  Databricks adapter: operation-id: b'\x01\xed\xdfu\xf4\x0e\x1e:\x957H`\xbc\x94\xe7\xcb'
2023-04-20 12:22:01.866883 (Thread-1): 12:22:01  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 12:22:01.115237 => 2023-04-20 12:22:01.866758
2023-04-20 12:22:01.867079 (Thread-1): 12:22:01  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 12:22:02.142111 (Thread-1): Got an exception: Runtime Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'a'.(line 4, pos 2)
  
  == SQL ==
  
  SELECT
    md5(a.accountid::string) as sk_accountid
    a.accountid,
  --^^^
    b.sk_brokerid,
    a.sk_customerid,
    a.accountdesc,
    a.TaxStatus,
    a.status,
    a.batchid,
    a.effectivedate,
    a.enddate
  FROM (
    SELECT
      a.* except(effectivedate, enddate, customerid),
      c.sk_customerid,
      if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
      if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
    FROM (
      SELECT *
      FROM (
        SELECT
          accountid,
          customerid,
          coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
            PARTITION BY accountid ORDER BY update_ts)) accountdesc,
          coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
            PARTITION BY accountid ORDER BY update_ts)) taxstatus,
          coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
            PARTITION BY accountid ORDER BY update_ts)) brokerid,
          coalesce(status, last_value(status) IGNORE NULLS OVER (
            PARTITION BY accountid ORDER BY update_ts)) status,
          date(update_ts) effectivedate,
          nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
          batchid
        FROM (
          SELECT
            accountid,
            customerid,
            accountdesc,
            taxstatus,
            brokerid,
            status,
            update_ts,
            1 batchid
          FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
          WHERE ActionType NOT IN ('UPDCUST', 'INACT')
          UNION ALL
          SELECT
            accountid,
            a.ca_c_id customerid,
            accountDesc,
            TaxStatus,
            a.ca_b_id brokerid,
            st_name as status,
            TIMESTAMP(bd.batchdate) update_ts,
            a.batchid
          FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
            ON a.batchid = bd.batchid
          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
            ON a.CA_ST_ID = st.st_id
        ) a
      ) a
      WHERE a.effectivedate < a.enddate
    ) a
    FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
      ON 
        a.customerid = c.customerid
        AND c.enddate > a.effectivedate
        AND c.effectivedate < a.enddate
  ) a
  LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
    ON a.brokerid = b.brokerid
  limit 500
  /* limit added automatically by dbt cloud */
  
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'a'.(line 4, pos 2)

== SQL ==

SELECT
  md5(a.accountid::string) as sk_accountid
  a.accountid,
--^^^
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'a'.(line 4, pos 2)
  
  == SQL ==
  
  SELECT
    md5(a.accountid::string) as sk_accountid
    a.accountid,
  --^^^
    b.sk_brokerid,
    a.sk_customerid,
    a.accountdesc,
    a.TaxStatus,
    a.status,
    a.batchid,
    a.effectivedate,
    a.enddate
  FROM (
    SELECT
      a.* except(effectivedate, enddate, customerid),
      c.sk_customerid,
      if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
      if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
    FROM (
      SELECT *
      FROM (
        SELECT
          accountid,
          customerid,
          coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
            PARTITION BY accountid ORDER BY update_ts)) accountdesc,
          coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
            PARTITION BY accountid ORDER BY update_ts)) taxstatus,
          coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
            PARTITION BY accountid ORDER BY update_ts)) brokerid,
          coalesce(status, last_value(status) IGNORE NULLS OVER (
            PARTITION BY accountid ORDER BY update_ts)) status,
          date(update_ts) effectivedate,
          nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
          batchid
        FROM (
          SELECT
            accountid,
            customerid,
            accountdesc,
            taxstatus,
            brokerid,
            status,
            update_ts,
            1 batchid
          FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
          WHERE ActionType NOT IN ('UPDCUST', 'INACT')
          UNION ALL
          SELECT
            accountid,
            a.ca_c_id customerid,
            accountDesc,
            TaxStatus,
            a.ca_b_id brokerid,
            st_name as status,
            TIMESTAMP(bd.batchdate) update_ts,
            a.batchid
          FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
            ON a.batchid = bd.batchid
          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
            ON a.CA_ST_ID = st.st_id
        ) a
      ) a
      WHERE a.effectivedate < a.enddate
    ) a
    FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
      ON 
        a.customerid = c.customerid
        AND c.enddate > a.effectivedate
        AND c.effectivedate < a.enddate
  ) a
  LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
    ON a.brokerid = b.brokerid
  limit 500
  /* limit added automatically by dbt cloud */
2023-04-20 12:22:02.144036 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': "Runtime Error in rpc request (from remote system.sql)\n  \n  [PARSE_SYNTAX_ERROR] Syntax error at or near 'a'.(line 4, pos 2)\n  \n  == SQL ==\n  \n  SELECT\n    md5(a.accountid::string) as sk_accountid\n    a.accountid,\n  --^^^\n    b.sk_brokerid,\n    a.sk_customerid,\n    a.accountdesc,\n    a.TaxStatus,\n    a.status,\n    a.batchid,\n    a.effectivedate,\n    a.enddate\n  FROM (\n    SELECT\n      a.* except(effectivedate, enddate, customerid),\n      c.sk_customerid,\n      if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n      if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n    FROM (\n      SELECT *\n      FROM (\n        SELECT\n          accountid,\n          customerid,\n          coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n            PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n          coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n            PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n          coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n            PARTITION BY accountid ORDER BY update_ts)) brokerid,\n          coalesce(status, last_value(status) IGNORE NULLS OVER (\n            PARTITION BY accountid ORDER BY update_ts)) status,\n          date(update_ts) effectivedate,\n          nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n          batchid\n        FROM (\n          SELECT\n            accountid,\n            customerid,\n            accountdesc,\n            taxstatus,\n            brokerid,\n            status,\n            update_ts,\n            1 batchid\n          FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n          WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n          UNION ALL\n          SELECT\n            accountid,\n            a.ca_c_id customerid,\n            accountDesc,\n            TaxStatus,\n            a.ca_b_id brokerid,\n            st_name as status,\n            TIMESTAMP(bd.batchdate) update_ts,\n            a.batchid\n          FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a\n          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd\n            ON a.batchid = bd.batchid\n          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st \n            ON a.CA_ST_ID = st.st_id\n        ) a\n      ) a\n      WHERE a.effectivedate < a.enddate\n    ) a\n    FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c \n      ON \n        a.customerid = c.customerid\n        AND c.enddate > a.effectivedate\n        AND c.effectivedate < a.enddate\n  ) a\n  LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b \n    ON a.brokerid = b.brokerid\n  limit 500\n  /* limit added automatically by dbt cloud */\n  ", 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT\n  md5(a.accountid::string) as sk_accountid\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM {{ source('tpcdi', 'AccountIncremental') }} a\n        JOIN {{ source('tpcdi', 'BatchDate') }} bd\n          ON a.batchid = bd.batchid\n        JOIN {{ source('tpcdi', 'StatusType') }} st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN {{ ref('DimCustomerStg') }} c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN {{ ref('DimBroker') }} b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT\n  md5(a.accountid::string) as sk_accountid\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd\n          ON a.batchid = bd.batchid\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN `dbt_shabbirkdb`.`DimBroker` b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': "Runtime Error in rpc request (from remote system.sql)\n  \n  [PARSE_SYNTAX_ERROR] Syntax error at or near 'a'.(line 4, pos 2)\n  \n  == SQL ==\n  \n  SELECT\n    md5(a.accountid::string) as sk_accountid\n    a.accountid,\n  --^^^\n    b.sk_brokerid,\n    a.sk_customerid,\n    a.accountdesc,\n    a.TaxStatus,\n    a.status,\n    a.batchid,\n    a.effectivedate,\n    a.enddate\n  FROM (\n    SELECT\n      a.* except(effectivedate, enddate, customerid),\n      c.sk_customerid,\n      if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n      if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n    FROM (\n      SELECT *\n      FROM (\n        SELECT\n          accountid,\n          customerid,\n          coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n            PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n          coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n            PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n          coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n            PARTITION BY accountid ORDER BY update_ts)) brokerid,\n          coalesce(status, last_value(status) IGNORE NULLS OVER (\n            PARTITION BY accountid ORDER BY update_ts)) status,\n          date(update_ts) effectivedate,\n          nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n          batchid\n        FROM (\n          SELECT\n            accountid,\n            customerid,\n            accountdesc,\n            taxstatus,\n            brokerid,\n            status,\n            update_ts,\n            1 batchid\n          FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n          WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n          UNION ALL\n          SELECT\n            accountid,\n            a.ca_c_id customerid,\n            accountDesc,\n            TaxStatus,\n            a.ca_b_id brokerid,\n            st_name as status,\n            TIMESTAMP(bd.batchdate) update_ts,\n            a.batchid\n          FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a\n          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd\n            ON a.batchid = bd.batchid\n          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st \n            ON a.CA_ST_ID = st.st_id\n        ) a\n      ) a\n      WHERE a.effectivedate < a.enddate\n    ) a\n    FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c \n      ON \n        a.customerid = c.customerid\n        AND c.enddate > a.effectivedate\n        AND c.effectivedate < a.enddate\n  ) a\n  LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b \n    ON a.brokerid = b.brokerid\n  limit 500\n  /* limit added automatically by dbt cloud */\n  ", 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT\n  md5(a.accountid::string) as sk_accountid\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM {{ source('tpcdi', 'AccountIncremental') }} a\n        JOIN {{ source('tpcdi', 'BatchDate') }} bd\n          ON a.batchid = bd.batchid\n        JOIN {{ source('tpcdi', 'StatusType') }} st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN {{ ref('DimCustomerStg') }} c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN {{ ref('DimBroker') }} b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT\n  md5(a.accountid::string) as sk_accountid\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd\n          ON a.batchid = bd.batchid\n        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN `dbt_shabbirkdb`.`DimBroker` b \n  ON a.brokerid = b.brokerid\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 12:22:02.195466 (Thread-1074): handling status request
2023-04-20 12:22:02.195975 (Thread-1074): 12:22:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528193ee0>]}
2023-04-20 12:22:02.196619 (Thread-1074): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:03.096861 (Thread-1075): handling poll request
2023-04-20 12:22:03.097357 (Thread-1075): 12:22:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528193100>]}
2023-04-20 12:22:03.098237 (Thread-1075): sending response (<Response 60403 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:04.343176 (Thread-1076): handling ps request
2023-04-20 12:22:04.343691 (Thread-1076): 12:22:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5281934f0>]}
2023-04-20 12:22:04.345610 (Thread-1076): sending response (<Response 26958 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:04.888768 (Thread-1077): handling status request
2023-04-20 12:22:04.889277 (Thread-1077): 12:22:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528175c70>]}
2023-04-20 12:22:04.889920 (Thread-1077): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:04.907583 (Thread-1078): handling poll request
2023-04-20 12:22:04.907933 (Thread-1078): 12:22:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528175ee0>]}
2023-04-20 12:22:04.908652 (Thread-1078): sending response (<Response 60403 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:06.350466 (Thread-1079): handling status request
2023-04-20 12:22:06.350950 (Thread-1079): 12:22:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528175190>]}
2023-04-20 12:22:06.351636 (Thread-1079): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:26.573870 (Thread-1080): handling run_sql request
2023-04-20 12:22:26.574375 (Thread-1080): 12:22:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5281756d0>]}
2023-04-20 12:22:26.594282 (Thread-1081): handling ps request
2023-04-20 12:22:26.600249 (Thread-1081): 12:22:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb562270310>]}
2023-04-20 12:22:26.606083 (Thread-1081): sending response (<Response 27481 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:27.139665 (Thread-1082): handling poll request
2023-04-20 12:22:27.140164 (Thread-1082): 12:22:27  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528175160>]}
2023-04-20 12:22:27.140706 (Thread-1082): sending response (<Response 432 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:27.560856 (Thread-1083): handling status request
2023-04-20 12:22:27.561361 (Thread-1083): 12:22:27  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8798ac0>]}
2023-04-20 12:22:27.562093 (Thread-1083): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:27.816980 (Thread-1084): handling ps request
2023-04-20 12:22:27.817465 (Thread-1084): 12:22:27  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528193af0>]}
2023-04-20 12:22:27.819334 (Thread-1084): sending response (<Response 27480 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:29.332749 (Thread-1080): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:29.369673 (MainThread): 12:22:29  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c26a84ce-9edd-4231-a3db-2b0d1b0ff5a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff98a646eb0>]}
2023-04-20 12:22:29.370291 (MainThread): 12:22:29  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:22:29.371764 (Thread-1): 12:22:29  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 12:22:29.372002 (Thread-1): 12:22:29  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 12:22:29.376904 (Thread-1): 12:22:29  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 12:22:29.372052 => 2023-04-20 12:22:29.376731
2023-04-20 12:22:29.377126 (Thread-1): 12:22:29  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 12:22:29.377475 (Thread-1): 12:22:29  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 12:22:29.377851 (Thread-1): 12:22:29  On rpc.dbsql_dbt_tpch.request: 
SELECT
  md5(a.accountid::string) as sk_accountid,
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 12:22:29.378000 (Thread-1): 12:22:29  Opening a new connection, currently in state init
2023-04-20 12:22:29.721854 (Thread-1085): handling poll request
2023-04-20 12:22:29.722426 (Thread-1085): 12:22:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437e76790>]}
2023-04-20 12:22:29.723087 (Thread-1085): sending response (<Response 5998 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:29.851665 (Thread-1086): handling ps request
2023-04-20 12:22:29.852176 (Thread-1086): 12:22:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5085a62e0>]}
2023-04-20 12:22:29.854253 (Thread-1086): sending response (<Response 27476 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:29.904614 (Thread-1087): handling ps request
2023-04-20 12:22:29.905073 (Thread-1087): 12:22:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5281933a0>]}
2023-04-20 12:22:29.906896 (Thread-1087): sending response (<Response 27476 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:30.142269 (Thread-1088): handling status request
2023-04-20 12:22:30.142757 (Thread-1088): 12:22:30  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5085a6460>]}
2023-04-20 12:22:30.143365 (Thread-1088): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:30.626182 (Thread-1089): handling poll request
2023-04-20 12:22:30.626669 (Thread-1089): 12:22:30  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528193070>]}
2023-04-20 12:22:30.627261 (Thread-1089): sending response (<Response 5998 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:30.919322 (Thread-1): 12:22:30  SQL status: OK in 1.54 seconds
2023-04-20 12:22:30.941350 (Thread-1): 12:22:30  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 12:22:29.377173 => 2023-04-20 12:22:30.941129
2023-04-20 12:22:30.941608 (Thread-1): 12:22:30  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 12:22:32.220140 (Thread-1090): handling poll request
2023-04-20 12:22:32.220646 (Thread-1090): 12:22:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508382bb0>]}
2023-04-20 12:22:32.226301 (Thread-1090): sending response (<Response 117886 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:32.313756 (Thread-1091): handling poll request
2023-04-20 12:22:32.314251 (Thread-1091): 12:22:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528063a90>]}
2023-04-20 12:22:32.318661 (Thread-1091): sending response (<Response 123459 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:32.434413 (Thread-1092): handling ps request
2023-04-20 12:22:32.434894 (Thread-1092): 12:22:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb50807b880>]}
2023-04-20 12:22:32.436762 (Thread-1092): sending response (<Response 27501 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:33.722792 (Thread-1093): handling status request
2023-04-20 12:22:33.723314 (Thread-1093): 12:22:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52bf9ca60>]}
2023-04-20 12:22:33.748638 (Thread-1093): sending response (<Response 6832 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:33.924698 (Thread-1094): handling ps request
2023-04-20 12:22:33.925178 (Thread-1094): 12:22:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f86e0430>]}
2023-04-20 12:22:33.927017 (Thread-1094): sending response (<Response 27501 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:40.005356 (Thread-1095): 12:22:40  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 12:22:40.215107 (Thread-1095): 12:22:40  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
2023-04-20 12:22:40.216407 (Thread-1095): 12:22:40  Partial parsing: updated file: dbsql_dbt_tpch://models/incremental/DimAccount.sql
2023-04-20 12:22:40.224485 (Thread-1095): 12:22:40  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 12:22:40.288577 (Thread-1095): 12:22:40  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8077d60>]}
2023-04-20 12:22:40.531810 (Thread-1096): handling status request
2023-04-20 12:22:40.532326 (Thread-1096): 12:22:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f84e5fa0>]}
2023-04-20 12:22:40.532815 (Thread-1096): sending response (<Response 1928 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:45.407001 (Thread-1097): handling status request
2023-04-20 12:22:45.407550 (Thread-1097): 12:22:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f828d160>]}
2023-04-20 12:22:45.408040 (Thread-1097): sending response (<Response 1928 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:46.044518 (Thread-1098): handling cli_args request
2023-04-20 12:22:46.045010 (Thread-1098): 12:22:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8478ca0>]}
2023-04-20 12:22:48.810701 (Thread-1098): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:48.867491 (MainThread): 12:22:48  checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
2023-04-20 12:22:48.886444 (MainThread): 12:22:48  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 12:22:48.886618 (MainThread): 12:22:48  previous checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, current checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd
2023-04-20 12:22:48.886770 (MainThread): 12:22:48  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7874bb0bb0>]}
2023-04-20 12:22:49.483241 (Thread-1099): handling ps request
2023-04-20 12:22:49.483922 (Thread-1099): 12:22:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8478ac0>]}
2023-04-20 12:22:49.487094 (Thread-1099): sending response (<Response 27883 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:49.659578 (Thread-1100): handling poll request
2023-04-20 12:22:49.660056 (Thread-1100): 12:22:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8523040>]}
2023-04-20 12:22:49.660627 (Thread-1100): sending response (<Response 1863 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:49.689004 (Thread-1101): handling status request
2023-04-20 12:22:49.689371 (Thread-1101): 12:22:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5082e6970>]}
2023-04-20 12:22:49.689840 (Thread-1101): sending response (<Response 1928 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:50.085574 (Thread-1102): handling ps request
2023-04-20 12:22:50.086066 (Thread-1102): 12:22:50  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f807d250>]}
2023-04-20 12:22:50.087914 (Thread-1102): sending response (<Response 27883 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:50.375083 (MainThread): 12:22:50  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 12:22:50.387319 (MainThread): 12:22:50  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 12:22:50.390372 (MainThread): 12:22:50  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 12:22:50.393425 (MainThread): 12:22:50  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 12:22:50.396363 (MainThread): 12:22:50  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 12:22:50.399260 (MainThread): 12:22:50  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 12:22:50.403087 (MainThread): 12:22:50  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 12:22:50.406244 (MainThread): 12:22:50  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 12:22:50.409812 (MainThread): 12:22:50  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 12:22:50.412947 (MainThread): 12:22:50  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 12:22:50.416377 (MainThread): 12:22:50  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 12:22:50.419386 (MainThread): 12:22:50  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 12:22:50.422249 (MainThread): 12:22:50  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 12:22:50.425189 (MainThread): 12:22:50  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 12:22:50.428283 (MainThread): 12:22:50  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 12:22:50.594483 (MainThread): 12:22:50  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f78740a7d00>]}
2023-04-20 12:22:50.626913 (MainThread): 12:22:50  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7874bb2370>]}
2023-04-20 12:22:50.627244 (MainThread): 12:22:50  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 12:22:50.627390 (MainThread): 12:22:50  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7874e1ba00>]}
2023-04-20 12:22:50.629276 (MainThread): 12:22:50  
2023-04-20 12:22:50.630493 (MainThread): 12:22:50  Acquiring new databricks connection 'master'
2023-04-20 12:22:50.632271 (ThreadPoolExecutor-0_0): 12:22:50  Acquiring new databricks connection 'list_schemas'
2023-04-20 12:22:50.643312 (ThreadPoolExecutor-0_0): 12:22:50  Using databricks connection "list_schemas"
2023-04-20 12:22:50.643693 (ThreadPoolExecutor-0_0): 12:22:50  On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_schemas"} */

    show databases
  
2023-04-20 12:22:50.643883 (ThreadPoolExecutor-0_0): 12:22:50  Opening a new connection, currently in state init
2023-04-20 12:22:51.633627 (ThreadPoolExecutor-0_0): 12:22:51  SQL status: OK in 0.99 seconds
2023-04-20 12:22:51.773404 (ThreadPoolExecutor-0_0): 12:22:51  On list_schemas: Close
2023-04-20 12:22:52.045351 (ThreadPoolExecutor-1_0): 12:22:52  Acquiring new databricks connection 'list_None_dbt_shabbirkdb'
2023-04-20 12:22:52.055984 (ThreadPoolExecutor-1_0): 12:22:52  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:22:52.056198 (ThreadPoolExecutor-1_0): 12:22:52  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 12:22:52.056360 (ThreadPoolExecutor-1_0): 12:22:52  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show tables in `dbt_shabbirkdb`
  
2023-04-20 12:22:52.056511 (ThreadPoolExecutor-1_0): 12:22:52  Opening a new connection, currently in state closed
2023-04-20 12:22:52.256832 (Thread-1103): handling poll request
2023-04-20 12:22:52.257330 (Thread-1103): 12:22:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f83f6310>]}
2023-04-20 12:22:52.258115 (Thread-1103): sending response (<Response 11070 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:52.785903 (ThreadPoolExecutor-1_0): 12:22:52  SQL status: OK in 0.73 seconds
2023-04-20 12:22:52.807555 (ThreadPoolExecutor-1_0): 12:22:52  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 12:22:52.807783 (ThreadPoolExecutor-1_0): 12:22:52  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show views in `dbt_shabbirkdb`
  
2023-04-20 12:22:53.017505 (Thread-1104): handling ps request
2023-04-20 12:22:53.018042 (Thread-1104): 12:22:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f807d310>]}
2023-04-20 12:22:53.020152 (Thread-1104): sending response (<Response 27883 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:53.275090 (ThreadPoolExecutor-1_0): 12:22:53  SQL status: OK in 0.47 seconds
2023-04-20 12:22:53.280383 (ThreadPoolExecutor-1_0): 12:22:53  On list_None_dbt_shabbirkdb: ROLLBACK
2023-04-20 12:22:53.280731 (ThreadPoolExecutor-1_0): 12:22:53  Databricks adapter: NotImplemented: rollback
2023-04-20 12:22:53.280904 (ThreadPoolExecutor-1_0): 12:22:53  On list_None_dbt_shabbirkdb: Close
2023-04-20 12:22:53.577084 (MainThread): 12:22:53  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7874bb0a90>]}
2023-04-20 12:22:53.577522 (MainThread): 12:22:53  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:22:53.577691 (MainThread): 12:22:53  Spark adapter: NotImplemented: commit
2023-04-20 12:22:53.578271 (MainThread): 12:22:53  Concurrency: 4 threads (target='default')
2023-04-20 12:22:53.578427 (MainThread): 12:22:53  
2023-04-20 12:22:53.582340 (Thread-1): 12:22:53  Began running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:22:53.582729 (Thread-1): 12:22:53  1 of 15 START sql table model dbt_shabbirkdb.DimBroker ......................... [RUN]
2023-04-20 12:22:53.583219 (Thread-1): 12:22:53  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
2023-04-20 12:22:53.583410 (Thread-1): 12:22:53  Began compiling node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:22:53.586290 (Thread-2): 12:22:53  Began running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:22:53.586592 (Thread-2): 12:22:53  2 of 15 START sql table model dbt_shabbirkdb.DimCompany ........................ [RUN]
2023-04-20 12:22:53.587109 (Thread-2): 12:22:53  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCompany'
2023-04-20 12:22:53.587287 (Thread-2): 12:22:53  Began compiling node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:22:53.591342 (Thread-3): 12:22:53  Began running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:22:53.591694 (Thread-3): 12:22:53  3 of 15 START sql table model dbt_shabbirkdb.DimCustomerStg .................... [RUN]
2023-04-20 12:22:53.592240 (Thread-3): 12:22:53  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomerStg'
2023-04-20 12:22:53.592422 (Thread-3): 12:22:53  Began compiling node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:22:53.596636 (Thread-2): 12:22:53  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:22:53.597935 (Thread-1): 12:22:53  Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:22:53.598301 (Thread-4): 12:22:53  Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:22:53.598596 (Thread-4): 12:22:53  4 of 15 START sql table model dbt_shabbirkdb.tempDailyMarketHistorical ......... [RUN]
2023-04-20 12:22:53.599103 (Thread-4): 12:22:53  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempDailyMarketHistorical'
2023-04-20 12:22:53.599283 (Thread-4): 12:22:53  Began compiling node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:22:53.602963 (Thread-4): 12:22:53  Writing injected SQL for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:22:53.603690 (Thread-3): 12:22:53  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:22:53.615046 (Thread-1): 12:22:53  Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 2023-04-20 12:22:53.583458 => 2023-04-20 12:22:53.614883
2023-04-20 12:22:53.615270 (Thread-1): 12:22:53  Began executing node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:22:53.620381 (Thread-2): 12:22:53  Timing info for model.dbsql_dbt_tpch.DimCompany (compile): 2023-04-20 12:22:53.587332 => 2023-04-20 12:22:53.620220
2023-04-20 12:22:53.620588 (Thread-2): 12:22:53  Began executing node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:22:53.625463 (Thread-4): 12:22:53  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (compile): 2023-04-20 12:22:53.599327 => 2023-04-20 12:22:53.625306
2023-04-20 12:22:53.625667 (Thread-4): 12:22:53  Began executing node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:22:53.636152 (Thread-3): 12:22:53  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (compile): 2023-04-20 12:22:53.592466 => 2023-04-20 12:22:53.635989
2023-04-20 12:22:53.636358 (Thread-3): 12:22:53  Began executing node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:22:53.663176 (Thread-3): 12:22:53  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:22:53.663354 (Thread-3): 12:22:53  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:22:53.663512 (Thread-3): 12:22:53  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

      describe extended `dbt_shabbirkdb`.`dimcustomerstg`
  
2023-04-20 12:22:53.664504 (Thread-3): 12:22:53  Opening a new connection, currently in state init
2023-04-20 12:22:53.664885 (Thread-2): 12:22:53  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:22:53.665056 (Thread-2): 12:22:53  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:22:53.665207 (Thread-2): 12:22:53  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

      describe extended `dbt_shabbirkdb`.`dimcompany`
  
2023-04-20 12:22:53.665330 (Thread-2): 12:22:53  Opening a new connection, currently in state init
2023-04-20 12:22:53.666981 (Thread-1): 12:22:53  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:22:53.667152 (Thread-1): 12:22:53  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:22:53.667303 (Thread-1): 12:22:53  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

      describe extended `dbt_shabbirkdb`.`dimbroker`
  
2023-04-20 12:22:53.667424 (Thread-1): 12:22:53  Opening a new connection, currently in state closed
2023-04-20 12:22:53.669913 (Thread-4): 12:22:53  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:22:53.670080 (Thread-4): 12:22:53  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:22:53.670232 (Thread-4): 12:22:53  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

      describe extended `dbt_shabbirkdb`.`tempdailymarkethistorical`
  
2023-04-20 12:22:53.670351 (Thread-4): 12:22:53  Opening a new connection, currently in state init
2023-04-20 12:22:54.377581 (Thread-1105): handling status request
2023-04-20 12:22:54.378083 (Thread-1105): 12:22:54  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f83f24f0>]}
2023-04-20 12:22:54.378568 (Thread-1105): sending response (<Response 1928 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:54.442480 (Thread-1): 12:22:54  SQL status: OK in 0.77 seconds
2023-04-20 12:22:54.491270 (Thread-1): 12:22:54  Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:22:54.491714 (Thread-2): 12:22:54  SQL status: OK in 0.83 seconds
2023-04-20 12:22:54.493271 (Thread-4): 12:22:54  SQL status: OK in 0.82 seconds
2023-04-20 12:22:54.498189 (Thread-4): 12:22:54  Writing runtime sql for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:22:54.502512 (Thread-2): 12:22:54  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:22:54.508516 (Thread-1): 12:22:54  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 12:22:54.508729 (Thread-1): 12:22:54  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT
  md5(employeeid) as sk_brokerid,
  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate`) effectivedate,
  date('9999-12-31') enddate
FROM  `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`HR`
WHERE employeejobcode = 314
  
2023-04-20 12:22:54.510414 (Thread-4): 12:22:54  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 12:22:54.510624 (Thread-4): 12:22:54  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempDailyMarketHistorical`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  dmh.*,
  sk_dateid,
  min(dm_low) OVER (
    PARTITION BY dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweeklow,
  max(dm_high) OVER (
    PARTITION by dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweekhigh
FROM (
  SELECT * FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketHistorical`
  UNION ALL
  SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketIncremental`) dmh
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
  ON d.datevalue = dm_date;
  
2023-04-20 12:22:54.517100 (Thread-2): 12:22:54  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 12:22:54.517366 (Thread-2): 12:22:54  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCompany`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  * FROM (
  SELECT
    md5(cik) sk_companyid,
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
  
2023-04-20 12:22:54.527637 (Thread-3): 12:22:54  SQL status: OK in 0.86 seconds
2023-04-20 12:22:54.532475 (Thread-3): 12:22:54  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:22:54.561161 (Thread-3): 12:22:54  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 12:22:54.561501 (Thread-3): 12:22:54  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomerStg`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT * FROM (
  SELECT
    sk_customerid,
    customerid,
    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) taxid,
    status,
    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) lastname,
    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) firstname,
    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) middleinitial,
    coalesce(gender, last_value(gender) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) gender,
    coalesce(tier, last_value(tier) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) tier,
    coalesce(dob, last_value(dob) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) dob,
    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline1,
    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline2,
    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) postalcode,
    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) CITY,
    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) stateprov,
    coalesce(country, last_value(country) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) country,
    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone1,
    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone2,
    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone3,
    coalesce(email1, last_value(email1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email1,
    coalesce(email2, last_value(email2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email2,
    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) LCL_TX_ID,
    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) NAT_TX_ID,
    batchid,
    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,
    date(update_ts) effectivedate,
    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate
  FROM (
    SELECT
      md5(customerid::string) as sk_customerid,
      customerid,
      taxid,
      status,
      lastname,
      firstname,
      middleinitial,
      gender,
      tier,
      dob,
      addressline1,
      addressline2,
      postalcode,
      city,
      stateprov,
      country,
      phone1,
      phone2,
      phone3,
      email1,
      email2,
      lcl_tx_id,
      nat_tx_id,
      1 batchid,
      update_ts
    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')
    UNION ALL
    SELECT
      md5(c.customerid::string) as sk_customerid,
      c.customerid,
      nullif(c.taxid, '') taxid,
      nullif(s.st_name, '') as status,
      nullif(c.lastname, '') lastname,
      nullif(c.firstname, '') firstname,
      nullif(c.middleinitial, '') middleinitial,
      gender,
      c.tier,
      c.dob,
      nullif(c.addressline1, '') addressline1,
      nullif(c.addressline2, '') addressline2,
      nullif(c.postalcode, '') postalcode,
      nullif(c.city, '') city,
      nullif(c.stateprov, '') stateprov,
      nullif(c.country, '') country,
      CASE
        WHEN isnull(c_local_1) then c_local_1
        ELSE concat(
          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),
          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),
          c_local_1,
          nvl(c_ext_1, '')) END as phone1,
      CASE
        WHEN isnull(c_local_2) then c_local_2
        ELSE concat(
          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),
          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),
          c_local_2,
          nvl(c_ext_2, '')) END as phone2,
      CASE
        WHEN isnull(c_local_3) then c_local_3
        ELSE concat(
          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),
          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),
          c_local_3,
          nvl(c_ext_3, '')) END as phone3,
      nullif(c.email1, '') email1,
      nullif(c.email2, '') email2,
      c.LCL_TX_ID, 
      c.NAT_TX_ID,
      c.batchid,
      timestamp(bd.batchdate) update_ts
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CustomerIncremental` c
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
      ON c.batchid = bd.batchid
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` s 
      ON c.status = s.st_id
  ) c
  )
  
2023-04-20 12:22:54.732930 (Thread-1106): handling poll request
2023-04-20 12:22:54.733431 (Thread-1106): 12:22:54  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f83f29d0>]}
2023-04-20 12:22:54.734695 (Thread-1106): sending response (<Response 39675 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:56.328983 (Thread-1107): handling poll request
2023-04-20 12:22:56.329468 (Thread-1107): 12:22:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437fcb4f0>]}
2023-04-20 12:22:56.329956 (Thread-1107): sending response (<Response 291 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:56.468402 (Thread-1108): handling ps request
2023-04-20 12:22:56.468886 (Thread-1108): 12:22:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437fcb550>]}
2023-04-20 12:22:56.470700 (Thread-1108): sending response (<Response 27884 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:56.918633 (Thread-1109): handling status request
2023-04-20 12:22:56.919115 (Thread-1109): 12:22:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437fcb8b0>]}
2023-04-20 12:22:56.919611 (Thread-1109): sending response (<Response 1928 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:57.762267 (Thread-1): 12:22:57  SQL status: OK in 3.25 seconds
2023-04-20 12:22:57.788879 (Thread-1): 12:22:57  Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 2023-04-20 12:22:53.615320 => 2023-04-20 12:22:57.788719
2023-04-20 12:22:57.789113 (Thread-1): 12:22:57  On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
2023-04-20 12:22:57.789276 (Thread-1): 12:22:57  Databricks adapter: NotImplemented: rollback
2023-04-20 12:22:57.789405 (Thread-1): 12:22:57  On model.dbsql_dbt_tpch.DimBroker: Close
2023-04-20 12:22:57.991043 (Thread-1110): handling poll request
2023-04-20 12:22:57.991541 (Thread-1110): 12:22:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437fcbb20>]}
2023-04-20 12:22:57.992073 (Thread-1110): sending response (<Response 2133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:58.002166 (Thread-2): 12:22:58  SQL status: OK in 3.48 seconds
2023-04-20 12:22:58.004525 (Thread-2): 12:22:58  Timing info for model.dbsql_dbt_tpch.DimCompany (execute): 2023-04-20 12:22:53.620637 => 2023-04-20 12:22:58.004374
2023-04-20 12:22:58.004731 (Thread-2): 12:22:58  On model.dbsql_dbt_tpch.DimCompany: ROLLBACK
2023-04-20 12:22:58.004875 (Thread-2): 12:22:58  Databricks adapter: NotImplemented: rollback
2023-04-20 12:22:58.005002 (Thread-2): 12:22:58  On model.dbsql_dbt_tpch.DimCompany: Close
2023-04-20 12:22:58.074170 (Thread-1): 12:22:58  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7874344130>]}
2023-04-20 12:22:58.074634 (Thread-1): 12:22:58  1 of 15 OK created sql table model dbt_shabbirkdb.DimBroker .................... [OK in 4.49s]
2023-04-20 12:22:58.075801 (Thread-1): 12:22:58  Finished running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 12:22:58.283670 (Thread-2): 12:22:58  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f784ab3d940>]}
2023-04-20 12:22:58.284149 (Thread-2): 12:22:58  2 of 15 OK created sql table model dbt_shabbirkdb.DimCompany ................... [OK in 4.70s]
2023-04-20 12:22:58.284371 (Thread-2): 12:22:58  Finished running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 12:22:58.285241 (Thread-1): 12:22:58  Began running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:22:58.285565 (Thread-1): 12:22:58  5 of 15 START sql table model dbt_shabbirkdb.DimSecurity ....................... [RUN]
2023-04-20 12:22:58.286089 (Thread-1): 12:22:58  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimSecurity'
2023-04-20 12:22:58.286277 (Thread-1): 12:22:58  Began compiling node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:22:58.290283 (Thread-2): 12:22:58  Began running node model.dbsql_dbt_tpch.Financial
2023-04-20 12:22:58.290571 (Thread-2): 12:22:58  6 of 15 START sql table model dbt_shabbirkdb.Financial ......................... [RUN]
2023-04-20 12:22:58.291026 (Thread-2): 12:22:58  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Financial'
2023-04-20 12:22:58.291226 (Thread-2): 12:22:58  Began compiling node model.dbsql_dbt_tpch.Financial
2023-04-20 12:22:58.296236 (Thread-1): 12:22:58  Writing injected SQL for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:22:58.297399 (Thread-2): 12:22:58  Writing injected SQL for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:22:58.310109 (Thread-2): 12:22:58  Timing info for model.dbsql_dbt_tpch.Financial (compile): 2023-04-20 12:22:58.291272 => 2023-04-20 12:22:58.309942
2023-04-20 12:22:58.310332 (Thread-2): 12:22:58  Began executing node model.dbsql_dbt_tpch.Financial
2023-04-20 12:22:58.314499 (Thread-2): 12:22:58  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:22:58.314673 (Thread-2): 12:22:58  Using databricks connection "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:22:58.314831 (Thread-2): 12:22:58  On model.dbsql_dbt_tpch.Financial: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

      describe extended `dbt_shabbirkdb`.`financial`
  
2023-04-20 12:22:58.314964 (Thread-2): 12:22:58  Opening a new connection, currently in state closed
2023-04-20 12:22:58.315363 (Thread-1): 12:22:58  Timing info for model.dbsql_dbt_tpch.DimSecurity (compile): 2023-04-20 12:22:58.286325 => 2023-04-20 12:22:58.315199
2023-04-20 12:22:58.315597 (Thread-1): 12:22:58  Began executing node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:22:58.319755 (Thread-1): 12:22:58  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:22:58.319997 (Thread-1): 12:22:58  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:22:58.320223 (Thread-1): 12:22:58  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

      describe extended `dbt_shabbirkdb`.`dimsecurity`
  
2023-04-20 12:22:58.320412 (Thread-1): 12:22:58  Opening a new connection, currently in state closed
2023-04-20 12:22:58.633674 (Thread-3): 12:22:58  SQL status: OK in 4.07 seconds
2023-04-20 12:22:58.636020 (Thread-3): 12:22:58  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (execute): 2023-04-20 12:22:53.636407 => 2023-04-20 12:22:58.635864
2023-04-20 12:22:58.636234 (Thread-3): 12:22:58  On model.dbsql_dbt_tpch.DimCustomerStg: ROLLBACK
2023-04-20 12:22:58.636379 (Thread-3): 12:22:58  Databricks adapter: NotImplemented: rollback
2023-04-20 12:22:58.636504 (Thread-3): 12:22:58  On model.dbsql_dbt_tpch.DimCustomerStg: Close
2023-04-20 12:22:58.917944 (Thread-3): 12:22:58  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f787416f970>]}
2023-04-20 12:22:58.918487 (Thread-3): 12:22:58  3 of 15 OK created sql table model dbt_shabbirkdb.DimCustomerStg ............... [OK in 5.33s]
2023-04-20 12:22:58.918728 (Thread-3): 12:22:58  Finished running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 12:22:58.919565 (Thread-3): 12:22:58  Began running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:22:58.919870 (Thread-3): 12:22:58  7 of 15 START sql table model dbt_shabbirkdb.DimAccount ........................ [RUN]
2023-04-20 12:22:58.920372 (Thread-3): 12:22:58  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimAccount'
2023-04-20 12:22:58.920552 (Thread-3): 12:22:58  Began compiling node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:22:58.925850 (Thread-3): 12:22:58  Writing injected SQL for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 12:22:58.940782 (Thread-3): 12:22:58  Timing info for model.dbsql_dbt_tpch.DimAccount (compile): 2023-04-20 12:22:58.920598 => 2023-04-20 12:22:58.940616
2023-04-20 12:22:58.941007 (Thread-3): 12:22:58  Began executing node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:22:58.946532 (Thread-3): 12:22:58  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:22:58.946710 (Thread-3): 12:22:58  Using databricks connection "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 12:22:58.946872 (Thread-3): 12:22:58  On model.dbsql_dbt_tpch.DimAccount: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

      describe extended `dbt_shabbirkdb`.`dimaccount`
  
2023-04-20 12:22:58.947006 (Thread-3): 12:22:58  Opening a new connection, currently in state closed
2023-04-20 12:22:59.089683 (Thread-1): 12:22:59  SQL status: OK in 0.77 seconds
2023-04-20 12:22:59.092659 (Thread-2): 12:22:59  SQL status: OK in 0.78 seconds
2023-04-20 12:22:59.097825 (Thread-1): 12:22:59  Writing runtime sql for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:22:59.101930 (Thread-2): 12:22:59  Writing runtime sql for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:22:59.113063 (Thread-1): 12:22:59  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 12:22:59.113335 (Thread-1): 12:22:59  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 12:22:59.113953 (Thread-2): 12:22:59  Using databricks connection "model.dbsql_dbt_tpch.Financial"
2023-04-20 12:22:59.114598 (Thread-2): 12:22:59  On model.dbsql_dbt_tpch.Financial: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 12:22:59.200249 (Thread-1111): handling ps request
2023-04-20 12:22:59.200687 (Thread-1111): 12:22:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437d6a250>]}
2023-04-20 12:22:59.202656 (Thread-1111): sending response (<Response 27884 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:59.619952 (Thread-1112): handling poll request
2023-04-20 12:22:59.620436 (Thread-1112): 12:22:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437d6a490>]}
2023-04-20 12:22:59.621453 (Thread-1112): sending response (<Response 29983 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:59.653791 (Thread-1113): handling status request
2023-04-20 12:22:59.654169 (Thread-1113): 12:22:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437d6a520>]}
2023-04-20 12:22:59.654589 (Thread-1113): sending response (<Response 1928 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:22:59.688917 (Thread-3): 12:22:59  SQL status: OK in 0.74 seconds
2023-04-20 12:22:59.694242 (Thread-3): 12:22:59  Writing runtime sql for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 12:22:59.706592 (Thread-3): 12:22:59  Using databricks connection "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 12:22:59.706871 (Thread-3): 12:22:59  On model.dbsql_dbt_tpch.DimAccount: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  md5(a.accountid::string) as sk_accountid,
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 12:23:01.273443 (Thread-1114): handling poll request
2023-04-20 12:23:01.273964 (Thread-1114): 12:23:01  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437d6a820>]}
2023-04-20 12:23:01.274529 (Thread-1114): sending response (<Response 4527 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:01.498049 (Thread-4): 12:23:01  SQL status: OK in 6.99 seconds
2023-04-20 12:23:01.804167 (Thread-4): 12:23:01  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (execute): 2023-04-20 12:22:53.625715 => 2023-04-20 12:23:01.803979
2023-04-20 12:23:01.804434 (Thread-4): 12:23:01  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: ROLLBACK
2023-04-20 12:23:01.804630 (Thread-4): 12:23:01  Databricks adapter: NotImplemented: rollback
2023-04-20 12:23:01.804793 (Thread-4): 12:23:01  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: Close
2023-04-20 12:23:02.014362 (Thread-1115): handling ps request
2023-04-20 12:23:02.014872 (Thread-1115): 12:23:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437d6aaf0>]}
2023-04-20 12:23:02.016764 (Thread-1115): sending response (<Response 27884 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:02.061004 (Thread-1): 12:23:02  SQL status: OK in 2.95 seconds
2023-04-20 12:23:02.063914 (Thread-1): 12:23:02  Timing info for model.dbsql_dbt_tpch.DimSecurity (execute): 2023-04-20 12:22:58.315648 => 2023-04-20 12:23:02.063757
2023-04-20 12:23:02.064144 (Thread-1): 12:23:02  On model.dbsql_dbt_tpch.DimSecurity: ROLLBACK
2023-04-20 12:23:02.064320 (Thread-1): 12:23:02  Databricks adapter: NotImplemented: rollback
2023-04-20 12:23:02.064458 (Thread-1): 12:23:02  On model.dbsql_dbt_tpch.DimSecurity: Close
2023-04-20 12:23:02.091229 (Thread-4): 12:23:02  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f784ab3e580>]}
2023-04-20 12:23:02.091778 (Thread-4): 12:23:02  4 of 15 OK created sql table model dbt_shabbirkdb.tempDailyMarketHistorical .... [OK in 8.49s]
2023-04-20 12:23:02.092016 (Thread-4): 12:23:02  Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 12:23:02.092298 (Thread-4): 12:23:02  Began running node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:23:02.092568 (Thread-4): 12:23:02  8 of 15 START sql table model dbt_shabbirkdb.Prospect .......................... [RUN]
2023-04-20 12:23:02.093053 (Thread-4): 12:23:02  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Prospect'
2023-04-20 12:23:02.093236 (Thread-4): 12:23:02  Began compiling node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:23:02.099095 (Thread-4): 12:23:02  Writing injected SQL for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:23:02.112942 (Thread-4): 12:23:02  Timing info for model.dbsql_dbt_tpch.Prospect (compile): 2023-04-20 12:23:02.093282 => 2023-04-20 12:23:02.112756
2023-04-20 12:23:02.113189 (Thread-4): 12:23:02  Began executing node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:23:02.117782 (Thread-4): 12:23:02  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:23:02.118080 (Thread-4): 12:23:02  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:23:02.118405 (Thread-4): 12:23:02  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

      describe extended `dbt_shabbirkdb`.`prospect`
  
2023-04-20 12:23:02.118548 (Thread-4): 12:23:02  Opening a new connection, currently in state closed
2023-04-20 12:23:02.342801 (Thread-1): 12:23:02  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f786c0b8ee0>]}
2023-04-20 12:23:02.343338 (Thread-1): 12:23:02  5 of 15 OK created sql table model dbt_shabbirkdb.DimSecurity .................. [OK in 4.06s]
2023-04-20 12:23:02.343605 (Thread-1): 12:23:02  Finished running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 12:23:02.488189 (Thread-1116): handling status request
2023-04-20 12:23:02.488665 (Thread-1116): 12:23:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437d6ae80>]}
2023-04-20 12:23:02.489141 (Thread-1116): sending response (<Response 1928 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:02.842381 (Thread-4): 12:23:02  SQL status: OK in 0.72 seconds
2023-04-20 12:23:02.848131 (Thread-4): 12:23:02  Writing runtime sql for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:23:02.863376 (Thread-4): 12:23:02  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 12:23:02.863803 (Thread-4): 12:23:02  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Prospect`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  agencyid,
  recdate.sk_dateid sk_recorddateid,
  origdate.sk_dateid sk_updatedateid,
  p.batchid,
  nvl2(c.customerid, True, False) iscustomer, 
  p.lastname,
  p.firstname,
  p.middleinitial,
  p.gender,
  p.addressline1,
  p.addressline2,
  p.postalcode,
  city,
  state,
  country,
  phone,
  income,
  numbercars,
  numberchildren,
  maritalstatus,
  age,
  creditrating,
  ownorrentflag,
  employer,
  numbercreditcards,
  networth,
  if(
    isnotnull(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+","")),
    left(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+",""),
      length(
        if(networth > 1000000 or income > 200000,"HighValue+","") || 
        if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
        if(age > 45, "Boomer+", "") ||
        if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
        if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
        if(age < 25 and networth > 1000000, "Inherited+",""))
      -1),
    NULL) marketingnameplate
FROM (
  SELECT 
    * FROM (
    SELECT
      agencyid,
      max(batchid) recordbatchid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth,
      min(batchid) batchid
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`ProspectRaw` p
    GROUP BY
      agencyid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth)
  QUALIFY ROW_NUMBER() OVER (PARTITION BY agencyid ORDER BY batchid DESC) = 1) p
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) recdate
  ON p.recordbatchid = recdate.batchid
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) origdate
  ON p.batchid = origdate.batchid
LEFT JOIN (
  SELECT 
    customerid,
    lastname,
    firstname,
    addressline1,
    addressline2,
    postalcode
  FROM `dbt_shabbirkdb`.`DimCustomerStg`
  WHERE iscurrent) c
  ON 
    upper(p.LastName) = upper(c.lastname)
    and upper(p.FirstName) = upper(c.firstname)
    and upper(p.AddressLine1) = upper(c.addressline1)
    and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
    and upper(p.PostalCode) = upper(c.postalcode)
  
2023-04-20 12:23:02.996109 (Thread-3): 12:23:02  SQL status: OK in 3.29 seconds
2023-04-20 12:23:03.000242 (Thread-3): 12:23:03  Timing info for model.dbsql_dbt_tpch.DimAccount (execute): 2023-04-20 12:22:58.941058 => 2023-04-20 12:23:03.000091
2023-04-20 12:23:03.000482 (Thread-3): 12:23:03  On model.dbsql_dbt_tpch.DimAccount: ROLLBACK
2023-04-20 12:23:03.000640 (Thread-3): 12:23:03  Databricks adapter: NotImplemented: rollback
2023-04-20 12:23:03.000763 (Thread-3): 12:23:03  On model.dbsql_dbt_tpch.DimAccount: Close
2023-04-20 12:23:03.006009 (Thread-1117): handling poll request
2023-04-20 12:23:03.006460 (Thread-1117): 12:23:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb508137190>]}
2023-04-20 12:23:03.007277 (Thread-1117): sending response (<Response 18979 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:03.293957 (Thread-3): 12:23:03  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f784ad534c0>]}
2023-04-20 12:23:03.294490 (Thread-3): 12:23:03  7 of 15 OK created sql table model dbt_shabbirkdb.DimAccount ................... [OK in 4.37s]
2023-04-20 12:23:03.294723 (Thread-3): 12:23:03  Finished running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 12:23:03.295670 (Thread-1): 12:23:03  Began running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 12:23:03.296007 (Thread-1): 12:23:03  9 of 15 START sql table model dbt_shabbirkdb.DimTrade .......................... [RUN]
2023-04-20 12:23:03.296525 (Thread-1): 12:23:03  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimTrade'
2023-04-20 12:23:03.296713 (Thread-1): 12:23:03  Began compiling node model.dbsql_dbt_tpch.DimTrade
2023-04-20 12:23:03.300870 (Thread-3): 12:23:03  Began running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 12:23:03.301207 (Thread-3): 12:23:03  10 of 15 START sql table model dbt_shabbirkdb.FactCashBalances ................. [RUN]
2023-04-20 12:23:03.301689 (Thread-3): 12:23:03  Acquiring new databricks connection 'model.dbsql_dbt_tpch.FactCashBalances'
2023-04-20 12:23:03.301882 (Thread-3): 12:23:03  Began compiling node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 12:23:03.308150 (Thread-1): 12:23:03  Writing injected SQL for node "model.dbsql_dbt_tpch.DimTrade"
2023-04-20 12:23:03.309417 (Thread-3): 12:23:03  Writing injected SQL for node "model.dbsql_dbt_tpch.FactCashBalances"
2023-04-20 12:23:03.323386 (Thread-3): 12:23:03  Timing info for model.dbsql_dbt_tpch.FactCashBalances (compile): 2023-04-20 12:23:03.301930 => 2023-04-20 12:23:03.323199
2023-04-20 12:23:03.323660 (Thread-3): 12:23:03  Began executing node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 12:23:03.328108 (Thread-3): 12:23:03  Writing runtime sql for node "model.dbsql_dbt_tpch.FactCashBalances"
2023-04-20 12:23:03.336793 (Thread-1): 12:23:03  Timing info for model.dbsql_dbt_tpch.DimTrade (compile): 2023-04-20 12:23:03.296758 => 2023-04-20 12:23:03.336619
2023-04-20 12:23:03.337032 (Thread-1): 12:23:03  Began executing node model.dbsql_dbt_tpch.DimTrade
2023-04-20 12:23:03.341558 (Thread-1): 12:23:03  Writing runtime sql for node "model.dbsql_dbt_tpch.DimTrade"
2023-04-20 12:23:03.342417 (Thread-3): 12:23:03  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:23:03.342617 (Thread-3): 12:23:03  Using databricks connection "model.dbsql_dbt_tpch.FactCashBalances"
2023-04-20 12:23:03.342818 (Thread-3): 12:23:03  On model.dbsql_dbt_tpch.FactCashBalances: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactCashBalances"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactCashBalances`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT
  a.sk_customerid, 
  a.sk_accountid, 
  d.sk_dateid, 
  sum(account_daily_total) OVER (partition by c.accountid order by c.datevalue) cash,
  c.batchid
FROM (
  SELECT 
    ct_ca_id accountid,
    to_date(ct_dts) datevalue,
    sum(ct_amt) account_daily_total,
    batchid
  FROM (
    SELECT * , 1 batchid
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CashTransactionHistory`
    UNION ALL
    SELECT * except(cdc_flag, cdc_dsn)
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CashTransactionIncremental`
  )
  GROUP BY
    accountid,
    datevalue,
    batchid) c 
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
  ON c.datevalue = d.datevalue
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Account IDs are missing from DimAccount, causing audit check failures. 
 JOIN `dbt_shabbirkdb`.`DimAccount` a 
  ON 
    c.accountid = a.accountid
    AND c.datevalue >= a.effectivedate 
    AND c.datevalue < a.enddate
  
2023-04-20 12:23:03.342985 (Thread-3): 12:23:03  Opening a new connection, currently in state closed
2023-04-20 12:23:03.368159 (Thread-1): 12:23:03  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:23:03.368374 (Thread-1): 12:23:03  Using databricks connection "model.dbsql_dbt_tpch.DimTrade"
2023-04-20 12:23:03.368666 (Thread-1): 12:23:03  On model.dbsql_dbt_tpch.DimTrade: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimTrade"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimTrade`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  trade.tradeid,
  sk_brokerid,
  trade.sk_createdateid,
  trade.sk_createtimeid,
  trade.sk_closedateid,
  trade.sk_closetimeid,
  st_name status,
  tt_name type,
  trade.cashflag,
  sk_securityid,
  sk_companyid,
  trade.quantity,
  trade.bidprice,
  sk_customerid,
  sk_accountid,
  trade.executedby,
  trade.tradeprice,
  trade.fee,
  trade.commission,
  trade.tax,
  trade.batchid
FROM (
  SELECT * EXCEPT(t_dts)
  FROM (
    SELECT
      tradeid,
      min(date(t_dts)) OVER (PARTITION BY tradeid) createdate,
      t_dts,
      coalesce(sk_createdateid, last_value(sk_createdateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createdateid,
      coalesce(sk_createtimeid, last_value(sk_createtimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createtimeid,
      coalesce(sk_closedateid, last_value(sk_closedateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closedateid,
      coalesce(sk_closetimeid, last_value(sk_closetimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closetimeid,
      cashflag,
      t_st_id,
      t_tt_id,
      t_s_symb,
      quantity,
      bidprice,
      t_ca_id,
      executedby,
      tradeprice,
      fee,
      commission,
      tax,
      batchid
    FROM (
      SELECT
        tradeid,
        t_dts,
        if(create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_createdateid,
        if(create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_createtimeid,
        if(!create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_closedateid,
        if(!create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_closetimeid,
        CASE 
          WHEN t_is_cash = 1 then TRUE
          WHEN t_is_cash = 0 then FALSE
          ELSE cast(null as BOOLEAN) END AS cashflag,
        t_st_id,
        t_tt_id,
        t_s_symb,
        quantity,
        bidprice,
        t_ca_id,
        executedby,
        tradeprice,
        fee,
        commission,
        tax,
        t.batchid
      FROM (
        SELECT
          t_id tradeid,
          th_dts t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          1 batchid,
          CASE 
            WHEN (th_st_id == "SBMT" AND t_tt_id IN ("TMB", "TMS")) OR th_st_id = "PNDG" THEN TRUE 
            WHEN th_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistory` t
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistoryRaw` th
          ON th_t_id = t_id
        UNION ALL
        SELECT
          t_id tradeid,
          t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          t.batchid,
          CASE 
            WHEN cdc_flag = 'I' THEN TRUE 
            WHEN t_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeIncremental` t
      ) t
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` dd
        ON date(t.t_dts) = dd.datevalue
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimTime` dt
        ON date_format(t.t_dts, 'HH:mm:ss') = dt.timevalue
    )
  )
  QUALIFY ROW_NUMBER() OVER (PARTITION BY tradeid ORDER BY t_dts desc) = 1
) trade
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` status
  ON status.st_id = trade.t_st_id
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeType` tt
  ON tt.tt_id == trade.t_tt_id
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Account IDs are missing from DimSecurity/DimAccount, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` ds
  ON 
    ds.symbol = trade.t_s_symb
    AND createdate >= ds.effectivedate 
    AND createdate < ds.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimAccount` da
  ON 
    trade.t_ca_id = da.accountid 
    AND createdate >= da.effectivedate 
    AND createdate < da.enddate
  
2023-04-20 12:23:03.368805 (Thread-1): 12:23:03  Opening a new connection, currently in state closed
2023-04-20 12:23:04.063916 (Thread-1): 12:23:04  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimTrade"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimTrade`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  trade.tradeid,
  sk_brokerid,
  trade.sk_createdateid,
  trade.sk_createtimeid,
  trade.sk_closedateid,
  trade.sk_closetimeid,
  st_name status,
  tt_name type,
  trade.cashflag,
  sk_securityid,
  sk_companyid,
  trade.quantity,
  trade.bidprice,
  sk_customerid,
  sk_accountid,
  trade.executedby,
  trade.tradeprice,
  trade.fee,
  trade.commission,
  trade.tax,
  trade.batchid
FROM (
  SELECT * EXCEPT(t_dts)
  FROM (
    SELECT
      tradeid,
      min(date(t_dts)) OVER (PARTITION BY tradeid) createdate,
      t_dts,
      coalesce(sk_createdateid, last_value(sk_createdateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createdateid,
      coalesce(sk_createtimeid, last_value(sk_createtimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createtimeid,
      coalesce(sk_closedateid, last_value(sk_closedateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closedateid,
      coalesce(sk_closetimeid, last_value(sk_closetimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closetimeid,
      cashflag,
      t_st_id,
      t_tt_id,
      t_s_symb,
      quantity,
      bidprice,
      t_ca_id,
      executedby,
      tradeprice,
      fee,
      commission,
      tax,
      batchid
    FROM (
      SELECT
        tradeid,
        t_dts,
        if(create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_createdateid,
        if(create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_createtimeid,
        if(!create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_closedateid,
        if(!create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_closetimeid,
        CASE 
          WHEN t_is_cash = 1 then TRUE
          WHEN t_is_cash = 0 then FALSE
          ELSE cast(null as BOOLEAN) END AS cashflag,
        t_st_id,
        t_tt_id,
        t_s_symb,
        quantity,
        bidprice,
        t_ca_id,
        executedby,
        tradeprice,
        fee,
        commission,
        tax,
        t.batchid
      FROM (
        SELECT
          t_id tradeid,
          th_dts t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          1 batchid,
          CASE 
            WHEN (th_st_id == "SBMT" AND t_tt_id IN ("TMB", "TMS")) OR th_st_id = "PNDG" THEN TRUE 
            WHEN th_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistory` t
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistoryRaw` th
          ON th_t_id = t_id
        UNION ALL
        SELECT
          t_id tradeid,
          t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          t.batchid,
          CASE 
            WHEN cdc_flag = 'I' THEN TRUE 
            WHEN t_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeIncremental` t
      ) t
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` dd
        ON date(t.t_dts) = dd.datevalue
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimTime` dt
        ON date_format(t.t_dts, 'HH:mm:ss') = dt.timevalue
    )
  )
  QUALIFY ROW_NUMBER() OVER (PARTITION BY tradeid ORDER BY t_dts desc) = 1
) trade
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` status
  ON status.st_id = trade.t_st_id
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeType` tt
  ON tt.tt_id == trade.t_tt_id
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Account IDs are missing from DimSecurity/DimAccount, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` ds
  ON 
    ds.symbol = trade.t_s_symb
    AND createdate >= ds.effectivedate 
    AND createdate < ds.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimAccount` da
  ON 
    trade.t_ca_id = da.accountid 
    AND createdate >= da.effectivedate 
    AND createdate < da.enddate
  
2023-04-20 12:23:04.064149 (Thread-1): 12:23:04  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 151, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimTrade"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimTrade`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  trade.tradeid,
  sk_brokerid,
  trade.sk_createdateid,
  trade.sk_createtimeid,
  trade.sk_closedateid,
  trade.sk_closetimeid,
  st_name status,
  tt_name type,
  trade.cashflag,
  sk_securityid,
  sk_companyid,
  trade.quantity,
  trade.bidprice,
  sk_customerid,
  sk_accountid,
  trade.executedby,
  trade.tradeprice,
  trade.fee,
  trade.commission,
  trade.tax,
  trade.batchid
FROM (
  SELECT * EXCEPT(t_dts)
  FROM (
    SELECT
      tradeid,
      min(date(t_dts)) OVER (PARTITION BY tradeid) createdate,
      t_dts,
      coalesce(sk_createdateid, last_value(sk_createdateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createdateid,
      coalesce(sk_createtimeid, last_value(sk_createtimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createtimeid,
      coalesce(sk_closedateid, last_value(sk_closedateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closedateid,
      coalesce(sk_closetimeid, last_value(sk_closetimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closetimeid,
      cashflag,
      t_st_id,
      t_tt_id,
      t_s_symb,
      quantity,
      bidprice,
      t_ca_id,
      executedby,
      tradeprice,
      fee,
      commission,
      tax,
      batchid
    FROM (
      SELECT
        tradeid,
        t_dts,
        if(create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_createdateid,
        if(create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_createtimeid,
        if(!create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_closedateid,
        if(!create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_closetimeid,
        CASE 
          WHEN t_is_cash = 1 then TRUE
          WHEN t_is_cash = 0 then FALSE
          ELSE cast(null as BOOLEAN) END AS cashflag,
        t_st_id,
        t_tt_id,
        t_s_symb,
        quantity,
        bidprice,
        t_ca_id,
        executedby,
        tradeprice,
        fee,
        commission,
        tax,
        t.batchid
      FROM (
        SELECT
          t_id tradeid,
          th_dts t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          1 batchid,
          CASE 
            WHEN (th_st_id == "SBMT" AND t_tt_id IN ("TMB", "TMS")) OR th_st_id = "PNDG" THEN TRUE 
            WHEN th_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistory` t
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistoryRaw` th
          ON th_t_id = t_id
        UNION ALL
        SELECT
          t_id tradeid,
          t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          t.batchid,
          CASE 
            WHEN cdc_flag = 'I' THEN TRUE 
            WHEN t_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeIncremental` t
      ) t
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` dd
        ON date(t.t_dts) = dd.datevalue
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimTime` dt
        ON date_format(t.t_dts, 'HH:mm:ss') = dt.timevalue
    )
  )
  QUALIFY ROW_NUMBER() OVER (PARTITION BY tradeid ORDER BY t_dts desc) = 1
) trade
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` status
  ON status.st_id = trade.t_st_id
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeType` tt
  ON tt.tt_id == trade.t_tt_id
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Account IDs are missing from DimSecurity/DimAccount, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` ds
^^^
  ON 
    ds.symbol = trade.t_s_symb
    AND createdate >= ds.effectivedate 
    AND createdate < ds.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimAccount` da
  ON 
    trade.t_ca_id = da.accountid 
    AND createdate >= da.effectivedate 
    AND createdate < da.enddate
  

2023-04-20 12:23:04.064282 (Thread-1): 12:23:04  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 151, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimTrade"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimTrade`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  trade.tradeid,
  sk_brokerid,
  trade.sk_createdateid,
  trade.sk_createtimeid,
  trade.sk_closedateid,
  trade.sk_closetimeid,
  st_name status,
  tt_name type,
  trade.cashflag,
  sk_securityid,
  sk_companyid,
  trade.quantity,
  trade.bidprice,
  sk_customerid,
  sk_accountid,
  trade.executedby,
  trade.tradeprice,
  trade.fee,
  trade.commission,
  trade.tax,
  trade.batchid
FROM (
  SELECT * EXCEPT(t_dts)
  FROM (
    SELECT
      tradeid,
      min(date(t_dts)) OVER (PARTITION BY tradeid) createdate,
      t_dts,
      coalesce(sk_createdateid, last_value(sk_createdateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createdateid,
      coalesce(sk_createtimeid, last_value(sk_createtimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createtimeid,
      coalesce(sk_closedateid, last_value(sk_closedateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closedateid,
      coalesce(sk_closetimeid, last_value(sk_closetimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closetimeid,
      cashflag,
      t_st_id,
      t_tt_id,
      t_s_symb,
      quantity,
      bidprice,
      t_ca_id,
      executedby,
      tradeprice,
      fee,
      commission,
      tax,
      batchid
    FROM (
      SELECT
        tradeid,
        t_dts,
        if(create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_createdateid,
        if(create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_createtimeid,
        if(!create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_closedateid,
        if(!create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_closetimeid,
        CASE 
          WHEN t_is_cash = 1 then TRUE
          WHEN t_is_cash = 0 then FALSE
          ELSE cast(null as BOOLEAN) END AS cashflag,
        t_st_id,
        t_tt_id,
        t_s_symb,
        quantity,
        bidprice,
        t_ca_id,
        executedby,
        tradeprice,
        fee,
        commission,
        tax,
        t.batchid
      FROM (
        SELECT
          t_id tradeid,
          th_dts t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          1 batchid,
          CASE 
            WHEN (th_st_id == "SBMT" AND t_tt_id IN ("TMB", "TMS")) OR th_st_id = "PNDG" THEN TRUE 
            WHEN th_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistory` t
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistoryRaw` th
          ON th_t_id = t_id
        UNION ALL
        SELECT
          t_id tradeid,
          t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          t.batchid,
          CASE 
            WHEN cdc_flag = 'I' THEN TRUE 
            WHEN t_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeIncremental` t
      ) t
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` dd
        ON date(t.t_dts) = dd.datevalue
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimTime` dt
        ON date_format(t.t_dts, 'HH:mm:ss') = dt.timevalue
    )
  )
  QUALIFY ROW_NUMBER() OVER (PARTITION BY tradeid ORDER BY t_dts desc) = 1
) trade
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` status
  ON status.st_id = trade.t_st_id
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeType` tt
  ON tt.tt_id == trade.t_tt_id
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Account IDs are missing from DimSecurity/DimAccount, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` ds
^^^
  ON 
    ds.symbol = trade.t_s_symb
    AND createdate >= ds.effectivedate 
    AND createdate < ds.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimAccount` da
  ON 
    trade.t_ca_id = da.accountid 
    AND createdate >= da.effectivedate 
    AND createdate < da.enddate
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 151, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimTrade"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimTrade`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  trade.tradeid,
  sk_brokerid,
  trade.sk_createdateid,
  trade.sk_createtimeid,
  trade.sk_closedateid,
  trade.sk_closetimeid,
  st_name status,
  tt_name type,
  trade.cashflag,
  sk_securityid,
  sk_companyid,
  trade.quantity,
  trade.bidprice,
  sk_customerid,
  sk_accountid,
  trade.executedby,
  trade.tradeprice,
  trade.fee,
  trade.commission,
  trade.tax,
  trade.batchid
FROM (
  SELECT * EXCEPT(t_dts)
  FROM (
    SELECT
      tradeid,
      min(date(t_dts)) OVER (PARTITION BY tradeid) createdate,
      t_dts,
      coalesce(sk_createdateid, last_value(sk_createdateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createdateid,
      coalesce(sk_createtimeid, last_value(sk_createtimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_createtimeid,
      coalesce(sk_closedateid, last_value(sk_closedateid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closedateid,
      coalesce(sk_closetimeid, last_value(sk_closetimeid) IGNORE NULLS OVER (
        PARTITION BY tradeid ORDER BY t_dts)) sk_closetimeid,
      cashflag,
      t_st_id,
      t_tt_id,
      t_s_symb,
      quantity,
      bidprice,
      t_ca_id,
      executedby,
      tradeprice,
      fee,
      commission,
      tax,
      batchid
    FROM (
      SELECT
        tradeid,
        t_dts,
        if(create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_createdateid,
        if(create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_createtimeid,
        if(!create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_closedateid,
        if(!create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_closetimeid,
        CASE 
          WHEN t_is_cash = 1 then TRUE
          WHEN t_is_cash = 0 then FALSE
          ELSE cast(null as BOOLEAN) END AS cashflag,
        t_st_id,
        t_tt_id,
        t_s_symb,
        quantity,
        bidprice,
        t_ca_id,
        executedby,
        tradeprice,
        fee,
        commission,
        tax,
        t.batchid
      FROM (
        SELECT
          t_id tradeid,
          th_dts t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          1 batchid,
          CASE 
            WHEN (th_st_id == "SBMT" AND t_tt_id IN ("TMB", "TMS")) OR th_st_id = "PNDG" THEN TRUE 
            WHEN th_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistory` t
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistoryRaw` th
          ON th_t_id = t_id
        UNION ALL
        SELECT
          t_id tradeid,
          t_dts,
          t_st_id,
          t_tt_id,
          t_is_cash,
          t_s_symb,
          t_qty AS quantity,
          t_bid_price AS bidprice,
          t_ca_id,
          t_exec_name AS executedby,
          t_trade_price AS tradeprice,
          t_chrg AS fee,
          t_comm AS commission,
          t_tax AS tax,
          t.batchid,
          CASE 
            WHEN cdc_flag = 'I' THEN TRUE 
            WHEN t_st_id IN ("CMPT", "CNCL") THEN FALSE 
            ELSE cast(null as boolean) END AS create_flg
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeIncremental` t
      ) t
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` dd
        ON date(t.t_dts) = dd.datevalue
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimTime` dt
        ON date_format(t.t_dts, 'HH:mm:ss') = dt.timevalue
    )
  )
  QUALIFY ROW_NUMBER() OVER (PARTITION BY tradeid ORDER BY t_dts desc) = 1
) trade
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` status
  ON status.st_id = trade.t_st_id
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeType` tt
  ON tt.tt_id == trade.t_tt_id
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Account IDs are missing from DimSecurity/DimAccount, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` ds
^^^
  ON 
    ds.symbol = trade.t_s_symb
    AND createdate >= ds.effectivedate 
    AND createdate < ds.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimAccount` da
  ON 
    trade.t_ca_id = da.accountid 
    AND createdate >= da.effectivedate 
    AND createdate < da.enddate
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 12:23:04.064400 (Thread-1): 12:23:04  Databricks adapter: operation-id: b'\x01\xed\xdfv\x19\x1d\x1fx\xb4"&\x14\xab#\xe9\x8c'
2023-04-20 12:23:04.064644 (Thread-1): 12:23:04  Timing info for model.dbsql_dbt_tpch.DimTrade (execute): 2023-04-20 12:23:03.337083 => 2023-04-20 12:23:04.064513
2023-04-20 12:23:04.064822 (Thread-1): 12:23:04  On model.dbsql_dbt_tpch.DimTrade: ROLLBACK
2023-04-20 12:23:04.064949 (Thread-1): 12:23:04  Databricks adapter: NotImplemented: rollback
2023-04-20 12:23:04.065072 (Thread-1): 12:23:04  On model.dbsql_dbt_tpch.DimTrade: Close
2023-04-20 12:23:04.343247 (Thread-1): 12:23:04  Runtime Error in model DimTrade (models/incremental/DimTrade.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 151, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimTrade"} */
  
    
      
          create or replace table `dbt_shabbirkdb`.`DimTrade`
        
        
      using delta
        
        
        
        
        
        
        as
        
  SELECT
    trade.tradeid,
    sk_brokerid,
    trade.sk_createdateid,
    trade.sk_createtimeid,
    trade.sk_closedateid,
    trade.sk_closetimeid,
    st_name status,
    tt_name type,
    trade.cashflag,
    sk_securityid,
    sk_companyid,
    trade.quantity,
    trade.bidprice,
    sk_customerid,
    sk_accountid,
    trade.executedby,
    trade.tradeprice,
    trade.fee,
    trade.commission,
    trade.tax,
    trade.batchid
  FROM (
    SELECT * EXCEPT(t_dts)
    FROM (
      SELECT
        tradeid,
        min(date(t_dts)) OVER (PARTITION BY tradeid) createdate,
        t_dts,
        coalesce(sk_createdateid, last_value(sk_createdateid) IGNORE NULLS OVER (
          PARTITION BY tradeid ORDER BY t_dts)) sk_createdateid,
        coalesce(sk_createtimeid, last_value(sk_createtimeid) IGNORE NULLS OVER (
          PARTITION BY tradeid ORDER BY t_dts)) sk_createtimeid,
        coalesce(sk_closedateid, last_value(sk_closedateid) IGNORE NULLS OVER (
          PARTITION BY tradeid ORDER BY t_dts)) sk_closedateid,
        coalesce(sk_closetimeid, last_value(sk_closetimeid) IGNORE NULLS OVER (
          PARTITION BY tradeid ORDER BY t_dts)) sk_closetimeid,
        cashflag,
        t_st_id,
        t_tt_id,
        t_s_symb,
        quantity,
        bidprice,
        t_ca_id,
        executedby,
        tradeprice,
        fee,
        commission,
        tax,
        batchid
      FROM (
        SELECT
          tradeid,
          t_dts,
          if(create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_createdateid,
          if(create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_createtimeid,
          if(!create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_closedateid,
          if(!create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_closetimeid,
          CASE 
            WHEN t_is_cash = 1 then TRUE
            WHEN t_is_cash = 0 then FALSE
            ELSE cast(null as BOOLEAN) END AS cashflag,
          t_st_id,
          t_tt_id,
          t_s_symb,
          quantity,
          bidprice,
          t_ca_id,
          executedby,
          tradeprice,
          fee,
          commission,
          tax,
          t.batchid
        FROM (
          SELECT
            t_id tradeid,
            th_dts t_dts,
            t_st_id,
            t_tt_id,
            t_is_cash,
            t_s_symb,
            t_qty AS quantity,
            t_bid_price AS bidprice,
            t_ca_id,
            t_exec_name AS executedby,
            t_trade_price AS tradeprice,
            t_chrg AS fee,
            t_comm AS commission,
            t_tax AS tax,
            1 batchid,
            CASE 
              WHEN (th_st_id == "SBMT" AND t_tt_id IN ("TMB", "TMS")) OR th_st_id = "PNDG" THEN TRUE 
              WHEN th_st_id IN ("CMPT", "CNCL") THEN FALSE 
              ELSE cast(null as boolean) END AS create_flg
          FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistory` t
          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistoryRaw` th
            ON th_t_id = t_id
          UNION ALL
          SELECT
            t_id tradeid,
            t_dts,
            t_st_id,
            t_tt_id,
            t_is_cash,
            t_s_symb,
            t_qty AS quantity,
            t_bid_price AS bidprice,
            t_ca_id,
            t_exec_name AS executedby,
            t_trade_price AS tradeprice,
            t_chrg AS fee,
            t_comm AS commission,
            t_tax AS tax,
            t.batchid,
            CASE 
              WHEN cdc_flag = 'I' THEN TRUE 
              WHEN t_st_id IN ("CMPT", "CNCL") THEN FALSE 
              ELSE cast(null as boolean) END AS create_flg
          FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeIncremental` t
        ) t
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` dd
          ON date(t.t_dts) = dd.datevalue
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimTime` dt
          ON date_format(t.t_dts, 'HH:mm:ss') = dt.timevalue
      )
    )
    QUALIFY ROW_NUMBER() OVER (PARTITION BY tradeid ORDER BY t_dts desc) = 1
  ) trade
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` status
    ON status.st_id = trade.t_st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeType` tt
    ON tt.tt_id == trade.t_tt_id
  -- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Account IDs are missing from DimSecurity/DimAccount, causing audit check failures. 
  ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` ds
  ^^^
    ON 
      ds.symbol = trade.t_s_symb
      AND createdate >= ds.effectivedate 
      AND createdate < ds.enddate
  ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimAccount` da
    ON 
      trade.t_ca_id = da.accountid 
      AND createdate >= da.effectivedate 
      AND createdate < da.enddate
    
  
2023-04-20 12:23:04.343977 (Thread-1): 12:23:04  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f784ad47eb0>]}
2023-04-20 12:23:04.344546 (Thread-1): 12:23:04  9 of 15 ERROR creating sql table model dbt_shabbirkdb.DimTrade ................. [ERROR in 1.05s]
2023-04-20 12:23:04.344856 (Thread-1): 12:23:04  Finished running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 12:23:04.345769 (Thread-1): 12:23:04  Began running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 12:23:04.346101 (Thread-1): 12:23:04  11 of 15 SKIP relation dbt_shabbirkdb.FactHoldings ............................. [SKIP]
2023-04-20 12:23:04.346357 (Thread-1): 12:23:04  Finished running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 12:23:04.599694 (Thread-1118): handling poll request
2023-04-20 12:23:04.600173 (Thread-1118): 12:23:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f81212b0>]}
2023-04-20 12:23:04.601253 (Thread-1118): sending response (<Response 56296 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:04.622023 (Thread-1119): handling ps request
2023-04-20 12:23:04.622431 (Thread-1119): 12:23:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f81213a0>]}
2023-04-20 12:23:04.624246 (Thread-1119): sending response (<Response 27884 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:05.075911 (Thread-1120): handling status request
2023-04-20 12:23:05.076402 (Thread-1120): 12:23:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f81215e0>]}
2023-04-20 12:23:05.076883 (Thread-1120): sending response (<Response 1928 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:05.496884 (Thread-2): 12:23:05  SQL status: OK in 6.38 seconds
2023-04-20 12:23:05.798829 (Thread-2): 12:23:05  Timing info for model.dbsql_dbt_tpch.Financial (execute): 2023-04-20 12:22:58.310383 => 2023-04-20 12:23:05.798618
2023-04-20 12:23:05.799104 (Thread-2): 12:23:05  On model.dbsql_dbt_tpch.Financial: ROLLBACK
2023-04-20 12:23:05.799257 (Thread-2): 12:23:05  Databricks adapter: NotImplemented: rollback
2023-04-20 12:23:05.799385 (Thread-2): 12:23:05  On model.dbsql_dbt_tpch.Financial: Close
2023-04-20 12:23:06.084247 (Thread-2): 12:23:06  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f784ad125b0>]}
2023-04-20 12:23:06.084810 (Thread-2): 12:23:06  6 of 15 OK created sql table model dbt_shabbirkdb.Financial .................... [OK in 7.79s]
2023-04-20 12:23:06.085048 (Thread-2): 12:23:06  Finished running node model.dbsql_dbt_tpch.Financial
2023-04-20 12:23:06.085952 (Thread-1): 12:23:06  Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:23:06.086291 (Thread-1): 12:23:06  12 of 15 START sql table model dbt_shabbirkdb.tempSumpFiBasicEps ............... [RUN]
2023-04-20 12:23:06.086802 (Thread-1): 12:23:06  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempSumpFiBasicEps'
2023-04-20 12:23:06.086988 (Thread-1): 12:23:06  Began compiling node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:23:06.091431 (Thread-1): 12:23:06  Writing injected SQL for node "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:23:06.104421 (Thread-1): 12:23:06  Timing info for model.dbsql_dbt_tpch.tempSumpFiBasicEps (compile): 2023-04-20 12:23:06.087035 => 2023-04-20 12:23:06.104259
2023-04-20 12:23:06.104647 (Thread-1): 12:23:06  Began executing node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:23:06.110814 (Thread-1): 12:23:06  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:23:06.110988 (Thread-1): 12:23:06  Using databricks connection "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:23:06.111147 (Thread-1): 12:23:06  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempSumpFiBasicEps"} */

      describe extended `dbt_shabbirkdb`.`tempsumpfibasiceps`
  
2023-04-20 12:23:06.111278 (Thread-1): 12:23:06  Opening a new connection, currently in state closed
2023-04-20 12:23:06.228797 (Thread-1121): handling poll request
2023-04-20 12:23:06.229276 (Thread-1121): 12:23:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8121dc0>]}
2023-04-20 12:23:06.229985 (Thread-1121): sending response (<Response 8262 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:06.768089 (Thread-4): 12:23:06  SQL status: OK in 3.9 seconds
2023-04-20 12:23:06.770454 (Thread-4): 12:23:06  Timing info for model.dbsql_dbt_tpch.Prospect (execute): 2023-04-20 12:23:02.113242 => 2023-04-20 12:23:06.770297
2023-04-20 12:23:06.770668 (Thread-4): 12:23:06  On model.dbsql_dbt_tpch.Prospect: ROLLBACK
2023-04-20 12:23:06.770813 (Thread-4): 12:23:06  Databricks adapter: NotImplemented: rollback
2023-04-20 12:23:06.770937 (Thread-4): 12:23:06  On model.dbsql_dbt_tpch.Prospect: Close
2023-04-20 12:23:06.854577 (Thread-1): 12:23:06  SQL status: OK in 0.74 seconds
2023-04-20 12:23:06.860219 (Thread-1): 12:23:06  Writing runtime sql for node "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:23:06.874253 (Thread-1): 12:23:06  Using databricks connection "model.dbsql_dbt_tpch.tempSumpFiBasicEps"
2023-04-20 12:23:06.874469 (Thread-1): 12:23:06  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempSumpFiBasicEps"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempSumpFiBasicEps`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  sk_companyid,
  fi_qtr_start_date,
  sum(fi_basic_eps) OVER (PARTITION BY companyid ORDER BY fi_qtr_start_date ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) - fi_basic_eps sum_fi_basic_eps
FROM `dbt_shabbirkdb`.`Financial`
JOIN `dbt_shabbirkdb`.`DimCompany`
  USING (sk_companyid);
  
2023-04-20 12:23:07.046245 (Thread-4): 12:23:07  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f786c0a8100>]}
2023-04-20 12:23:07.047024 (Thread-4): 12:23:07  8 of 15 OK created sql table model dbt_shabbirkdb.Prospect ..................... [OK in 4.95s]
2023-04-20 12:23:07.047420 (Thread-4): 12:23:07  Finished running node model.dbsql_dbt_tpch.Prospect
2023-04-20 12:23:07.048368 (Thread-2): 12:23:07  Began running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:23:07.048696 (Thread-2): 12:23:07  13 of 15 START sql table model dbt_shabbirkdb.DimCustomer ...................... [RUN]
2023-04-20 12:23:07.049292 (Thread-2): 12:23:07  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomer'
2023-04-20 12:23:07.049512 (Thread-2): 12:23:07  Began compiling node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:23:07.054632 (Thread-2): 12:23:07  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 12:23:07.069241 (Thread-2): 12:23:07  Timing info for model.dbsql_dbt_tpch.DimCustomer (compile): 2023-04-20 12:23:07.049564 => 2023-04-20 12:23:07.069070
2023-04-20 12:23:07.069493 (Thread-2): 12:23:07  Began executing node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:23:07.074022 (Thread-2): 12:23:07  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:23:07.074295 (Thread-2): 12:23:07  Using databricks connection "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 12:23:07.074616 (Thread-2): 12:23:07  On model.dbsql_dbt_tpch.DimCustomer: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

      describe extended `dbt_shabbirkdb`.`dimcustomer`
  
2023-04-20 12:23:07.074772 (Thread-2): 12:23:07  Opening a new connection, currently in state closed
2023-04-20 12:23:07.257030 (Thread-1122): handling ps request
2023-04-20 12:23:07.257525 (Thread-1122): 12:23:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f84d26d0>]}
2023-04-20 12:23:07.259505 (Thread-1122): sending response (<Response 27884 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:07.706155 (Thread-1123): handling status request
2023-04-20 12:23:07.706650 (Thread-1123): 12:23:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f84d29a0>]}
2023-04-20 12:23:07.731781 (Thread-1123): sending response (<Response 1928 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:07.798528 (Thread-2): 12:23:07  SQL status: OK in 0.72 seconds
2023-04-20 12:23:07.804244 (Thread-2): 12:23:07  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 12:23:07.820472 (Thread-2): 12:23:07  Using databricks connection "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 12:23:07.820719 (Thread-2): 12:23:07  On model.dbsql_dbt_tpch.DimCustomer: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 12:23:07.837133 (Thread-1124): handling poll request
2023-04-20 12:23:07.837584 (Thread-1124): 12:23:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f84d2c70>]}
2023-04-20 12:23:07.838347 (Thread-1124): sending response (<Response 13257 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:09.532881 (Thread-1125): handling poll request
2023-04-20 12:23:09.533364 (Thread-1125): 12:23:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f84d2eb0>]}
2023-04-20 12:23:09.533853 (Thread-1125): sending response (<Response 290 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:09.903724 (Thread-1126): handling ps request
2023-04-20 12:23:09.904207 (Thread-1126): 12:23:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f84d29d0>]}
2023-04-20 12:23:09.906046 (Thread-1126): sending response (<Response 27884 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:10.046568 (Thread-3): 12:23:10  SQL status: OK in 6.7 seconds
2023-04-20 12:23:10.248698 (Thread-1): 12:23:10  SQL status: OK in 3.37 seconds
2023-04-20 12:23:10.251471 (Thread-1): 12:23:10  Timing info for model.dbsql_dbt_tpch.tempSumpFiBasicEps (execute): 2023-04-20 12:23:06.104699 => 2023-04-20 12:23:10.251316
2023-04-20 12:23:10.251718 (Thread-1): 12:23:10  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: ROLLBACK
2023-04-20 12:23:10.251866 (Thread-1): 12:23:10  Databricks adapter: NotImplemented: rollback
2023-04-20 12:23:10.251988 (Thread-1): 12:23:10  On model.dbsql_dbt_tpch.tempSumpFiBasicEps: Close
2023-04-20 12:23:10.360647 (Thread-1127): handling status request
2023-04-20 12:23:10.361124 (Thread-1127): 12:23:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f84d22b0>]}
2023-04-20 12:23:10.361610 (Thread-1127): sending response (<Response 1928 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:10.362435 (Thread-3): 12:23:10  Timing info for model.dbsql_dbt_tpch.FactCashBalances (execute): 2023-04-20 12:23:03.323714 => 2023-04-20 12:23:10.362237
2023-04-20 12:23:10.362688 (Thread-3): 12:23:10  On model.dbsql_dbt_tpch.FactCashBalances: ROLLBACK
2023-04-20 12:23:10.362835 (Thread-3): 12:23:10  Databricks adapter: NotImplemented: rollback
2023-04-20 12:23:10.362957 (Thread-3): 12:23:10  On model.dbsql_dbt_tpch.FactCashBalances: Close
2023-04-20 12:23:10.539563 (Thread-1): 12:23:10  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f784ad6b610>]}
2023-04-20 12:23:10.540089 (Thread-1): 12:23:10  12 of 15 OK created sql table model dbt_shabbirkdb.tempSumpFiBasicEps .......... [OK in 4.45s]
2023-04-20 12:23:10.540324 (Thread-1): 12:23:10  Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 12:23:10.541208 (Thread-4): 12:23:10  Began running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:23:10.541545 (Thread-4): 12:23:10  14 of 15 START sql table model dbt_shabbirkdb.FactMarketHistory ................ [RUN]
2023-04-20 12:23:10.542079 (Thread-4): 12:23:10  Acquiring new databricks connection 'model.dbsql_dbt_tpch.FactMarketHistory'
2023-04-20 12:23:10.542267 (Thread-4): 12:23:10  Began compiling node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:23:10.548526 (Thread-4): 12:23:10  Writing injected SQL for node "model.dbsql_dbt_tpch.FactMarketHistory"
2023-04-20 12:23:10.563589 (Thread-4): 12:23:10  Timing info for model.dbsql_dbt_tpch.FactMarketHistory (compile): 2023-04-20 12:23:10.542314 => 2023-04-20 12:23:10.563410
2023-04-20 12:23:10.563818 (Thread-4): 12:23:10  Began executing node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:23:10.567816 (Thread-4): 12:23:10  Writing runtime sql for node "model.dbsql_dbt_tpch.FactMarketHistory"
2023-04-20 12:23:10.581800 (Thread-4): 12:23:10  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:23:10.581997 (Thread-4): 12:23:10  Using databricks connection "model.dbsql_dbt_tpch.FactMarketHistory"
2023-04-20 12:23:10.582209 (Thread-4): 12:23:10  On model.dbsql_dbt_tpch.FactMarketHistory: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date);
  
2023-04-20 12:23:10.582347 (Thread-4): 12:23:10  Opening a new connection, currently in state closed
2023-04-20 12:23:10.635153 (Thread-3): 12:23:10  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f784adf6d60>]}
2023-04-20 12:23:10.635582 (Thread-3): 12:23:10  10 of 15 OK created sql table model dbt_shabbirkdb.FactCashBalances ............ [OK in 7.33s]
2023-04-20 12:23:10.635808 (Thread-3): 12:23:10  Finished running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 12:23:11.022081 (Thread-2): 12:23:11  SQL status: OK in 3.2 seconds
2023-04-20 12:23:11.025108 (Thread-2): 12:23:11  Timing info for model.dbsql_dbt_tpch.DimCustomer (execute): 2023-04-20 12:23:07.069559 => 2023-04-20 12:23:11.024900
2023-04-20 12:23:11.025390 (Thread-2): 12:23:11  On model.dbsql_dbt_tpch.DimCustomer: ROLLBACK
2023-04-20 12:23:11.025578 (Thread-2): 12:23:11  Databricks adapter: NotImplemented: rollback
2023-04-20 12:23:11.025755 (Thread-2): 12:23:11  On model.dbsql_dbt_tpch.DimCustomer: Close
2023-04-20 12:23:11.149343 (Thread-1128): handling poll request
2023-04-20 12:23:11.149811 (Thread-1128): 12:23:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f84cb4f0>]}
2023-04-20 12:23:11.150619 (Thread-1128): sending response (<Response 15760 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:11.310595 (Thread-2): 12:23:11  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f786c06c820>]}
2023-04-20 12:23:11.311121 (Thread-2): 12:23:11  13 of 15 OK created sql table model dbt_shabbirkdb.DimCustomer ................. [OK in 4.26s]
2023-04-20 12:23:11.311354 (Thread-2): 12:23:11  Finished running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 12:23:11.312254 (Thread-1): 12:23:11  Began running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 12:23:11.312589 (Thread-1): 12:23:11  15 of 15 START sql table model dbt_shabbirkdb.FactWatches ...................... [RUN]
2023-04-20 12:23:11.313091 (Thread-1): 12:23:11  Acquiring new databricks connection 'model.dbsql_dbt_tpch.FactWatches'
2023-04-20 12:23:11.313275 (Thread-1): 12:23:11  Began compiling node model.dbsql_dbt_tpch.FactWatches
2023-04-20 12:23:11.318375 (Thread-1): 12:23:11  Writing injected SQL for node "model.dbsql_dbt_tpch.FactWatches"
2023-04-20 12:23:11.325713 (Thread-4): 12:23:11  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date);
  
2023-04-20 12:23:11.325900 (Thread-4): 12:23:11  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date)

2023-04-20 12:23:11.326035 (Thread-4): 12:23:11  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date)

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  s.sk_securityid,
  s.sk_companyid,
  sk_dateid,
  fmh.dm_close / sum_fi_basic_eps AS peratio,
  (s.dividend / fmh.dm_close) / 100 yield,
  fiftytwoweekhigh,
  sk_fiftytwoweekhighdate,
  fiftytwoweeklow,
  sk_fiftytwoweeklowdate,
  dm_close closeprice,
  dm_high dayhigh,
  dm_low daylow,
  dm_vol volume,
  fmh.batchid
FROM (
  SELECT * FROM (
    SELECT 
      a.*,
      b.sk_dateid AS sk_fiftytwoweeklowdate,
      c.sk_dateid AS sk_fiftytwoweekhighdate
    FROM
      `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
      ON
        a.dm_s_symb = b.dm_s_symb
        AND a.fiftytwoweeklow = b.dm_low
        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
    JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
      ON 
        a.dm_s_symb = c.dm_s_symb
        AND a.fiftytwoweekhigh = c.dm_high
        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY dm_s_symb, dm_date 
    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = fmh.dm_s_symb
    AND fmh.dm_date >= s.effectivedate 
    AND fmh.dm_date < s.enddate
LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
  ON 
    f.sk_companyid = s.sk_companyid
    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
    AND year(fmh.dm_date) = year(fi_qtr_start_date)

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 12:23:11.326152 (Thread-4): 12:23:11  Databricks adapter: operation-id: b'\x01\xed\xdfv\x1dr\x16\x89\x8d>r3\xf9\x08Th'
2023-04-20 12:23:11.326370 (Thread-4): 12:23:11  Timing info for model.dbsql_dbt_tpch.FactMarketHistory (execute): 2023-04-20 12:23:10.563870 => 2023-04-20 12:23:11.326250
2023-04-20 12:23:11.326537 (Thread-4): 12:23:11  On model.dbsql_dbt_tpch.FactMarketHistory: ROLLBACK
2023-04-20 12:23:11.326664 (Thread-4): 12:23:11  Databricks adapter: NotImplemented: rollback
2023-04-20 12:23:11.326784 (Thread-4): 12:23:11  On model.dbsql_dbt_tpch.FactMarketHistory: Close
2023-04-20 12:23:11.332857 (Thread-1): 12:23:11  Timing info for model.dbsql_dbt_tpch.FactWatches (compile): 2023-04-20 12:23:11.313323 => 2023-04-20 12:23:11.332706
2023-04-20 12:23:11.333080 (Thread-1): 12:23:11  Began executing node model.dbsql_dbt_tpch.FactWatches
2023-04-20 12:23:11.337152 (Thread-1): 12:23:11  Writing runtime sql for node "model.dbsql_dbt_tpch.FactWatches"
2023-04-20 12:23:11.352457 (Thread-1): 12:23:11  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:23:11.352642 (Thread-1): 12:23:11  Using databricks connection "model.dbsql_dbt_tpch.FactWatches"
2023-04-20 12:23:11.352863 (Thread-1): 12:23:11  On model.dbsql_dbt_tpch.FactWatches: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate;
  
2023-04-20 12:23:11.352997 (Thread-1): 12:23:11  Opening a new connection, currently in state closed
2023-04-20 12:23:11.598083 (Thread-4): 12:23:11  Runtime Error in model FactMarketHistory (models/incremental/FactMarketHistory.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */
  
    
      
          create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
        
        
      using delta
        
        
        
        
        
        
        as
        
  SELECT 
    s.sk_securityid,
    s.sk_companyid,
    sk_dateid,
    fmh.dm_close / sum_fi_basic_eps AS peratio,
    (s.dividend / fmh.dm_close) / 100 yield,
    fiftytwoweekhigh,
    sk_fiftytwoweekhighdate,
    fiftytwoweeklow,
    sk_fiftytwoweeklowdate,
    dm_close closeprice,
    dm_high dayhigh,
    dm_low daylow,
    dm_vol volume,
    fmh.batchid
  FROM (
    SELECT * FROM (
      SELECT 
        a.*,
        b.sk_dateid AS sk_fiftytwoweeklowdate,
        c.sk_dateid AS sk_fiftytwoweekhighdate
      FROM
        `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
      JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
        ON
          a.dm_s_symb = b.dm_s_symb
          AND a.fiftytwoweeklow = b.dm_low
          AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
      JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
        ON 
          a.dm_s_symb = c.dm_s_symb
          AND a.fiftytwoweekhigh = c.dm_high
          AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
    QUALIFY ROW_NUMBER() OVER (
      PARTITION BY dm_s_symb, dm_date 
      ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
  -- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
  ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ^^^
    ON 
      s.symbol = fmh.dm_s_symb
      AND fmh.dm_date >= s.effectivedate 
      AND fmh.dm_date < s.enddate
  LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
    ON 
      f.sk_companyid = s.sk_companyid
      AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
      AND year(fmh.dm_date) = year(fi_qtr_start_date)
  
2023-04-20 12:23:11.598503 (Thread-4): 12:23:11  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f784ab3d940>]}
2023-04-20 12:23:11.598939 (Thread-4): 12:23:11  14 of 15 ERROR creating sql table model dbt_shabbirkdb.FactMarketHistory ....... [ERROR in 1.06s]
2023-04-20 12:23:11.599151 (Thread-4): 12:23:11  Finished running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 12:23:12.026006 (Thread-1): 12:23:12  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate;
  
2023-04-20 12:23:12.026232 (Thread-1): 12:23:12  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate

2023-04-20 12:23:12.026364 (Thread-1): 12:23:12  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */

  
    
        create or replace table `dbt_shabbirkdb`.`FactWatches`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  c.sk_customerid sk_customerid,
  s.sk_securityid sk_securityid,
  sk_dateid_dateplaced,
  sk_dateid_dateremoved,
  wh.batchid
FROM (
  SELECT * EXCEPT(w_dts)
  FROM (
    SELECT
      customerid,
      symbol,
      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
      w_dts,
      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
    FROM ( 
      SELECT 
        wh.w_c_id customerid,
        wh.w_s_symb symbol,
        if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
        if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
        if(w_action = 'ACTV', d.datevalue, null) dateplaced,
        wh.w_dts,
        batchid 
      FROM (
        SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
        UNION ALL
        SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
      JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
        ON d.datevalue = date(wh.w_dts)))
  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
^^^
  ON 
    s.symbol = wh.symbol
    AND wh.dateplaced >= s.effectivedate 
    AND wh.dateplaced < s.enddate
${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
  ON
    wh.customerid = c.customerid
    AND wh.dateplaced >= c.effectivedate 
    AND wh.dateplaced < c.enddate

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 12:23:12.026482 (Thread-1): 12:23:12  Databricks adapter: operation-id: b'\x01\xed\xdfv\x1d\xdf\x13\x15\xaef+ZB$\x07\r'
2023-04-20 12:23:12.026726 (Thread-1): 12:23:12  Timing info for model.dbsql_dbt_tpch.FactWatches (execute): 2023-04-20 12:23:11.333131 => 2023-04-20 12:23:12.026591
2023-04-20 12:23:12.026909 (Thread-1): 12:23:12  On model.dbsql_dbt_tpch.FactWatches: ROLLBACK
2023-04-20 12:23:12.027039 (Thread-1): 12:23:12  Databricks adapter: NotImplemented: rollback
2023-04-20 12:23:12.027162 (Thread-1): 12:23:12  On model.dbsql_dbt_tpch.FactWatches: Close
2023-04-20 12:23:12.296112 (Thread-1): 12:23:12  Runtime Error in model FactWatches (models/incremental/FactWatches.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */
  
    
      
          create or replace table `dbt_shabbirkdb`.`FactWatches`
        
        
      using delta
        
        
        
        
        
        
        as
        
  SELECT
    c.sk_customerid sk_customerid,
    s.sk_securityid sk_securityid,
    sk_dateid_dateplaced,
    sk_dateid_dateremoved,
    wh.batchid
  FROM (
    SELECT * EXCEPT(w_dts)
    FROM (
      SELECT
        customerid,
        symbol,
        coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
          PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
        coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
          PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
        coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
          PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
        w_dts,
        coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
          PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
      FROM ( 
        SELECT 
          wh.w_c_id customerid,
          wh.w_s_symb symbol,
          if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
          if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
          if(w_action = 'ACTV', d.datevalue, null) dateplaced,
          wh.w_dts,
          batchid 
        FROM (
          SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
          UNION ALL
          SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
          ON d.datevalue = date(wh.w_dts)))
    QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
  -- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
  ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
  ^^^
    ON 
      s.symbol = wh.symbol
      AND wh.dateplaced >= s.effectivedate 
      AND wh.dateplaced < s.enddate
  ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
    ON
      wh.customerid = c.customerid
      AND wh.dateplaced >= c.effectivedate 
      AND wh.dateplaced < c.enddate
  
2023-04-20 12:23:12.296683 (Thread-1): 12:23:12  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6fe138e-c0a3-4432-b084-4bb13b916dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f784abbdee0>]}
2023-04-20 12:23:12.297136 (Thread-1): 12:23:12  15 of 15 ERROR creating sql table model dbt_shabbirkdb.FactWatches ............. [ERROR in 0.98s]
2023-04-20 12:23:12.297344 (Thread-1): 12:23:12  Finished running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 12:23:12.299765 (MainThread): 12:23:12  Acquiring new databricks connection 'master'
2023-04-20 12:23:12.300048 (MainThread): 12:23:12  On master: ROLLBACK
2023-04-20 12:23:12.300214 (MainThread): 12:23:12  Opening a new connection, currently in state init
2023-04-20 12:23:12.511134 (Thread-1129): handling ps request
2023-04-20 12:23:12.511631 (Thread-1129): 12:23:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f84c25e0>]}
2023-04-20 12:23:12.513437 (Thread-1129): sending response (<Response 27884 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:12.573907 (MainThread): 12:23:12  Databricks adapter: NotImplemented: rollback
2023-04-20 12:23:12.574148 (MainThread): 12:23:12  Spark adapter: NotImplemented: add_begin_query
2023-04-20 12:23:12.574290 (MainThread): 12:23:12  Spark adapter: NotImplemented: commit
2023-04-20 12:23:12.574451 (MainThread): 12:23:12  On master: ROLLBACK
2023-04-20 12:23:12.574599 (MainThread): 12:23:12  Databricks adapter: NotImplemented: rollback
2023-04-20 12:23:12.574746 (MainThread): 12:23:12  On master: Close
2023-04-20 12:23:12.733473 (Thread-1130): handling poll request
2023-04-20 12:23:12.733943 (Thread-1130): 12:23:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f84c2a00>]}
2023-04-20 12:23:12.734986 (Thread-1130): sending response (<Response 57023 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:12.861135 (MainThread): 12:23:12  Connection 'master' was properly closed.
2023-04-20 12:23:12.861344 (MainThread): 12:23:12  Connection 'model.dbsql_dbt_tpch.FactWatches' was properly closed.
2023-04-20 12:23:12.861458 (MainThread): 12:23:12  Connection 'model.dbsql_dbt_tpch.DimCustomer' was properly closed.
2023-04-20 12:23:12.861561 (MainThread): 12:23:12  Connection 'model.dbsql_dbt_tpch.FactCashBalances' was properly closed.
2023-04-20 12:23:12.861660 (MainThread): 12:23:12  Connection 'model.dbsql_dbt_tpch.FactMarketHistory' was properly closed.
2023-04-20 12:23:12.861980 (MainThread): 12:23:12  
2023-04-20 12:23:12.862137 (MainThread): 12:23:12  Finished running 15 table models in 0 hours 0 minutes and 22.23 seconds (22.23s).
2023-04-20 12:23:12.970290 (Thread-1131): handling status request
2023-04-20 12:23:12.970712 (Thread-1131): 12:23:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f84c2d90>]}
2023-04-20 12:23:12.971174 (Thread-1131): sending response (<Response 1928 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:13.025415 (MainThread): 12:23:13  
2023-04-20 12:23:13.025678 (MainThread): 12:23:13  Completed with 3 errors and 0 warnings:
2023-04-20 12:23:13.025812 (MainThread): 12:23:13  
2023-04-20 12:23:13.025985 (MainThread): 12:23:13  Runtime Error in model DimTrade (models/incremental/DimTrade.sql)
2023-04-20 12:23:13.026114 (MainThread): 12:23:13    
2023-04-20 12:23:13.026223 (MainThread): 12:23:13    [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 151, pos 0)
2023-04-20 12:23:13.026328 (MainThread): 12:23:13    
2023-04-20 12:23:13.026433 (MainThread): 12:23:13    == SQL ==
2023-04-20 12:23:13.026534 (MainThread): 12:23:13    /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimTrade"} */
2023-04-20 12:23:13.026635 (MainThread): 12:23:13    
2023-04-20 12:23:13.026736 (MainThread): 12:23:13      
2023-04-20 12:23:13.026837 (MainThread): 12:23:13        
2023-04-20 12:23:13.026936 (MainThread): 12:23:13            create or replace table `dbt_shabbirkdb`.`DimTrade`
2023-04-20 12:23:13.027035 (MainThread): 12:23:13          
2023-04-20 12:23:13.027135 (MainThread): 12:23:13          
2023-04-20 12:23:13.027233 (MainThread): 12:23:13        using delta
2023-04-20 12:23:13.027331 (MainThread): 12:23:13          
2023-04-20 12:23:13.027429 (MainThread): 12:23:13          
2023-04-20 12:23:13.027552 (MainThread): 12:23:13          
2023-04-20 12:23:13.027660 (MainThread): 12:23:13          
2023-04-20 12:23:13.027759 (MainThread): 12:23:13          
2023-04-20 12:23:13.027855 (MainThread): 12:23:13          
2023-04-20 12:23:13.027951 (MainThread): 12:23:13          as
2023-04-20 12:23:13.028047 (MainThread): 12:23:13          
2023-04-20 12:23:13.028144 (MainThread): 12:23:13    SELECT
2023-04-20 12:23:13.028240 (MainThread): 12:23:13      trade.tradeid,
2023-04-20 12:23:13.028334 (MainThread): 12:23:13      sk_brokerid,
2023-04-20 12:23:13.028429 (MainThread): 12:23:13      trade.sk_createdateid,
2023-04-20 12:23:13.028523 (MainThread): 12:23:13      trade.sk_createtimeid,
2023-04-20 12:23:13.028617 (MainThread): 12:23:13      trade.sk_closedateid,
2023-04-20 12:23:13.028713 (MainThread): 12:23:13      trade.sk_closetimeid,
2023-04-20 12:23:13.028809 (MainThread): 12:23:13      st_name status,
2023-04-20 12:23:13.028904 (MainThread): 12:23:13      tt_name type,
2023-04-20 12:23:13.028998 (MainThread): 12:23:13      trade.cashflag,
2023-04-20 12:23:13.029092 (MainThread): 12:23:13      sk_securityid,
2023-04-20 12:23:13.029188 (MainThread): 12:23:13      sk_companyid,
2023-04-20 12:23:13.029286 (MainThread): 12:23:13      trade.quantity,
2023-04-20 12:23:13.029384 (MainThread): 12:23:13      trade.bidprice,
2023-04-20 12:23:13.029480 (MainThread): 12:23:13      sk_customerid,
2023-04-20 12:23:13.029575 (MainThread): 12:23:13      sk_accountid,
2023-04-20 12:23:13.029670 (MainThread): 12:23:13      trade.executedby,
2023-04-20 12:23:13.029763 (MainThread): 12:23:13      trade.tradeprice,
2023-04-20 12:23:13.029864 (MainThread): 12:23:13      trade.fee,
2023-04-20 12:23:13.029962 (MainThread): 12:23:13      trade.commission,
2023-04-20 12:23:13.030056 (MainThread): 12:23:13      trade.tax,
2023-04-20 12:23:13.030150 (MainThread): 12:23:13      trade.batchid
2023-04-20 12:23:13.030244 (MainThread): 12:23:13    FROM (
2023-04-20 12:23:13.030338 (MainThread): 12:23:13      SELECT * EXCEPT(t_dts)
2023-04-20 12:23:13.030433 (MainThread): 12:23:13      FROM (
2023-04-20 12:23:13.030527 (MainThread): 12:23:13        SELECT
2023-04-20 12:23:13.030753 (MainThread): 12:23:13          tradeid,
2023-04-20 12:23:13.030864 (MainThread): 12:23:13          min(date(t_dts)) OVER (PARTITION BY tradeid) createdate,
2023-04-20 12:23:13.030965 (MainThread): 12:23:13          t_dts,
2023-04-20 12:23:13.031063 (MainThread): 12:23:13          coalesce(sk_createdateid, last_value(sk_createdateid) IGNORE NULLS OVER (
2023-04-20 12:23:13.031160 (MainThread): 12:23:13            PARTITION BY tradeid ORDER BY t_dts)) sk_createdateid,
2023-04-20 12:23:13.031258 (MainThread): 12:23:13          coalesce(sk_createtimeid, last_value(sk_createtimeid) IGNORE NULLS OVER (
2023-04-20 12:23:13.031354 (MainThread): 12:23:13            PARTITION BY tradeid ORDER BY t_dts)) sk_createtimeid,
2023-04-20 12:23:13.031449 (MainThread): 12:23:13          coalesce(sk_closedateid, last_value(sk_closedateid) IGNORE NULLS OVER (
2023-04-20 12:23:13.031569 (MainThread): 12:23:13            PARTITION BY tradeid ORDER BY t_dts)) sk_closedateid,
2023-04-20 12:23:13.031670 (MainThread): 12:23:13          coalesce(sk_closetimeid, last_value(sk_closetimeid) IGNORE NULLS OVER (
2023-04-20 12:23:13.031765 (MainThread): 12:23:13            PARTITION BY tradeid ORDER BY t_dts)) sk_closetimeid,
2023-04-20 12:23:13.031860 (MainThread): 12:23:13          cashflag,
2023-04-20 12:23:13.031954 (MainThread): 12:23:13          t_st_id,
2023-04-20 12:23:13.032049 (MainThread): 12:23:13          t_tt_id,
2023-04-20 12:23:13.032143 (MainThread): 12:23:13          t_s_symb,
2023-04-20 12:23:13.032238 (MainThread): 12:23:13          quantity,
2023-04-20 12:23:13.032332 (MainThread): 12:23:13          bidprice,
2023-04-20 12:23:13.032427 (MainThread): 12:23:13          t_ca_id,
2023-04-20 12:23:13.032521 (MainThread): 12:23:13          executedby,
2023-04-20 12:23:13.032615 (MainThread): 12:23:13          tradeprice,
2023-04-20 12:23:13.032712 (MainThread): 12:23:13          fee,
2023-04-20 12:23:13.032808 (MainThread): 12:23:13          commission,
2023-04-20 12:23:13.032903 (MainThread): 12:23:13          tax,
2023-04-20 12:23:13.032997 (MainThread): 12:23:13          batchid
2023-04-20 12:23:13.033090 (MainThread): 12:23:13        FROM (
2023-04-20 12:23:13.033183 (MainThread): 12:23:13          SELECT
2023-04-20 12:23:13.033275 (MainThread): 12:23:13            tradeid,
2023-04-20 12:23:13.033369 (MainThread): 12:23:13            t_dts,
2023-04-20 12:23:13.033462 (MainThread): 12:23:13            if(create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_createdateid,
2023-04-20 12:23:13.033611 (MainThread): 12:23:13            if(create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_createtimeid,
2023-04-20 12:23:13.033721 (MainThread): 12:23:13            if(!create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_closedateid,
2023-04-20 12:23:13.033822 (MainThread): 12:23:13            if(!create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_closetimeid,
2023-04-20 12:23:13.033930 (MainThread): 12:23:13            CASE 
2023-04-20 12:23:13.034027 (MainThread): 12:23:13              WHEN t_is_cash = 1 then TRUE
2023-04-20 12:23:13.034124 (MainThread): 12:23:13              WHEN t_is_cash = 0 then FALSE
2023-04-20 12:23:13.034220 (MainThread): 12:23:13              ELSE cast(null as BOOLEAN) END AS cashflag,
2023-04-20 12:23:13.034317 (MainThread): 12:23:13            t_st_id,
2023-04-20 12:23:13.034412 (MainThread): 12:23:13            t_tt_id,
2023-04-20 12:23:13.034508 (MainThread): 12:23:13            t_s_symb,
2023-04-20 12:23:13.034601 (MainThread): 12:23:13            quantity,
2023-04-20 12:23:13.034694 (MainThread): 12:23:13            bidprice,
2023-04-20 12:23:13.034788 (MainThread): 12:23:13            t_ca_id,
2023-04-20 12:23:13.034881 (MainThread): 12:23:13            executedby,
2023-04-20 12:23:13.034974 (MainThread): 12:23:13            tradeprice,
2023-04-20 12:23:13.035069 (MainThread): 12:23:13            fee,
2023-04-20 12:23:13.035164 (MainThread): 12:23:13            commission,
2023-04-20 12:23:13.035259 (MainThread): 12:23:13            tax,
2023-04-20 12:23:13.035356 (MainThread): 12:23:13            t.batchid
2023-04-20 12:23:13.035451 (MainThread): 12:23:13          FROM (
2023-04-20 12:23:13.035566 (MainThread): 12:23:13            SELECT
2023-04-20 12:23:13.035667 (MainThread): 12:23:13              t_id tradeid,
2023-04-20 12:23:13.035765 (MainThread): 12:23:13              th_dts t_dts,
2023-04-20 12:23:13.035953 (MainThread): 12:23:13              t_st_id,
2023-04-20 12:23:13.036059 (MainThread): 12:23:13              t_tt_id,
2023-04-20 12:23:13.036159 (MainThread): 12:23:13              t_is_cash,
2023-04-20 12:23:13.036255 (MainThread): 12:23:13              t_s_symb,
2023-04-20 12:23:13.036352 (MainThread): 12:23:13              t_qty AS quantity,
2023-04-20 12:23:13.036448 (MainThread): 12:23:13              t_bid_price AS bidprice,
2023-04-20 12:23:13.036543 (MainThread): 12:23:13              t_ca_id,
2023-04-20 12:23:13.036637 (MainThread): 12:23:13              t_exec_name AS executedby,
2023-04-20 12:23:13.036731 (MainThread): 12:23:13              t_trade_price AS tradeprice,
2023-04-20 12:23:13.036825 (MainThread): 12:23:13              t_chrg AS fee,
2023-04-20 12:23:13.036919 (MainThread): 12:23:13              t_comm AS commission,
2023-04-20 12:23:13.037014 (MainThread): 12:23:13              t_tax AS tax,
2023-04-20 12:23:13.037108 (MainThread): 12:23:13              1 batchid,
2023-04-20 12:23:13.037203 (MainThread): 12:23:13              CASE 
2023-04-20 12:23:13.037299 (MainThread): 12:23:13                WHEN (th_st_id == "SBMT" AND t_tt_id IN ("TMB", "TMS")) OR th_st_id = "PNDG" THEN TRUE 
2023-04-20 12:23:13.037395 (MainThread): 12:23:13                WHEN th_st_id IN ("CMPT", "CNCL") THEN FALSE 
2023-04-20 12:23:13.037491 (MainThread): 12:23:13                ELSE cast(null as boolean) END AS create_flg
2023-04-20 12:23:13.037586 (MainThread): 12:23:13            FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistory` t
2023-04-20 12:23:13.037682 (MainThread): 12:23:13            JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeHistoryRaw` th
2023-04-20 12:23:13.037777 (MainThread): 12:23:13              ON th_t_id = t_id
2023-04-20 12:23:13.037881 (MainThread): 12:23:13            UNION ALL
2023-04-20 12:23:13.037978 (MainThread): 12:23:13            SELECT
2023-04-20 12:23:13.038071 (MainThread): 12:23:13              t_id tradeid,
2023-04-20 12:23:13.038164 (MainThread): 12:23:13              t_dts,
2023-04-20 12:23:13.038258 (MainThread): 12:23:13              t_st_id,
2023-04-20 12:23:13.038351 (MainThread): 12:23:13              t_tt_id,
2023-04-20 12:23:13.038445 (MainThread): 12:23:13              t_is_cash,
2023-04-20 12:23:13.038539 (MainThread): 12:23:13              t_s_symb,
2023-04-20 12:23:13.038633 (MainThread): 12:23:13              t_qty AS quantity,
2023-04-20 12:23:13.038727 (MainThread): 12:23:13              t_bid_price AS bidprice,
2023-04-20 12:23:13.038822 (MainThread): 12:23:13              t_ca_id,
2023-04-20 12:23:13.038917 (MainThread): 12:23:13              t_exec_name AS executedby,
2023-04-20 12:23:13.039012 (MainThread): 12:23:13              t_trade_price AS tradeprice,
2023-04-20 12:23:13.039106 (MainThread): 12:23:13              t_chrg AS fee,
2023-04-20 12:23:13.039203 (MainThread): 12:23:13              t_comm AS commission,
2023-04-20 12:23:13.039300 (MainThread): 12:23:13              t_tax AS tax,
2023-04-20 12:23:13.039397 (MainThread): 12:23:13              t.batchid,
2023-04-20 12:23:13.039491 (MainThread): 12:23:13              CASE 
2023-04-20 12:23:13.039619 (MainThread): 12:23:13                WHEN cdc_flag = 'I' THEN TRUE 
2023-04-20 12:23:13.039718 (MainThread): 12:23:13                WHEN t_st_id IN ("CMPT", "CNCL") THEN FALSE 
2023-04-20 12:23:13.039814 (MainThread): 12:23:13                ELSE cast(null as boolean) END AS create_flg
2023-04-20 12:23:13.039908 (MainThread): 12:23:13            FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeIncremental` t
2023-04-20 12:23:13.040002 (MainThread): 12:23:13          ) t
2023-04-20 12:23:13.040095 (MainThread): 12:23:13          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` dd
2023-04-20 12:23:13.040190 (MainThread): 12:23:13            ON date(t.t_dts) = dd.datevalue
2023-04-20 12:23:13.040286 (MainThread): 12:23:13          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimTime` dt
2023-04-20 12:23:13.040380 (MainThread): 12:23:13            ON date_format(t.t_dts, 'HH:mm:ss') = dt.timevalue
2023-04-20 12:23:13.040475 (MainThread): 12:23:13        )
2023-04-20 12:23:13.040569 (MainThread): 12:23:13      )
2023-04-20 12:23:13.040664 (MainThread): 12:23:13      QUALIFY ROW_NUMBER() OVER (PARTITION BY tradeid ORDER BY t_dts desc) = 1
2023-04-20 12:23:13.040760 (MainThread): 12:23:13    ) trade
2023-04-20 12:23:13.040856 (MainThread): 12:23:13    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` status
2023-04-20 12:23:13.041055 (MainThread): 12:23:13      ON status.st_id = trade.t_st_id
2023-04-20 12:23:13.041160 (MainThread): 12:23:13    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TradeType` tt
2023-04-20 12:23:13.041259 (MainThread): 12:23:13      ON tt.tt_id == trade.t_tt_id
2023-04-20 12:23:13.041354 (MainThread): 12:23:13    -- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Account IDs are missing from DimSecurity/DimAccount, causing audit check failures. 
2023-04-20 12:23:13.041497 (MainThread): 12:23:13    ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` ds
2023-04-20 12:23:13.041603 (MainThread): 12:23:13    ^^^
2023-04-20 12:23:13.041701 (MainThread): 12:23:13      ON 
2023-04-20 12:23:13.041797 (MainThread): 12:23:13        ds.symbol = trade.t_s_symb
2023-04-20 12:23:13.041902 (MainThread): 12:23:13        AND createdate >= ds.effectivedate 
2023-04-20 12:23:13.041999 (MainThread): 12:23:13        AND createdate < ds.enddate
2023-04-20 12:23:13.042094 (MainThread): 12:23:13    ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimAccount` da
2023-04-20 12:23:13.042188 (MainThread): 12:23:13      ON 
2023-04-20 12:23:13.042282 (MainThread): 12:23:13        trade.t_ca_id = da.accountid 
2023-04-20 12:23:13.042376 (MainThread): 12:23:13        AND createdate >= da.effectivedate 
2023-04-20 12:23:13.042470 (MainThread): 12:23:13        AND createdate < da.enddate
2023-04-20 12:23:13.042565 (MainThread): 12:23:13      
2023-04-20 12:23:13.042658 (MainThread): 12:23:13    
2023-04-20 12:23:13.042772 (MainThread): 12:23:13  
2023-04-20 12:23:13.042901 (MainThread): 12:23:13  Runtime Error in model FactMarketHistory (models/incremental/FactMarketHistory.sql)
2023-04-20 12:23:13.043008 (MainThread): 12:23:13    
2023-04-20 12:23:13.043109 (MainThread): 12:23:13    [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 54, pos 0)
2023-04-20 12:23:13.043207 (MainThread): 12:23:13    
2023-04-20 12:23:13.043303 (MainThread): 12:23:13    == SQL ==
2023-04-20 12:23:13.043398 (MainThread): 12:23:13    /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactMarketHistory"} */
2023-04-20 12:23:13.043494 (MainThread): 12:23:13    
2023-04-20 12:23:13.043618 (MainThread): 12:23:13      
2023-04-20 12:23:13.043715 (MainThread): 12:23:13        
2023-04-20 12:23:13.043810 (MainThread): 12:23:13            create or replace table `dbt_shabbirkdb`.`FactMarketHistory`
2023-04-20 12:23:13.043904 (MainThread): 12:23:13          
2023-04-20 12:23:13.043999 (MainThread): 12:23:13          
2023-04-20 12:23:13.044094 (MainThread): 12:23:13        using delta
2023-04-20 12:23:13.044191 (MainThread): 12:23:13          
2023-04-20 12:23:13.044288 (MainThread): 12:23:13          
2023-04-20 12:23:13.044384 (MainThread): 12:23:13          
2023-04-20 12:23:13.044481 (MainThread): 12:23:13          
2023-04-20 12:23:13.044577 (MainThread): 12:23:13          
2023-04-20 12:23:13.044673 (MainThread): 12:23:13          
2023-04-20 12:23:13.044769 (MainThread): 12:23:13          as
2023-04-20 12:23:13.044864 (MainThread): 12:23:13          
2023-04-20 12:23:13.044958 (MainThread): 12:23:13    SELECT 
2023-04-20 12:23:13.045053 (MainThread): 12:23:13      s.sk_securityid,
2023-04-20 12:23:13.045148 (MainThread): 12:23:13      s.sk_companyid,
2023-04-20 12:23:13.045243 (MainThread): 12:23:13      sk_dateid,
2023-04-20 12:23:13.045337 (MainThread): 12:23:13      fmh.dm_close / sum_fi_basic_eps AS peratio,
2023-04-20 12:23:13.045432 (MainThread): 12:23:13      (s.dividend / fmh.dm_close) / 100 yield,
2023-04-20 12:23:13.045528 (MainThread): 12:23:13      fiftytwoweekhigh,
2023-04-20 12:23:13.045624 (MainThread): 12:23:13      sk_fiftytwoweekhighdate,
2023-04-20 12:23:13.045719 (MainThread): 12:23:13      fiftytwoweeklow,
2023-04-20 12:23:13.045815 (MainThread): 12:23:13      sk_fiftytwoweeklowdate,
2023-04-20 12:23:13.045920 (MainThread): 12:23:13      dm_close closeprice,
2023-04-20 12:23:13.046017 (MainThread): 12:23:13      dm_high dayhigh,
2023-04-20 12:23:13.046187 (MainThread): 12:23:13      dm_low daylow,
2023-04-20 12:23:13.046295 (MainThread): 12:23:13      dm_vol volume,
2023-04-20 12:23:13.046394 (MainThread): 12:23:13      fmh.batchid
2023-04-20 12:23:13.046490 (MainThread): 12:23:13    FROM (
2023-04-20 12:23:13.046587 (MainThread): 12:23:13      SELECT * FROM (
2023-04-20 12:23:13.046682 (MainThread): 12:23:13        SELECT 
2023-04-20 12:23:13.046777 (MainThread): 12:23:13          a.*,
2023-04-20 12:23:13.046871 (MainThread): 12:23:13          b.sk_dateid AS sk_fiftytwoweeklowdate,
2023-04-20 12:23:13.046967 (MainThread): 12:23:13          c.sk_dateid AS sk_fiftytwoweekhighdate
2023-04-20 12:23:13.047063 (MainThread): 12:23:13        FROM
2023-04-20 12:23:13.047158 (MainThread): 12:23:13          `dbt_shabbirkdb`.`tempDailyMarketHistorical`a
2023-04-20 12:23:13.047254 (MainThread): 12:23:13        JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` b 
2023-04-20 12:23:13.047348 (MainThread): 12:23:13          ON
2023-04-20 12:23:13.047443 (MainThread): 12:23:13            a.dm_s_symb = b.dm_s_symb
2023-04-20 12:23:13.047560 (MainThread): 12:23:13            AND a.fiftytwoweeklow = b.dm_low
2023-04-20 12:23:13.047659 (MainThread): 12:23:13            AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date
2023-04-20 12:23:13.047754 (MainThread): 12:23:13        JOIN  `dbt_shabbirkdb`.`tempDailyMarketHistorical` c 
2023-04-20 12:23:13.047849 (MainThread): 12:23:13          ON 
2023-04-20 12:23:13.047942 (MainThread): 12:23:13            a.dm_s_symb = c.dm_s_symb
2023-04-20 12:23:13.048035 (MainThread): 12:23:13            AND a.fiftytwoweekhigh = c.dm_high
2023-04-20 12:23:13.048129 (MainThread): 12:23:13            AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh
2023-04-20 12:23:13.048223 (MainThread): 12:23:13      QUALIFY ROW_NUMBER() OVER (
2023-04-20 12:23:13.048318 (MainThread): 12:23:13        PARTITION BY dm_s_symb, dm_date 
2023-04-20 12:23:13.048415 (MainThread): 12:23:13        ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh
2023-04-20 12:23:13.048511 (MainThread): 12:23:13    -- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. 
2023-04-20 12:23:13.048606 (MainThread): 12:23:13    ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
2023-04-20 12:23:13.048700 (MainThread): 12:23:13    ^^^
2023-04-20 12:23:13.048794 (MainThread): 12:23:13      ON 
2023-04-20 12:23:13.048886 (MainThread): 12:23:13        s.symbol = fmh.dm_s_symb
2023-04-20 12:23:13.048980 (MainThread): 12:23:13        AND fmh.dm_date >= s.effectivedate 
2023-04-20 12:23:13.049072 (MainThread): 12:23:13        AND fmh.dm_date < s.enddate
2023-04-20 12:23:13.049164 (MainThread): 12:23:13    LEFT JOIN  `dbt_shabbirkdb`.`tempSumpFiBasicEps` f 
2023-04-20 12:23:13.049315 (MainThread): 12:23:13      ON 
2023-04-20 12:23:13.049413 (MainThread): 12:23:13        f.sk_companyid = s.sk_companyid
2023-04-20 12:23:13.049507 (MainThread): 12:23:13        AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)
2023-04-20 12:23:13.049603 (MainThread): 12:23:13        AND year(fmh.dm_date) = year(fi_qtr_start_date)
2023-04-20 12:23:13.049700 (MainThread): 12:23:13    
2023-04-20 12:23:13.049811 (MainThread): 12:23:13  
2023-04-20 12:23:13.049951 (MainThread): 12:23:13  Runtime Error in model FactWatches (models/incremental/FactWatches.sql)
2023-04-20 12:23:13.050060 (MainThread): 12:23:13    
2023-04-20 12:23:13.050160 (MainThread): 12:23:13    [PARSE_SYNTAX_ERROR] Syntax error at or near '$'.(line 55, pos 0)
2023-04-20 12:23:13.050259 (MainThread): 12:23:13    
2023-04-20 12:23:13.050355 (MainThread): 12:23:13    == SQL ==
2023-04-20 12:23:13.050451 (MainThread): 12:23:13    /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.FactWatches"} */
2023-04-20 12:23:13.050546 (MainThread): 12:23:13    
2023-04-20 12:23:13.050640 (MainThread): 12:23:13      
2023-04-20 12:23:13.050736 (MainThread): 12:23:13        
2023-04-20 12:23:13.050831 (MainThread): 12:23:13            create or replace table `dbt_shabbirkdb`.`FactWatches`
2023-04-20 12:23:13.050925 (MainThread): 12:23:13          
2023-04-20 12:23:13.051020 (MainThread): 12:23:13          
2023-04-20 12:23:13.051116 (MainThread): 12:23:13        using delta
2023-04-20 12:23:13.051213 (MainThread): 12:23:13          
2023-04-20 12:23:13.051437 (MainThread): 12:23:13          
2023-04-20 12:23:13.051584 (MainThread): 12:23:13          
2023-04-20 12:23:13.051703 (MainThread): 12:23:13          
2023-04-20 12:23:13.051804 (MainThread): 12:23:13          
2023-04-20 12:23:13.051903 (MainThread): 12:23:13          
2023-04-20 12:23:13.051999 (MainThread): 12:23:13          as
2023-04-20 12:23:13.052095 (MainThread): 12:23:13          
2023-04-20 12:23:13.052191 (MainThread): 12:23:13    SELECT
2023-04-20 12:23:13.052287 (MainThread): 12:23:13      c.sk_customerid sk_customerid,
2023-04-20 12:23:13.052382 (MainThread): 12:23:13      s.sk_securityid sk_securityid,
2023-04-20 12:23:13.052476 (MainThread): 12:23:13      sk_dateid_dateplaced,
2023-04-20 12:23:13.052569 (MainThread): 12:23:13      sk_dateid_dateremoved,
2023-04-20 12:23:13.052662 (MainThread): 12:23:13      wh.batchid
2023-04-20 12:23:13.052755 (MainThread): 12:23:13    FROM (
2023-04-20 12:23:13.052847 (MainThread): 12:23:13      SELECT * EXCEPT(w_dts)
2023-04-20 12:23:13.052939 (MainThread): 12:23:13      FROM (
2023-04-20 12:23:13.053033 (MainThread): 12:23:13        SELECT
2023-04-20 12:23:13.053128 (MainThread): 12:23:13          customerid,
2023-04-20 12:23:13.053223 (MainThread): 12:23:13          symbol,
2023-04-20 12:23:13.053317 (MainThread): 12:23:13          coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (
2023-04-20 12:23:13.053413 (MainThread): 12:23:13            PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,
2023-04-20 12:23:13.053507 (MainThread): 12:23:13          coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (
2023-04-20 12:23:13.053600 (MainThread): 12:23:13            PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,
2023-04-20 12:23:13.053693 (MainThread): 12:23:13          coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (
2023-04-20 12:23:13.053785 (MainThread): 12:23:13            PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,
2023-04-20 12:23:13.053887 (MainThread): 12:23:13          w_dts,
2023-04-20 12:23:13.053982 (MainThread): 12:23:13          coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (
2023-04-20 12:23:13.054076 (MainThread): 12:23:13            PARTITION BY customerid, symbol ORDER BY w_dts)) batchid
2023-04-20 12:23:13.054170 (MainThread): 12:23:13        FROM ( 
2023-04-20 12:23:13.054264 (MainThread): 12:23:13          SELECT 
2023-04-20 12:23:13.054358 (MainThread): 12:23:13            wh.w_c_id customerid,
2023-04-20 12:23:13.054452 (MainThread): 12:23:13            wh.w_s_symb symbol,
2023-04-20 12:23:13.054549 (MainThread): 12:23:13            if(w_action = 'ACTV', d.sk_dateid, null) sk_dateid_dateplaced,
2023-04-20 12:23:13.054644 (MainThread): 12:23:13            if(w_action = 'CNCL', d.sk_dateid, null) sk_dateid_dateremoved,
2023-04-20 12:23:13.054738 (MainThread): 12:23:13            if(w_action = 'ACTV', d.datevalue, null) dateplaced,
2023-04-20 12:23:13.054832 (MainThread): 12:23:13            wh.w_dts,
2023-04-20 12:23:13.054925 (MainThread): 12:23:13            batchid 
2023-04-20 12:23:13.055018 (MainThread): 12:23:13          FROM (
2023-04-20 12:23:13.055112 (MainThread): 12:23:13            SELECT *, 1 batchid FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchHistory`
2023-04-20 12:23:13.055205 (MainThread): 12:23:13            UNION ALL
2023-04-20 12:23:13.055298 (MainThread): 12:23:13            SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`WatchIncremental`) wh
2023-04-20 12:23:13.055393 (MainThread): 12:23:13          JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d
2023-04-20 12:23:13.055487 (MainThread): 12:23:13            ON d.datevalue = date(wh.w_dts)))
2023-04-20 12:23:13.055602 (MainThread): 12:23:13      QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh
2023-04-20 12:23:13.055700 (MainThread): 12:23:13    -- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. 
2023-04-20 12:23:13.055794 (MainThread): 12:23:13    ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimSecurity` s 
2023-04-20 12:23:13.055889 (MainThread): 12:23:13    ^^^
2023-04-20 12:23:13.055984 (MainThread): 12:23:13      ON 
2023-04-20 12:23:13.056078 (MainThread): 12:23:13        s.symbol = wh.symbol
2023-04-20 12:23:13.056173 (MainThread): 12:23:13        AND wh.dateplaced >= s.effectivedate 
2023-04-20 12:23:13.056268 (MainThread): 12:23:13        AND wh.dateplaced < s.enddate
2023-04-20 12:23:13.056362 (MainThread): 12:23:13    ${dq_left_flg} JOIN `dbt_shabbirkdb`.`DimCustomer` c 
2023-04-20 12:23:13.056554 (MainThread): 12:23:13      ON
2023-04-20 12:23:13.056659 (MainThread): 12:23:13        wh.customerid = c.customerid
2023-04-20 12:23:13.056756 (MainThread): 12:23:13        AND wh.dateplaced >= c.effectivedate 
2023-04-20 12:23:13.056852 (MainThread): 12:23:13        AND wh.dateplaced < c.enddate
2023-04-20 12:23:13.056947 (MainThread): 12:23:13    
2023-04-20 12:23:13.057077 (MainThread): 12:23:13  
2023-04-20 12:23:13.057259 (MainThread): 12:23:13  Done. PASS=11 WARN=0 ERROR=3 SKIP=1 TOTAL=15
2023-04-20 12:23:14.451658 (Thread-1132): handling poll request
2023-04-20 12:23:14.452160 (Thread-1132): 12:23:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437cfbbb0>]}
2023-04-20 12:23:14.458363 (Thread-1132): sending response (<Response 239767 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:15.357975 (Thread-1133): handling ps request
2023-04-20 12:23:15.358514 (Thread-1133): 12:23:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb437cf2cd0>]}
2023-04-20 12:23:15.360576 (Thread-1133): sending response (<Response 27908 bytes [200 OK]>) to 10.0.136.211
2023-04-20 12:23:15.788033 (Thread-1134): handling status request
2023-04-20 12:23:15.788581 (Thread-1134): 12:23:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4f8536280>]}
2023-04-20 12:23:15.789074 (Thread-1134): sending response (<Response 1928 bytes [200 OK]>) to 10.0.136.211
