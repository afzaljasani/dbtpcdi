2023-04-20 10:51:24.397121 (MainThread): Running with dbt=1.4.6
2023-04-20 10:51:24.829998 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt_rpc.task.server.RPCServerTask'>, debug=None, defer=None, exclude=None, fail_fast=None, host='0.0.0.0', log_cache_events=False, log_format=None, models=None, partial_parse=True, port=8580, printer_width=None, profile='user', profiles_dir='/usr/src/develop/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, send_anonymous_usage_stats=None, single_threaded=False, state=None, static_parser=None, target=None, threads=None, use_colors=None, use_experimental_parser=None, vars='{}', version_check=None, warn_error=None, which='rpc', write_json=None)
2023-04-20 10:51:24.842670 (MainThread): Tracking: tracking
2023-04-20 10:51:24.844251 (MainThread): 10:51:24  Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb57fecb820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5699efa60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5699ef5e0>]}
2023-04-20 10:51:24.844621 (MainThread): Serving RPC server at 0.0.0.0:8580, pid=39
2023-04-20 10:51:24.844937 (MainThread): Supported methods: ['build', 'cli_args', 'compile', 'compile_sql', 'deps', 'docs.generate', 'gc', 'get-manifest', 'kill', 'list', 'poll', 'ps', 'run', 'run-operation', 'run_sql', 'seed', 'snapshot', 'snapshot-freshness', 'source-freshness', 'status', 'test']
2023-04-20 10:51:24.845108 (MainThread): Send requests to http://localhost:8580/jsonrpc
2023-04-20 10:51:28.054058 (Thread-13): handling status request
2023-04-20 10:51:28.054550 (Thread-13): 10:51:28  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5699f9550>]}
2023-04-20 10:51:28.055147 (Thread-13): sending response (<Response 339 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:28.845769 (Thread-14): handling ps request
2023-04-20 10:51:28.846285 (Thread-14): 10:51:28  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5699f9700>]}
2023-04-20 10:51:28.846743 (Thread-14): sending response (<Response 100 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:29.286980 (Thread-15): handling status request
2023-04-20 10:51:29.287483 (Thread-15): 10:51:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5699f98b0>]}
2023-04-20 10:51:29.287959 (Thread-15): sending response (<Response 339 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:39.891489 (Thread-16): handling deps request
2023-04-20 10:51:39.893479 (Thread-16): 10:51:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5699f9940>]}
2023-04-20 10:51:40.020563 (Thread-16): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:40.581551 (Thread-17): handling status request
2023-04-20 10:51:40.582222 (Thread-17): 10:51:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a21d60>]}
2023-04-20 10:51:40.582891 (Thread-17): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:40.636084 (Thread-18): handling ps request
2023-04-20 10:51:40.636563 (Thread-18): 10:51:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a21280>]}
2023-04-20 10:51:40.637138 (Thread-18): sending response (<Response 465 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:40.760029 (Thread-19): handling poll request
2023-04-20 10:51:40.760515 (Thread-19): 10:51:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a21940>]}
2023-04-20 10:51:40.761049 (Thread-19): sending response (<Response 279 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:41.095434 (Thread-20): handling status request
2023-04-20 10:51:41.095949 (Thread-20): 10:51:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a21580>]}
2023-04-20 10:51:41.096363 (Thread-20): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:42.056507 (Thread-21): handling status request
2023-04-20 10:51:42.057004 (Thread-21): 10:51:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a215b0>]}
2023-04-20 10:51:42.057423 (Thread-21): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:42.408016 (Thread-22): handling poll request
2023-04-20 10:51:42.408811 (Thread-22): 10:51:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a219a0>]}
2023-04-20 10:51:42.409573 (Thread-22): sending response (<Response 279 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:42.805425 (MainThread): 10:51:42  Set downloads directory='/tmp/dbt-downloads-px3mtwmk'
2023-04-20 10:51:42.836156 (MainThread): 10:51:42  Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
2023-04-20 10:51:42.859107 (MainThread): 10:51:42  Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
2023-04-20 10:51:42.859464 (MainThread): 10:51:42  Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
2023-04-20 10:51:42.873893 (MainThread): 10:51:42  Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
2023-04-20 10:51:42.880049 (MainThread): 10:51:42  Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/spark_utils.json
2023-04-20 10:51:42.894075 (MainThread): 10:51:42  Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/spark_utils.json 200
2023-04-20 10:51:42.900879 (MainThread): 10:51:42  Installing dbt-labs/dbt_utils
2023-04-20 10:51:43.007994 (Thread-23): handling status request
2023-04-20 10:51:43.008483 (Thread-23): 10:51:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a21fd0>]}
2023-04-20 10:51:43.008901 (Thread-23): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:43.333801 (Thread-24): handling ps request
2023-04-20 10:51:43.334297 (Thread-24): 10:51:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56998aa90>]}
2023-04-20 10:51:43.334788 (Thread-24): sending response (<Response 465 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:44.324803 (Thread-25): handling status request
2023-04-20 10:51:44.325329 (Thread-25): 10:51:44  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56998a700>]}
2023-04-20 10:51:44.325777 (Thread-25): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:44.443736 (Thread-26): handling poll request
2023-04-20 10:51:44.444236 (Thread-26): 10:51:44  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56998adf0>]}
2023-04-20 10:51:44.444784 (Thread-26): sending response (<Response 2825 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:45.265859 (Thread-27): handling status request
2023-04-20 10:51:45.266349 (Thread-27): 10:51:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56998ad00>]}
2023-04-20 10:51:45.266763 (Thread-27): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:46.067842 (Thread-28): handling poll request
2023-04-20 10:51:46.068344 (Thread-28): 10:51:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56998aee0>]}
2023-04-20 10:51:46.068823 (Thread-28): sending response (<Response 279 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:46.218422 (Thread-29): handling status request
2023-04-20 10:51:46.218867 (Thread-29): 10:51:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56998a280>]}
2023-04-20 10:51:46.219265 (Thread-29): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:46.475490 (Thread-30): handling ps request
2023-04-20 10:51:46.476061 (Thread-30): 10:51:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a21220>]}
2023-04-20 10:51:46.476564 (Thread-30): sending response (<Response 465 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:46.928489 (Thread-31): handling status request
2023-04-20 10:51:46.928995 (Thread-31): 10:51:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56998a850>]}
2023-04-20 10:51:46.929425 (Thread-31): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:47.482463 (MainThread): 10:51:47    Installed from version 0.8.0
2023-04-20 10:51:47.482817 (MainThread): 10:51:47    Updated version available: 1.0.0
2023-04-20 10:51:47.483179 (MainThread): 10:51:47  Sending event: {'category': 'dbt', 'action': 'package', 'label': '25bbcb94-0a47-46e4-babe-6ef82036ec83', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fde11daf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fde13d1f0>]}
2023-04-20 10:51:47.483676 (MainThread): 10:51:47  Installing dbt-labs/spark_utils
2023-04-20 10:51:47.737119 (Thread-32): handling poll request
2023-04-20 10:51:47.737617 (Thread-32): 10:51:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56998aca0>]}
2023-04-20 10:51:47.738152 (Thread-32): sending response (<Response 1656 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:47.851289 (Thread-33): handling status request
2023-04-20 10:51:47.852195 (Thread-33): 10:51:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56998a190>]}
2023-04-20 10:51:47.852589 (Thread-33): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:48.264640 (MainThread): 10:51:48    Installed from version 0.3.0
2023-04-20 10:51:48.264846 (MainThread): 10:51:48    Up to date!
2023-04-20 10:51:48.265049 (MainThread): 10:51:48  Sending event: {'category': 'dbt', 'action': 'package', 'label': '25bbcb94-0a47-46e4-babe-6ef82036ec83', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fde13dd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fde13dca0>]}
2023-04-20 10:51:48.265304 (MainThread): 10:51:48  
2023-04-20 10:51:48.265439 (MainThread): 10:51:48  Updates available for packages: ['dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
2023-04-20 10:51:48.518302 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:48  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 10:51:48.526374 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:48  Unable to do partial parsing because saved manifest not found. Starting full parse.
2023-04-20 10:51:48.526723 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:48  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a1b0a0>]}
2023-04-20 10:51:48.820412 (Thread-34): handling status request
2023-04-20 10:51:48.820947 (Thread-34): 10:51:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a21dc0>]}
2023-04-20 10:51:48.821368 (Thread-34): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:49.110348 (Thread-35): handling ps request
2023-04-20 10:51:49.131257 (Thread-35): 10:51:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5698fddf0>]}
2023-04-20 10:51:49.136930 (Thread-35): sending response (<Response 465 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:49.493967 (Thread-36): handling poll request
2023-04-20 10:51:49.514853 (Thread-36): 10:51:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56990b4f0>]}
2023-04-20 10:51:49.520540 (Thread-36): sending response (<Response 1975 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:49.608580 (Thread-37): handling status request
2023-04-20 10:51:49.619283 (Thread-37): 10:51:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56991f3a0>]}
2023-04-20 10:51:49.630050 (Thread-37): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:50.487385 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 10:51:50.501353 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 10:51:50.504705 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 10:51:50.507644 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 10:51:50.510545 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 10:51:50.513540 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 10:51:50.516539 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 10:51:50.519942 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 10:51:50.522970 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 10:51:50.525858 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 10:51:50.528820 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 10:51:50.532115 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 10:51:50.534797 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 10:51:50.537709 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 10:51:50.540675 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 10:51:50.565926 (Thread-38): handling status request
2023-04-20 10:51:50.566352 (Thread-38): 10:51:50  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5622ab490>]}
2023-04-20 10:51:50.566765 (Thread-38): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:50.714672 (1a54fdcc-fe3d-4fae-abc3-7d9194de3e68-handler-deps): 10:51:50  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56229f9a0>]}
2023-04-20 10:51:51.089000 (Thread-39): handling poll request
2023-04-20 10:51:51.089545 (Thread-39): 10:51:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569997fa0>]}
2023-04-20 10:51:51.111633 (Thread-39): sending response (<Response 352 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:51.473764 (Thread-40): handling status request
2023-04-20 10:51:51.474270 (Thread-40): 10:51:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56229ffd0>]}
2023-04-20 10:51:51.474855 (Thread-40): sending response (<Response 5782 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:51.776548 (Thread-41): handling ps request
2023-04-20 10:51:51.777066 (Thread-41): 10:51:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56229fa90>]}
2023-04-20 10:51:51.777585 (Thread-41): sending response (<Response 491 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:52.203659 (Thread-42): handling status request
2023-04-20 10:51:52.204151 (Thread-42): 10:51:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56229f880>]}
2023-04-20 10:51:52.204737 (Thread-42): sending response (<Response 5782 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:57.366942 (Thread-43): 10:51:57  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 10:51:57.724306 (Thread-44): handling status request
2023-04-20 10:51:57.724928 (Thread-44): 10:51:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa11ca0>]}
2023-04-20 10:51:57.725370 (Thread-44): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:51:57.782516 (Thread-43): 10:51:57  Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
2023-04-20 10:51:57.782788 (Thread-43): 10:51:57  Partial parsing enabled, no changes found, skipping parsing
2023-04-20 10:51:57.791062 (Thread-43): 10:51:57  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f979f40>]}
2023-04-20 10:51:58.969279 (Thread-45): handling status request
2023-04-20 10:51:58.969776 (Thread-45): 10:51:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa11d00>]}
2023-04-20 10:51:58.970260 (Thread-45): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:04.030870 (Thread-46): handling ps request
2023-04-20 10:52:04.031366 (Thread-46): 10:52:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa11af0>]}
2023-04-20 10:52:04.031897 (Thread-46): sending response (<Response 491 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:04.042442 (Thread-47): handling ps request
2023-04-20 10:52:04.042775 (Thread-47): 10:52:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa11e80>]}
2023-04-20 10:52:04.043161 (Thread-47): sending response (<Response 491 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:04.232933 (Thread-48): handling status request
2023-04-20 10:52:04.233423 (Thread-48): 10:52:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa119d0>]}
2023-04-20 10:52:04.233906 (Thread-48): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:04.375629 (Thread-49): handling status request
2023-04-20 10:52:04.376115 (Thread-49): 10:52:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa11a00>]}
2023-04-20 10:52:04.376573 (Thread-49): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:04.381145 (Thread-50): handling list request
2023-04-20 10:52:04.381464 (Thread-50): 10:52:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa11730>]}
2023-04-20 10:52:04.410242 (Thread-50): 10:52:04  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa11100>]}
2023-04-20 10:52:04.410734 (Thread-50): 10:52:04  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 10:52:04.411071 (Thread-50): 10:52:04  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb57b790be0>]}
2023-04-20 10:52:04.413486 (Thread-50): sending response (<Response 5675 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:13.699783 (Thread-51): handling status request
2023-04-20 10:52:13.700295 (Thread-51): 10:52:13  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa11f40>]}
2023-04-20 10:52:13.700792 (Thread-51): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:17.159658 (Thread-52): handling status request
2023-04-20 10:52:17.160165 (Thread-52): 10:52:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa11c10>]}
2023-04-20 10:52:17.160635 (Thread-52): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:17.494576 (Thread-53): handling cli_args request
2023-04-20 10:52:17.495070 (Thread-53): 10:52:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5699efeb0>]}
2023-04-20 10:52:20.242811 (Thread-53): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:20.304440 (MainThread): 10:52:20  checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
2023-04-20 10:52:20.326492 (MainThread): 10:52:20  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 10:52:20.326671 (MainThread): 10:52:20  previous checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, current checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd
2023-04-20 10:52:20.326827 (MainThread): 10:52:20  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4fe7467ca0>]}
2023-04-20 10:52:20.908293 (Thread-54): handling ps request
2023-04-20 10:52:20.908951 (Thread-54): 10:52:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa1ad00>]}
2023-04-20 10:52:20.909843 (Thread-54): sending response (<Response 1164 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:20.965186 (Thread-55): handling poll request
2023-04-20 10:52:20.965647 (Thread-55): 10:52:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa1aa00>]}
2023-04-20 10:52:20.966222 (Thread-55): sending response (<Response 1859 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:21.210435 (Thread-56): handling status request
2023-04-20 10:52:21.210913 (Thread-56): 10:52:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa1ab50>]}
2023-04-20 10:52:21.211414 (Thread-56): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:21.299685 (Thread-57): handling ps request
2023-04-20 10:52:21.300152 (Thread-57): 10:52:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa1ac10>]}
2023-04-20 10:52:21.300698 (Thread-57): sending response (<Response 1165 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:22.033183 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 10:52:22.052164 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 10:52:22.056773 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 10:52:22.061244 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 10:52:22.065803 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 10:52:22.070276 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 10:52:22.076233 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 10:52:22.081096 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 10:52:22.085846 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 10:52:22.090362 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 10:52:22.095578 (MainThread): 10:52:22  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 10:52:22.100250 (MainThread): 10:52:22  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 10:52:22.104336 (MainThread): 10:52:22  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 10:52:22.107061 (MainThread): 10:52:22  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 10:52:22.110280 (MainThread): 10:52:22  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 10:52:22.274660 (MainThread): 10:52:22  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f9a56fb20>]}
2023-04-20 10:52:22.303932 (MainThread): 10:52:22  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4fe70a2430>]}
2023-04-20 10:52:22.304302 (MainThread): 10:52:22  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 10:52:22.304457 (MainThread): 10:52:22  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4fe73059a0>]}
2023-04-20 10:52:22.306391 (MainThread): 10:52:22  
2023-04-20 10:52:22.307692 (MainThread): 10:52:22  Acquiring new databricks connection 'master'
2023-04-20 10:52:22.309445 (ThreadPoolExecutor-0_0): 10:52:22  Acquiring new databricks connection 'list_schemas'
2023-04-20 10:52:22.320669 (ThreadPoolExecutor-0_0): 10:52:22  Using databricks connection "list_schemas"
2023-04-20 10:52:22.320993 (ThreadPoolExecutor-0_0): 10:52:22  On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_schemas"} */

    show databases
  
2023-04-20 10:52:22.321156 (ThreadPoolExecutor-0_0): 10:52:22  Opening a new connection, currently in state init
2023-04-20 10:52:22.687695 (Thread-58): handling poll request
2023-04-20 10:52:22.688194 (Thread-58): 10:52:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f97b940>]}
2023-04-20 10:52:22.688962 (Thread-58): sending response (<Response 8664 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:23.391639 (ThreadPoolExecutor-0_0): 10:52:23  SQL status: OK in 1.07 seconds
2023-04-20 10:52:23.516664 (ThreadPoolExecutor-0_0): 10:52:23  On list_schemas: Close
2023-04-20 10:52:23.679581 (Thread-59): handling ps request
2023-04-20 10:52:23.680107 (Thread-59): 10:52:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa2d4c0>]}
2023-04-20 10:52:23.680676 (Thread-59): sending response (<Response 1165 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:23.872199 (ThreadPoolExecutor-1_0): 10:52:23  Acquiring new databricks connection 'list_None_dbt_shabbirkdb'
2023-04-20 10:52:23.882664 (ThreadPoolExecutor-1_0): 10:52:23  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:23.882857 (ThreadPoolExecutor-1_0): 10:52:23  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 10:52:23.883015 (ThreadPoolExecutor-1_0): 10:52:23  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show tables in `dbt_shabbirkdb`
  
2023-04-20 10:52:23.883166 (ThreadPoolExecutor-1_0): 10:52:23  Opening a new connection, currently in state closed
2023-04-20 10:52:24.416515 (Thread-60): handling status request
2023-04-20 10:52:24.417011 (Thread-60): 10:52:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f97dee0>]}
2023-04-20 10:52:24.417493 (Thread-60): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:24.608985 (ThreadPoolExecutor-1_0): 10:52:24  SQL status: OK in 0.73 seconds
2023-04-20 10:52:24.618169 (ThreadPoolExecutor-1_0): 10:52:24  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 10:52:24.618393 (ThreadPoolExecutor-1_0): 10:52:24  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show views in `dbt_shabbirkdb`
  
2023-04-20 10:52:24.682714 (Thread-61): handling poll request
2023-04-20 10:52:24.683207 (Thread-61): 10:52:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f986490>]}
2023-04-20 10:52:24.683838 (Thread-61): sending response (<Response 3830 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:25.034823 (ThreadPoolExecutor-1_0): 10:52:25  SQL status: OK in 0.42 seconds
2023-04-20 10:52:25.038347 (ThreadPoolExecutor-1_0): 10:52:25  On list_None_dbt_shabbirkdb: ROLLBACK
2023-04-20 10:52:25.038560 (ThreadPoolExecutor-1_0): 10:52:25  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:25.038709 (ThreadPoolExecutor-1_0): 10:52:25  On list_None_dbt_shabbirkdb: Close
2023-04-20 10:52:25.321001 (MainThread): 10:52:25  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f9a56fd60>]}
2023-04-20 10:52:25.321416 (MainThread): 10:52:25  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:25.321577 (MainThread): 10:52:25  Spark adapter: NotImplemented: commit
2023-04-20 10:52:25.322094 (MainThread): 10:52:25  Concurrency: 4 threads (target='default')
2023-04-20 10:52:25.322245 (MainThread): 10:52:25  
2023-04-20 10:52:25.325079 (Thread-1): 10:52:25  Began running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 10:52:25.325455 (Thread-1): 10:52:25  1 of 15 START sql table model dbt_shabbirkdb.DimBroker ......................... [RUN]
2023-04-20 10:52:25.325985 (Thread-1): 10:52:25  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
2023-04-20 10:52:25.326220 (Thread-1): 10:52:25  Began compiling node model.dbsql_dbt_tpch.DimBroker
2023-04-20 10:52:25.330150 (Thread-2): 10:52:25  Began running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 10:52:25.330466 (Thread-2): 10:52:25  2 of 15 START sql table model dbt_shabbirkdb.DimCompany ........................ [RUN]
2023-04-20 10:52:25.330973 (Thread-2): 10:52:25  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCompany'
2023-04-20 10:52:25.331283 (Thread-2): 10:52:25  Began compiling node model.dbsql_dbt_tpch.DimCompany
2023-04-20 10:52:25.335211 (Thread-3): 10:52:25  Began running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 10:52:25.335506 (Thread-3): 10:52:25  3 of 15 START sql table model dbt_shabbirkdb.DimCustomerStg .................... [RUN]
2023-04-20 10:52:25.336044 (Thread-3): 10:52:25  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomerStg'
2023-04-20 10:52:25.336218 (Thread-3): 10:52:25  Began compiling node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 10:52:25.340278 (Thread-4): 10:52:25  Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 10:52:25.340567 (Thread-4): 10:52:25  4 of 15 START sql table model dbt_shabbirkdb.tempDailyMarketHistorical ......... [RUN]
2023-04-20 10:52:25.341070 (Thread-4): 10:52:25  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempDailyMarketHistorical'
2023-04-20 10:52:25.341247 (Thread-4): 10:52:25  Began compiling node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 10:52:25.344962 (Thread-4): 10:52:25  Writing injected SQL for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 10:52:25.346244 (Thread-1): 10:52:25  Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 10:52:25.346742 (Thread-2): 10:52:25  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 10:52:25.347304 (Thread-3): 10:52:25  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 10:52:25.381598 (Thread-4): 10:52:25  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (compile): 2023-04-20 10:52:25.341290 => 2023-04-20 10:52:25.381368
2023-04-20 10:52:25.381880 (Thread-4): 10:52:25  Began executing node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 10:52:25.387089 (Thread-1): 10:52:25  Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 2023-04-20 10:52:25.326274 => 2023-04-20 10:52:25.386909
2023-04-20 10:52:25.387316 (Thread-1): 10:52:25  Began executing node model.dbsql_dbt_tpch.DimBroker
2023-04-20 10:52:25.397390 (Thread-3): 10:52:25  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (compile): 2023-04-20 10:52:25.336262 => 2023-04-20 10:52:25.397214
2023-04-20 10:52:25.397609 (Thread-3): 10:52:25  Began executing node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 10:52:25.415304 (Thread-4): 10:52:25  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:25.415491 (Thread-4): 10:52:25  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 10:52:25.415680 (Thread-4): 10:52:25  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

      describe extended `dbt_shabbirkdb`.`tempdailymarkethistorical`
  
2023-04-20 10:52:25.415818 (Thread-4): 10:52:25  Opening a new connection, currently in state init
2023-04-20 10:52:25.421315 (Thread-1): 10:52:25  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:25.421504 (Thread-1): 10:52:25  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 10:52:25.421665 (Thread-1): 10:52:25  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

      describe extended `dbt_shabbirkdb`.`dimbroker`
  
2023-04-20 10:52:25.421794 (Thread-1): 10:52:25  Opening a new connection, currently in state closed
2023-04-20 10:52:25.434011 (Thread-2): 10:52:25  Timing info for model.dbsql_dbt_tpch.DimCompany (compile): 2023-04-20 10:52:25.331340 => 2023-04-20 10:52:25.433839
2023-04-20 10:52:25.434232 (Thread-2): 10:52:25  Began executing node model.dbsql_dbt_tpch.DimCompany
2023-04-20 10:52:25.479700 (Thread-2): 10:52:25  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 10:52:25.482305 (Thread-3): 10:52:25  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 10:52:25.517408 (Thread-2): 10:52:25  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:25.517624 (Thread-2): 10:52:25  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 10:52:25.517862 (Thread-2): 10:52:25  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCompany`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
  
2023-04-20 10:52:25.518009 (Thread-2): 10:52:25  Opening a new connection, currently in state init
2023-04-20 10:52:25.533638 (Thread-3): 10:52:25  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:25.533966 (Thread-3): 10:52:25  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 10:52:25.534495 (Thread-3): 10:52:25  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomerStg`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT * FROM (
  SELECT
    customerid,
    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) taxid,
    status,
    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) lastname,
    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) firstname,
    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) middleinitial,
    coalesce(gender, last_value(gender) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) gender,
    coalesce(tier, last_value(tier) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) tier,
    coalesce(dob, last_value(dob) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) dob,
    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline1,
    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline2,
    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) postalcode,
    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) CITY,
    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) stateprov,
    coalesce(country, last_value(country) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) country,
    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone1,
    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone2,
    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone3,
    coalesce(email1, last_value(email1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email1,
    coalesce(email2, last_value(email2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email2,
    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) LCL_TX_ID,
    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) NAT_TX_ID,
    batchid,
    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,
    date(update_ts) effectivedate,
    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate
  FROM (
    SELECT
      customerid,
      taxid,
      status,
      lastname,
      firstname,
      middleinitial,
      gender,
      tier,
      dob,
      addressline1,
      addressline2,
      postalcode,
      city,
      stateprov,
      country,
      phone1,
      phone2,
      phone3,
      email1,
      email2,
      lcl_tx_id,
      nat_tx_id,
      1 batchid,
      update_ts
    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')
    UNION ALL
    SELECT
      c.customerid,
      nullif(c.taxid, '') taxid,
      nullif(s.st_name, '') as status,
      nullif(c.lastname, '') lastname,
      nullif(c.firstname, '') firstname,
      nullif(c.middleinitial, '') middleinitial,
      gender,
      c.tier,
      c.dob,
      nullif(c.addressline1, '') addressline1,
      nullif(c.addressline2, '') addressline2,
      nullif(c.postalcode, '') postalcode,
      nullif(c.city, '') city,
      nullif(c.stateprov, '') stateprov,
      nullif(c.country, '') country,
      CASE
        WHEN isnull(c_local_1) then c_local_1
        ELSE concat(
          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),
          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),
          c_local_1,
          nvl(c_ext_1, '')) END as phone1,
      CASE
        WHEN isnull(c_local_2) then c_local_2
        ELSE concat(
          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),
          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),
          c_local_2,
          nvl(c_ext_2, '')) END as phone2,
      CASE
        WHEN isnull(c_local_3) then c_local_3
        ELSE concat(
          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),
          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),
          c_local_3,
          nvl(c_ext_3, '')) END as phone3,
      nullif(c.email1, '') email1,
      nullif(c.email2, '') email2,
      c.LCL_TX_ID, 
      c.NAT_TX_ID,
      c.batchid,
      timestamp(bd.batchdate) update_ts
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CustomerIncremental` c
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
      ON c.batchid = bd.batchid
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` s 
      ON c.status = s.st_id
  ) c
  )
  
2023-04-20 10:52:25.534652 (Thread-3): 10:52:25  Opening a new connection, currently in state init
2023-04-20 10:52:26.179434 (Thread-4): 10:52:26  SQL status: OK in 0.76 seconds
2023-04-20 10:52:26.185390 (Thread-4): 10:52:26  Writing runtime sql for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 10:52:26.197423 (Thread-4): 10:52:26  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 10:52:26.197657 (Thread-4): 10:52:26  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempDailyMarketHistorical`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  dmh.*,
  sk_dateid,
  min(dm_low) OVER (
    PARTITION BY dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweeklow,
  max(dm_high) OVER (
    PARTITION by dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweekhigh
FROM (
  SELECT * FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketHistorical`
  UNION ALL
  SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketIncremental`) dmh
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
  ON d.datevalue = dm_date;
  
2023-04-20 10:52:26.251879 (Thread-1): 10:52:26  SQL status: OK in 0.83 seconds
2023-04-20 10:52:26.257394 (Thread-1): 10:52:26  Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 10:52:26.269725 (Thread-1): 10:52:26  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 10:52:26.269962 (Thread-1): 10:52:26  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT
  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate`) effectivedate,
  date('9999-12-31') enddate
FROM  `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`HR`
WHERE employeejobcode = 314
  
2023-04-20 10:52:26.316404 (Thread-62): handling poll request
2023-04-20 10:52:26.316875 (Thread-62): 10:52:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f90b190>]}
2023-04-20 10:52:26.318109 (Thread-62): sending response (<Response 35562 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:26.539986 (Thread-63): handling ps request
2023-04-20 10:52:26.540531 (Thread-63): 10:52:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f90b280>]}
2023-04-20 10:52:26.541111 (Thread-63): sending response (<Response 1165 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:26.972830 (Thread-64): handling status request
2023-04-20 10:52:26.973321 (Thread-64): 10:52:26  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f90b520>]}
2023-04-20 10:52:26.973789 (Thread-64): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:27.085595 (Thread-2): 10:52:27  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCompany`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
  
2023-04-20 10:52:27.085834 (Thread-2): 10:52:27  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 10:52:27.085975 (Thread-2): 10:52:27  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3545.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3545.0 (TID 28490) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3545.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3545.0 (TID 28490) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
Caused by: org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

2023-04-20 10:52:27.086095 (Thread-2): 10:52:27  Databricks adapter: operation-id: b'\x01\xed\xdfio\xe6\x1aT\xb2\xbefA\xbf\x9f\xa9\xba'
2023-04-20 10:52:27.086342 (Thread-2): 10:52:27  Timing info for model.dbsql_dbt_tpch.DimCompany (execute): 2023-04-20 10:52:25.434280 => 2023-04-20 10:52:27.086210
2023-04-20 10:52:27.086529 (Thread-2): 10:52:27  On model.dbsql_dbt_tpch.DimCompany: ROLLBACK
2023-04-20 10:52:27.086656 (Thread-2): 10:52:27  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:27.086779 (Thread-2): 10:52:27  On model.dbsql_dbt_tpch.DimCompany: Close
2023-04-20 10:52:27.370372 (Thread-2): 10:52:27  Runtime Error in model DimCompany (models/silver/DimCompany.sql)
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 10:52:27.370801 (Thread-2): 10:52:27  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4fe6877280>]}
2023-04-20 10:52:27.371251 (Thread-2): 10:52:27  2 of 15 ERROR creating sql table model dbt_shabbirkdb.DimCompany ............... [ERROR in 2.04s]
2023-04-20 10:52:27.372425 (Thread-2): 10:52:27  Finished running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 10:52:27.373345 (Thread-2): 10:52:27  Began running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 10:52:27.373597 (Thread-2): 10:52:27  5 of 15 SKIP relation dbt_shabbirkdb.DimSecurity ............................... [SKIP]
2023-04-20 10:52:27.373786 (Thread-2): 10:52:27  Finished running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 10:52:27.374034 (Thread-2): 10:52:27  Began running node model.dbsql_dbt_tpch.Financial
2023-04-20 10:52:27.374228 (Thread-2): 10:52:27  6 of 15 SKIP relation dbt_shabbirkdb.Financial ................................. [SKIP]
2023-04-20 10:52:27.374400 (Thread-2): 10:52:27  Finished running node model.dbsql_dbt_tpch.Financial
2023-04-20 10:52:27.375061 (Thread-2): 10:52:27  Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 10:52:27.375275 (Thread-2): 10:52:27  7 of 15 SKIP relation dbt_shabbirkdb.tempSumpFiBasicEps ........................ [SKIP]
2023-04-20 10:52:27.375450 (Thread-2): 10:52:27  Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 10:52:27.900641 (Thread-65): handling poll request
2023-04-20 10:52:27.901135 (Thread-65): 10:52:27  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f90bd30>]}
2023-04-20 10:52:27.901944 (Thread-65): sending response (<Response 34219 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:29.126298 (Thread-66): handling ps request
2023-04-20 10:52:29.126791 (Thread-66): 10:52:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f90be20>]}
2023-04-20 10:52:29.127346 (Thread-66): sending response (<Response 1166 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:29.187887 (Thread-1): 10:52:29  SQL status: OK in 2.92 seconds
2023-04-20 10:52:29.216028 (Thread-1): 10:52:29  Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 2023-04-20 10:52:25.387364 => 2023-04-20 10:52:29.215861
2023-04-20 10:52:29.216272 (Thread-1): 10:52:29  On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
2023-04-20 10:52:29.216422 (Thread-1): 10:52:29  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:29.216545 (Thread-1): 10:52:29  On model.dbsql_dbt_tpch.DimBroker: Close
2023-04-20 10:52:29.518348 (Thread-1): 10:52:29  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4fe6867490>]}
2023-04-20 10:52:29.518877 (Thread-1): 10:52:29  1 of 15 OK created sql table model dbt_shabbirkdb.DimBroker .................... [OK in 4.19s]
2023-04-20 10:52:29.519113 (Thread-1): 10:52:29  Finished running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 10:52:29.523471 (Thread-67): handling poll request
2023-04-20 10:52:29.523887 (Thread-67): 10:52:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f90e2b0>]}
2023-04-20 10:52:29.546471 (Thread-67): sending response (<Response 3405 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:29.583967 (Thread-68): handling status request
2023-04-20 10:52:29.584314 (Thread-68): 10:52:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f90ef40>]}
2023-04-20 10:52:29.584704 (Thread-68): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:29.917313 (Thread-3): 10:52:29  SQL status: OK in 4.38 seconds
2023-04-20 10:52:29.919620 (Thread-3): 10:52:29  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (execute): 2023-04-20 10:52:25.397658 => 2023-04-20 10:52:29.919422
2023-04-20 10:52:29.919839 (Thread-3): 10:52:29  On model.dbsql_dbt_tpch.DimCustomerStg: ROLLBACK
2023-04-20 10:52:29.919983 (Thread-3): 10:52:29  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:29.920108 (Thread-3): 10:52:29  On model.dbsql_dbt_tpch.DimCustomerStg: Close
2023-04-20 10:52:30.195809 (Thread-3): 10:52:30  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f9a54f7f0>]}
2023-04-20 10:52:30.196342 (Thread-3): 10:52:30  3 of 15 OK created sql table model dbt_shabbirkdb.DimCustomerStg ............... [OK in 4.86s]
2023-04-20 10:52:30.196579 (Thread-3): 10:52:30  Finished running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 10:52:30.197516 (Thread-2): 10:52:30  Began running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 10:52:30.197880 (Thread-2): 10:52:30  8 of 15 START sql table model dbt_shabbirkdb.DimAccount ........................ [RUN]
2023-04-20 10:52:30.198390 (Thread-2): 10:52:30  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimAccount'
2023-04-20 10:52:30.198580 (Thread-2): 10:52:30  Began compiling node model.dbsql_dbt_tpch.DimAccount
2023-04-20 10:52:30.202577 (Thread-1): 10:52:30  Began running node model.dbsql_dbt_tpch.Prospect
2023-04-20 10:52:30.202869 (Thread-1): 10:52:30  9 of 15 START sql table model dbt_shabbirkdb.Prospect .......................... [RUN]
2023-04-20 10:52:30.203343 (Thread-1): 10:52:30  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Prospect'
2023-04-20 10:52:30.203559 (Thread-1): 10:52:30  Began compiling node model.dbsql_dbt_tpch.Prospect
2023-04-20 10:52:30.208885 (Thread-2): 10:52:30  Writing injected SQL for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 10:52:30.210347 (Thread-1): 10:52:30  Writing injected SQL for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 10:52:30.220022 (Thread-2): 10:52:30  Timing info for model.dbsql_dbt_tpch.DimAccount (compile): 2023-04-20 10:52:30.198626 => 2023-04-20 10:52:30.219860
2023-04-20 10:52:30.220250 (Thread-2): 10:52:30  Began executing node model.dbsql_dbt_tpch.DimAccount
2023-04-20 10:52:30.224231 (Thread-2): 10:52:30  Writing runtime sql for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 10:52:30.224607 (Thread-1): 10:52:30  Timing info for model.dbsql_dbt_tpch.Prospect (compile): 2023-04-20 10:52:30.203627 => 2023-04-20 10:52:30.224440
2023-04-20 10:52:30.224819 (Thread-1): 10:52:30  Began executing node model.dbsql_dbt_tpch.Prospect
2023-04-20 10:52:30.230096 (Thread-1): 10:52:30  Writing runtime sql for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 10:52:30.238748 (Thread-2): 10:52:30  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:30.238932 (Thread-2): 10:52:30  Using databricks connection "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 10:52:30.239164 (Thread-2): 10:52:30  On model.dbsql_dbt_tpch.DimAccount: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 10:52:30.239299 (Thread-2): 10:52:30  Opening a new connection, currently in state closed
2023-04-20 10:52:30.242528 (Thread-1): 10:52:30  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:30.242714 (Thread-1): 10:52:30  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 10:52:30.242973 (Thread-1): 10:52:30  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Prospect`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  agencyid,
  recdate.sk_dateid sk_recorddateid,
  origdate.sk_dateid sk_updatedateid,
  p.batchid,
  nvl2(c.customerid, True, False) iscustomer, 
  p.lastname,
  p.firstname,
  p.middleinitial,
  p.gender,
  p.addressline1,
  p.addressline2,
  p.postalcode,
  city,
  state,
  country,
  phone,
  income,
  numbercars,
  numberchildren,
  maritalstatus,
  age,
  creditrating,
  ownorrentflag,
  employer,
  numbercreditcards,
  networth,
  if(
    isnotnull(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+","")),
    left(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+",""),
      length(
        if(networth > 1000000 or income > 200000,"HighValue+","") || 
        if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
        if(age > 45, "Boomer+", "") ||
        if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
        if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
        if(age < 25 and networth > 1000000, "Inherited+",""))
      -1),
    NULL) marketingnameplate
FROM (
  SELECT 
    * FROM (
    SELECT
      agencyid,
      max(batchid) recordbatchid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth,
      min(batchid) batchid
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`ProspectRaw` p
    GROUP BY
      agencyid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth)
  QUALIFY ROW_NUMBER() OVER (PARTITION BY agencyid ORDER BY batchid DESC) = 1) p
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) recdate
  ON p.recordbatchid = recdate.batchid
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) origdate
  ON p.batchid = origdate.batchid
LEFT JOIN (
  SELECT 
    customerid,
    lastname,
    firstname,
    addressline1,
    addressline2,
    postalcode
  FROM `dbt_shabbirkdb`.`DimCustomerStg`
  WHERE iscurrent) c
  ON 
    upper(p.LastName) = upper(c.lastname)
    and upper(p.FirstName) = upper(c.firstname)
    and upper(p.AddressLine1) = upper(c.addressline1)
    and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
    and upper(p.PostalCode) = upper(c.postalcode)
  
2023-04-20 10:52:30.243102 (Thread-1): 10:52:30  Opening a new connection, currently in state closed
2023-04-20 10:52:31.056354 (Thread-2): 10:52:31  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 10:52:31.056593 (Thread-2): 10:52:31  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 10:52:31.056755 (Thread-2): 10:52:31  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 10:52:31.056895 (Thread-2): 10:52:31  Databricks adapter: operation-id: b'\x01\xed\xdfir\xb6\x19\x98\xbb\xc9\x94\xf7\xd0\x82\xe2\xbb'
2023-04-20 10:52:31.057142 (Thread-2): 10:52:31  Timing info for model.dbsql_dbt_tpch.DimAccount (execute): 2023-04-20 10:52:30.220301 => 2023-04-20 10:52:31.057008
2023-04-20 10:52:31.057328 (Thread-2): 10:52:31  On model.dbsql_dbt_tpch.DimAccount: ROLLBACK
2023-04-20 10:52:31.057455 (Thread-2): 10:52:31  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:31.057577 (Thread-2): 10:52:31  On model.dbsql_dbt_tpch.DimAccount: Close
2023-04-20 10:52:31.179564 (Thread-69): handling poll request
2023-04-20 10:52:31.180068 (Thread-69): 10:52:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa1a2b0>]}
2023-04-20 10:52:31.181003 (Thread-69): sending response (<Response 29512 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:31.343123 (Thread-2): 10:52:31  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 10:52:31.343713 (Thread-2): 10:52:31  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4f9930d040>]}
2023-04-20 10:52:31.344177 (Thread-2): 10:52:31  8 of 15 ERROR creating sql table model dbt_shabbirkdb.DimAccount ............... [ERROR in 1.15s]
2023-04-20 10:52:31.344392 (Thread-2): 10:52:31  Finished running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 10:52:31.345258 (Thread-3): 10:52:31  Began running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 10:52:31.345534 (Thread-3): 10:52:31  10 of 15 SKIP relation dbt_shabbirkdb.DimTrade ................................. [SKIP]
2023-04-20 10:52:31.345728 (Thread-3): 10:52:31  Finished running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 10:52:31.345987 (Thread-3): 10:52:31  Began running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 10:52:31.346188 (Thread-3): 10:52:31  11 of 15 SKIP relation dbt_shabbirkdb.FactCashBalances ......................... [SKIP]
2023-04-20 10:52:31.346366 (Thread-3): 10:52:31  Finished running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 10:52:31.347056 (Thread-3): 10:52:31  Began running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 10:52:31.347284 (Thread-3): 10:52:31  12 of 15 SKIP relation dbt_shabbirkdb.FactHoldings ............................. [SKIP]
2023-04-20 10:52:31.347469 (Thread-3): 10:52:31  Finished running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 10:52:31.757851 (Thread-70): handling ps request
2023-04-20 10:52:31.758345 (Thread-70): 10:52:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f919490>]}
2023-04-20 10:52:31.758940 (Thread-70): sending response (<Response 1166 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:32.321532 (Thread-71): handling status request
2023-04-20 10:52:32.322028 (Thread-71): 10:52:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f919940>]}
2023-04-20 10:52:32.322494 (Thread-71): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:32.751834 (Thread-72): handling poll request
2023-04-20 10:52:32.752324 (Thread-72): 10:52:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f9195b0>]}
2023-04-20 10:52:32.752960 (Thread-72): sending response (<Response 7112 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:33.218266 (Thread-4): 10:52:33  SQL status: OK in 7.02 seconds
2023-04-20 10:52:33.525068 (Thread-4): 10:52:33  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (execute): 2023-04-20 10:52:25.381935 => 2023-04-20 10:52:33.524880
2023-04-20 10:52:33.525342 (Thread-4): 10:52:33  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: ROLLBACK
2023-04-20 10:52:33.525498 (Thread-4): 10:52:33  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:33.525624 (Thread-4): 10:52:33  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: Close
2023-04-20 10:52:33.813337 (Thread-4): 10:52:33  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4fe40561f0>]}
2023-04-20 10:52:33.814130 (Thread-4): 10:52:33  4 of 15 OK created sql table model dbt_shabbirkdb.tempDailyMarketHistorical .... [OK in 8.47s]
2023-04-20 10:52:33.814530 (Thread-4): 10:52:33  Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 10:52:33.815464 (Thread-3): 10:52:33  Began running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 10:52:33.815769 (Thread-3): 10:52:33  13 of 15 SKIP relation dbt_shabbirkdb.FactMarketHistory ........................ [SKIP]
2023-04-20 10:52:33.815965 (Thread-3): 10:52:33  Finished running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 10:52:34.319174 (Thread-73): handling poll request
2023-04-20 10:52:34.319695 (Thread-73): 10:52:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f919dc0>]}
2023-04-20 10:52:34.320331 (Thread-73): sending response (<Response 5279 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:34.352131 (Thread-74): handling ps request
2023-04-20 10:52:34.352514 (Thread-74): 10:52:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f919ee0>]}
2023-04-20 10:52:34.353009 (Thread-74): sending response (<Response 1166 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:34.802459 (Thread-75): handling status request
2023-04-20 10:52:34.802944 (Thread-75): 10:52:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f91d040>]}
2023-04-20 10:52:34.803404 (Thread-75): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:35.165012 (Thread-1): 10:52:35  SQL status: OK in 4.92 seconds
2023-04-20 10:52:35.167178 (Thread-1): 10:52:35  Timing info for model.dbsql_dbt_tpch.Prospect (execute): 2023-04-20 10:52:30.224868 => 2023-04-20 10:52:35.167026
2023-04-20 10:52:35.167385 (Thread-1): 10:52:35  On model.dbsql_dbt_tpch.Prospect: ROLLBACK
2023-04-20 10:52:35.167549 (Thread-1): 10:52:35  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:35.167680 (Thread-1): 10:52:35  On model.dbsql_dbt_tpch.Prospect: Close
2023-04-20 10:52:35.440429 (Thread-1): 10:52:35  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4fe400cd30>]}
2023-04-20 10:52:35.440963 (Thread-1): 10:52:35  9 of 15 OK created sql table model dbt_shabbirkdb.Prospect ..................... [OK in 5.24s]
2023-04-20 10:52:35.441207 (Thread-1): 10:52:35  Finished running node model.dbsql_dbt_tpch.Prospect
2023-04-20 10:52:35.442311 (Thread-4): 10:52:35  Began running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 10:52:35.442640 (Thread-4): 10:52:35  14 of 15 START sql table model dbt_shabbirkdb.DimCustomer ...................... [RUN]
2023-04-20 10:52:35.443175 (Thread-4): 10:52:35  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomer'
2023-04-20 10:52:35.443363 (Thread-4): 10:52:35  Began compiling node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 10:52:35.448357 (Thread-4): 10:52:35  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 10:52:35.459577 (Thread-4): 10:52:35  Timing info for model.dbsql_dbt_tpch.DimCustomer (compile): 2023-04-20 10:52:35.443410 => 2023-04-20 10:52:35.459378
2023-04-20 10:52:35.459811 (Thread-4): 10:52:35  Began executing node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 10:52:35.463887 (Thread-4): 10:52:35  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 10:52:35.474833 (Thread-4): 10:52:35  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:35.475019 (Thread-4): 10:52:35  Using databricks connection "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 10:52:35.475240 (Thread-4): 10:52:35  On model.dbsql_dbt_tpch.DimCustomer: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 10:52:35.475379 (Thread-4): 10:52:35  Opening a new connection, currently in state closed
2023-04-20 10:52:35.908833 (Thread-76): handling poll request
2023-04-20 10:52:35.909308 (Thread-76): 10:52:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f91d910>]}
2023-04-20 10:52:35.910033 (Thread-76): sending response (<Response 9778 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:36.282032 (Thread-4): 10:52:36  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 10:52:36.282261 (Thread-4): 10:52:36  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 10:52:36.282389 (Thread-4): 10:52:36  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 10:52:36.282505 (Thread-4): 10:52:36  Databricks adapter: operation-id: b'\x01\xed\xdfiu\xd4\x1c~\xb3X\xa3\x0e\xe1.\x82h'
2023-04-20 10:52:36.283080 (Thread-4): 10:52:36  Timing info for model.dbsql_dbt_tpch.DimCustomer (execute): 2023-04-20 10:52:35.459863 => 2023-04-20 10:52:36.282615
2023-04-20 10:52:36.283322 (Thread-4): 10:52:36  On model.dbsql_dbt_tpch.DimCustomer: ROLLBACK
2023-04-20 10:52:36.283459 (Thread-4): 10:52:36  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:36.283625 (Thread-4): 10:52:36  On model.dbsql_dbt_tpch.DimCustomer: Close
2023-04-20 10:52:36.555306 (Thread-4): 10:52:36  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 10:52:36.555749 (Thread-4): 10:52:36  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4cb6905b-9462-4de8-878a-a280f30e27ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4fe4006910>]}
2023-04-20 10:52:36.556196 (Thread-4): 10:52:36  14 of 15 ERROR creating sql table model dbt_shabbirkdb.DimCustomer ............. [ERROR in 1.11s]
2023-04-20 10:52:36.556411 (Thread-4): 10:52:36  Finished running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 10:52:36.557563 (Thread-2): 10:52:36  Began running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 10:52:36.557858 (Thread-2): 10:52:36  15 of 15 SKIP relation dbt_shabbirkdb.FactWatches .............................. [SKIP]
2023-04-20 10:52:36.558058 (Thread-2): 10:52:36  Finished running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 10:52:36.561273 (MainThread): 10:52:36  Acquiring new databricks connection 'master'
2023-04-20 10:52:36.561664 (MainThread): 10:52:36  On master: ROLLBACK
2023-04-20 10:52:36.561956 (MainThread): 10:52:36  Opening a new connection, currently in state init
2023-04-20 10:52:36.850095 (MainThread): 10:52:36  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:36.850345 (MainThread): 10:52:36  Spark adapter: NotImplemented: add_begin_query
2023-04-20 10:52:36.850487 (MainThread): 10:52:36  Spark adapter: NotImplemented: commit
2023-04-20 10:52:36.850654 (MainThread): 10:52:36  On master: ROLLBACK
2023-04-20 10:52:36.850789 (MainThread): 10:52:36  Databricks adapter: NotImplemented: rollback
2023-04-20 10:52:36.850927 (MainThread): 10:52:36  On master: Close
2023-04-20 10:52:36.918271 (Thread-77): handling ps request
2023-04-20 10:52:36.918752 (Thread-77): 10:52:36  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f925370>]}
2023-04-20 10:52:36.919303 (Thread-77): sending response (<Response 1166 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:37.135251 (MainThread): 10:52:37  Connection 'master' was properly closed.
2023-04-20 10:52:37.135449 (MainThread): 10:52:37  Connection 'model.dbsql_dbt_tpch.Prospect' was properly closed.
2023-04-20 10:52:37.135587 (MainThread): 10:52:37  Connection 'model.dbsql_dbt_tpch.DimAccount' was properly closed.
2023-04-20 10:52:37.135696 (MainThread): 10:52:37  Connection 'model.dbsql_dbt_tpch.DimCustomerStg' was properly closed.
2023-04-20 10:52:37.135799 (MainThread): 10:52:37  Connection 'model.dbsql_dbt_tpch.DimCustomer' was properly closed.
2023-04-20 10:52:37.136096 (MainThread): 10:52:37  
2023-04-20 10:52:37.136243 (MainThread): 10:52:37  Finished running 15 table models in 0 hours 0 minutes and 14.83 seconds (14.83s).
2023-04-20 10:52:37.227948 (MainThread): 10:52:37  
2023-04-20 10:52:37.228232 (MainThread): 10:52:37  Completed with 3 errors and 0 warnings:
2023-04-20 10:52:37.228370 (MainThread): 10:52:37  
2023-04-20 10:52:37.228515 (MainThread): 10:52:37  Runtime Error in model DimCompany (models/silver/DimCompany.sql)
2023-04-20 10:52:37.228643 (MainThread): 10:52:37    Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 10:52:37.228756 (MainThread): 10:52:37  
2023-04-20 10:52:37.228875 (MainThread): 10:52:37  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
2023-04-20 10:52:37.228988 (MainThread): 10:52:37    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 10:52:37.229098 (MainThread): 10:52:37  
2023-04-20 10:52:37.229216 (MainThread): 10:52:37  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
2023-04-20 10:52:37.229326 (MainThread): 10:52:37    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 10:52:37.229452 (MainThread): 10:52:37  
2023-04-20 10:52:37.229581 (MainThread): 10:52:37  Done. PASS=4 WARN=0 ERROR=3 SKIP=8 TOTAL=15
2023-04-20 10:52:37.651984 (Thread-78): handling status request
2023-04-20 10:52:37.652495 (Thread-78): 10:52:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f92c190>]}
2023-04-20 10:52:37.652971 (Thread-78): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:37.668831 (Thread-79): handling poll request
2023-04-20 10:52:37.669162 (Thread-79): 10:52:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f933640>]}
2023-04-20 10:52:37.671573 (Thread-79): sending response (<Response 100133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:39.506112 (Thread-80): handling ps request
2023-04-20 10:52:39.506585 (Thread-80): 10:52:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f933850>]}
2023-04-20 10:52:39.507125 (Thread-80): sending response (<Response 1190 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:52:39.946771 (Thread-81): handling status request
2023-04-20 10:52:39.947284 (Thread-81): 10:52:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f933a30>]}
2023-04-20 10:52:39.947804 (Thread-81): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:51.186362 (Thread-82): handling status request
2023-04-20 10:53:51.188362 (Thread-82): 10:53:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f933910>]}
2023-04-20 10:53:51.188852 (Thread-82): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:51.297688 (Thread-83): handling ps request
2023-04-20 10:53:51.298156 (Thread-83): 10:53:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f933af0>]}
2023-04-20 10:53:51.298694 (Thread-83): sending response (<Response 1190 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:51.972444 (Thread-84): handling ps request
2023-04-20 10:53:51.972976 (Thread-84): 10:53:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f933cd0>]}
2023-04-20 10:53:51.973518 (Thread-84): sending response (<Response 1190 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:51.988653 (Thread-85): handling run_sql request
2023-04-20 10:53:51.988989 (Thread-85): 10:53:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f933df0>]}
2023-04-20 10:53:54.762668 (Thread-85): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:54.802841 (MainThread): 10:53:54  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9ba7cc92-65e2-46bc-80f2-bde4d32276c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a5048fca0>]}
2023-04-20 10:53:54.803447 (MainThread): 10:53:54  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 10:53:54.804864 (Thread-1): 10:53:54  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 10:53:54.805099 (Thread-1): 10:53:54  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 10:53:54.809856 (Thread-1): 10:53:54  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 10:53:54.805147 => 2023-04-20 10:53:54.809677
2023-04-20 10:53:54.810082 (Thread-1): 10:53:54  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 10:53:54.810480 (Thread-1): 10:53:54  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 10:53:54.810973 (Thread-1): 10:53:54  On rpc.dbsql_dbt_tpch.request: 
SELECT * FROM (
  SELECT
    customerid,
    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) taxid,
    status,
    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) lastname,
    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) firstname,
    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) middleinitial,
    coalesce(gender, last_value(gender) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) gender,
    coalesce(tier, last_value(tier) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) tier,
    coalesce(dob, last_value(dob) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) dob,
    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline1,
    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline2,
    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) postalcode,
    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) CITY,
    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) stateprov,
    coalesce(country, last_value(country) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) country,
    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone1,
    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone2,
    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone3,
    coalesce(email1, last_value(email1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email1,
    coalesce(email2, last_value(email2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email2,
    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) LCL_TX_ID,
    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) NAT_TX_ID,
    batchid,
    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,
    date(update_ts) effectivedate,
    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate
  FROM (
    SELECT
      customerid,
      taxid,
      status,
      lastname,
      firstname,
      middleinitial,
      gender,
      tier,
      dob,
      addressline1,
      addressline2,
      postalcode,
      city,
      stateprov,
      country,
      phone1,
      phone2,
      phone3,
      email1,
      email2,
      lcl_tx_id,
      nat_tx_id,
      1 batchid,
      update_ts
    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')
    UNION ALL
    SELECT
      c.customerid,
      nullif(c.taxid, '') taxid,
      nullif(s.st_name, '') as status,
      nullif(c.lastname, '') lastname,
      nullif(c.firstname, '') firstname,
      nullif(c.middleinitial, '') middleinitial,
      gender,
      c.tier,
      c.dob,
      nullif(c.addressline1, '') addressline1,
      nullif(c.addressline2, '') addressline2,
      nullif(c.postalcode, '') postalcode,
      nullif(c.city, '') city,
      nullif(c.stateprov, '') stateprov,
      nullif(c.country, '') country,
      CASE
        WHEN isnull(c_local_1) then c_local_1
        ELSE concat(
          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),
          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),
          c_local_1,
          nvl(c_ext_1, '')) END as phone1,
      CASE
        WHEN isnull(c_local_2) then c_local_2
        ELSE concat(
          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),
          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),
          c_local_2,
          nvl(c_ext_2, '')) END as phone2,
      CASE
        WHEN isnull(c_local_3) then c_local_3
        ELSE concat(
          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),
          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),
          c_local_3,
          nvl(c_ext_3, '')) END as phone3,
      nullif(c.email1, '') email1,
      nullif(c.email2, '') email2,
      c.LCL_TX_ID, 
      c.NAT_TX_ID,
      c.batchid,
      timestamp(bd.batchdate) update_ts
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CustomerIncremental` c
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
      ON c.batchid = bd.batchid
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` s 
      ON c.status = s.st_id
  ) c
  )
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 10:53:54.811132 (Thread-1): 10:53:54  Opening a new connection, currently in state init
2023-04-20 10:53:55.364574 (Thread-86): handling ps request
2023-04-20 10:53:55.365221 (Thread-86): 10:53:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56997aee0>]}
2023-04-20 10:53:55.366141 (Thread-86): sending response (<Response 1716 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:55.374075 (Thread-87): handling ps request
2023-04-20 10:53:55.374406 (Thread-87): 10:53:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93e040>]}
2023-04-20 10:53:55.374864 (Thread-87): sending response (<Response 1716 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:55.840424 (Thread-88): handling status request
2023-04-20 10:53:55.840903 (Thread-88): 10:53:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93e2b0>]}
2023-04-20 10:53:55.841425 (Thread-88): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:56.034514 (Thread-89): handling poll request
2023-04-20 10:53:56.035002 (Thread-89): 10:53:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93e250>]}
2023-04-20 10:53:56.035664 (Thread-89): sending response (<Response 8910 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:56.116440 (Thread-90): handling poll request
2023-04-20 10:53:56.116899 (Thread-90): 10:53:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93e400>]}
2023-04-20 10:53:56.117471 (Thread-90): sending response (<Response 8910 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:56.580943 (Thread-91): handling status request
2023-04-20 10:53:56.581411 (Thread-91): 10:53:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93e370>]}
2023-04-20 10:53:56.581874 (Thread-91): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:56.790517 (Thread-1): 10:53:56  SQL status: OK in 1.98 seconds
2023-04-20 10:53:56.828426 (Thread-1): 10:53:56  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 10:53:54.810135 => 2023-04-20 10:53:56.828190
2023-04-20 10:53:56.828684 (Thread-1): 10:53:56  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 10:53:57.631833 (Thread-92): handling poll request
2023-04-20 10:53:57.632328 (Thread-92): 10:53:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564928880>]}
2023-04-20 10:53:57.638749 (Thread-92): sending response (<Response 183382 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:58.010215 (Thread-93): handling ps request
2023-04-20 10:53:58.010698 (Thread-93): 10:53:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564928d30>]}
2023-04-20 10:53:58.011295 (Thread-93): sending response (<Response 1741 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:58.173415 (Thread-94): handling poll request
2023-04-20 10:53:58.173904 (Thread-94): 10:53:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5621ea1c0>]}
2023-04-20 10:53:58.179790 (Thread-94): sending response (<Response 191859 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:58.551291 (Thread-95): handling status request
2023-04-20 10:53:58.551804 (Thread-95): 10:53:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5621ea220>]}
2023-04-20 10:53:58.552291 (Thread-95): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:59.778027 (Thread-96): handling ps request
2023-04-20 10:53:59.778534 (Thread-96): 10:53:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5621ea340>]}
2023-04-20 10:53:59.779129 (Thread-96): sending response (<Response 1741 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:53:59.816699 (Thread-97): handling run_sql request
2023-04-20 10:53:59.817057 (Thread-97): 10:53:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5621ea4f0>]}
2023-04-20 10:54:02.578526 (Thread-97): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:02.614472 (MainThread): 10:54:02  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2348f96b-a6cd-433b-b38f-71a5b6449a42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fce88e72d30>]}
2023-04-20 10:54:02.615060 (MainThread): 10:54:02  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 10:54:02.616413 (Thread-1): 10:54:02  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 10:54:02.616648 (Thread-1): 10:54:02  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 10:54:02.620905 (Thread-1): 10:54:02  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 10:54:02.616697 => 2023-04-20 10:54:02.620731
2023-04-20 10:54:02.621122 (Thread-1): 10:54:02  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 10:54:02.621468 (Thread-1): 10:54:02  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 10:54:02.621923 (Thread-1): 10:54:02  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 10:54:02.622072 (Thread-1): 10:54:02  Opening a new connection, currently in state init
2023-04-20 10:54:03.235823 (Thread-98): handling ps request
2023-04-20 10:54:03.236509 (Thread-98): 10:54:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2a51f0>]}
2023-04-20 10:54:03.237465 (Thread-98): sending response (<Response 2249 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:03.382510 (Thread-99): handling ps request
2023-04-20 10:54:03.383016 (Thread-99): 10:54:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2a5c10>]}
2023-04-20 10:54:03.383701 (Thread-99): sending response (<Response 2249 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:03.850021 (Thread-1): 10:54:03  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 10:54:03.850265 (Thread-1): 10:54:03  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 10:54:03.850398 (Thread-1): 10:54:03  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3612.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3612.0 (TID 28609) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3612.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3612.0 (TID 28609) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
Caused by: org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

2023-04-20 10:54:03.850512 (Thread-1): 10:54:03  Databricks adapter: operation-id: b'\x01\xed\xdfi\xa9\xcc\x12o\xbcL\xff\x1fP.n\t'
2023-04-20 10:54:03.850715 (Thread-1): 10:54:03  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 10:54:02.621167 => 2023-04-20 10:54:03.850584
2023-04-20 10:54:03.850901 (Thread-1): 10:54:03  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 10:54:03.972596 (Thread-100): handling status request
2023-04-20 10:54:03.973085 (Thread-100): 10:54:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2ab3d0>]}
2023-04-20 10:54:03.973645 (Thread-100): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:04.065797 (Thread-101): handling poll request
2023-04-20 10:54:04.066296 (Thread-101): 10:54:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2ab520>]}
2023-04-20 10:54:04.067586 (Thread-101): sending response (<Response 31903 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:04.145210 (Thread-102): handling poll request
2023-04-20 10:54:04.145682 (Thread-102): 10:54:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564fee700>]}
2023-04-20 10:54:04.146486 (Thread-102): sending response (<Response 31903 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:04.144922 (Thread-1): Got an exception: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 10:54:04.146741 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 10:54:04.509519 (Thread-103): handling status request
2023-04-20 10:54:04.510028 (Thread-103): 10:54:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564fee8e0>]}
2023-04-20 10:54:04.510516 (Thread-103): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:05.778844 (Thread-104): handling poll request
2023-04-20 10:54:05.779337 (Thread-104): 10:54:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564feea00>]}
2023-04-20 10:54:05.780212 (Thread-104): sending response (<Response 50997 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:05.817173 (Thread-105): handling ps request
2023-04-20 10:54:05.817559 (Thread-105): 10:54:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56499ebe0>]}
2023-04-20 10:54:05.818141 (Thread-105): sending response (<Response 2272 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:06.357130 (Thread-106): handling status request
2023-04-20 10:54:06.357620 (Thread-106): 10:54:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5621ea1c0>]}
2023-04-20 10:54:06.358108 (Thread-106): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:06.663226 (Thread-107): handling poll request
2023-04-20 10:54:06.663751 (Thread-107): 10:54:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564fee400>]}
2023-04-20 10:54:06.664511 (Thread-107): sending response (<Response 50997 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:07.418358 (Thread-108): handling status request
2023-04-20 10:54:07.418852 (Thread-108): 10:54:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564928880>]}
2023-04-20 10:54:07.419359 (Thread-108): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:34.103170 (Thread-109): handling ps request
2023-04-20 10:54:34.105119 (Thread-109): 10:54:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5649288b0>]}
2023-04-20 10:54:34.105741 (Thread-109): sending response (<Response 2272 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:34.126011 (Thread-110): handling run_sql request
2023-04-20 10:54:34.126373 (Thread-110): 10:54:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564928af0>]}
2023-04-20 10:54:36.895963 (Thread-110): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:36.931843 (MainThread): 10:54:36  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4305e098-6241-41ca-8cc8-ee8b6282db05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f5bf62c70>]}
2023-04-20 10:54:36.932425 (MainThread): 10:54:36  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 10:54:36.933777 (Thread-1): 10:54:36  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 10:54:36.934019 (Thread-1): 10:54:36  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 10:54:36.938185 (Thread-1): 10:54:36  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 10:54:36.934070 => 2023-04-20 10:54:36.938017
2023-04-20 10:54:36.938402 (Thread-1): 10:54:36  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 10:54:36.938759 (Thread-1): 10:54:36  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 10:54:36.939206 (Thread-1): 10:54:36  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
     -- to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 10:54:36.939358 (Thread-1): 10:54:36  Opening a new connection, currently in state init
2023-04-20 10:54:37.470143 (Thread-111): handling ps request
2023-04-20 10:54:37.470820 (Thread-111): 10:54:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93edc0>]}
2023-04-20 10:54:37.471846 (Thread-111): sending response (<Response 2780 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:37.569758 (Thread-112): handling ps request
2023-04-20 10:54:37.570291 (Thread-112): 10:54:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564928910>]}
2023-04-20 10:54:37.570928 (Thread-112): sending response (<Response 2780 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:37.692523 (Thread-1): 10:54:37  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
     -- to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 10:54:37.692754 (Thread-1): 10:54:37  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pts` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `st`.`st_id`, `cmp`.`Status`, `st`.`st_name`].; line 27 pos 14
2023-04-20 10:54:37.692881 (Thread-1): 10:54:37  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pts` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `st`.`st_id`, `cmp`.`Status`, `st`.`st_name`].; line 27 pos 14
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pts` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `st`.`st_id`, `cmp`.`Status`, `st`.`st_name`].; line 27 pos 14
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 10:54:37.692999 (Thread-1): 10:54:37  Databricks adapter: operation-id: b'\x01\xed\xdfi\xbe@\x1b@\xa8\x19\xfaI,\x13+\x10'
2023-04-20 10:54:37.693200 (Thread-1): 10:54:37  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 10:54:36.938449 => 2023-04-20 10:54:37.693070
2023-04-20 10:54:37.693392 (Thread-1): 10:54:37  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 10:54:37.978008 (Thread-1): Got an exception: Runtime Error
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pts` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `st`.`st_id`, `cmp`.`Status`, `st`.`st_name`].; line 27 pos 14
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pts` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `st`.`st_id`, `cmp`.`Status`, `st`.`st_name`].; line 27 pos 14

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pts` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `st`.`st_id`, `cmp`.`Status`, `st`.`st_name`].; line 27 pos 14
2023-04-20 10:54:37.979971 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pts` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `st`.`st_id`, `cmp`.`Status`, `st`.`st_name`].; line 27 pos 14', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n     -- to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n     -- to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pts` cannot be resolved. Did you mean one of the following? [`cmp`.`CIK`, `cmp`.`City`, `st`.`st_id`, `cmp`.`Status`, `st`.`st_name`].; line 27 pos 14', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n     -- to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n     -- to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 10:54:38.040222 (Thread-113): handling status request
2023-04-20 10:54:38.040710 (Thread-113): 10:54:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56997adc0>]}
2023-04-20 10:54:38.041227 (Thread-113): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:38.138902 (Thread-114): handling poll request
2023-04-20 10:54:38.139394 (Thread-114): 10:54:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56997a8e0>]}
2023-04-20 10:54:38.140210 (Thread-114): sending response (<Response 28275 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:38.241554 (Thread-115): handling poll request
2023-04-20 10:54:38.242057 (Thread-115): 10:54:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56997afa0>]}
2023-04-20 10:54:38.242792 (Thread-115): sending response (<Response 32783 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:38.591451 (Thread-116): handling status request
2023-04-20 10:54:38.591961 (Thread-116): 10:54:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2b9ee0>]}
2023-04-20 10:54:38.592423 (Thread-116): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:40.138442 (Thread-117): handling ps request
2023-04-20 10:54:40.138938 (Thread-117): 10:54:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2bf2b0>]}
2023-04-20 10:54:40.139615 (Thread-117): sending response (<Response 2803 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:40.607792 (Thread-118): handling status request
2023-04-20 10:54:40.608274 (Thread-118): 10:54:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2bfb20>]}
2023-04-20 10:54:40.608740 (Thread-118): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:40.802036 (Thread-119): handling poll request
2023-04-20 10:54:40.802513 (Thread-119): 10:54:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2bf070>]}
2023-04-20 10:54:40.803191 (Thread-119): sending response (<Response 32783 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:54:41.522106 (Thread-120): handling status request
2023-04-20 10:54:41.522591 (Thread-120): 10:54:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2c6970>]}
2023-04-20 10:54:41.523052 (Thread-120): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:56:03.436583 (Thread-121): handling status request
2023-04-20 10:56:03.438367 (Thread-121): 10:56:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56997a970>]}
2023-04-20 10:56:03.438849 (Thread-121): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:56:43.940853 (Thread-122): handling status request
2023-04-20 10:56:43.941376 (Thread-122): 10:56:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93e400>]}
2023-04-20 10:56:43.941884 (Thread-122): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:56:59.627640 (Thread-123): handling status request
2023-04-20 10:56:59.628136 (Thread-123): 10:56:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93ed60>]}
2023-04-20 10:56:59.628606 (Thread-123): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:14.495193 (Thread-124): handling ps request
2023-04-20 10:57:14.495721 (Thread-124): 10:57:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93e6d0>]}
2023-04-20 10:57:14.496384 (Thread-124): sending response (<Response 2803 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:14.534619 (Thread-125): handling run_sql request
2023-04-20 10:57:14.535030 (Thread-125): 10:57:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f93e0d0>]}
2023-04-20 10:57:17.371184 (Thread-125): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:17.411068 (MainThread): 10:57:17  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '99f1d357-8114-41fe-9af2-a95895f245f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ec2feaca0>]}
2023-04-20 10:57:17.411751 (MainThread): 10:57:17  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 10:57:17.413170 (Thread-1): 10:57:17  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 10:57:17.413403 (Thread-1): 10:57:17  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 10:57:17.417678 (Thread-1): 10:57:17  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 10:57:17.413451 => 2023-04-20 10:57:17.417511
2023-04-20 10:57:17.417908 (Thread-1): 10:57:17  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 10:57:17.418259 (Thread-1): 10:57:17  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 10:57:17.418706 (Thread-1): 10:57:17  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 0, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 10:57:17.418853 (Thread-1): 10:57:17  Opening a new connection, currently in state init
2023-04-20 10:57:17.986277 (Thread-126): handling ps request
2023-04-20 10:57:17.986944 (Thread-126): 10:57:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d254400>]}
2023-04-20 10:57:17.988008 (Thread-126): sending response (<Response 3311 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:17.999258 (Thread-127): handling ps request
2023-04-20 10:57:17.999634 (Thread-127): 10:57:17  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d254d60>]}
2023-04-20 10:57:18.025433 (Thread-127): sending response (<Response 3311 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:18.470753 (Thread-128): handling status request
2023-04-20 10:57:18.471245 (Thread-128): 10:57:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2547f0>]}
2023-04-20 10:57:18.471807 (Thread-128): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:18.575122 (Thread-129): handling poll request
2023-04-20 10:57:18.575663 (Thread-129): 10:57:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d25b7f0>]}
2023-04-20 10:57:18.576312 (Thread-129): sending response (<Response 5691 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:18.700004 (Thread-1): 10:57:18  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 0, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 10:57:18.700232 (Thread-1): 10:57:18  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 10:57:18.700362 (Thread-1): 10:57:18  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3629.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3629.0 (TID 28813) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3629.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3629.0 (TID 28813) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
Caused by: org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

2023-04-20 10:57:18.700477 (Thread-1): 10:57:18  Databricks adapter: operation-id: b'\x01\xed\xdfj\x1d\xea\x1e\xe0\x8e2S\xe4\xcf\xe8\xbct'
2023-04-20 10:57:18.700677 (Thread-1): 10:57:18  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 10:57:17.417955 => 2023-04-20 10:57:18.700549
2023-04-20 10:57:18.700864 (Thread-1): 10:57:18  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 10:57:18.744143 (Thread-130): handling poll request
2023-04-20 10:57:18.744626 (Thread-130): 10:57:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d25beb0>]}
2023-04-20 10:57:18.745342 (Thread-130): sending response (<Response 31912 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:18.983764 (Thread-131): handling status request
2023-04-20 10:57:18.984262 (Thread-131): 10:57:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d26a5b0>]}
2023-04-20 10:57:18.984734 (Thread-131): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:19.050629 (Thread-1): Got an exception: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 10:57:19.052464 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 0, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 0, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 0, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 0, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 10:57:20.275972 (Thread-132): handling poll request
2023-04-20 10:57:20.276470 (Thread-132): 10:57:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d26a5b0>]}
2023-04-20 10:57:20.277246 (Thread-132): sending response (<Response 51007 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:20.589984 (Thread-133): handling ps request
2023-04-20 10:57:20.590481 (Thread-133): 10:57:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d262ee0>]}
2023-04-20 10:57:20.591144 (Thread-133): sending response (<Response 3334 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:21.025791 (Thread-134): handling status request
2023-04-20 10:57:21.026294 (Thread-134): 10:57:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f90ed90>]}
2023-04-20 10:57:21.026754 (Thread-134): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:21.138082 (Thread-135): handling poll request
2023-04-20 10:57:21.138498 (Thread-135): 10:57:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d262f70>]}
2023-04-20 10:57:21.139220 (Thread-135): sending response (<Response 51007 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:21.896514 (Thread-136): handling status request
2023-04-20 10:57:21.897009 (Thread-136): 10:57:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa7b730>]}
2023-04-20 10:57:21.897480 (Thread-136): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:32.074361 (Thread-137): handling run_sql request
2023-04-20 10:57:32.074867 (Thread-137): 10:57:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f987220>]}
2023-04-20 10:57:32.112660 (Thread-138): handling ps request
2023-04-20 10:57:32.113589 (Thread-138): 10:57:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d26acd0>]}
2023-04-20 10:57:32.114739 (Thread-138): sending response (<Response 3847 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:32.664912 (Thread-139): handling poll request
2023-04-20 10:57:32.665421 (Thread-139): 10:57:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d25beb0>]}
2023-04-20 10:57:32.665964 (Thread-139): sending response (<Response 422 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:33.099371 (Thread-140): handling status request
2023-04-20 10:57:33.099888 (Thread-140): 10:57:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d254d60>]}
2023-04-20 10:57:33.100405 (Thread-140): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:33.394273 (Thread-141): handling ps request
2023-04-20 10:57:33.394762 (Thread-141): 10:57:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d254550>]}
2023-04-20 10:57:33.395449 (Thread-141): sending response (<Response 3847 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:34.886762 (Thread-137): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:34.924022 (MainThread): 10:57:34  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dfc9645e-9c14-4145-a1ad-38eb2cb92e13', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb723ca2c70>]}
2023-04-20 10:57:34.924664 (MainThread): 10:57:34  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 10:57:34.926063 (Thread-1): 10:57:34  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 10:57:34.926299 (Thread-1): 10:57:34  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 10:57:34.930589 (Thread-1): 10:57:34  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 10:57:34.926348 => 2023-04-20 10:57:34.930417
2023-04-20 10:57:34.930810 (Thread-1): 10:57:34  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 10:57:34.931161 (Thread-1): 10:57:34  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 10:57:34.931640 (Thread-1): 10:57:34  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 10:57:34.931790 (Thread-1): 10:57:34  Opening a new connection, currently in state init
2023-04-20 10:57:35.492467 (Thread-142): handling poll request
2023-04-20 10:57:35.492990 (Thread-142): 10:57:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d24d4c0>]}
2023-04-20 10:57:35.493613 (Thread-142): sending response (<Response 5691 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:35.552163 (Thread-143): handling ps request
2023-04-20 10:57:35.552658 (Thread-143): 10:57:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d254c70>]}
2023-04-20 10:57:35.553361 (Thread-143): sending response (<Response 3842 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:35.668466 (Thread-144): handling ps request
2023-04-20 10:57:35.668953 (Thread-144): 10:57:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2546a0>]}
2023-04-20 10:57:35.669630 (Thread-144): sending response (<Response 3842 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:35.971226 (Thread-145): handling status request
2023-04-20 10:57:35.971745 (Thread-145): 10:57:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56997a970>]}
2023-04-20 10:57:35.972244 (Thread-145): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:36.114381 (Thread-1): 10:57:36  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 10:57:36.114616 (Thread-1): 10:57:36  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 10:57:36.114751 (Thread-1): 10:57:36  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3634.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3634.0 (TID 28819) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3634.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3634.0 (TID 28819) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
Caused by: org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

2023-04-20 10:57:36.114867 (Thread-1): 10:57:36  Databricks adapter: operation-id: b'\x01\xed\xdfj(S\x1b\xa4\xa2\x11V\xf2\x80\x8f\xd3\xaf'
2023-04-20 10:57:36.115064 (Thread-1): 10:57:36  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 10:57:34.930856 => 2023-04-20 10:57:36.114938
2023-04-20 10:57:36.115249 (Thread-1): 10:57:36  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 10:57:36.258886 (Thread-146): handling poll request
2023-04-20 10:57:36.259365 (Thread-146): 10:57:36  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2b9b50>]}
2023-04-20 10:57:36.260126 (Thread-146): sending response (<Response 31913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:36.389838 (Thread-1): Got an exception: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 10:57:36.391807 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 10:57:38.575790 (Thread-147): handling ps request
2023-04-20 10:57:38.576277 (Thread-147): 10:57:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2548e0>]}
2023-04-20 10:57:38.576958 (Thread-147): sending response (<Response 3865 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:38.602314 (Thread-148): handling poll request
2023-04-20 10:57:38.602702 (Thread-148): 10:57:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f933640>]}
2023-04-20 10:57:38.603434 (Thread-148): sending response (<Response 51007 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:38.620193 (Thread-149): handling poll request
2023-04-20 10:57:38.620564 (Thread-149): 10:57:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f986460>]}
2023-04-20 10:57:38.621230 (Thread-149): sending response (<Response 51007 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:39.033328 (Thread-150): handling status request
2023-04-20 10:57:39.033830 (Thread-150): 10:57:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f97b670>]}
2023-04-20 10:57:39.034314 (Thread-150): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:57:56.324528 (Thread-151): handling status request
2023-04-20 10:57:56.325007 (Thread-151): 10:57:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f97b760>]}
2023-04-20 10:57:56.325460 (Thread-151): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 10:58:08.090005 (Thread-152): handling status request
2023-04-20 10:58:08.090567 (Thread-152): 10:58:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56997a1c0>]}
2023-04-20 10:58:08.091045 (Thread-152): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:30.321365 (Thread-153): handling status request
2023-04-20 11:02:30.323431 (Thread-153): 11:02:30  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5622a7c10>]}
2023-04-20 11:02:30.323941 (Thread-153): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:42.496002 (Thread-154): handling ps request
2023-04-20 11:02:42.496546 (Thread-154): 11:02:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5622a7160>]}
2023-04-20 11:02:42.497530 (Thread-154): sending response (<Response 3865 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:42.498077 (Thread-155): handling run_sql request
2023-04-20 11:02:42.498847 (Thread-155): 11:02:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5622a73d0>]}
2023-04-20 11:02:45.327769 (Thread-155): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:45.367685 (MainThread): 11:02:45  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '22b05fdf-8aac-4115-aa37-8709f49ebf0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe9d5c8fbe0>]}
2023-04-20 11:02:45.368322 (MainThread): 11:02:45  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:02:45.369740 (Thread-1): 11:02:45  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:02:45.369996 (Thread-1): 11:02:45  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:02:45.374572 (Thread-1): 11:02:45  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:02:45.370047 => 2023-04-20 11:02:45.374392
2023-04-20 11:02:45.374796 (Thread-1): 11:02:45  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:02:45.375166 (Thread-1): 11:02:45  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:02:45.375665 (Thread-1): 11:02:45  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:02:45.375824 (Thread-1): 11:02:45  Opening a new connection, currently in state init
2023-04-20 11:02:45.946768 (Thread-156): handling ps request
2023-04-20 11:02:45.947502 (Thread-156): 11:02:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f986070>]}
2023-04-20 11:02:45.948629 (Thread-156): sending response (<Response 4373 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:45.982513 (Thread-157): handling ps request
2023-04-20 11:02:45.983009 (Thread-157): 11:02:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d20df70>]}
2023-04-20 11:02:46.009195 (Thread-157): sending response (<Response 4373 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:46.420671 (Thread-158): handling status request
2023-04-20 11:02:46.421170 (Thread-158): 11:02:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d20d850>]}
2023-04-20 11:02:46.421721 (Thread-158): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:46.615877 (Thread-159): handling poll request
2023-04-20 11:02:46.616373 (Thread-159): 11:02:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2077f0>]}
2023-04-20 11:02:46.616987 (Thread-159): sending response (<Response 5695 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:46.715271 (Thread-1): 11:02:46  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:02:46.715509 (Thread-1): 11:02:46  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:02:46.715681 (Thread-1): 11:02:46  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3671.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3671.0 (TID 29152) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3671.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3671.0 (TID 29152) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
Caused by: org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

2023-04-20 11:02:46.715799 (Thread-1): 11:02:46  Databricks adapter: operation-id: b'\x01\xed\xdfj\xe1l\x16J\x86\xd0t\\\xa1\xe6\xfa\x83'
2023-04-20 11:02:46.716000 (Thread-1): 11:02:46  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:02:45.374843 => 2023-04-20 11:02:46.715872
2023-04-20 11:02:46.716189 (Thread-1): 11:02:46  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:02:46.719540 (Thread-160): handling poll request
2023-04-20 11:02:46.719977 (Thread-160): 11:02:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d24d8e0>]}
2023-04-20 11:02:46.720702 (Thread-160): sending response (<Response 31920 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:47.002848 (Thread-1): Got an exception: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:02:47.004776 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 11:02:47.185684 (Thread-161): handling status request
2023-04-20 11:02:47.186200 (Thread-161): 11:02:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d21b880>]}
2023-04-20 11:02:47.186697 (Thread-161): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:48.271249 (Thread-162): handling poll request
2023-04-20 11:02:48.271789 (Thread-162): 11:02:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d24dc70>]}
2023-04-20 11:02:48.272602 (Thread-162): sending response (<Response 51038 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:48.561988 (Thread-163): handling ps request
2023-04-20 11:02:48.562508 (Thread-163): 11:02:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d20d6d0>]}
2023-04-20 11:02:48.563266 (Thread-163): sending response (<Response 4396 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:49.080529 (Thread-164): handling status request
2023-04-20 11:02:49.081027 (Thread-164): 11:02:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2549a0>]}
2023-04-20 11:02:49.081509 (Thread-164): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:49.342424 (Thread-165): handling poll request
2023-04-20 11:02:49.342919 (Thread-165): 11:02:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d254c70>]}
2023-04-20 11:02:49.343713 (Thread-165): sending response (<Response 51038 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:02:50.101257 (Thread-166): handling status request
2023-04-20 11:02:50.101764 (Thread-166): 11:02:50  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d26ae50>]}
2023-04-20 11:02:50.102265 (Thread-166): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:08.110803 (Thread-167): handling status request
2023-04-20 11:04:08.112737 (Thread-167): 11:04:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d2541f0>]}
2023-04-20 11:04:08.113217 (Thread-167): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:27.206917 (Thread-168): handling ps request
2023-04-20 11:04:27.207432 (Thread-168): 11:04:27  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d207730>]}
2023-04-20 11:04:27.208161 (Thread-168): sending response (<Response 4396 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:27.242644 (Thread-169): handling run_sql request
2023-04-20 11:04:27.242996 (Thread-169): 11:04:27  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d222ca0>]}
2023-04-20 11:04:29.960802 (Thread-169): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:29.999145 (MainThread): 11:04:29  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bfac63d5-cac9-46a9-882e-2fd273172687', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd226742dc0>]}
2023-04-20 11:04:29.999754 (MainThread): 11:04:29  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:04:30.001093 (Thread-1): 11:04:30  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:04:30.001328 (Thread-1): 11:04:30  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:04:30.005696 (Thread-1): 11:04:30  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:04:30.001383 => 2023-04-20 11:04:30.005522
2023-04-20 11:04:30.005927 (Thread-1): 11:04:30  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:04:30.006279 (Thread-1): 11:04:30  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:04:30.006766 (Thread-1): 11:04:30  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      substring(value, 1, 15) as PTS,
      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:04:30.006996 (Thread-1): 11:04:30  Opening a new connection, currently in state init
2023-04-20 11:04:30.525459 (Thread-170): handling ps request
2023-04-20 11:04:30.526131 (Thread-170): 11:04:30  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d237fd0>]}
2023-04-20 11:04:30.527293 (Thread-170): sending response (<Response 4904 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:30.528504 (Thread-171): handling ps request
2023-04-20 11:04:30.528912 (Thread-171): 11:04:30  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d23e220>]}
2023-04-20 11:04:30.529525 (Thread-171): sending response (<Response 4904 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:31.020802 (Thread-172): handling status request
2023-04-20 11:04:31.021284 (Thread-172): 11:04:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d222ca0>]}
2023-04-20 11:04:31.021797 (Thread-172): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:31.133763 (Thread-173): handling poll request
2023-04-20 11:04:31.134264 (Thread-173): 11:04:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d231ee0>]}
2023-04-20 11:04:31.134893 (Thread-173): sending response (<Response 5745 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:31.186553 (Thread-1): 11:04:31  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      substring(value, 1, 15) as PTS,
      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:04:31.186788 (Thread-1): 11:04:31  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:04:31.186918 (Thread-1): 11:04:31  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3682.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3682.0 (TID 29257) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3682.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3682.0 (TID 29257) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
Caused by: org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

2023-04-20 11:04:31.187032 (Thread-1): 11:04:31  Databricks adapter: operation-id: b"\x01\xed\xdfk\x1f\xbe\x13\xd7\x98\xd1\x11\xd8I\x7f'\x14"
2023-04-20 11:04:31.187230 (Thread-1): 11:04:31  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:04:30.005975 => 2023-04-20 11:04:31.187105
2023-04-20 11:04:31.187418 (Thread-1): 11:04:31  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:04:31.330612 (Thread-174): handling poll request
2023-04-20 11:04:31.331061 (Thread-174): 11:04:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d246e80>]}
2023-04-20 11:04:31.331832 (Thread-174): sending response (<Response 32024 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:31.470601 (Thread-1): Got an exception: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:04:31.472444 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n        ,ansi_mode = 'false'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      substring(value, 1, 15) as PTS,\n      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      substring(value, 1, 15) as PTS,\n      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n        ,ansi_mode = 'false'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      substring(value, 1, 15) as PTS,\n      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      substring(value, 1, 15) as PTS,\n      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 11:04:31.624495 (Thread-175): handling status request
2023-04-20 11:04:31.624994 (Thread-175): 11:04:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1cf880>]}
2023-04-20 11:04:31.625457 (Thread-175): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:32.922325 (Thread-176): handling poll request
2023-04-20 11:04:32.922807 (Thread-176): 11:04:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1cf100>]}
2023-04-20 11:04:32.923620 (Thread-176): sending response (<Response 51486 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:33.179183 (Thread-177): handling ps request
2023-04-20 11:04:33.179722 (Thread-177): 11:04:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8850>]}
2023-04-20 11:04:33.180462 (Thread-177): sending response (<Response 4927 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:33.611324 (Thread-178): handling status request
2023-04-20 11:04:33.611842 (Thread-178): 11:04:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8e50>]}
2023-04-20 11:04:33.612898 (Thread-178): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:33.782323 (Thread-179): handling poll request
2023-04-20 11:04:33.782816 (Thread-179): 11:04:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d283e20>]}
2023-04-20 11:04:33.783605 (Thread-179): sending response (<Response 51486 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:04:34.627737 (Thread-180): handling status request
2023-04-20 11:04:34.628235 (Thread-180): 11:04:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d237970>]}
2023-04-20 11:04:34.628744 (Thread-180): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:07.509688 (Thread-181): handling ps request
2023-04-20 11:05:07.510445 (Thread-181): 11:05:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d222d90>]}
2023-04-20 11:05:07.510960 (Thread-182): handling run_sql request
2023-04-20 11:05:07.511933 (Thread-181): sending response (<Response 4927 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:07.512419 (Thread-182): 11:05:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f986be0>]}
2023-04-20 11:05:09.043073 (Thread-183): handling ps request
2023-04-20 11:05:09.044123 (Thread-183): 11:05:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d237fd0>]}
2023-04-20 11:05:09.045424 (Thread-183): sending response (<Response 5440 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:09.630683 (Thread-184): handling poll request
2023-04-20 11:05:09.631160 (Thread-184): 11:05:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df700>]}
2023-04-20 11:05:09.631686 (Thread-184): sending response (<Response 422 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:10.114692 (Thread-185): handling status request
2023-04-20 11:05:10.115188 (Thread-185): 11:05:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df6a0>]}
2023-04-20 11:05:10.115735 (Thread-185): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:10.377888 (Thread-182): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:10.414789 (MainThread): 11:05:10  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ac2bdbbc-0707-4c45-b7db-317abd51750f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9bbea2cd0>]}
2023-04-20 11:05:10.415408 (MainThread): 11:05:10  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:05:10.416796 (Thread-1): 11:05:10  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:05:10.417027 (Thread-1): 11:05:10  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:05:10.421380 (Thread-1): 11:05:10  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:05:10.417076 => 2023-04-20 11:05:10.421207
2023-04-20 11:05:10.421601 (Thread-1): 11:05:10  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:05:10.421964 (Thread-1): 11:05:10  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:05:10.422414 (Thread-1): 11:05:10  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    --nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      substring(value, 1, 15) as PTS,
      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:05:10.422561 (Thread-1): 11:05:10  Opening a new connection, currently in state init
2023-04-20 11:05:10.926790 (Thread-186): handling ps request
2023-04-20 11:05:10.927278 (Thread-186): 11:05:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df820>]}
2023-04-20 11:05:10.928080 (Thread-186): sending response (<Response 5435 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:10.951301 (Thread-187): handling ps request
2023-04-20 11:05:10.951695 (Thread-187): 11:05:10  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df910>]}
2023-04-20 11:05:10.978625 (Thread-187): sending response (<Response 5435 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:11.379889 (Thread-188): handling status request
2023-04-20 11:05:11.380383 (Thread-188): 11:05:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df460>]}
2023-04-20 11:05:11.380859 (Thread-188): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:11.666026 (Thread-1): 11:05:11  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    --nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      substring(value, 1, 15) as PTS,
      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP') cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:05:11.666258 (Thread-1): 11:05:11  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:05:11.666386 (Thread-1): 11:05:11  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3687.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3687.0 (TID 29263) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3687.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3687.0 (TID 29263) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
Caused by: org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

2023-04-20 11:05:11.666500 (Thread-1): 11:05:11  Databricks adapter: operation-id: b'\x01\xed\xdfk7\xd8\x14s\xb6\xb2\xc2\xbb\xaa\x81L:'
2023-04-20 11:05:11.666695 (Thread-1): 11:05:11  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:05:10.421648 => 2023-04-20 11:05:11.666571
2023-04-20 11:05:11.666889 (Thread-1): 11:05:11  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:05:11.699925 (Thread-189): handling poll request
2023-04-20 11:05:11.700473 (Thread-189): 11:05:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f97b1c0>]}
2023-04-20 11:05:11.701208 (Thread-189): sending response (<Response 32018 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:11.972183 (Thread-1): Got an exception: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '        ' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:05:11.974064 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    --nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      substring(value, 1, 15) as PTS,\n      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    --nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      substring(value, 1, 15) as PTS,\n      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'        \' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    --nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      substring(value, 1, 15) as PTS,\n      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP') cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    --nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      substring(value, 1, 15) as PTS,\n      --try_to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP') cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 11:05:12.180961 (Thread-190): handling poll request
2023-04-20 11:05:12.181458 (Thread-190): 11:05:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1dfc40>]}
2023-04-20 11:05:12.182275 (Thread-190): sending response (<Response 46858 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:12.591497 (Thread-191): handling status request
2023-04-20 11:05:12.591997 (Thread-191): 11:05:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df9a0>]}
2023-04-20 11:05:12.592460 (Thread-191): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:13.278674 (Thread-192): handling poll request
2023-04-20 11:05:13.279155 (Thread-192): 11:05:13  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df7c0>]}
2023-04-20 11:05:13.279923 (Thread-192): sending response (<Response 51400 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:13.509574 (Thread-193): handling ps request
2023-04-20 11:05:13.510062 (Thread-193): 11:05:13  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1b8310>]}
2023-04-20 11:05:13.510795 (Thread-193): sending response (<Response 5458 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:13.982360 (Thread-194): handling status request
2023-04-20 11:05:13.982870 (Thread-194): 11:05:13  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1b8370>]}
2023-04-20 11:05:13.983345 (Thread-194): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:15.095248 (Thread-195): handling poll request
2023-04-20 11:05:15.095808 (Thread-195): 11:05:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1b8640>]}
2023-04-20 11:05:15.096589 (Thread-195): sending response (<Response 51400 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:05:15.877695 (Thread-196): handling status request
2023-04-20 11:05:15.878210 (Thread-196): 11:05:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1b8b80>]}
2023-04-20 11:05:15.878705 (Thread-196): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:09:25.195307 (Thread-197): handling status request
2023-04-20 11:09:25.197228 (Thread-197): 11:09:25  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df8e0>]}
2023-04-20 11:09:25.197699 (Thread-197): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:09:59.193495 (Thread-198): handling status request
2023-04-20 11:09:59.194031 (Thread-198): 11:09:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1c40a0>]}
2023-04-20 11:09:59.194522 (Thread-198): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:10:03.634887 (Thread-199): handling status request
2023-04-20 11:10:03.635378 (Thread-199): 11:10:03  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1c4250>]}
2023-04-20 11:10:03.635873 (Thread-199): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:10:07.023066 (Thread-200): handling status request
2023-04-20 11:10:07.023972 (Thread-200): 11:10:07  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d20d6d0>]}
2023-04-20 11:10:07.024442 (Thread-200): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:10:53.861934 (Thread-201): handling status request
2023-04-20 11:10:53.862435 (Thread-201): 11:10:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1dff10>]}
2023-04-20 11:10:53.862904 (Thread-201): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:11:34.198468 (Thread-202): handling status request
2023-04-20 11:11:34.198970 (Thread-202): 11:11:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1dfb50>]}
2023-04-20 11:11:34.199464 (Thread-202): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:12:58.624800 (Thread-203): handling status request
2023-04-20 11:12:58.626772 (Thread-203): 11:12:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df430>]}
2023-04-20 11:12:58.627250 (Thread-203): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:13:45.231807 (Thread-204): handling status request
2023-04-20 11:13:45.232315 (Thread-204): 11:13:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1dfc70>]}
2023-04-20 11:13:45.232784 (Thread-204): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:14:04.428647 (Thread-205): handling status request
2023-04-20 11:14:04.429145 (Thread-205): 11:14:04  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1df1c0>]}
2023-04-20 11:14:04.429639 (Thread-205): sending response (<Response 1589 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:14:26.654819 (Thread-206): 11:14:26  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:14:26.680446 (Thread-206): 11:14:26  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 11:14:26.680839 (Thread-206): 11:14:26  previous checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, current checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886
2023-04-20 11:14:26.681054 (Thread-206): 11:14:26  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564fdfcd0>]}
2023-04-20 11:14:27.070979 (Thread-207): handling status request
2023-04-20 11:14:27.071506 (Thread-207): 11:14:27  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9db8b0>]}
2023-04-20 11:14:27.071964 (Thread-207): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:14:28.386743 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 11:14:28.390445 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 11:14:28.394106 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 11:14:28.397199 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 11:14:28.400299 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 11:14:28.403148 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 11:14:28.406481 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 11:14:28.409649 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 11:14:28.412847 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 11:14:28.415792 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 11:14:28.419139 (Thread-206): 11:14:28  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 11:14:28.422148 (Thread-206): 11:14:28  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 11:14:28.424927 (Thread-206): 11:14:28  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 11:14:28.427840 (Thread-206): 11:14:28  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 11:14:28.431029 (Thread-206): 11:14:28  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:14:28.603725 (Thread-206): 11:14:28  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b8d30>]}
2023-04-20 11:14:29.840945 (Thread-208): handling status request
2023-04-20 11:14:29.841488 (Thread-208): 11:14:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b8be0>]}
2023-04-20 11:14:29.842131 (Thread-208): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:14:48.560204 (Thread-209): 11:14:48  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:14:48.873143 (Thread-209): 11:14:48  Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
2023-04-20 11:14:48.873726 (Thread-209): 11:14:48  Partial parsing: added file: dbsql_dbt_tpch://tests/not-null-founding-date.sql
2023-04-20 11:14:48.941612 (Thread-209): 11:14:48  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb555976940>]}
2023-04-20 11:14:49.000790 (Thread-210): handling status request
2023-04-20 11:14:49.001243 (Thread-210): 11:14:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559afbb0>]}
2023-04-20 11:14:49.001720 (Thread-210): sending response (<Response 1612 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:15:15.585116 (Thread-211): handling status request
2023-04-20 11:15:15.585614 (Thread-211): 11:15:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559afb20>]}
2023-04-20 11:15:15.586095 (Thread-211): sending response (<Response 1612 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:15:39.118389 (Thread-212): handling status request
2023-04-20 11:15:39.118924 (Thread-212): 11:15:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559af4f0>]}
2023-04-20 11:15:39.119419 (Thread-212): sending response (<Response 1612 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:15:55.326037 (Thread-213): 11:15:55  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:15:55.716882 (Thread-214): handling status request
2023-04-20 11:15:55.717429 (Thread-214): 11:15:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1c4bb0>]}
2023-04-20 11:15:55.717968 (Thread-214): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:15:55.932969 (Thread-213): 11:15:55  Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
2023-04-20 11:15:55.933464 (Thread-213): 11:15:55  Partial parsing: added file: dbsql_dbt_tpch://tests/null-founding-date.sql
2023-04-20 11:15:55.933648 (Thread-213): 11:15:55  Partial parsing: deleted file: dbsql_dbt_tpch://tests/not-null-founding-date.sql
2023-04-20 11:15:56.000710 (Thread-213): 11:15:56  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5649389a0>]}
2023-04-20 11:15:57.251007 (Thread-215): handling status request
2023-04-20 11:15:57.251544 (Thread-215): 11:15:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b13a0>]}
2023-04-20 11:15:57.252054 (Thread-215): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:16:34.074240 (Thread-216): handling status request
2023-04-20 11:16:34.074729 (Thread-216): 11:16:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1130>]}
2023-04-20 11:16:34.099355 (Thread-216): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:18:24.762112 (Thread-217): handling status request
2023-04-20 11:18:24.763986 (Thread-217): 11:18:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b17f0>]}
2023-04-20 11:18:24.764476 (Thread-217): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:19:25.094123 (Thread-218): handling status request
2023-04-20 11:19:25.094639 (Thread-218): 11:19:25  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1cd0>]}
2023-04-20 11:19:25.095129 (Thread-218): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:08.742042 (Thread-219): handling status request
2023-04-20 11:20:08.742542 (Thread-219): 11:20:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1730>]}
2023-04-20 11:20:08.743023 (Thread-219): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:31.462748 (Thread-220): handling status request
2023-04-20 11:20:31.463261 (Thread-220): 11:20:31  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1fa0>]}
2023-04-20 11:20:31.463794 (Thread-220): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:41.829746 (Thread-221): handling ps request
2023-04-20 11:20:41.830294 (Thread-221): 11:20:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1c40>]}
2023-04-20 11:20:41.831104 (Thread-221): sending response (<Response 5458 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:41.860655 (Thread-222): handling run_sql request
2023-04-20 11:20:41.861144 (Thread-222): 11:20:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1220>]}
2023-04-20 11:20:44.637900 (Thread-222): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:44.674551 (MainThread): 11:20:44  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5f9ebbff-f62f-413c-a4cb-dbdd0dd9fb0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd7a47e0220>]}
2023-04-20 11:20:44.675166 (MainThread): 11:20:44  Found 15 models, 2 tests, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:20:44.676568 (Thread-1): 11:20:44  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:20:44.676811 (Thread-1): 11:20:44  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:20:44.678800 (Thread-1): 11:20:44  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:20:44.676859 => 2023-04-20 11:20:44.678629
2023-04-20 11:20:44.679030 (Thread-1): 11:20:44  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:20:44.679402 (Thread-1): 11:20:44  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:20:44.679785 (Thread-1): 11:20:44  On rpc.dbsql_dbt_tpch.request: AND trim(substring(value, 99, 8)) <> ''
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:20:44.679943 (Thread-1): 11:20:44  Opening a new connection, currently in state init
2023-04-20 11:20:45.176631 (Thread-223): handling ps request
2023-04-20 11:20:45.177349 (Thread-223): 11:20:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b9b20>]}
2023-04-20 11:20:45.178698 (Thread-223): sending response (<Response 5966 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:45.206498 (Thread-224): handling ps request
2023-04-20 11:20:45.206878 (Thread-224): 11:20:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b98e0>]}
2023-04-20 11:20:45.207657 (Thread-224): sending response (<Response 5966 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:45.335716 (Thread-1): 11:20:45  Databricks adapter: Error while running:
AND trim(substring(value, 99, 8)) <> ''
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:20:45.335955 (Thread-1): 11:20:45  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'AND'.(line 1, pos 0)

== SQL ==
AND trim(substring(value, 99, 8)) <> ''
^^^
limit 500
/* limit added automatically by dbt cloud */

2023-04-20 11:20:45.336088 (Thread-1): 11:20:45  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'AND'.(line 1, pos 0)

== SQL ==
AND trim(substring(value, 99, 8)) <> ''
^^^
limit 500
/* limit added automatically by dbt cloud */

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'AND'.(line 1, pos 0)

== SQL ==
AND trim(substring(value, 99, 8)) <> ''
^^^
limit 500
/* limit added automatically by dbt cloud */

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 11:20:45.336205 (Thread-1): 11:20:45  Databricks adapter: operation-id: b'\x01\xed\xdfmd\xb5\x10-\x85\xe4ECx\xfb\x12\xab'
2023-04-20 11:20:45.336409 (Thread-1): 11:20:45  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:20:44.679079 => 2023-04-20 11:20:45.336276
2023-04-20 11:20:45.336611 (Thread-1): 11:20:45  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:20:45.625044 (Thread-1): Got an exception: Runtime Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'AND'.(line 1, pos 0)
  
  == SQL ==
  AND trim(substring(value, 99, 8)) <> ''
  ^^^
  limit 500
  /* limit added automatically by dbt cloud */
  
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'AND'.(line 1, pos 0)

== SQL ==
AND trim(substring(value, 99, 8)) <> ''
^^^
limit 500
/* limit added automatically by dbt cloud */


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'AND'.(line 1, pos 0)
  
  == SQL ==
  AND trim(substring(value, 99, 8)) <> ''
  ^^^
  limit 500
  /* limit added automatically by dbt cloud */
2023-04-20 11:20:45.626939 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': "Runtime Error in rpc request (from remote system.sql)\n  \n  [PARSE_SYNTAX_ERROR] Syntax error at or near 'AND'.(line 1, pos 0)\n  \n  == SQL ==\n  AND trim(substring(value, 99, 8)) <> ''\n  ^^^\n  limit 500\n  /* limit added automatically by dbt cloud */\n  ", 'raw_code': "AND trim(substring(value, 99, 8)) <> ''\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "AND trim(substring(value, 99, 8)) <> ''\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': "Runtime Error in rpc request (from remote system.sql)\n  \n  [PARSE_SYNTAX_ERROR] Syntax error at or near 'AND'.(line 1, pos 0)\n  \n  == SQL ==\n  AND trim(substring(value, 99, 8)) <> ''\n  ^^^\n  limit 500\n  /* limit added automatically by dbt cloud */\n  ", 'raw_code': "AND trim(substring(value, 99, 8)) <> ''\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "AND trim(substring(value, 99, 8)) <> ''\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 11:20:45.664605 (Thread-225): handling status request
2023-04-20 11:20:45.665180 (Thread-225): 11:20:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b9bb0>]}
2023-04-20 11:20:45.665756 (Thread-225): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:45.866693 (Thread-226): handling poll request
2023-04-20 11:20:45.867168 (Thread-226): 11:20:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b9a30>]}
2023-04-20 11:20:45.867857 (Thread-226): sending response (<Response 18150 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:45.870570 (Thread-227): handling poll request
2023-04-20 11:20:45.870904 (Thread-227): 11:20:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9ac460>]}
2023-04-20 11:20:45.871393 (Thread-227): sending response (<Response 18150 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:46.792842 (Thread-228): handling status request
2023-04-20 11:20:46.793339 (Thread-228): 11:20:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9acdc0>]}
2023-04-20 11:20:46.793835 (Thread-228): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:47.782674 (Thread-229): handling ps request
2023-04-20 11:20:47.783173 (Thread-229): 11:20:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5581fbb80>]}
2023-04-20 11:20:47.783982 (Thread-229): sending response (<Response 5989 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:20:48.203483 (Thread-230): handling status request
2023-04-20 11:20:48.204007 (Thread-230): 11:20:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559732b0>]}
2023-04-20 11:20:48.204489 (Thread-230): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:05.283370 (Thread-231): handling run_sql request
2023-04-20 11:21:05.283890 (Thread-231): 11:21:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5581fb790>]}
2023-04-20 11:21:05.339670 (Thread-232): handling ps request
2023-04-20 11:21:05.345418 (Thread-232): 11:21:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b9af0>]}
2023-04-20 11:21:05.347706 (Thread-232): sending response (<Response 6502 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:05.912466 (Thread-233): handling poll request
2023-04-20 11:21:05.912954 (Thread-233): 11:21:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1c70>]}
2023-04-20 11:21:05.913487 (Thread-233): sending response (<Response 422 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:06.334872 (Thread-234): handling status request
2023-04-20 11:21:06.335354 (Thread-234): 11:21:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1910>]}
2023-04-20 11:21:06.335934 (Thread-234): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:06.601055 (Thread-235): handling ps request
2023-04-20 11:21:06.601578 (Thread-235): 11:21:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b11c0>]}
2023-04-20 11:21:06.602449 (Thread-235): sending response (<Response 6502 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:08.112852 (Thread-231): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:08.149058 (MainThread): 11:21:08  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c3202b7c-9952-42de-8a8e-872d324aed9e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57bfa2a100>]}
2023-04-20 11:21:08.149660 (MainThread): 11:21:08  Found 15 models, 2 tests, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:21:08.151038 (Thread-1): 11:21:08  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:21:08.151277 (Thread-1): 11:21:08  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:21:08.155459 (Thread-1): 11:21:08  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:21:08.151326 => 2023-04-20 11:21:08.155288
2023-04-20 11:21:08.155786 (Thread-1): 11:21:08  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:21:08.156152 (Thread-1): 11:21:08  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:21:08.156600 (Thread-1): 11:21:08  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:21:08.156749 (Thread-1): 11:21:08  Opening a new connection, currently in state init
2023-04-20 11:21:08.450953 (Thread-236): handling poll request
2023-04-20 11:21:08.451450 (Thread-236): 11:21:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1be0>]}
2023-04-20 11:21:08.452076 (Thread-236): sending response (<Response 5758 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:08.636317 (Thread-237): handling ps request
2023-04-20 11:21:08.636799 (Thread-237): 11:21:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1d90>]}
2023-04-20 11:21:08.637585 (Thread-237): sending response (<Response 6497 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:08.674716 (Thread-238): handling ps request
2023-04-20 11:21:08.675169 (Thread-238): 11:21:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9c4130>]}
2023-04-20 11:21:08.675969 (Thread-238): sending response (<Response 6496 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:08.861644 (Thread-239): handling status request
2023-04-20 11:21:08.862146 (Thread-239): 11:21:08  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9c4cd0>]}
2023-04-20 11:21:08.862622 (Thread-239): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:09.220811 (Thread-240): handling poll request
2023-04-20 11:21:09.221311 (Thread-240): 11:21:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9c4280>]}
2023-04-20 11:21:09.221902 (Thread-240): sending response (<Response 5758 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:09.763613 (Thread-1): 11:21:09  SQL status: OK in 1.61 seconds
2023-04-20 11:21:09.794466 (Thread-1): 11:21:09  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:21:08.155841 => 2023-04-20 11:21:09.794248
2023-04-20 11:21:09.794712 (Thread-1): 11:21:09  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:21:11.491774 (Thread-241): handling ps request
2023-04-20 11:21:11.492529 (Thread-241): 11:21:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb57b95e850>]}
2023-04-20 11:21:11.493037 (Thread-242): handling poll request
2023-04-20 11:21:11.494015 (Thread-241): sending response (<Response 6522 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:11.494448 (Thread-242): 11:21:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1c47c0>]}
2023-04-20 11:21:11.500586 (Thread-242): sending response (<Response 175642 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:11.510484 (Thread-243): handling poll request
2023-04-20 11:21:11.510854 (Thread-243): 11:21:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52bf448b0>]}
2023-04-20 11:21:11.515723 (Thread-243): sending response (<Response 180985 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:11.966255 (Thread-244): handling status request
2023-04-20 11:21:11.966745 (Thread-244): 11:21:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56229f370>]}
2023-04-20 11:21:11.967232 (Thread-244): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:12.986842 (Thread-245): handling ps request
2023-04-20 11:21:12.987377 (Thread-245): 11:21:12  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1c4b20>]}
2023-04-20 11:21:12.988243 (Thread-245): sending response (<Response 6522 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:21:36.436492 (Thread-246): handling status request
2023-04-20 11:21:36.437000 (Thread-246): 11:21:36  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f9799d0>]}
2023-04-20 11:21:36.468398 (Thread-246): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:02.595441 (Thread-247): handling status request
2023-04-20 11:22:02.595968 (Thread-247): 11:22:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f979dc0>]}
2023-04-20 11:22:02.596451 (Thread-247): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:37.178183 (Thread-248): handling run_sql request
2023-04-20 11:22:37.178684 (Thread-248): 11:22:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f979a00>]}
2023-04-20 11:22:37.197067 (Thread-249): handling ps request
2023-04-20 11:22:37.203081 (Thread-249): 11:22:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8be0>]}
2023-04-20 11:22:37.205661 (Thread-249): sending response (<Response 7035 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:37.754623 (Thread-250): handling poll request
2023-04-20 11:22:37.755123 (Thread-250): 11:22:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8eb0>]}
2023-04-20 11:22:37.755709 (Thread-250): sending response (<Response 422 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:38.206397 (Thread-251): handling status request
2023-04-20 11:22:38.206890 (Thread-251): 11:22:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8c10>]}
2023-04-20 11:22:38.207432 (Thread-251): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:38.396396 (Thread-252): handling ps request
2023-04-20 11:22:38.396934 (Thread-252): 11:22:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8fa0>]}
2023-04-20 11:22:38.397791 (Thread-252): sending response (<Response 7035 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:39.968521 (Thread-248): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:40.008381 (MainThread): 11:22:40  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2339e5f5-7f32-49f2-ac12-b09ff95c0e77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c300a6160>]}
2023-04-20 11:22:40.008999 (MainThread): 11:22:40  Found 15 models, 2 tests, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:22:40.010375 (Thread-1): 11:22:40  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:22:40.010607 (Thread-1): 11:22:40  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:22:40.014882 (Thread-1): 11:22:40  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:22:40.010654 => 2023-04-20 11:22:40.014710
2023-04-20 11:22:40.015103 (Thread-1): 11:22:40  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:22:40.015467 (Thread-1): 11:22:40  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:22:40.015960 (Thread-1): 11:22:40  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(replace(trim(substring(value, 99, 8)),'','19010101'), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:22:40.016113 (Thread-1): 11:22:40  Opening a new connection, currently in state init
2023-04-20 11:22:40.419456 (Thread-253): handling poll request
2023-04-20 11:22:40.419977 (Thread-253): 11:22:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8700>]}
2023-04-20 11:22:40.420595 (Thread-253): sending response (<Response 5739 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:40.585618 (Thread-254): handling ps request
2023-04-20 11:22:40.586056 (Thread-254): 11:22:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8040>]}
2023-04-20 11:22:40.586901 (Thread-254): sending response (<Response 7030 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:40.642628 (Thread-255): handling ps request
2023-04-20 11:22:40.642982 (Thread-255): 11:22:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8250>]}
2023-04-20 11:22:40.643737 (Thread-255): sending response (<Response 7030 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:40.903182 (Thread-256): handling status request
2023-04-20 11:22:40.903673 (Thread-256): 11:22:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d165fd0>]}
2023-04-20 11:22:40.904148 (Thread-256): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:41.184069 (Thread-257): handling poll request
2023-04-20 11:22:41.184555 (Thread-257): 11:22:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1652b0>]}
2023-04-20 11:22:41.185133 (Thread-257): sending response (<Response 5739 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:41.232967 (Thread-1): 11:22:41  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(replace(trim(substring(value, 99, 8)),'','19010101'), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:22:41.233201 (Thread-1): 11:22:41  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:22:41.233333 (Thread-1): 11:22:41  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3768.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3768.0 (TID 30180) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3768.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3768.0 (TID 30180) (10.5.135.185 executor 7): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3330)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3262)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3253)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1423)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3542)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3468)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)
Caused by: org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:325)
	at org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)
	at com.databricks.photon.NativeToTimestampWrapper.evalOutput(NativeToTimestampWrapper.java:80)
	at com.databricks.photon.NativeToTimestampWrapper.eval(NativeToTimestampWrapper.java:106)
	at 0x899c692 <photon>.Eval(external/workspace_spark_3_3/photon/exprs/to-timestamp-expr.cc:133)
	at 0x5e23d3c <photon>.Eval(external/workspace_spark_3_3/photon/exprs/unary-expr.h:66)
	at 0x4bc90c1 <photon>.NextImpl(external/workspace_spark_3_3/photon/exec-nodes/project-node.cc:72)
	at 0x4abb61b <photon>.Next(external/workspace_spark_3_3/photon/exec-nodes/exec-node.cc:154)
	at 0x4af9182 <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:306)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.GetAndLookupNextProbeBatch(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:302)
	at 0x4bacc84 <photon>.DoTransition(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:100)
	at 0x4bacb68 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/join-probe-base.cc:50)
	at 0x4ba6110 <photon>.HasNext(external/workspace_spark_3_3/photon/exec-nodes/hash-join.cc:273)
	at 0x4b99774 <photon>.HasNextImpl(external/workspace_spark_3_3/photon/exec-nodes/hash-join-node.cc:207)
	at 0x4abb58f <photon>.OpenImpl(external/workspace_spark_3_3/photon/exec-nodes/shuffle-sink-node.cc:163)
	at com.databricks.photon.JniApiImpl.open(Native Method)
	at com.databricks.photon.JniApi.open(JniApi.scala)
	at com.databricks.photon.JniExecNode.open(JniExecNode.java:64)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$9(PhotonExec.scala:809)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.photon.PhotonExec.timeit(PhotonExec.scala:344)
	at com.databricks.photon.PhotonExec.timeit$(PhotonExec.scala:342)
	at com.databricks.photon.PhotonShuffleExchangeExec.timeit(PhotonExec.scala:637)
	at com.databricks.photon.PhotonShuffleMapStageExec.$anonfun$preShuffleRDDInternal$8(PhotonExec.scala:809)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6(PhotonExec.scala:578)
	at com.databricks.photon.PhotonExec.$anonfun$executePhoton$6$adapted(PhotonExec.scala:421)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:881)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:881)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:371)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

2023-04-20 11:22:41.233445 (Thread-1): 11:22:41  Databricks adapter: operation-id: b'\x01\xed\xdfm\xa9n\x1c\xbe\xa5\x16\xa3rb\\~;'
2023-04-20 11:22:41.233641 (Thread-1): 11:22:41  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:22:40.015150 => 2023-04-20 11:22:41.233515
2023-04-20 11:22:41.233829 (Thread-1): 11:22:41  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:22:41.516705 (Thread-1): Got an exception: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text '' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.
2023-04-20 11:22:41.518533 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'\' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(replace(trim(substring(value, 99, 8)),'','19010101'), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP'\n       ) cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(replace(trim(substring(value, 99, 8)),'','19010101'), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP'\n       ) cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': 'Runtime Error in rpc request (from remote system.sql)\n  Job aborted due to stage failure: [CANNOT_PARSE_TIMESTAMP] Text \'\' could not be parsed at index 0. If necessary set "ansi_mode" to "false" to bypass this error.', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(replace(trim(substring(value, 99, 8)),'','19010101'), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP'\n       ) cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(replace(trim(substring(value, 99, 8)),'','19010101'), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP'\n       ) cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 11:22:42.787254 (Thread-258): handling poll request
2023-04-20 11:22:42.787792 (Thread-258): 11:22:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d12c1f0>]}
2023-04-20 11:22:42.788604 (Thread-258): sending response (<Response 51244 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:43.001158 (Thread-259): handling poll request
2023-04-20 11:22:43.001654 (Thread-259): 11:22:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d12cd60>]}
2023-04-20 11:22:43.002433 (Thread-259): sending response (<Response 51244 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:43.223856 (Thread-260): handling ps request
2023-04-20 11:22:43.224345 (Thread-260): 11:22:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1fa0>]}
2023-04-20 11:22:43.225202 (Thread-260): sending response (<Response 7053 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:22:43.822691 (Thread-261): handling status request
2023-04-20 11:22:43.823197 (Thread-261): 11:22:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b1df0>]}
2023-04-20 11:22:43.823774 (Thread-261): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:18.192230 (Thread-262): handling run_sql request
2023-04-20 11:23:18.192721 (Thread-262): 11:23:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9b11f0>]}
2023-04-20 11:23:18.252648 (Thread-263): handling ps request
2023-04-20 11:23:18.253588 (Thread-263): 11:23:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559557f0>]}
2023-04-20 11:23:18.254998 (Thread-263): sending response (<Response 7565 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:18.811943 (Thread-264): handling poll request
2023-04-20 11:23:18.812429 (Thread-264): 11:23:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55a9c4cd0>]}
2023-04-20 11:23:18.812940 (Thread-264): sending response (<Response 422 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:19.270419 (Thread-265): handling status request
2023-04-20 11:23:19.270901 (Thread-265): 11:23:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d12cbb0>]}
2023-04-20 11:23:19.271419 (Thread-265): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:19.726495 (Thread-266): handling ps request
2023-04-20 11:23:19.726970 (Thread-266): 11:23:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564936550>]}
2023-04-20 11:23:19.727856 (Thread-266): sending response (<Response 7566 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:20.992050 (Thread-262): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:21.028412 (MainThread): 11:23:21  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0b0d509f-6990-4c3f-b325-c4d9604505ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4ed79c100>]}
2023-04-20 11:23:21.029006 (MainThread): 11:23:21  Found 15 models, 2 tests, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:23:21.030348 (Thread-1): 11:23:21  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:23:21.030579 (Thread-1): 11:23:21  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:23:21.034766 (Thread-1): 11:23:21  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:23:21.030628 => 2023-04-20 11:23:21.034599
2023-04-20 11:23:21.034983 (Thread-1): 11:23:21  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:23:21.035337 (Thread-1): 11:23:21  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:23:21.035820 (Thread-1): 11:23:21  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:23:21.035967 (Thread-1): 11:23:21  Opening a new connection, currently in state init
2023-04-20 11:23:21.398554 (Thread-267): handling poll request
2023-04-20 11:23:21.399092 (Thread-267): 11:23:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d148a90>]}
2023-04-20 11:23:21.399741 (Thread-267): sending response (<Response 5758 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:21.565111 (Thread-268): handling ps request
2023-04-20 11:23:21.565588 (Thread-268): 11:23:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d8eb0>]}
2023-04-20 11:23:21.566478 (Thread-268): sending response (<Response 7561 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:21.588458 (Thread-269): handling ps request
2023-04-20 11:23:21.588799 (Thread-269): 11:23:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56229f250>]}
2023-04-20 11:23:21.589534 (Thread-269): sending response (<Response 7561 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:21.836193 (Thread-270): handling status request
2023-04-20 11:23:21.836667 (Thread-270): 11:23:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d1d84c0>]}
2023-04-20 11:23:21.837152 (Thread-270): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:22.028127 (Thread-1): 11:23:22  SQL status: OK in 0.99 seconds
2023-04-20 11:23:22.059292 (Thread-1): 11:23:22  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:23:21.035030 => 2023-04-20 11:23:22.059068
2023-04-20 11:23:22.059575 (Thread-1): 11:23:22  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:23:22.176988 (Thread-271): handling poll request
2023-04-20 11:23:22.177489 (Thread-271): 11:23:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56229f310>]}
2023-04-20 11:23:22.178194 (Thread-271): sending response (<Response 6791 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:23.740016 (Thread-272): handling poll request
2023-04-20 11:23:23.740521 (Thread-272): 11:23:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d237100>]}
2023-04-20 11:23:23.746394 (Thread-272): sending response (<Response 174611 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:23.960357 (Thread-273): handling poll request
2023-04-20 11:23:23.960863 (Thread-273): 11:23:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d237430>]}
2023-04-20 11:23:23.966309 (Thread-273): sending response (<Response 180985 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:24.187677 (Thread-274): handling ps request
2023-04-20 11:23:24.188152 (Thread-274): 11:23:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55fa7bb50>]}
2023-04-20 11:23:24.189017 (Thread-274): sending response (<Response 7586 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:24.761465 (Thread-275): handling status request
2023-04-20 11:23:24.761981 (Thread-275): 11:23:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559ad610>]}
2023-04-20 11:23:24.762482 (Thread-275): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:24.997657 (Thread-276): handling ps request
2023-04-20 11:23:24.998171 (Thread-276): 11:23:24  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559ad5b0>]}
2023-04-20 11:23:25.021736 (Thread-276): sending response (<Response 7586 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:29.973443 (Thread-277): handling status request
2023-04-20 11:23:29.973962 (Thread-277): 11:23:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559ad4f0>]}
2023-04-20 11:23:29.974466 (Thread-277): sending response (<Response 1930 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:30.497860 (Thread-278): handling cli_args request
2023-04-20 11:23:30.498462 (Thread-278): 11:23:30  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56229f370>]}
2023-04-20 11:23:33.094063 (Thread-279): 11:23:33  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:23:33.277996 (Thread-278): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:33.337291 (MainThread): 11:23:33  checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
2023-04-20 11:23:33.358159 (MainThread): 11:23:33  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 11:23:33.358336 (MainThread): 11:23:33  previous checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, current checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd
2023-04-20 11:23:33.358489 (MainThread): 11:23:33  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4d0f5e970>]}
2023-04-20 11:23:33.477649 (Thread-279): 11:23:33  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
2023-04-20 11:23:33.478393 (Thread-279): 11:23:33  Partial parsing: updated file: dbsql_dbt_tpch://models/silver/DimCompany.sql
2023-04-20 11:23:33.486916 (Thread-279): 11:23:33  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:23:33.547455 (Thread-279): 11:23:33  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb555a82670>]}
2023-04-20 11:23:33.570421 (Thread-280): handling status request
2023-04-20 11:23:33.570792 (Thread-280): 11:23:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5622c7d30>]}
2023-04-20 11:23:33.571374 (Thread-280): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:34.249421 (Thread-281): handling ps request
2023-04-20 11:23:34.249924 (Thread-281): 11:23:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5622c7c40>]}
2023-04-20 11:23:34.251001 (Thread-281): sending response (<Response 7968 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:34.329296 (Thread-282): handling poll request
2023-04-20 11:23:34.329805 (Thread-282): 11:23:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569924070>]}
2023-04-20 11:23:34.330348 (Thread-282): sending response (<Response 1863 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:34.610591 (Thread-283): handling ps request
2023-04-20 11:23:34.611131 (Thread-283): 11:23:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569924b20>]}
2023-04-20 11:23:34.612073 (Thread-283): sending response (<Response 7968 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:34.968995 (MainThread): 11:23:34  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 11:23:34.981556 (MainThread): 11:23:34  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 11:23:34.984655 (MainThread): 11:23:34  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 11:23:34.987677 (MainThread): 11:23:34  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 11:23:34.990582 (MainThread): 11:23:34  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 11:23:34.993405 (MainThread): 11:23:34  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 11:23:34.997018 (MainThread): 11:23:34  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 11:23:35.000079 (MainThread): 11:23:35  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 11:23:35.003110 (MainThread): 11:23:35  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 11:23:35.006046 (MainThread): 11:23:35  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 11:23:35.009401 (MainThread): 11:23:35  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 11:23:35.012273 (MainThread): 11:23:35  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 11:23:35.015178 (MainThread): 11:23:35  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 11:23:35.018013 (MainThread): 11:23:35  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 11:23:35.020872 (MainThread): 11:23:35  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:23:35.189010 (MainThread): 11:23:35  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4d05c76d0>]}
2023-04-20 11:23:35.224633 (MainThread): 11:23:35  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4d100fe20>]}
2023-04-20 11:23:35.225011 (MainThread): 11:23:35  Found 15 models, 2 tests, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:23:35.225160 (MainThread): 11:23:35  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4d11c4a90>]}
2023-04-20 11:23:35.227090 (MainThread): 11:23:35  
2023-04-20 11:23:35.228380 (MainThread): 11:23:35  Acquiring new databricks connection 'master'
2023-04-20 11:23:35.230108 (ThreadPoolExecutor-0_0): 11:23:35  Acquiring new databricks connection 'list_schemas'
2023-04-20 11:23:35.240990 (ThreadPoolExecutor-0_0): 11:23:35  Using databricks connection "list_schemas"
2023-04-20 11:23:35.241311 (ThreadPoolExecutor-0_0): 11:23:35  On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_schemas"} */

    show databases
  
2023-04-20 11:23:35.241470 (ThreadPoolExecutor-0_0): 11:23:35  Opening a new connection, currently in state init
2023-04-20 11:23:35.790849 (Thread-284): handling status request
2023-04-20 11:23:35.791329 (Thread-284): 11:23:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb555966220>]}
2023-04-20 11:23:35.791851 (Thread-284): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:36.310193 (ThreadPoolExecutor-0_0): 11:23:36  SQL status: OK in 1.07 seconds
2023-04-20 11:23:36.434620 (ThreadPoolExecutor-0_0): 11:23:36  On list_schemas: Close
2023-04-20 11:23:36.723590 (ThreadPoolExecutor-1_0): 11:23:36  Acquiring new databricks connection 'list_None_dbt_shabbirkdb'
2023-04-20 11:23:36.734015 (ThreadPoolExecutor-1_0): 11:23:36  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:36.734218 (ThreadPoolExecutor-1_0): 11:23:36  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 11:23:36.734376 (ThreadPoolExecutor-1_0): 11:23:36  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show tables in `dbt_shabbirkdb`
  
2023-04-20 11:23:36.734523 (ThreadPoolExecutor-1_0): 11:23:36  Opening a new connection, currently in state closed
2023-04-20 11:23:37.404691 (Thread-285): handling poll request
2023-04-20 11:23:37.405181 (Thread-285): 11:23:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5622b4220>]}
2023-04-20 11:23:37.405963 (Thread-285): sending response (<Response 11071 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:37.452849 (ThreadPoolExecutor-1_0): 11:23:37  SQL status: OK in 0.72 seconds
2023-04-20 11:23:37.462330 (ThreadPoolExecutor-1_0): 11:23:37  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 11:23:37.462544 (ThreadPoolExecutor-1_0): 11:23:37  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show views in `dbt_shabbirkdb`
  
2023-04-20 11:23:37.873220 (Thread-286): handling ps request
2023-04-20 11:23:37.872536 (ThreadPoolExecutor-1_0): 11:23:37  SQL status: OK in 0.41 seconds
2023-04-20 11:23:37.873994 (Thread-286): 11:23:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5622b41c0>]}
2023-04-20 11:23:37.874966 (Thread-286): sending response (<Response 7968 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:37.875905 (ThreadPoolExecutor-1_0): 11:23:37  On list_None_dbt_shabbirkdb: ROLLBACK
2023-04-20 11:23:37.876113 (ThreadPoolExecutor-1_0): 11:23:37  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:37.876257 (ThreadPoolExecutor-1_0): 11:23:37  On list_None_dbt_shabbirkdb: Close
2023-04-20 11:23:38.160297 (MainThread): 11:23:38  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4d0117850>]}
2023-04-20 11:23:38.160728 (MainThread): 11:23:38  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:38.160891 (MainThread): 11:23:38  Spark adapter: NotImplemented: commit
2023-04-20 11:23:38.161422 (MainThread): 11:23:38  Concurrency: 4 threads (target='default')
2023-04-20 11:23:38.161572 (MainThread): 11:23:38  
2023-04-20 11:23:38.164642 (Thread-1): 11:23:38  Began running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:23:38.165005 (Thread-1): 11:23:38  1 of 15 START sql table model dbt_shabbirkdb.DimBroker ......................... [RUN]
2023-04-20 11:23:38.165487 (Thread-1): 11:23:38  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
2023-04-20 11:23:38.165668 (Thread-1): 11:23:38  Began compiling node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:23:38.169746 (Thread-2): 11:23:38  Began running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:23:38.170093 (Thread-2): 11:23:38  2 of 15 START sql table model dbt_shabbirkdb.DimCompany ........................ [RUN]
2023-04-20 11:23:38.170660 (Thread-2): 11:23:38  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCompany'
2023-04-20 11:23:38.170944 (Thread-2): 11:23:38  Began compiling node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:23:38.175054 (Thread-3): 11:23:38  Began running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:23:38.175343 (Thread-3): 11:23:38  3 of 15 START sql table model dbt_shabbirkdb.DimCustomerStg .................... [RUN]
2023-04-20 11:23:38.175987 (Thread-3): 11:23:38  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomerStg'
2023-04-20 11:23:38.176169 (Thread-3): 11:23:38  Began compiling node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:23:38.180044 (Thread-1): 11:23:38  Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:23:38.180797 (Thread-2): 11:23:38  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:23:38.181481 (Thread-3): 11:23:38  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:23:38.181825 (Thread-4): 11:23:38  Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:23:38.182131 (Thread-4): 11:23:38  4 of 15 START sql table model dbt_shabbirkdb.tempDailyMarketHistorical ......... [RUN]
2023-04-20 11:23:38.182628 (Thread-4): 11:23:38  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempDailyMarketHistorical'
2023-04-20 11:23:38.182802 (Thread-4): 11:23:38  Began compiling node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:23:38.186391 (Thread-4): 11:23:38  Writing injected SQL for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:23:38.202438 (Thread-4): 11:23:38  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (compile): 2023-04-20 11:23:38.182845 => 2023-04-20 11:23:38.202265
2023-04-20 11:23:38.202660 (Thread-4): 11:23:38  Began executing node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:23:38.207800 (Thread-1): 11:23:38  Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 2023-04-20 11:23:38.165716 => 2023-04-20 11:23:38.207643
2023-04-20 11:23:38.208003 (Thread-1): 11:23:38  Began executing node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:23:38.212956 (Thread-2): 11:23:38  Timing info for model.dbsql_dbt_tpch.DimCompany (compile): 2023-04-20 11:23:38.171021 => 2023-04-20 11:23:38.212795
2023-04-20 11:23:38.213170 (Thread-2): 11:23:38  Began executing node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:23:38.235071 (Thread-1): 11:23:38  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:38.235246 (Thread-1): 11:23:38  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:23:38.235403 (Thread-1): 11:23:38  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

      describe extended `dbt_shabbirkdb`.`dimbroker`
  
2023-04-20 11:23:38.235559 (Thread-1): 11:23:38  Opening a new connection, currently in state closed
2023-04-20 11:23:38.235974 (Thread-3): 11:23:38  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (compile): 2023-04-20 11:23:38.176213 => 2023-04-20 11:23:38.235811
2023-04-20 11:23:38.236181 (Thread-3): 11:23:38  Began executing node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:23:38.241288 (Thread-4): 11:23:38  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:38.241452 (Thread-4): 11:23:38  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:23:38.241607 (Thread-4): 11:23:38  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

      describe extended `dbt_shabbirkdb`.`tempdailymarkethistorical`
  
2023-04-20 11:23:38.241729 (Thread-4): 11:23:38  Opening a new connection, currently in state init
2023-04-20 11:23:38.247067 (Thread-3): 11:23:38  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:38.247263 (Thread-3): 11:23:38  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:23:38.247417 (Thread-3): 11:23:38  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

      describe extended `dbt_shabbirkdb`.`dimcustomerstg`
  
2023-04-20 11:23:38.247572 (Thread-3): 11:23:38  Opening a new connection, currently in state init
2023-04-20 11:23:38.292329 (Thread-2): 11:23:38  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:23:38.307472 (Thread-2): 11:23:38  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:38.307674 (Thread-2): 11:23:38  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:23:38.307889 (Thread-2): 11:23:38  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCompany`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
  
2023-04-20 11:23:38.308014 (Thread-2): 11:23:38  Opening a new connection, currently in state init
2023-04-20 11:23:38.676571 (Thread-287): handling status request
2023-04-20 11:23:38.677097 (Thread-287): 11:23:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5650bca30>]}
2023-04-20 11:23:38.677611 (Thread-287): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:39.032437 (Thread-288): handling poll request
2023-04-20 11:23:39.032920 (Thread-288): 11:23:39  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb565079190>]}
2023-04-20 11:23:39.034033 (Thread-288): sending response (<Response 26162 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:39.050761 (Thread-1): 11:23:39  SQL status: OK in 0.82 seconds
2023-04-20 11:23:39.051219 (Thread-3): 11:23:39  SQL status: OK in 0.8 seconds
2023-04-20 11:23:39.054857 (Thread-4): 11:23:39  SQL status: OK in 0.81 seconds
2023-04-20 11:23:39.060767 (Thread-1): 11:23:39  Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:23:39.065131 (Thread-3): 11:23:39  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:23:39.069200 (Thread-4): 11:23:39  Writing runtime sql for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:23:39.076048 (Thread-1): 11:23:39  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:23:39.076260 (Thread-1): 11:23:39  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT
  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate`) effectivedate,
  date('9999-12-31') enddate
FROM  `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`HR`
WHERE employeejobcode = 314
  
2023-04-20 11:23:39.083720 (Thread-4): 11:23:39  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:23:39.083930 (Thread-4): 11:23:39  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempDailyMarketHistorical`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  dmh.*,
  sk_dateid,
  min(dm_low) OVER (
    PARTITION BY dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweeklow,
  max(dm_high) OVER (
    PARTITION by dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweekhigh
FROM (
  SELECT * FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketHistorical`
  UNION ALL
  SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketIncremental`) dmh
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
  ON d.datevalue = dm_date;
  
2023-04-20 11:23:39.091761 (Thread-3): 11:23:39  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:23:39.092088 (Thread-3): 11:23:39  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomerStg`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT * FROM (
  SELECT
    customerid,
    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) taxid,
    status,
    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) lastname,
    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) firstname,
    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) middleinitial,
    coalesce(gender, last_value(gender) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) gender,
    coalesce(tier, last_value(tier) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) tier,
    coalesce(dob, last_value(dob) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) dob,
    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline1,
    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline2,
    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) postalcode,
    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) CITY,
    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) stateprov,
    coalesce(country, last_value(country) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) country,
    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone1,
    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone2,
    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone3,
    coalesce(email1, last_value(email1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email1,
    coalesce(email2, last_value(email2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email2,
    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) LCL_TX_ID,
    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) NAT_TX_ID,
    batchid,
    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,
    date(update_ts) effectivedate,
    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate
  FROM (
    SELECT
      customerid,
      taxid,
      status,
      lastname,
      firstname,
      middleinitial,
      gender,
      tier,
      dob,
      addressline1,
      addressline2,
      postalcode,
      city,
      stateprov,
      country,
      phone1,
      phone2,
      phone3,
      email1,
      email2,
      lcl_tx_id,
      nat_tx_id,
      1 batchid,
      update_ts
    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')
    UNION ALL
    SELECT
      c.customerid,
      nullif(c.taxid, '') taxid,
      nullif(s.st_name, '') as status,
      nullif(c.lastname, '') lastname,
      nullif(c.firstname, '') firstname,
      nullif(c.middleinitial, '') middleinitial,
      gender,
      c.tier,
      c.dob,
      nullif(c.addressline1, '') addressline1,
      nullif(c.addressline2, '') addressline2,
      nullif(c.postalcode, '') postalcode,
      nullif(c.city, '') city,
      nullif(c.stateprov, '') stateprov,
      nullif(c.country, '') country,
      CASE
        WHEN isnull(c_local_1) then c_local_1
        ELSE concat(
          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),
          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),
          c_local_1,
          nvl(c_ext_1, '')) END as phone1,
      CASE
        WHEN isnull(c_local_2) then c_local_2
        ELSE concat(
          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),
          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),
          c_local_2,
          nvl(c_ext_2, '')) END as phone2,
      CASE
        WHEN isnull(c_local_3) then c_local_3
        ELSE concat(
          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),
          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),
          c_local_3,
          nvl(c_ext_3, '')) END as phone3,
      nullif(c.email1, '') email1,
      nullif(c.email2, '') email2,
      c.LCL_TX_ID, 
      c.NAT_TX_ID,
      c.batchid,
      timestamp(bd.batchdate) update_ts
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CustomerIncremental` c
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
      ON c.batchid = bd.batchid
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` s 
      ON c.status = s.st_id
  ) c
  )
  
2023-04-20 11:23:40.590840 (Thread-289): handling poll request
2023-04-20 11:23:40.591381 (Thread-289): 11:23:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5650797f0>]}
2023-04-20 11:23:40.592055 (Thread-289): sending response (<Response 12315 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:40.814313 (Thread-290): handling ps request
2023-04-20 11:23:40.814793 (Thread-290): 11:23:40  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500dc40>]}
2023-04-20 11:23:40.815708 (Thread-290): sending response (<Response 7969 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:41.239492 (Thread-291): handling status request
2023-04-20 11:23:41.239999 (Thread-291): 11:23:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569969340>]}
2023-04-20 11:23:41.240476 (Thread-291): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:42.090494 (Thread-2): 11:23:42  SQL status: OK in 3.78 seconds
2023-04-20 11:23:42.109983 (Thread-292): 11:23:42  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:23:42.115043 (Thread-2): 11:23:42  Timing info for model.dbsql_dbt_tpch.DimCompany (execute): 2023-04-20 11:23:38.213227 => 2023-04-20 11:23:42.114886
2023-04-20 11:23:42.115275 (Thread-2): 11:23:42  On model.dbsql_dbt_tpch.DimCompany: ROLLBACK
2023-04-20 11:23:42.115435 (Thread-2): 11:23:42  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:42.115600 (Thread-2): 11:23:42  On model.dbsql_dbt_tpch.DimCompany: Close
2023-04-20 11:23:42.130286 (Thread-292): 11:23:42  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 11:23:42.130648 (Thread-292): 11:23:42  previous checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, current checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886
2023-04-20 11:23:42.130848 (Thread-292): 11:23:42  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55817d3d0>]}
2023-04-20 11:23:42.139748 (Thread-293): handling poll request
2023-04-20 11:23:42.140182 (Thread-293): 11:23:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb558160310>]}
2023-04-20 11:23:42.140696 (Thread-293): sending response (<Response 2140 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:42.392126 (Thread-2): 11:23:42  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4c02ae640>]}
2023-04-20 11:23:42.392670 (Thread-2): 11:23:42  2 of 15 OK created sql table model dbt_shabbirkdb.DimCompany ................... [OK in 4.22s]
2023-04-20 11:23:42.393874 (Thread-2): 11:23:42  Finished running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:23:42.394737 (Thread-2): 11:23:42  Began running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:23:42.395056 (Thread-2): 11:23:42  5 of 15 START sql table model dbt_shabbirkdb.DimSecurity ....................... [RUN]
2023-04-20 11:23:42.395625 (Thread-2): 11:23:42  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimSecurity'
2023-04-20 11:23:42.395822 (Thread-2): 11:23:42  Began compiling node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:23:42.400551 (Thread-2): 11:23:42  Writing injected SQL for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:23:42.411545 (Thread-2): 11:23:42  Timing info for model.dbsql_dbt_tpch.DimSecurity (compile): 2023-04-20 11:23:42.395868 => 2023-04-20 11:23:42.411358
2023-04-20 11:23:42.411789 (Thread-2): 11:23:42  Began executing node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:23:42.415989 (Thread-2): 11:23:42  Writing runtime sql for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:23:42.425600 (Thread-2): 11:23:42  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:42.425781 (Thread-2): 11:23:42  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:23:42.426018 (Thread-2): 11:23:42  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 11:23:42.426151 (Thread-2): 11:23:42  Opening a new connection, currently in state closed
2023-04-20 11:23:42.571834 (Thread-1): 11:23:42  SQL status: OK in 3.5 seconds
2023-04-20 11:23:42.577425 (Thread-1): 11:23:42  Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 2023-04-20 11:23:38.208051 => 2023-04-20 11:23:42.577278
2023-04-20 11:23:42.577631 (Thread-1): 11:23:42  On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
2023-04-20 11:23:42.577770 (Thread-1): 11:23:42  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:42.589404 (Thread-294): handling status request
2023-04-20 11:23:42.577904 (Thread-1): 11:23:42  On model.dbsql_dbt_tpch.DimBroker: Close
2023-04-20 11:23:42.600292 (Thread-294): 11:23:42  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb555a503a0>]}
2023-04-20 11:23:42.626261 (Thread-294): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:42.857653 (Thread-1): 11:23:42  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4c012e8e0>]}
2023-04-20 11:23:42.858195 (Thread-1): 11:23:42  1 of 15 OK created sql table model dbt_shabbirkdb.DimBroker .................... [OK in 4.69s]
2023-04-20 11:23:42.858434 (Thread-1): 11:23:42  Finished running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:23:42.858717 (Thread-1): 11:23:42  Began running node model.dbsql_dbt_tpch.Financial
2023-04-20 11:23:42.859021 (Thread-1): 11:23:42  6 of 15 START sql table model dbt_shabbirkdb.Financial ......................... [RUN]
2023-04-20 11:23:42.859653 (Thread-1): 11:23:42  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Financial'
2023-04-20 11:23:42.859842 (Thread-1): 11:23:42  Began compiling node model.dbsql_dbt_tpch.Financial
2023-04-20 11:23:42.864765 (Thread-1): 11:23:42  Writing injected SQL for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:23:42.875820 (Thread-1): 11:23:42  Timing info for model.dbsql_dbt_tpch.Financial (compile): 2023-04-20 11:23:42.859888 => 2023-04-20 11:23:42.875628
2023-04-20 11:23:42.876095 (Thread-1): 11:23:42  Began executing node model.dbsql_dbt_tpch.Financial
2023-04-20 11:23:42.881690 (Thread-1): 11:23:42  Writing runtime sql for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:23:42.891879 (Thread-1): 11:23:42  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:42.892067 (Thread-1): 11:23:42  Using databricks connection "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:23:42.892292 (Thread-1): 11:23:42  On model.dbsql_dbt_tpch.Financial: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 11:23:42.892431 (Thread-1): 11:23:42  Opening a new connection, currently in state closed
2023-04-20 11:23:43.153721 (Thread-3): 11:23:43  SQL status: OK in 4.06 seconds
2023-04-20 11:23:43.156532 (Thread-3): 11:23:43  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (execute): 2023-04-20 11:23:38.236228 => 2023-04-20 11:23:43.156377
2023-04-20 11:23:43.156746 (Thread-3): 11:23:43  On model.dbsql_dbt_tpch.DimCustomerStg: ROLLBACK
2023-04-20 11:23:43.156886 (Thread-3): 11:23:43  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:43.157008 (Thread-3): 11:23:43  On model.dbsql_dbt_tpch.DimCustomerStg: Close
2023-04-20 11:23:43.201311 (Thread-2): 11:23:43  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 11:23:43.201522 (Thread-2): 11:23:43  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 78 pos 6
2023-04-20 11:23:43.201648 (Thread-2): 11:23:43  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 78 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 78 pos 6
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:23:43.201761 (Thread-2): 11:23:43  Databricks adapter: operation-id: b'\x01\xed\xdfm\xce\xa0\x11\x83\xa4\xd1\xf3\xfdB{\xdew'
2023-04-20 11:23:43.202008 (Thread-2): 11:23:43  Timing info for model.dbsql_dbt_tpch.DimSecurity (execute): 2023-04-20 11:23:42.411841 => 2023-04-20 11:23:43.201870
2023-04-20 11:23:43.202204 (Thread-2): 11:23:43  On model.dbsql_dbt_tpch.DimSecurity: ROLLBACK
2023-04-20 11:23:43.202334 (Thread-2): 11:23:43  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:43.202452 (Thread-2): 11:23:43  On model.dbsql_dbt_tpch.DimSecurity: Close
2023-04-20 11:23:43.434771 (Thread-3): 11:23:43  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4c02adb20>]}
2023-04-20 11:23:43.435290 (Thread-3): 11:23:43  3 of 15 OK created sql table model dbt_shabbirkdb.DimCustomerStg ............... [OK in 5.26s]
2023-04-20 11:23:43.435549 (Thread-3): 11:23:43  Finished running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:23:43.436395 (Thread-3): 11:23:43  Began running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:23:43.436694 (Thread-3): 11:23:43  7 of 15 START sql table model dbt_shabbirkdb.DimAccount ........................ [RUN]
2023-04-20 11:23:43.437170 (Thread-3): 11:23:43  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimAccount'
2023-04-20 11:23:43.437348 (Thread-3): 11:23:43  Began compiling node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:23:43.442563 (Thread-3): 11:23:43  Writing injected SQL for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:23:43.456971 (Thread-3): 11:23:43  Timing info for model.dbsql_dbt_tpch.DimAccount (compile): 2023-04-20 11:23:43.437394 => 2023-04-20 11:23:43.456810
2023-04-20 11:23:43.457189 (Thread-3): 11:23:43  Began executing node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:23:43.461196 (Thread-3): 11:23:43  Writing runtime sql for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:23:43.475660 (Thread-3): 11:23:43  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:43.475843 (Thread-3): 11:23:43  Using databricks connection "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:23:43.525739 (Thread-295): handling ps request
2023-04-20 11:23:43.476070 (Thread-3): 11:23:43  On model.dbsql_dbt_tpch.DimAccount: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 11:23:43.541558 (Thread-295): 11:23:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb565040e80>]}
2023-04-20 11:23:43.476202 (Thread-3): 11:23:43  Opening a new connection, currently in state closed
2023-04-20 11:23:43.567651 (Thread-295): sending response (<Response 7969 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:43.490743 (Thread-2): 11:23:43  Runtime Error in model DimSecurity (models/silver/DimSecurity.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 78 pos 6
2023-04-20 11:23:43.491215 (Thread-2): 11:23:43  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4c02ae130>]}
2023-04-20 11:23:43.491682 (Thread-2): 11:23:43  5 of 15 ERROR creating sql table model dbt_shabbirkdb.DimSecurity .............. [ERROR in 1.10s]
2023-04-20 11:23:43.491891 (Thread-2): 11:23:43  Finished running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:23:43.492166 (Thread-2): 11:23:43  Began running node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:23:43.492434 (Thread-2): 11:23:43  8 of 15 START sql table model dbt_shabbirkdb.Prospect .......................... [RUN]
2023-04-20 11:23:43.492896 (Thread-2): 11:23:43  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Prospect'
2023-04-20 11:23:43.493096 (Thread-2): 11:23:43  Began compiling node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:23:43.499073 (Thread-2): 11:23:43  Writing injected SQL for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:23:43.512769 (Thread-2): 11:23:43  Timing info for model.dbsql_dbt_tpch.Prospect (compile): 2023-04-20 11:23:43.493142 => 2023-04-20 11:23:43.512603
2023-04-20 11:23:43.513000 (Thread-2): 11:23:43  Began executing node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:23:43.518703 (Thread-2): 11:23:43  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:43.518903 (Thread-2): 11:23:43  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:23:43.519067 (Thread-2): 11:23:43  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

      describe extended `dbt_shabbirkdb`.`prospect`
  
2023-04-20 11:23:43.519196 (Thread-2): 11:23:43  Opening a new connection, currently in state closed
2023-04-20 11:23:43.622087 (Thread-1): 11:23:43  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 11:23:43.622300 (Thread-1): 11:23:43  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 61 pos 4
2023-04-20 11:23:43.730660 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 11:23:43.622423 (Thread-1): 11:23:43  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 61 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 61 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:23:43.734058 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 11:23:43.622534 (Thread-1): 11:23:43  Databricks adapter: operation-id: b'\x01\xed\xdfm\xce\xe5\x19\xc0\x84\xf9\x91\x066\x1c#\xe4'
2023-04-20 11:23:43.737579 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 11:23:43.622767 (Thread-1): 11:23:43  Timing info for model.dbsql_dbt_tpch.Financial (execute): 2023-04-20 11:23:42.876158 => 2023-04-20 11:23:43.622641
2023-04-20 11:23:43.740668 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 11:23:43.622945 (Thread-1): 11:23:43  On model.dbsql_dbt_tpch.Financial: ROLLBACK
2023-04-20 11:23:43.743901 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 11:23:43.623070 (Thread-1): 11:23:43  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:43.746868 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 11:23:43.623189 (Thread-1): 11:23:43  On model.dbsql_dbt_tpch.Financial: Close
2023-04-20 11:23:43.750241 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 11:23:43.753453 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 11:23:43.756459 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 11:23:43.759255 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 11:23:43.762514 (Thread-292): 11:23:43  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 11:23:43.765498 (Thread-292): 11:23:43  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 11:23:43.768175 (Thread-292): 11:23:43  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 11:23:43.770933 (Thread-292): 11:23:43  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 11:23:43.773784 (Thread-292): 11:23:43  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:23:43.796072 (Thread-296): handling status request
2023-04-20 11:23:43.801200 (Thread-296): 11:23:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564982520>]}
2023-04-20 11:23:43.801641 (Thread-296): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:43.899442 (Thread-1): 11:23:43  Runtime Error in model Financial (models/silver/Financial.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 61 pos 4
2023-04-20 11:23:43.899870 (Thread-1): 11:23:43  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4c0229940>]}
2023-04-20 11:23:43.900299 (Thread-1): 11:23:43  6 of 15 ERROR creating sql table model dbt_shabbirkdb.Financial ................ [ERROR in 1.04s]
2023-04-20 11:23:43.900509 (Thread-1): 11:23:43  Finished running node model.dbsql_dbt_tpch.Financial
2023-04-20 11:23:43.901374 (Thread-1): 11:23:43  Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 11:23:43.901631 (Thread-1): 11:23:43  9 of 15 SKIP relation dbt_shabbirkdb.tempSumpFiBasicEps ........................ [SKIP]
2023-04-20 11:23:43.901816 (Thread-1): 11:23:43  Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 11:23:43.928310 (Thread-292): 11:23:43  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb558281640>]}
2023-04-20 11:23:44.253850 (Thread-2): 11:23:44  SQL status: OK in 0.73 seconds
2023-04-20 11:23:44.260205 (Thread-2): 11:23:44  Writing runtime sql for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:23:44.267271 (Thread-3): 11:23:44  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 11:23:44.267448 (Thread-3): 11:23:44  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:23:44.267606 (Thread-3): 11:23:44  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:23:44.267727 (Thread-3): 11:23:44  Databricks adapter: operation-id: b'\x01\xed\xdfm\xcf?\x18"\xbc\x90\xd6GuF\x83\xa0'
2023-04-20 11:23:44.267952 (Thread-3): 11:23:44  Timing info for model.dbsql_dbt_tpch.DimAccount (execute): 2023-04-20 11:23:43.457238 => 2023-04-20 11:23:44.267826
2023-04-20 11:23:44.268124 (Thread-3): 11:23:44  On model.dbsql_dbt_tpch.DimAccount: ROLLBACK
2023-04-20 11:23:44.268246 (Thread-3): 11:23:44  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:44.268366 (Thread-3): 11:23:44  On model.dbsql_dbt_tpch.DimAccount: Close
2023-04-20 11:23:44.275102 (Thread-2): 11:23:44  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:23:44.275407 (Thread-2): 11:23:44  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Prospect`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  agencyid,
  recdate.sk_dateid sk_recorddateid,
  origdate.sk_dateid sk_updatedateid,
  p.batchid,
  nvl2(c.customerid, True, False) iscustomer, 
  p.lastname,
  p.firstname,
  p.middleinitial,
  p.gender,
  p.addressline1,
  p.addressline2,
  p.postalcode,
  city,
  state,
  country,
  phone,
  income,
  numbercars,
  numberchildren,
  maritalstatus,
  age,
  creditrating,
  ownorrentflag,
  employer,
  numbercreditcards,
  networth,
  if(
    isnotnull(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+","")),
    left(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+",""),
      length(
        if(networth > 1000000 or income > 200000,"HighValue+","") || 
        if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
        if(age > 45, "Boomer+", "") ||
        if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
        if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
        if(age < 25 and networth > 1000000, "Inherited+",""))
      -1),
    NULL) marketingnameplate
FROM (
  SELECT 
    * FROM (
    SELECT
      agencyid,
      max(batchid) recordbatchid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth,
      min(batchid) batchid
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`ProspectRaw` p
    GROUP BY
      agencyid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth)
  QUALIFY ROW_NUMBER() OVER (PARTITION BY agencyid ORDER BY batchid DESC) = 1) p
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) recdate
  ON p.recordbatchid = recdate.batchid
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) origdate
  ON p.batchid = origdate.batchid
LEFT JOIN (
  SELECT 
    customerid,
    lastname,
    firstname,
    addressline1,
    addressline2,
    postalcode
  FROM `dbt_shabbirkdb`.`DimCustomerStg`
  WHERE iscurrent) c
  ON 
    upper(p.LastName) = upper(c.lastname)
    and upper(p.FirstName) = upper(c.firstname)
    and upper(p.AddressLine1) = upper(c.addressline1)
    and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
    and upper(p.PostalCode) = upper(c.postalcode)
  
2023-04-20 11:23:44.293407 (Thread-297): handling poll request
2023-04-20 11:23:44.293824 (Thread-297): 11:23:44  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5649b6190>]}
2023-04-20 11:23:44.295551 (Thread-297): sending response (<Response 74900 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:44.557602 (Thread-3): 11:23:44  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:23:44.558187 (Thread-3): 11:23:44  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4c0214ca0>]}
2023-04-20 11:23:44.558821 (Thread-3): 11:23:44  7 of 15 ERROR creating sql table model dbt_shabbirkdb.DimAccount ............... [ERROR in 1.12s]
2023-04-20 11:23:44.559054 (Thread-3): 11:23:44  Finished running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:23:44.559859 (Thread-1): 11:23:44  Began running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 11:23:44.560119 (Thread-1): 11:23:44  10 of 15 SKIP relation dbt_shabbirkdb.DimTrade ................................. [SKIP]
2023-04-20 11:23:44.560309 (Thread-1): 11:23:44  Finished running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 11:23:44.560555 (Thread-1): 11:23:44  Began running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 11:23:44.560749 (Thread-1): 11:23:44  11 of 15 SKIP relation dbt_shabbirkdb.FactCashBalances ......................... [SKIP]
2023-04-20 11:23:44.560918 (Thread-1): 11:23:44  Finished running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 11:23:44.561658 (Thread-1): 11:23:44  Began running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 11:23:44.561874 (Thread-1): 11:23:44  12 of 15 SKIP relation dbt_shabbirkdb.FactHoldings ............................. [SKIP]
2023-04-20 11:23:44.562061 (Thread-1): 11:23:44  Finished running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 11:23:44.851966 (Thread-298): handling status request
2023-04-20 11:23:44.852447 (Thread-298): 11:23:44  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5649b6070>]}
2023-04-20 11:23:44.853062 (Thread-298): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:45.567111 (Thread-4): 11:23:45  SQL status: OK in 6.48 seconds
2023-04-20 11:23:45.852976 (Thread-299): handling poll request
2023-04-20 11:23:45.853576 (Thread-299): 11:23:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5649b62e0>]}
2023-04-20 11:23:45.854310 (Thread-299): sending response (<Response 7461 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:45.893870 (Thread-4): 11:23:45  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (execute): 2023-04-20 11:23:38.202710 => 2023-04-20 11:23:45.893687
2023-04-20 11:23:45.894141 (Thread-4): 11:23:45  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: ROLLBACK
2023-04-20 11:23:45.894288 (Thread-4): 11:23:45  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:45.894412 (Thread-4): 11:23:45  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: Close
2023-04-20 11:23:46.180582 (Thread-4): 11:23:46  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4a2e95190>]}
2023-04-20 11:23:46.181119 (Thread-4): 11:23:46  4 of 15 OK created sql table model dbt_shabbirkdb.tempDailyMarketHistorical .... [OK in 8.00s]
2023-04-20 11:23:46.181365 (Thread-4): 11:23:46  Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:23:46.182258 (Thread-1): 11:23:46  Began running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 11:23:46.182563 (Thread-1): 11:23:46  13 of 15 SKIP relation dbt_shabbirkdb.FactMarketHistory ........................ [SKIP]
2023-04-20 11:23:46.182756 (Thread-1): 11:23:46  Finished running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 11:23:46.383148 (Thread-300): handling ps request
2023-04-20 11:23:46.383647 (Thread-300): 11:23:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5649b6df0>]}
2023-04-20 11:23:46.384557 (Thread-300): sending response (<Response 7968 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:46.843699 (Thread-301): handling status request
2023-04-20 11:23:46.844196 (Thread-301): 11:23:46  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5649b6af0>]}
2023-04-20 11:23:46.844808 (Thread-301): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:47.500030 (Thread-302): handling poll request
2023-04-20 11:23:47.500525 (Thread-302): 11:23:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5649b6040>]}
2023-04-20 11:23:47.501129 (Thread-302): sending response (<Response 4955 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:48.303302 (Thread-2): 11:23:48  SQL status: OK in 4.03 seconds
2023-04-20 11:23:48.305786 (Thread-2): 11:23:48  Timing info for model.dbsql_dbt_tpch.Prospect (execute): 2023-04-20 11:23:43.513054 => 2023-04-20 11:23:48.305616
2023-04-20 11:23:48.306006 (Thread-2): 11:23:48  On model.dbsql_dbt_tpch.Prospect: ROLLBACK
2023-04-20 11:23:48.306146 (Thread-2): 11:23:48  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:48.306266 (Thread-2): 11:23:48  On model.dbsql_dbt_tpch.Prospect: Close
2023-04-20 11:23:48.550155 (Thread-303): handling status request
2023-04-20 11:23:48.550681 (Thread-303): 11:23:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb562203df0>]}
2023-04-20 11:23:48.551397 (Thread-303): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:48.589146 (Thread-2): 11:23:48  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4c028e5b0>]}
2023-04-20 11:23:48.589677 (Thread-2): 11:23:48  8 of 15 OK created sql table model dbt_shabbirkdb.Prospect ..................... [OK in 5.10s]
2023-04-20 11:23:48.589919 (Thread-2): 11:23:48  Finished running node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:23:48.591054 (Thread-4): 11:23:48  Began running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:23:48.591386 (Thread-4): 11:23:48  14 of 15 START sql table model dbt_shabbirkdb.DimCustomer ...................... [RUN]
2023-04-20 11:23:48.592023 (Thread-4): 11:23:48  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomer'
2023-04-20 11:23:48.592218 (Thread-4): 11:23:48  Began compiling node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:23:48.597511 (Thread-4): 11:23:48  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:23:48.611307 (Thread-4): 11:23:48  Timing info for model.dbsql_dbt_tpch.DimCustomer (compile): 2023-04-20 11:23:48.592265 => 2023-04-20 11:23:48.611133
2023-04-20 11:23:48.611562 (Thread-4): 11:23:48  Began executing node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:23:48.616044 (Thread-4): 11:23:48  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:23:48.631265 (Thread-4): 11:23:48  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:48.631454 (Thread-4): 11:23:48  Using databricks connection "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:23:48.631694 (Thread-4): 11:23:48  On model.dbsql_dbt_tpch.DimCustomer: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 11:23:48.631833 (Thread-4): 11:23:48  Opening a new connection, currently in state closed
2023-04-20 11:23:48.667852 (Thread-304): handling ps request
2023-04-20 11:23:48.668348 (Thread-304): 11:23:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb562203fa0>]}
2023-04-20 11:23:48.669259 (Thread-304): sending response (<Response 7969 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:48.942021 (Thread-305): handling ps request
2023-04-20 11:23:48.942542 (Thread-305): 11:23:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb562203250>]}
2023-04-20 11:23:48.966278 (Thread-305): sending response (<Response 7969 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:49.356507 (Thread-4): 11:23:49  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 11:23:49.356734 (Thread-4): 11:23:49  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:23:49.356858 (Thread-4): 11:23:49  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:23:49.356970 (Thread-4): 11:23:49  Databricks adapter: operation-id: b'\x01\xed\xdfm\xd2Q\x1dn\x9b\xc9g\xb7\xc3\x98\xc2\x1e'
2023-04-20 11:23:49.357203 (Thread-4): 11:23:49  Timing info for model.dbsql_dbt_tpch.DimCustomer (execute): 2023-04-20 11:23:48.611617 => 2023-04-20 11:23:49.357073
2023-04-20 11:23:49.357387 (Thread-4): 11:23:49  On model.dbsql_dbt_tpch.DimCustomer: ROLLBACK
2023-04-20 11:23:49.357512 (Thread-4): 11:23:49  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:49.357631 (Thread-4): 11:23:49  On model.dbsql_dbt_tpch.DimCustomer: Close
2023-04-20 11:23:49.631753 (Thread-4): 11:23:49  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:23:49.632149 (Thread-4): 11:23:49  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1241d838-66a0-4705-92bd-c1ffa533769c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4c0223460>]}
2023-04-20 11:23:49.632730 (Thread-4): 11:23:49  14 of 15 ERROR creating sql table model dbt_shabbirkdb.DimCustomer ............. [ERROR in 1.04s]
2023-04-20 11:23:49.632956 (Thread-4): 11:23:49  Finished running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:23:49.634110 (Thread-3): 11:23:49  Began running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 11:23:49.634387 (Thread-3): 11:23:49  15 of 15 SKIP relation dbt_shabbirkdb.FactWatches .............................. [SKIP]
2023-04-20 11:23:49.634579 (Thread-3): 11:23:49  Finished running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 11:23:49.636355 (MainThread): 11:23:49  Acquiring new databricks connection 'master'
2023-04-20 11:23:49.636574 (MainThread): 11:23:49  On master: ROLLBACK
2023-04-20 11:23:49.636726 (MainThread): 11:23:49  Opening a new connection, currently in state init
2023-04-20 11:23:49.690996 (Thread-306): handling poll request
2023-04-20 11:23:49.691426 (Thread-306): 11:23:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564946070>]}
2023-04-20 11:23:49.692380 (Thread-306): sending response (<Response 22275 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:49.858151 (Thread-307): handling status request
2023-04-20 11:23:49.858604 (Thread-307): 11:23:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb564946550>]}
2023-04-20 11:23:49.859204 (Thread-307): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:49.916055 (MainThread): 11:23:49  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:49.916291 (MainThread): 11:23:49  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:23:49.916426 (MainThread): 11:23:49  Spark adapter: NotImplemented: commit
2023-04-20 11:23:49.916584 (MainThread): 11:23:49  On master: ROLLBACK
2023-04-20 11:23:49.916715 (MainThread): 11:23:49  Databricks adapter: NotImplemented: rollback
2023-04-20 11:23:49.916851 (MainThread): 11:23:49  On master: Close
2023-04-20 11:23:50.189813 (MainThread): 11:23:50  Connection 'master' was properly closed.
2023-04-20 11:23:50.190017 (MainThread): 11:23:50  Connection 'model.dbsql_dbt_tpch.Financial' was properly closed.
2023-04-20 11:23:50.190127 (MainThread): 11:23:50  Connection 'model.dbsql_dbt_tpch.Prospect' was properly closed.
2023-04-20 11:23:50.190227 (MainThread): 11:23:50  Connection 'model.dbsql_dbt_tpch.DimAccount' was properly closed.
2023-04-20 11:23:50.190325 (MainThread): 11:23:50  Connection 'model.dbsql_dbt_tpch.DimCustomer' was properly closed.
2023-04-20 11:23:50.190632 (MainThread): 11:23:50  
2023-04-20 11:23:50.190781 (MainThread): 11:23:50  Finished running 15 table models in 0 hours 0 minutes and 14.96 seconds (14.96s).
2023-04-20 11:23:50.291430 (MainThread): 11:23:50  
2023-04-20 11:23:50.291708 (MainThread): 11:23:50  Completed with 4 errors and 0 warnings:
2023-04-20 11:23:50.291847 (MainThread): 11:23:50  
2023-04-20 11:23:50.291986 (MainThread): 11:23:50  Runtime Error in model DimSecurity (models/silver/DimSecurity.sql)
2023-04-20 11:23:50.292109 (MainThread): 11:23:50    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 78 pos 6
2023-04-20 11:23:50.292218 (MainThread): 11:23:50  
2023-04-20 11:23:50.292335 (MainThread): 11:23:50  Runtime Error in model Financial (models/silver/Financial.sql)
2023-04-20 11:23:50.292442 (MainThread): 11:23:50    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sk_companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`companyid`].; line 61 pos 4
2023-04-20 11:23:50.292547 (MainThread): 11:23:50  
2023-04-20 11:23:50.292660 (MainThread): 11:23:50  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
2023-04-20 11:23:50.292767 (MainThread): 11:23:50    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:23:50.292870 (MainThread): 11:23:50  
2023-04-20 11:23:50.292981 (MainThread): 11:23:50  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
2023-04-20 11:23:50.293086 (MainThread): 11:23:50    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:23:50.293208 (MainThread): 11:23:50  
2023-04-20 11:23:50.293332 (MainThread): 11:23:50  Done. PASS=5 WARN=0 ERROR=4 SKIP=6 TOTAL=15
2023-04-20 11:23:51.172995 (Thread-308): handling status request
2023-04-20 11:23:51.173494 (Thread-308): 11:23:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500da30>]}
2023-04-20 11:23:51.174160 (Thread-308): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:51.308340 (Thread-309): handling ps request
2023-04-20 11:23:51.308835 (Thread-309): 11:23:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500df70>]}
2023-04-20 11:23:51.309752 (Thread-309): sending response (<Response 7993 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:51.714686 (Thread-310): handling poll request
2023-04-20 11:23:51.715183 (Thread-310): 11:23:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500d790>]}
2023-04-20 11:23:51.717741 (Thread-310): sending response (<Response 95532 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:23:51.942886 (Thread-311): handling status request
2023-04-20 11:23:51.943380 (Thread-311): 11:23:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500df70>]}
2023-04-20 11:23:51.944062 (Thread-311): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:24:37.984997 (Thread-312): handling status request
2023-04-20 11:24:37.985491 (Thread-312): 11:24:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500d7c0>]}
2023-04-20 11:24:37.986128 (Thread-312): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:24:38.127036 (Thread-313): handling ps request
2023-04-20 11:24:38.127548 (Thread-313): 11:24:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500d6d0>]}
2023-04-20 11:24:38.128446 (Thread-313): sending response (<Response 7993 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:05.171881 (Thread-314): handling status request
2023-04-20 11:25:05.172430 (Thread-314): 11:25:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500d670>]}
2023-04-20 11:25:05.173091 (Thread-314): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:05.297542 (Thread-315): handling status request
2023-04-20 11:25:05.298026 (Thread-315): 11:25:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500dc70>]}
2023-04-20 11:25:05.298673 (Thread-315): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:05.303165 (Thread-316): handling list request
2023-04-20 11:25:05.303483 (Thread-316): 11:25:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56500ddf0>]}
2023-04-20 11:25:05.339912 (Thread-317): handling ps request
2023-04-20 11:25:05.340343 (Thread-317): 11:25:05  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56501bdf0>]}
2023-04-20 11:25:05.340905 (Thread-316): 11:25:05  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56501bf40>]}
2023-04-20 11:25:05.341797 (Thread-317): sending response (<Response 8265 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:05.342195 (Thread-316): 11:25:05  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:25:05.342807 (Thread-316): 11:25:05  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb565075ee0>]}
2023-04-20 11:25:05.345810 (Thread-316): sending response (<Response 6528 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:06.160116 (Thread-318): handling status request
2023-04-20 11:25:06.160605 (Thread-318): 11:25:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb553274d90>]}
2023-04-20 11:25:06.161265 (Thread-318): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:06.611085 (Thread-319): handling poll request
2023-04-20 11:25:06.611619 (Thread-319): 11:25:06  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5532746d0>]}
2023-04-20 11:25:06.612225 (Thread-319): sending response (<Response 303 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:11.218365 (Thread-320): handling status request
2023-04-20 11:25:11.218895 (Thread-320): 11:25:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56501beb0>]}
2023-04-20 11:25:11.219568 (Thread-320): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:11.224536 (Thread-321): handling list request
2023-04-20 11:25:11.224873 (Thread-321): 11:25:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56501bc70>]}
2023-04-20 11:25:11.254402 (Thread-321): 11:25:11  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5650c64f0>]}
2023-04-20 11:25:11.254914 (Thread-321): 11:25:11  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:25:11.255238 (Thread-321): 11:25:11  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55f96eac0>]}
2023-04-20 11:25:11.257776 (Thread-321): sending response (<Response 6923 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:16.881504 (Thread-322): handling status request
2023-04-20 11:25:16.882070 (Thread-322): 11:25:16  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5532746d0>]}
2023-04-20 11:25:16.882724 (Thread-322): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:16.887853 (Thread-323): handling list request
2023-04-20 11:25:16.888171 (Thread-323): 11:25:16  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569976820>]}
2023-04-20 11:25:16.927497 (Thread-323): 11:25:16  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb58983c610>]}
2023-04-20 11:25:16.928126 (Thread-323): 11:25:16  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:25:16.928479 (Thread-323): 11:25:16  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5650c6700>]}
2023-04-20 11:25:16.930895 (Thread-323): sending response (<Response 5535 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:31.790482 (Thread-324): 11:25:31  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:25:32.177789 (Thread-324): 11:25:32  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
2023-04-20 11:25:32.178402 (Thread-324): 11:25:32  Partial parsing: updated file: dbsql_dbt_tpch://models/silver/DimCompany.sql
2023-04-20 11:25:32.185959 (Thread-324): 11:25:32  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:25:32.199973 (Thread-325): handling status request
2023-04-20 11:25:32.200398 (Thread-325): 11:25:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb555a3b250>]}
2023-04-20 11:25:32.200803 (Thread-325): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:32.247672 (Thread-324): 11:25:32  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55aa0d910>]}
2023-04-20 11:25:32.473882 (Thread-326): handling status request
2023-04-20 11:25:32.474435 (Thread-326): 11:25:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56222aa90>]}
2023-04-20 11:25:32.474918 (Thread-326): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:32.479750 (Thread-327): handling list request
2023-04-20 11:25:32.480075 (Thread-327): 11:25:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56222aa60>]}
2023-04-20 11:25:32.513340 (Thread-327): 11:25:32  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55822a400>]}
2023-04-20 11:25:32.513817 (Thread-327): 11:25:32  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:25:32.514147 (Thread-327): 11:25:32  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5650c6c10>]}
2023-04-20 11:25:32.540039 (Thread-327): sending response (<Response 5535 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:33.489684 (Thread-328): handling status request
2023-04-20 11:25:33.490197 (Thread-328): 11:25:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56222aa00>]}
2023-04-20 11:25:33.490688 (Thread-328): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:33.495236 (Thread-329): handling list request
2023-04-20 11:25:33.495596 (Thread-329): 11:25:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb558281700>]}
2023-04-20 11:25:33.525722 (Thread-329): 11:25:33  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb56222af70>]}
2023-04-20 11:25:33.526270 (Thread-329): 11:25:33  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:25:33.526595 (Thread-329): 11:25:33  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb569a6ed90>]}
2023-04-20 11:25:33.530847 (Thread-329): sending response (<Response 13286 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:33.673896 (Thread-330): handling status request
2023-04-20 11:25:33.674371 (Thread-330): 11:25:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55aa0dc70>]}
2023-04-20 11:25:33.674849 (Thread-330): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:34.717717 (Thread-331): 11:25:34  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:25:34.998013 (Thread-331): 11:25:34  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
2023-04-20 11:25:34.999102 (Thread-331): 11:25:34  Partial parsing: updated file: dbsql_dbt_tpch://models/sources.yml
2023-04-20 11:25:35.070641 (Thread-331): 11:25:35  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a1e070>]}
2023-04-20 11:25:35.148140 (Thread-332): handling status request
2023-04-20 11:25:35.148639 (Thread-332): 11:25:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a7db80>]}
2023-04-20 11:25:35.149138 (Thread-332): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:35.403274 (Thread-333): handling status request
2023-04-20 11:25:35.403807 (Thread-333): 11:25:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a7dbb0>]}
2023-04-20 11:25:35.404286 (Thread-333): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:35.409655 (Thread-334): handling list request
2023-04-20 11:25:35.409992 (Thread-334): 11:25:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a7dbe0>]}
2023-04-20 11:25:35.447901 (Thread-334): 11:25:35  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a7d700>]}
2023-04-20 11:25:35.448473 (Thread-334): 11:25:35  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:25:35.448802 (Thread-334): 11:25:35  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55d157910>]}
2023-04-20 11:25:35.453021 (Thread-334): sending response (<Response 13286 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:37.464566 (Thread-335): handling status request
2023-04-20 11:25:37.465062 (Thread-335): 11:25:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55822a070>]}
2023-04-20 11:25:37.465536 (Thread-335): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:37.931932 (Thread-336): handling cli_args request
2023-04-20 11:25:37.932462 (Thread-336): 11:25:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a7d3d0>]}
2023-04-20 11:25:40.683276 (Thread-336): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:40.742563 (MainThread): 11:25:40  checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
2023-04-20 11:25:40.763727 (MainThread): 11:25:40  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 11:25:40.763901 (MainThread): 11:25:40  previous checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, current checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd
2023-04-20 11:25:40.764053 (MainThread): 11:25:40  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd24b33bac0>]}
2023-04-20 11:25:41.263395 (Thread-337): handling ps request
2023-04-20 11:25:41.264083 (Thread-337): 11:25:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5581a5e50>]}
2023-04-20 11:25:41.265761 (Thread-337): sending response (<Response 10127 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:41.346061 (Thread-338): handling status request
2023-04-20 11:25:41.346514 (Thread-338): 11:25:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a1e190>]}
2023-04-20 11:25:41.347013 (Thread-338): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:41.414194 (Thread-339): handling poll request
2023-04-20 11:25:41.414620 (Thread-339): 11:25:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a19a30>]}
2023-04-20 11:25:41.415170 (Thread-339): sending response (<Response 1863 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:41.630446 (Thread-340): handling ps request
2023-04-20 11:25:41.630935 (Thread-340): 11:25:41  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a19d30>]}
2023-04-20 11:25:41.631975 (Thread-340): sending response (<Response 10127 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:42.351039 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 11:25:42.363427 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 11:25:42.366562 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 11:25:42.369725 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 11:25:42.372746 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 11:25:42.375575 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 11:25:42.379378 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 11:25:42.382386 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 11:25:42.385414 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 11:25:42.388234 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 11:25:42.391609 (MainThread): 11:25:42  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 11:25:42.394495 (MainThread): 11:25:42  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 11:25:42.397258 (MainThread): 11:25:42  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 11:25:42.400108 (MainThread): 11:25:42  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 11:25:42.403082 (MainThread): 11:25:42  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:25:42.564914 (MainThread): 11:25:42  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1f67f2c10>]}
2023-04-20 11:25:42.596669 (MainThread): 11:25:42  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd24b342580>]}
2023-04-20 11:25:42.596977 (MainThread): 11:25:42  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:25:42.597119 (MainThread): 11:25:42  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd24b5ab970>]}
2023-04-20 11:25:42.598935 (MainThread): 11:25:42  
2023-04-20 11:25:42.600179 (MainThread): 11:25:42  Acquiring new databricks connection 'master'
2023-04-20 11:25:42.601839 (ThreadPoolExecutor-0_0): 11:25:42  Acquiring new databricks connection 'list_schemas'
2023-04-20 11:25:42.612478 (ThreadPoolExecutor-0_0): 11:25:42  Using databricks connection "list_schemas"
2023-04-20 11:25:42.612794 (ThreadPoolExecutor-0_0): 11:25:42  On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_schemas"} */

    show databases
  
2023-04-20 11:25:42.612951 (ThreadPoolExecutor-0_0): 11:25:42  Opening a new connection, currently in state init
2023-04-20 11:25:43.376342 (Thread-341): handling poll request
2023-04-20 11:25:43.376835 (Thread-341): 11:25:43  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559f1400>]}
2023-04-20 11:25:43.377555 (Thread-341): sending response (<Response 8689 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:43.693658 (ThreadPoolExecutor-0_0): 11:25:43  SQL status: OK in 1.08 seconds
2023-04-20 11:25:43.819414 (ThreadPoolExecutor-0_0): 11:25:43  On list_schemas: Close
2023-04-20 11:25:44.095992 (ThreadPoolExecutor-1_0): 11:25:44  Acquiring new databricks connection 'list_None_dbt_shabbirkdb'
2023-04-20 11:25:44.106578 (ThreadPoolExecutor-1_0): 11:25:44  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:44.106791 (ThreadPoolExecutor-1_0): 11:25:44  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 11:25:44.106950 (ThreadPoolExecutor-1_0): 11:25:44  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show tables in `dbt_shabbirkdb`
  
2023-04-20 11:25:44.107096 (ThreadPoolExecutor-1_0): 11:25:44  Opening a new connection, currently in state closed
2023-04-20 11:25:44.259308 (Thread-342): handling ps request
2023-04-20 11:25:44.259870 (Thread-342): 11:25:44  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559f1040>]}
2023-04-20 11:25:44.260900 (Thread-342): sending response (<Response 10126 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:44.803816 (ThreadPoolExecutor-1_0): 11:25:44  SQL status: OK in 0.7 seconds
2023-04-20 11:25:44.813003 (ThreadPoolExecutor-1_0): 11:25:44  Using databricks connection "list_None_dbt_shabbirkdb"
2023-04-20 11:25:44.813228 (ThreadPoolExecutor-1_0): 11:25:44  On list_None_dbt_shabbirkdb: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "connection_name": "list_None_dbt_shabbirkdb"} */
show views in `dbt_shabbirkdb`
  
2023-04-20 11:25:45.158334 (Thread-343): handling status request
2023-04-20 11:25:45.158845 (Thread-343): 11:25:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559f1cd0>]}
2023-04-20 11:25:45.159333 (Thread-343): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:45.281299 (ThreadPoolExecutor-1_0): 11:25:45  SQL status: OK in 0.47 seconds
2023-04-20 11:25:45.284957 (ThreadPoolExecutor-1_0): 11:25:45  On list_None_dbt_shabbirkdb: ROLLBACK
2023-04-20 11:25:45.285178 (ThreadPoolExecutor-1_0): 11:25:45  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:45.285326 (ThreadPoolExecutor-1_0): 11:25:45  On list_None_dbt_shabbirkdb: Close
2023-04-20 11:25:45.475070 (Thread-344): handling poll request
2023-04-20 11:25:45.475592 (Thread-344): 11:25:45  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5531c0100>]}
2023-04-20 11:25:45.476411 (Thread-344): sending response (<Response 5012 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:45.567866 (MainThread): 11:25:45  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1f67f2f10>]}
2023-04-20 11:25:45.568429 (MainThread): 11:25:45  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:45.568600 (MainThread): 11:25:45  Spark adapter: NotImplemented: commit
2023-04-20 11:25:45.569115 (MainThread): 11:25:45  Concurrency: 4 threads (target='default')
2023-04-20 11:25:45.569262 (MainThread): 11:25:45  
2023-04-20 11:25:45.572102 (Thread-1): 11:25:45  Began running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:25:45.572477 (Thread-1): 11:25:45  1 of 15 START sql table model dbt_shabbirkdb.DimBroker ......................... [RUN]
2023-04-20 11:25:45.573006 (Thread-1): 11:25:45  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
2023-04-20 11:25:45.573200 (Thread-1): 11:25:45  Began compiling node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:25:45.577242 (Thread-2): 11:25:45  Began running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:25:45.577554 (Thread-2): 11:25:45  2 of 15 START sql table model dbt_shabbirkdb.DimCompany ........................ [RUN]
2023-04-20 11:25:45.578071 (Thread-2): 11:25:45  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCompany'
2023-04-20 11:25:45.578247 (Thread-2): 11:25:45  Began compiling node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:25:45.582225 (Thread-3): 11:25:45  Began running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:25:45.582515 (Thread-3): 11:25:45  3 of 15 START sql table model dbt_shabbirkdb.DimCustomerStg .................... [RUN]
2023-04-20 11:25:45.583028 (Thread-3): 11:25:45  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomerStg'
2023-04-20 11:25:45.583202 (Thread-3): 11:25:45  Began compiling node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:25:45.588433 (Thread-1): 11:25:45  Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:25:45.588912 (Thread-2): 11:25:45  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:25:45.589274 (Thread-4): 11:25:45  Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:25:45.589564 (Thread-4): 11:25:45  4 of 15 START sql table model dbt_shabbirkdb.tempDailyMarketHistorical ......... [RUN]
2023-04-20 11:25:45.590082 (Thread-4): 11:25:45  Acquiring new databricks connection 'model.dbsql_dbt_tpch.tempDailyMarketHistorical'
2023-04-20 11:25:45.590259 (Thread-4): 11:25:45  Began compiling node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:25:45.594711 (Thread-3): 11:25:45  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:25:45.594813 (Thread-4): 11:25:45  Writing injected SQL for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:25:45.609071 (Thread-4): 11:25:45  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (compile): 2023-04-20 11:25:45.590303 => 2023-04-20 11:25:45.608890
2023-04-20 11:25:45.609328 (Thread-4): 11:25:45  Began executing node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:25:45.614421 (Thread-1): 11:25:45  Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 2023-04-20 11:25:45.574444 => 2023-04-20 11:25:45.614255
2023-04-20 11:25:45.614639 (Thread-1): 11:25:45  Began executing node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:25:45.630152 (Thread-3): 11:25:45  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (compile): 2023-04-20 11:25:45.583246 => 2023-04-20 11:25:45.629973
2023-04-20 11:25:45.630380 (Thread-3): 11:25:45  Began executing node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:25:45.642757 (Thread-1): 11:25:45  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:45.642946 (Thread-1): 11:25:45  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:25:45.643105 (Thread-1): 11:25:45  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

      describe extended `dbt_shabbirkdb`.`dimbroker`
  
2023-04-20 11:25:45.643236 (Thread-1): 11:25:45  Opening a new connection, currently in state closed
2023-04-20 11:25:45.643928 (Thread-3): 11:25:45  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:45.644106 (Thread-3): 11:25:45  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:25:45.644258 (Thread-3): 11:25:45  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

      describe extended `dbt_shabbirkdb`.`dimcustomerstg`
  
2023-04-20 11:25:45.644383 (Thread-3): 11:25:45  Opening a new connection, currently in state init
2023-04-20 11:25:45.644700 (Thread-2): 11:25:45  Timing info for model.dbsql_dbt_tpch.DimCompany (compile): 2023-04-20 11:25:45.578290 => 2023-04-20 11:25:45.644532
2023-04-20 11:25:45.644914 (Thread-2): 11:25:45  Began executing node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:25:45.640378 (Thread-4): 11:25:45  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:45.650749 (Thread-4): 11:25:45  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:25:45.650920 (Thread-4): 11:25:45  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

      describe extended `dbt_shabbirkdb`.`tempdailymarkethistorical`
  
2023-04-20 11:25:45.651048 (Thread-4): 11:25:45  Opening a new connection, currently in state init
2023-04-20 11:25:45.651335 (Thread-2): 11:25:45  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:45.651644 (Thread-2): 11:25:45  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:25:45.651822 (Thread-2): 11:25:45  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

      describe extended `dbt_shabbirkdb`.`dimcompany`
  
2023-04-20 11:25:45.651945 (Thread-2): 11:25:45  Opening a new connection, currently in state init
2023-04-20 11:25:46.412584 (Thread-3): 11:25:46  SQL status: OK in 0.77 seconds
2023-04-20 11:25:46.459940 (Thread-3): 11:25:46  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:25:46.460355 (Thread-1): 11:25:46  SQL status: OK in 0.82 seconds
2023-04-20 11:25:46.465461 (Thread-1): 11:25:46  Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:25:46.468268 (Thread-4): 11:25:46  SQL status: OK in 0.82 seconds
2023-04-20 11:25:46.468630 (Thread-2): 11:25:46  SQL status: OK in 0.82 seconds
2023-04-20 11:25:46.474684 (Thread-2): 11:25:46  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:25:46.478562 (Thread-4): 11:25:46  Writing runtime sql for node "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:25:46.485667 (Thread-1): 11:25:46  Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
2023-04-20 11:25:46.485890 (Thread-1): 11:25:46  On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT
  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate`) effectivedate,
  date('9999-12-31') enddate
FROM  `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`HR`
WHERE employeejobcode = 314
  
2023-04-20 11:25:46.488231 (Thread-2): 11:25:46  Using databricks connection "model.dbsql_dbt_tpch.DimCompany"
2023-04-20 11:25:46.488480 (Thread-2): 11:25:46  On model.dbsql_dbt_tpch.DimCompany: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCompany"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCompany`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) sk_companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(substring(value, 99, 8), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
       AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
  
2023-04-20 11:25:46.493704 (Thread-3): 11:25:46  Using databricks connection "model.dbsql_dbt_tpch.DimCustomerStg"
2023-04-20 11:25:46.494043 (Thread-3): 11:25:46  On model.dbsql_dbt_tpch.DimCustomerStg: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomerStg"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomerStg`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT * FROM (
  SELECT
    customerid,
    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) taxid,
    status,
    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) lastname,
    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) firstname,
    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) middleinitial,
    coalesce(gender, last_value(gender) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) gender,
    coalesce(tier, last_value(tier) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) tier,
    coalesce(dob, last_value(dob) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) dob,
    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline1,
    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) addressline2,
    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) postalcode,
    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) CITY,
    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) stateprov,
    coalesce(country, last_value(country) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) country,
    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone1,
    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone2,
    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) phone3,
    coalesce(email1, last_value(email1) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email1,
    coalesce(email2, last_value(email2) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) email2,
    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) LCL_TX_ID,
    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (
        PARTITION BY customerid
        ORDER BY update_ts)) NAT_TX_ID,
    batchid,
    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,
    date(update_ts) effectivedate,
    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate
  FROM (
    SELECT
      customerid,
      taxid,
      status,
      lastname,
      firstname,
      middleinitial,
      gender,
      tier,
      dob,
      addressline1,
      addressline2,
      postalcode,
      city,
      stateprov,
      country,
      phone1,
      phone2,
      phone3,
      email1,
      email2,
      lcl_tx_id,
      nat_tx_id,
      1 batchid,
      update_ts
    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')
    UNION ALL
    SELECT
      c.customerid,
      nullif(c.taxid, '') taxid,
      nullif(s.st_name, '') as status,
      nullif(c.lastname, '') lastname,
      nullif(c.firstname, '') firstname,
      nullif(c.middleinitial, '') middleinitial,
      gender,
      c.tier,
      c.dob,
      nullif(c.addressline1, '') addressline1,
      nullif(c.addressline2, '') addressline2,
      nullif(c.postalcode, '') postalcode,
      nullif(c.city, '') city,
      nullif(c.stateprov, '') stateprov,
      nullif(c.country, '') country,
      CASE
        WHEN isnull(c_local_1) then c_local_1
        ELSE concat(
          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),
          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),
          c_local_1,
          nvl(c_ext_1, '')) END as phone1,
      CASE
        WHEN isnull(c_local_2) then c_local_2
        ELSE concat(
          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),
          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),
          c_local_2,
          nvl(c_ext_2, '')) END as phone2,
      CASE
        WHEN isnull(c_local_3) then c_local_3
        ELSE concat(
          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),
          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),
          c_local_3,
          nvl(c_ext_3, '')) END as phone3,
      nullif(c.email1, '') email1,
      nullif(c.email2, '') email2,
      c.LCL_TX_ID, 
      c.NAT_TX_ID,
      c.batchid,
      timestamp(bd.batchdate) update_ts
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`CustomerIncremental` c
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
      ON c.batchid = bd.batchid
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` s 
      ON c.status = s.st_id
  ) c
  )
  
2023-04-20 11:25:46.494527 (Thread-4): 11:25:46  Using databricks connection "model.dbsql_dbt_tpch.tempDailyMarketHistorical"
2023-04-20 11:25:46.494721 (Thread-4): 11:25:46  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.tempDailyMarketHistorical"} */

  
    
        create or replace table `dbt_shabbirkdb`.`tempDailyMarketHistorical`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  dmh.*,
  sk_dateid,
  min(dm_low) OVER (
    PARTITION BY dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweeklow,
  max(dm_high) OVER (
    PARTITION by dm_s_symb
    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW
  ) fiftytwoweekhigh
FROM (
  SELECT * FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketHistorical`
  UNION ALL
  SELECT * except(cdc_flag, cdc_dsn) FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DailyMarketIncremental`) dmh
JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
  ON d.datevalue = dm_date;
  
2023-04-20 11:25:47.010224 (Thread-345): handling poll request
2023-04-20 11:25:47.010721 (Thread-345): 11:25:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559dca30>]}
2023-04-20 11:25:47.012007 (Thread-345): sending response (<Response 37156 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:47.268634 (Thread-346): handling ps request
2023-04-20 11:25:47.269127 (Thread-346): 11:25:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559dc9d0>]}
2023-04-20 11:25:47.270149 (Thread-346): sending response (<Response 10127 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:47.663201 (Thread-347): handling status request
2023-04-20 11:25:47.663709 (Thread-347): 11:25:47  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559dcfd0>]}
2023-04-20 11:25:47.664193 (Thread-347): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:48.550734 (Thread-348): handling poll request
2023-04-20 11:25:48.551229 (Thread-348): 11:25:48  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559dc910>]}
2023-04-20 11:25:48.551727 (Thread-348): sending response (<Response 291 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:49.259090 (Thread-2): 11:25:49  SQL status: OK in 2.77 seconds
2023-04-20 11:25:49.286078 (Thread-2): 11:25:49  Timing info for model.dbsql_dbt_tpch.DimCompany (execute): 2023-04-20 11:25:45.644961 => 2023-04-20 11:25:49.285920
2023-04-20 11:25:49.286305 (Thread-2): 11:25:49  On model.dbsql_dbt_tpch.DimCompany: ROLLBACK
2023-04-20 11:25:49.286450 (Thread-2): 11:25:49  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:49.286573 (Thread-2): 11:25:49  On model.dbsql_dbt_tpch.DimCompany: Close
2023-04-20 11:25:49.308087 (Thread-1): 11:25:49  SQL status: OK in 2.82 seconds
2023-04-20 11:25:49.310108 (Thread-1): 11:25:49  Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 2023-04-20 11:25:45.614688 => 2023-04-20 11:25:49.309976
2023-04-20 11:25:49.310294 (Thread-1): 11:25:49  On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
2023-04-20 11:25:49.310430 (Thread-1): 11:25:49  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:49.310548 (Thread-1): 11:25:49  On model.dbsql_dbt_tpch.DimBroker: Close
2023-04-20 11:25:49.571221 (Thread-2): 11:25:49  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd24ab0da30>]}
2023-04-20 11:25:49.572032 (Thread-2): 11:25:49  2 of 15 OK created sql table model dbt_shabbirkdb.DimCompany ................... [OK in 3.99s]
2023-04-20 11:25:49.573262 (Thread-2): 11:25:49  Finished running node model.dbsql_dbt_tpch.DimCompany
2023-04-20 11:25:49.574360 (Thread-2): 11:25:49  Began running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:25:49.574693 (Thread-2): 11:25:49  5 of 15 START sql table model dbt_shabbirkdb.DimSecurity ....................... [RUN]
2023-04-20 11:25:49.575269 (Thread-2): 11:25:49  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimSecurity'
2023-04-20 11:25:49.575459 (Thread-2): 11:25:49  Began compiling node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:25:49.580453 (Thread-2): 11:25:49  Writing injected SQL for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:25:49.588178 (Thread-1): 11:25:49  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd2480fdbe0>]}
2023-04-20 11:25:49.588596 (Thread-1): 11:25:49  1 of 15 OK created sql table model dbt_shabbirkdb.DimBroker .................... [OK in 4.02s]
2023-04-20 11:25:49.588819 (Thread-1): 11:25:49  Finished running node model.dbsql_dbt_tpch.DimBroker
2023-04-20 11:25:49.589088 (Thread-1): 11:25:49  Began running node model.dbsql_dbt_tpch.Financial
2023-04-20 11:25:49.589373 (Thread-1): 11:25:49  6 of 15 START sql table model dbt_shabbirkdb.Financial ......................... [RUN]
2023-04-20 11:25:49.589840 (Thread-1): 11:25:49  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Financial'
2023-04-20 11:25:49.590033 (Thread-1): 11:25:49  Began compiling node model.dbsql_dbt_tpch.Financial
2023-04-20 11:25:49.596365 (Thread-1): 11:25:49  Writing injected SQL for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:25:49.602908 (Thread-2): 11:25:49  Timing info for model.dbsql_dbt_tpch.DimSecurity (compile): 2023-04-20 11:25:49.575505 => 2023-04-20 11:25:49.602742
2023-04-20 11:25:49.603145 (Thread-2): 11:25:49  Began executing node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:25:49.607189 (Thread-2): 11:25:49  Writing runtime sql for node "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:25:49.613981 (Thread-1): 11:25:49  Timing info for model.dbsql_dbt_tpch.Financial (compile): 2023-04-20 11:25:49.590078 => 2023-04-20 11:25:49.613744
2023-04-20 11:25:49.614333 (Thread-1): 11:25:49  Began executing node model.dbsql_dbt_tpch.Financial
2023-04-20 11:25:49.619851 (Thread-1): 11:25:49  Writing runtime sql for node "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:25:49.626400 (Thread-2): 11:25:49  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:49.626585 (Thread-2): 11:25:49  Using databricks connection "model.dbsql_dbt_tpch.DimSecurity"
2023-04-20 11:25:49.626812 (Thread-2): 11:25:49  On model.dbsql_dbt_tpch.DimSecurity: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 11:25:49.626944 (Thread-2): 11:25:49  Opening a new connection, currently in state closed
2023-04-20 11:25:49.634807 (Thread-1): 11:25:49  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:49.634989 (Thread-1): 11:25:49  Using databricks connection "model.dbsql_dbt_tpch.Financial"
2023-04-20 11:25:49.635200 (Thread-1): 11:25:49  On model.dbsql_dbt_tpch.Financial: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 11:25:49.635326 (Thread-1): 11:25:49  Opening a new connection, currently in state closed
2023-04-20 11:25:49.803298 (Thread-349): handling ps request
2023-04-20 11:25:49.803816 (Thread-349): 11:25:49  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb555a20a00>]}
2023-04-20 11:25:49.804810 (Thread-349): sending response (<Response 10128 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:50.115312 (Thread-350): handling poll request
2023-04-20 11:25:50.115851 (Thread-350): 11:25:50  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb555a20c40>]}
2023-04-20 11:25:50.116738 (Thread-350): sending response (<Response 21225 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:50.218095 (Thread-351): handling status request
2023-04-20 11:25:50.218577 (Thread-351): 11:25:50  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb555a20cd0>]}
2023-04-20 11:25:50.219048 (Thread-351): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:50.393399 (Thread-2): 11:25:50  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimSecurity"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimSecurity`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  Symbol,
  issue,
  status,
  Name,
  exchangeid,
  sk_companyid,
  sharesoutstanding,
  firsttrade,
  firsttradeonexchange,
  Dividend,
  if(enddate = date('9999-12-31'), True, False) iscurrent,
  1 batchid,
  effectivedate,
  enddate
FROM (
  SELECT 
    fws.Symbol,
    fws.issue,
    fws.status,
    fws.Name,
    fws.exchangeid,
    dc.sk_companyid,
    fws.sharesoutstanding,
    fws.firsttrade,
    fws.firsttradeonexchange,
    fws.Dividend,
    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,
    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate
  FROM (
    SELECT 
      fws.* except(Status, conameorcik),
      nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik,
      s.ST_NAME as status,
      coalesce(
        lead(effectivedate) OVER (
          PARTITION BY symbol
          ORDER BY effectivedate),
        date('9999-12-31')
      ) enddate
    FROM (
      SELECT
        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,
        trim(substring(value, 19, 15)) AS Symbol,
        trim(substring(value, 34, 6)) AS issue,
        trim(substring(value, 40, 4)) AS Status,
        trim(substring(value, 44, 70)) AS Name,
        trim(substring(value, 114, 6)) AS exchangeid,
        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,
        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,
        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,
        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,
        trim(substring(value, 161, 60)) AS conameorcik
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'SEC'
      ) fws
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType`s 
      ON s.ST_ID = fws.status
    ) fws
  JOIN (
    SELECT 
      sk_companyid,
      name conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
    UNION ALL
    SELECT 
      sk_companyid,
      cast(companyid as string) conameorcik,
      EffectiveDate,
      EndDate
    FROM `dbt_shabbirkdb`.`DimCompany`
  ) dc 
  ON
    fws.conameorcik = dc.conameorcik 
    AND fws.EffectiveDate < dc.EndDate
    AND fws.EndDate > dc.EffectiveDate
) fws
WHERE effectivedate != enddate
  
2023-04-20 11:25:50.393637 (Thread-2): 11:25:50  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 86 pos 11
2023-04-20 11:25:50.393767 (Thread-2): 11:25:50  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 86 pos 11
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 86 pos 11
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:25:50.393891 (Thread-2): 11:25:50  Databricks adapter: operation-id: b'\x01\xed\xdfn\x1ap\x19K\xb1`\xe2L\xd6=\xb8t'
2023-04-20 11:25:50.394137 (Thread-2): 11:25:50  Timing info for model.dbsql_dbt_tpch.DimSecurity (execute): 2023-04-20 11:25:49.603197 => 2023-04-20 11:25:50.394005
2023-04-20 11:25:50.394324 (Thread-2): 11:25:50  On model.dbsql_dbt_tpch.DimSecurity: ROLLBACK
2023-04-20 11:25:50.394459 (Thread-2): 11:25:50  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:50.394590 (Thread-2): 11:25:50  On model.dbsql_dbt_tpch.DimSecurity: Close
2023-04-20 11:25:50.401764 (Thread-1): 11:25:50  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Financial"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Financial`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  sk_companyid,
  fi_year,
  fi_qtr,
  fi_qtr_start_date,
  fi_revenue,
  fi_net_earn,
  fi_basic_eps,
  fi_dilut_eps,
  fi_margin,
  fi_inventory,
  fi_assets,
  fi_liability,
  fi_out_basic,
  fi_out_dilut
FROM (
  SELECT 
    * except(conameorcik),
    nvl(string(cast(conameorcik as bigint)), conameorcik) conameorcik
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      cast(substring(value, 19, 4) AS INT) AS fi_year,
      cast(substring(value, 23, 1) AS INT) AS fi_qtr,
      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,
      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,
      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,
      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,
      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,
      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,
      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,
      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,
      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,
      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,
      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,
      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,
      trim(substring(value, 187, 60)) AS conameorcik
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'FIN'
  ) f 
) f
JOIN (
  SELECT 
    sk_companyid,
    name conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
  UNION ALL
  SELECT 
    sk_companyid,
    cast(companyid as string) conameorcik,
    EffectiveDate,
    EndDate
  FROM `dbt_shabbirkdb`.`DimCompany`
) dc 
ON
  f.conameorcik = dc.conameorcik 
  AND date(PTS) >= dc.effectivedate 
  AND date(PTS) < dc.enddate
  
2023-04-20 11:25:50.401964 (Thread-1): 11:25:50  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 69 pos 9
2023-04-20 11:25:50.402093 (Thread-1): 11:25:50  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 69 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 69 pos 9
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:25:50.402210 (Thread-1): 11:25:50  Databricks adapter: operation-id: b'\x01\xed\xdfn\x1ap\x1e\xfd\xbch\xc7\xa3D$j\xd1'
2023-04-20 11:25:50.402419 (Thread-1): 11:25:50  Timing info for model.dbsql_dbt_tpch.Financial (execute): 2023-04-20 11:25:49.614420 => 2023-04-20 11:25:50.402305
2023-04-20 11:25:50.402584 (Thread-1): 11:25:50  On model.dbsql_dbt_tpch.Financial: ROLLBACK
2023-04-20 11:25:50.402706 (Thread-1): 11:25:50  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:50.402820 (Thread-1): 11:25:50  On model.dbsql_dbt_tpch.Financial: Close
2023-04-20 11:25:50.677257 (Thread-2): 11:25:50  Runtime Error in model DimSecurity (models/silver/DimSecurity.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 86 pos 11
2023-04-20 11:25:50.677891 (Thread-2): 11:25:50  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1f53e8580>]}
2023-04-20 11:25:50.678413 (Thread-2): 11:25:50  5 of 15 ERROR creating sql table model dbt_shabbirkdb.DimSecurity .............. [ERROR in 1.10s]
2023-04-20 11:25:50.678632 (Thread-2): 11:25:50  Finished running node model.dbsql_dbt_tpch.DimSecurity
2023-04-20 11:25:50.691340 (Thread-1): 11:25:50  Runtime Error in model Financial (models/silver/Financial.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 69 pos 9
2023-04-20 11:25:50.691692 (Thread-1): 11:25:50  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1f53f7100>]}
2023-04-20 11:25:50.692094 (Thread-1): 11:25:50  6 of 15 ERROR creating sql table model dbt_shabbirkdb.Financial ................ [ERROR in 1.10s]
2023-04-20 11:25:50.692295 (Thread-1): 11:25:50  Finished running node model.dbsql_dbt_tpch.Financial
2023-04-20 11:25:50.693269 (Thread-2): 11:25:50  Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 11:25:50.693564 (Thread-2): 11:25:50  7 of 15 SKIP relation dbt_shabbirkdb.tempSumpFiBasicEps ........................ [SKIP]
2023-04-20 11:25:50.693760 (Thread-2): 11:25:50  Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
2023-04-20 11:25:50.736989 (Thread-3): 11:25:50  SQL status: OK in 4.24 seconds
2023-04-20 11:25:50.739480 (Thread-3): 11:25:50  Timing info for model.dbsql_dbt_tpch.DimCustomerStg (execute): 2023-04-20 11:25:45.630429 => 2023-04-20 11:25:50.739312
2023-04-20 11:25:50.739760 (Thread-3): 11:25:50  On model.dbsql_dbt_tpch.DimCustomerStg: ROLLBACK
2023-04-20 11:25:50.739930 (Thread-3): 11:25:50  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:50.740077 (Thread-3): 11:25:50  On model.dbsql_dbt_tpch.DimCustomerStg: Close
2023-04-20 11:25:51.027773 (Thread-3): 11:25:51  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd24a9e85b0>]}
2023-04-20 11:25:51.028317 (Thread-3): 11:25:51  3 of 15 OK created sql table model dbt_shabbirkdb.DimCustomerStg ............... [OK in 5.44s]
2023-04-20 11:25:51.028554 (Thread-3): 11:25:51  Finished running node model.dbsql_dbt_tpch.DimCustomerStg
2023-04-20 11:25:51.029564 (Thread-2): 11:25:51  Began running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:25:51.029952 (Thread-2): 11:25:51  8 of 15 START sql table model dbt_shabbirkdb.DimAccount ........................ [RUN]
2023-04-20 11:25:51.030461 (Thread-2): 11:25:51  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimAccount'
2023-04-20 11:25:51.030648 (Thread-2): 11:25:51  Began compiling node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:25:51.034518 (Thread-1): 11:25:51  Began running node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:25:51.034799 (Thread-1): 11:25:51  9 of 15 START sql table model dbt_shabbirkdb.Prospect .......................... [RUN]
2023-04-20 11:25:51.035256 (Thread-1): 11:25:51  Acquiring new databricks connection 'model.dbsql_dbt_tpch.Prospect'
2023-04-20 11:25:51.035445 (Thread-1): 11:25:51  Began compiling node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:25:51.040585 (Thread-1): 11:25:51  Writing injected SQL for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:25:51.042351 (Thread-2): 11:25:51  Writing injected SQL for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:25:51.056769 (Thread-1): 11:25:51  Timing info for model.dbsql_dbt_tpch.Prospect (compile): 2023-04-20 11:25:51.035504 => 2023-04-20 11:25:51.056591
2023-04-20 11:25:51.056997 (Thread-1): 11:25:51  Began executing node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:25:51.062238 (Thread-2): 11:25:51  Timing info for model.dbsql_dbt_tpch.DimAccount (compile): 2023-04-20 11:25:51.030695 => 2023-04-20 11:25:51.062024
2023-04-20 11:25:51.062472 (Thread-2): 11:25:51  Began executing node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:25:51.066376 (Thread-2): 11:25:51  Writing runtime sql for node "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:25:51.067055 (Thread-1): 11:25:51  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:51.067251 (Thread-1): 11:25:51  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:25:51.067414 (Thread-1): 11:25:51  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

      describe extended `dbt_shabbirkdb`.`prospect`
  
2023-04-20 11:25:51.067578 (Thread-1): 11:25:51  Opening a new connection, currently in state closed
2023-04-20 11:25:51.080091 (Thread-2): 11:25:51  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:51.080269 (Thread-2): 11:25:51  Using databricks connection "model.dbsql_dbt_tpch.DimAccount"
2023-04-20 11:25:51.080489 (Thread-2): 11:25:51  On model.dbsql_dbt_tpch.DimAccount: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 11:25:51.080615 (Thread-2): 11:25:51  Opening a new connection, currently in state closed
2023-04-20 11:25:51.650585 (Thread-352): handling poll request
2023-04-20 11:25:51.651077 (Thread-352): 11:25:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a2f400>]}
2023-04-20 11:25:51.652197 (Thread-352): sending response (<Response 40777 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:51.825991 (Thread-1): 11:25:51  SQL status: OK in 0.76 seconds
2023-04-20 11:25:51.832037 (Thread-1): 11:25:51  Writing runtime sql for node "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:25:51.846552 (Thread-1): 11:25:51  Using databricks connection "model.dbsql_dbt_tpch.Prospect"
2023-04-20 11:25:51.846875 (Thread-1): 11:25:51  On model.dbsql_dbt_tpch.Prospect: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.Prospect"} */

  
    
        create or replace table `dbt_shabbirkdb`.`Prospect`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT 
  agencyid,
  recdate.sk_dateid sk_recorddateid,
  origdate.sk_dateid sk_updatedateid,
  p.batchid,
  nvl2(c.customerid, True, False) iscustomer, 
  p.lastname,
  p.firstname,
  p.middleinitial,
  p.gender,
  p.addressline1,
  p.addressline2,
  p.postalcode,
  city,
  state,
  country,
  phone,
  income,
  numbercars,
  numberchildren,
  maritalstatus,
  age,
  creditrating,
  ownorrentflag,
  employer,
  numbercreditcards,
  networth,
  if(
    isnotnull(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+","")),
    left(
      if(networth > 1000000 or income > 200000,"HighValue+","") || 
      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
      if(age > 45, "Boomer+", "") ||
      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
      if(age < 25 and networth > 1000000, "Inherited+",""),
      length(
        if(networth > 1000000 or income > 200000,"HighValue+","") || 
        if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||
        if(age > 45, "Boomer+", "") ||
        if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||
        if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||
        if(age < 25 and networth > 1000000, "Inherited+",""))
      -1),
    NULL) marketingnameplate
FROM (
  SELECT 
    * FROM (
    SELECT
      agencyid,
      max(batchid) recordbatchid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth,
      min(batchid) batchid
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`ProspectRaw` p
    GROUP BY
      agencyid,
      lastname,
      firstname,
      middleinitial,
      gender,
      addressline1,
      addressline2,
      postalcode,
      city,
      state,
      country,
      phone,
      income,
      numbercars,
      numberchildren,
      maritalstatus,
      age,
      creditrating,
      ownorrentflag,
      employer,
      numbercreditcards,
      networth)
  QUALIFY ROW_NUMBER() OVER (PARTITION BY agencyid ORDER BY batchid DESC) = 1) p
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) recdate
  ON p.recordbatchid = recdate.batchid
JOIN (
  SELECT 
    sk_dateid,
    batchid
  FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` b 
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`DimDate` d 
    ON b.batchdate = d.datevalue) origdate
  ON p.batchid = origdate.batchid
LEFT JOIN (
  SELECT 
    customerid,
    lastname,
    firstname,
    addressline1,
    addressline2,
    postalcode
  FROM `dbt_shabbirkdb`.`DimCustomerStg`
  WHERE iscurrent) c
  ON 
    upper(p.LastName) = upper(c.lastname)
    and upper(p.FirstName) = upper(c.firstname)
    and upper(p.AddressLine1) = upper(c.addressline1)
    and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
    and upper(p.PostalCode) = upper(c.postalcode)
  
2023-04-20 11:25:51.864322 (Thread-2): 11:25:51  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimAccount"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimAccount`
      
      
    using delta
      
      
      
      
      
      
      as
      
SELECT
  a.accountid,
  b.sk_brokerid,
  a.sk_customerid,
  a.accountdesc,
  a.TaxStatus,
  a.status,
  a.batchid,
  a.effectivedate,
  a.enddate
FROM (
  SELECT
    a.* except(effectivedate, enddate, customerid),
    c.sk_customerid,
    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,
    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate
  FROM (
    SELECT *
    FROM (
      SELECT
        accountid,
        customerid,
        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) accountdesc,
        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) taxstatus,
        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) brokerid,
        coalesce(status, last_value(status) IGNORE NULLS OVER (
          PARTITION BY accountid ORDER BY update_ts)) status,
        date(update_ts) effectivedate,
        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,
        batchid
      FROM (
        SELECT
          accountid,
          customerid,
          accountdesc,
          taxstatus,
          brokerid,
          status,
          update_ts,
          1 batchid
        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c
        WHERE ActionType NOT IN ('UPDCUST', 'INACT')
        UNION ALL
        SELECT
          accountid,
          a.ca_c_id customerid,
          accountDesc,
          TaxStatus,
          a.ca_b_id brokerid,
          st_name as status,
          TIMESTAMP(bd.batchdate) update_ts,
          a.batchid
        FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`AccountIncremental` a
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`BatchDate` bd
          ON a.batchid = bd.batchid
        JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st 
          ON a.CA_ST_ID = st.st_id
      ) a
    ) a
    WHERE a.effectivedate < a.enddate
  ) a
  FULL OUTER JOIN `dbt_shabbirkdb`.`DimCustomerStg` c 
    ON 
      a.customerid = c.customerid
      AND c.enddate > a.effectivedate
      AND c.effectivedate < a.enddate
) a
LEFT JOIN `dbt_shabbirkdb`.`DimBroker` b 
  ON a.brokerid = b.brokerid;
  
2023-04-20 11:25:51.864520 (Thread-2): 11:25:51  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:25:51.864646 (Thread-2): 11:25:51  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:25:51.864761 (Thread-2): 11:25:51  Databricks adapter: operation-id: b'\x01\xed\xdfn\x1bM\x16\x14\x9f\x812\xb54\xb7\xfd\x0e'
2023-04-20 11:25:51.864990 (Thread-2): 11:25:51  Timing info for model.dbsql_dbt_tpch.DimAccount (execute): 2023-04-20 11:25:51.062522 => 2023-04-20 11:25:51.864863
2023-04-20 11:25:51.865167 (Thread-2): 11:25:51  On model.dbsql_dbt_tpch.DimAccount: ROLLBACK
2023-04-20 11:25:51.865293 (Thread-2): 11:25:51  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:51.865416 (Thread-2): 11:25:51  On model.dbsql_dbt_tpch.DimAccount: Close
2023-04-20 11:25:52.151849 (Thread-2): 11:25:52  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:25:52.152530 (Thread-2): 11:25:52  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1f5650dc0>]}
2023-04-20 11:25:52.153167 (Thread-2): 11:25:52  8 of 15 ERROR creating sql table model dbt_shabbirkdb.DimAccount ............... [ERROR in 1.12s]
2023-04-20 11:25:52.153409 (Thread-2): 11:25:52  Finished running node model.dbsql_dbt_tpch.DimAccount
2023-04-20 11:25:52.154289 (Thread-3): 11:25:52  Began running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 11:25:52.154566 (Thread-3): 11:25:52  10 of 15 SKIP relation dbt_shabbirkdb.DimTrade ................................. [SKIP]
2023-04-20 11:25:52.154764 (Thread-3): 11:25:52  Finished running node model.dbsql_dbt_tpch.DimTrade
2023-04-20 11:25:52.155135 (Thread-3): 11:25:52  Began running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 11:25:52.155501 (Thread-3): 11:25:52  11 of 15 SKIP relation dbt_shabbirkdb.FactCashBalances ......................... [SKIP]
2023-04-20 11:25:52.155865 (Thread-3): 11:25:52  Finished running node model.dbsql_dbt_tpch.FactCashBalances
2023-04-20 11:25:52.156746 (Thread-3): 11:25:52  Began running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 11:25:52.156983 (Thread-3): 11:25:52  12 of 15 SKIP relation dbt_shabbirkdb.FactHoldings ............................. [SKIP]
2023-04-20 11:25:52.157164 (Thread-3): 11:25:52  Finished running node model.dbsql_dbt_tpch.FactHoldings
2023-04-20 11:25:52.345785 (Thread-353): handling ps request
2023-04-20 11:25:52.346307 (Thread-353): 11:25:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a2feb0>]}
2023-04-20 11:25:52.372189 (Thread-353): sending response (<Response 10128 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:52.813282 (Thread-354): handling status request
2023-04-20 11:25:52.813776 (Thread-354): 11:25:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a2fe80>]}
2023-04-20 11:25:52.814266 (Thread-354): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:52.997130 (Thread-4): 11:25:52  SQL status: OK in 6.5 seconds
2023-04-20 11:25:53.227386 (Thread-355): handling poll request
2023-04-20 11:25:53.227952 (Thread-355): 11:25:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550a2ff40>]}
2023-04-20 11:25:53.228759 (Thread-355): sending response (<Response 22446 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:53.300525 (Thread-4): 11:25:53  Timing info for model.dbsql_dbt_tpch.tempDailyMarketHistorical (execute): 2023-04-20 11:25:45.609379 => 2023-04-20 11:25:53.300336
2023-04-20 11:25:53.300797 (Thread-4): 11:25:53  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: ROLLBACK
2023-04-20 11:25:53.300945 (Thread-4): 11:25:53  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:53.301070 (Thread-4): 11:25:53  On model.dbsql_dbt_tpch.tempDailyMarketHistorical: Close
2023-04-20 11:25:53.591942 (Thread-4): 11:25:53  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd24aaf87c0>]}
2023-04-20 11:25:53.592471 (Thread-4): 11:25:53  4 of 15 OK created sql table model dbt_shabbirkdb.tempDailyMarketHistorical .... [OK in 8.00s]
2023-04-20 11:25:53.592704 (Thread-4): 11:25:53  Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
2023-04-20 11:25:53.593588 (Thread-3): 11:25:53  Began running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 11:25:53.593867 (Thread-3): 11:25:53  13 of 15 SKIP relation dbt_shabbirkdb.FactMarketHistory ........................ [SKIP]
2023-04-20 11:25:53.594075 (Thread-3): 11:25:53  Finished running node model.dbsql_dbt_tpch.FactMarketHistory
2023-04-20 11:25:54.804039 (Thread-356): handling poll request
2023-04-20 11:25:54.804534 (Thread-356): 11:25:54  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509836a0>]}
2023-04-20 11:25:54.805138 (Thread-356): sending response (<Response 4954 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:54.915307 (Thread-357): handling ps request
2023-04-20 11:25:54.915704 (Thread-357): 11:25:54  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550983bb0>]}
2023-04-20 11:25:54.916667 (Thread-357): sending response (<Response 10128 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:55.350231 (Thread-358): handling status request
2023-04-20 11:25:55.350720 (Thread-358): 11:25:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509833a0>]}
2023-04-20 11:25:55.351194 (Thread-358): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:56.238142 (Thread-1): 11:25:56  SQL status: OK in 4.39 seconds
2023-04-20 11:25:56.241212 (Thread-1): 11:25:56  Timing info for model.dbsql_dbt_tpch.Prospect (execute): 2023-04-20 11:25:51.057047 => 2023-04-20 11:25:56.241055
2023-04-20 11:25:56.241423 (Thread-1): 11:25:56  On model.dbsql_dbt_tpch.Prospect: ROLLBACK
2023-04-20 11:25:56.241564 (Thread-1): 11:25:56  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:56.241685 (Thread-1): 11:25:56  On model.dbsql_dbt_tpch.Prospect: Close
2023-04-20 11:25:56.345778 (Thread-359): handling poll request
2023-04-20 11:25:56.346284 (Thread-359): 11:25:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509837c0>]}
2023-04-20 11:25:56.346821 (Thread-359): sending response (<Response 2125 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:56.528914 (Thread-1): 11:25:56  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1f55d81c0>]}
2023-04-20 11:25:56.529653 (Thread-1): 11:25:56  9 of 15 OK created sql table model dbt_shabbirkdb.Prospect ..................... [OK in 5.49s]
2023-04-20 11:25:56.530050 (Thread-1): 11:25:56  Finished running node model.dbsql_dbt_tpch.Prospect
2023-04-20 11:25:56.531279 (Thread-4): 11:25:56  Began running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:25:56.531638 (Thread-4): 11:25:56  14 of 15 START sql table model dbt_shabbirkdb.DimCustomer ...................... [RUN]
2023-04-20 11:25:56.532461 (Thread-4): 11:25:56  Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimCustomer'
2023-04-20 11:25:56.532755 (Thread-4): 11:25:56  Began compiling node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:25:56.538807 (Thread-4): 11:25:56  Writing injected SQL for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:25:56.554593 (Thread-4): 11:25:56  Timing info for model.dbsql_dbt_tpch.DimCustomer (compile): 2023-04-20 11:25:56.532837 => 2023-04-20 11:25:56.554393
2023-04-20 11:25:56.554847 (Thread-4): 11:25:56  Began executing node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:25:56.560985 (Thread-4): 11:25:56  Writing runtime sql for node "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:25:56.575540 (Thread-4): 11:25:56  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:56.575748 (Thread-4): 11:25:56  Using databricks connection "model.dbsql_dbt_tpch.DimCustomer"
2023-04-20 11:25:56.575963 (Thread-4): 11:25:56  On model.dbsql_dbt_tpch.DimCustomer: /* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 11:25:56.576096 (Thread-4): 11:25:56  Opening a new connection, currently in state closed
2023-04-20 11:25:57.318413 (Thread-4): 11:25:57  Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "dbt_databricks_version": "1.4.3", "databricks_sql_connector_version": "2.5.0", "profile_name": "user", "target_name": "default", "node_id": "model.dbsql_dbt_tpch.DimCustomer"} */

  
    
        create or replace table `dbt_shabbirkdb`.`DimCustomer`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT 
  c.sk_customerid,
  c.customerid,
  c.taxid,
  c.status,
  c.lastname,
  c.firstname,
  c.middleinitial,
  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,
  c.tier,
  c.dob,
  c.addressline1,
  c.addressline2,
  c.postalcode,
  c.city,
  c.stateprov,
  c.country,
  c.phone1,
  c.phone2,
  c.phone3,
  c.email1,
  c.email2,
  r_nat.TX_NAME as nationaltaxratedesc,
  r_nat.TX_RATE as nationaltaxrate,
  r_lcl.TX_NAME as localtaxratedesc,
  r_lcl.TX_RATE as localtaxrate,
  p.agencyid,
  p.creditrating,
  p.networth,
  p.marketingnameplate,
  c.iscurrent,
  c.batchid,
  c.effectivedate,
  c.enddate
FROM `dbt_shabbirkdb`.`DimCustomerStg` c
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_lcl 
  ON c.LCL_TX_ID = r_lcl.TX_ID
LEFT JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`TaxRate` r_nat 
  ON c.NAT_TX_ID = r_nat.TX_ID
LEFT JOIN `dbt_shabbirkdb`.`Prospect` p 
  on upper(p.lastname) = upper(c.lastname)
  and upper(p.firstname) = upper(c.firstname)
  and upper(p.addressline1) = upper(c.addressline1)
  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))
  and upper(p.postalcode) = upper(c.postalcode);
  
2023-04-20 11:25:57.318660 (Thread-4): 11:25:57  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:25:57.318808 (Thread-4): 11:25:57  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	... 21 more

2023-04-20 11:25:57.318940 (Thread-4): 11:25:57  Databricks adapter: operation-id: b'\x01\xed\xdfn\x1e\x97\x12\xbf\x8f\x91Z\xc8\\B\xce|'
2023-04-20 11:25:57.319228 (Thread-4): 11:25:57  Timing info for model.dbsql_dbt_tpch.DimCustomer (execute): 2023-04-20 11:25:56.554899 => 2023-04-20 11:25:57.319054
2023-04-20 11:25:57.319448 (Thread-4): 11:25:57  On model.dbsql_dbt_tpch.DimCustomer: ROLLBACK
2023-04-20 11:25:57.319635 (Thread-4): 11:25:57  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:57.319782 (Thread-4): 11:25:57  On model.dbsql_dbt_tpch.DimCustomer: Close
2023-04-20 11:25:57.439588 (Thread-360): handling ps request
2023-04-20 11:25:57.440079 (Thread-360): 11:25:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550989190>]}
2023-04-20 11:25:57.441084 (Thread-360): sending response (<Response 10128 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:57.601898 (Thread-4): 11:25:57  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:25:57.602322 (Thread-4): 11:25:57  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6560d22-94b3-48d2-9648-512016e54dad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd248043eb0>]}
2023-04-20 11:25:57.602773 (Thread-4): 11:25:57  14 of 15 ERROR creating sql table model dbt_shabbirkdb.DimCustomer ............. [ERROR in 1.07s]
2023-04-20 11:25:57.602985 (Thread-4): 11:25:57  Finished running node model.dbsql_dbt_tpch.DimCustomer
2023-04-20 11:25:57.604314 (Thread-2): 11:25:57  Began running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 11:25:57.604668 (Thread-2): 11:25:57  15 of 15 SKIP relation dbt_shabbirkdb.FactWatches .............................. [SKIP]
2023-04-20 11:25:57.604872 (Thread-2): 11:25:57  Finished running node model.dbsql_dbt_tpch.FactWatches
2023-04-20 11:25:57.606524 (MainThread): 11:25:57  Acquiring new databricks connection 'master'
2023-04-20 11:25:57.606741 (MainThread): 11:25:57  On master: ROLLBACK
2023-04-20 11:25:57.606892 (MainThread): 11:25:57  Opening a new connection, currently in state init
2023-04-20 11:25:57.874514 (Thread-361): handling status request
2023-04-20 11:25:57.875004 (Thread-361): 11:25:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509896d0>]}
2023-04-20 11:25:57.875495 (Thread-361): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:57.888687 (MainThread): 11:25:57  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:57.888931 (MainThread): 11:25:57  Spark adapter: NotImplemented: add_begin_query
2023-04-20 11:25:57.889071 (MainThread): 11:25:57  Spark adapter: NotImplemented: commit
2023-04-20 11:25:57.889228 (MainThread): 11:25:57  On master: ROLLBACK
2023-04-20 11:25:57.889378 (MainThread): 11:25:57  Databricks adapter: NotImplemented: rollback
2023-04-20 11:25:57.889517 (MainThread): 11:25:57  On master: Close
2023-04-20 11:25:57.903197 (Thread-362): handling poll request
2023-04-20 11:25:57.903582 (Thread-362): 11:25:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550989a00>]}
2023-04-20 11:25:57.904394 (Thread-362): sending response (<Response 22114 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:58.167015 (MainThread): 11:25:58  Connection 'master' was properly closed.
2023-04-20 11:25:58.167213 (MainThread): 11:25:58  Connection 'model.dbsql_dbt_tpch.Prospect' was properly closed.
2023-04-20 11:25:58.167322 (MainThread): 11:25:58  Connection 'model.dbsql_dbt_tpch.DimAccount' was properly closed.
2023-04-20 11:25:58.167426 (MainThread): 11:25:58  Connection 'model.dbsql_dbt_tpch.DimCustomerStg' was properly closed.
2023-04-20 11:25:58.167554 (MainThread): 11:25:58  Connection 'model.dbsql_dbt_tpch.DimCustomer' was properly closed.
2023-04-20 11:25:58.167856 (MainThread): 11:25:58  
2023-04-20 11:25:58.168002 (MainThread): 11:25:58  Finished running 15 table models in 0 hours 0 minutes and 15.57 seconds (15.57s).
2023-04-20 11:25:58.253557 (MainThread): 11:25:58  
2023-04-20 11:25:58.253795 (MainThread): 11:25:58  Completed with 4 errors and 0 warnings:
2023-04-20 11:25:58.253934 (MainThread): 11:25:58  
2023-04-20 11:25:58.254074 (MainThread): 11:25:58  Runtime Error in model DimSecurity (models/silver/DimSecurity.sql)
2023-04-20 11:25:58.254199 (MainThread): 11:25:58    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 86 pos 11
2023-04-20 11:25:58.254310 (MainThread): 11:25:58  
2023-04-20 11:25:58.254428 (MainThread): 11:25:58  Runtime Error in model Financial (models/silver/Financial.sql)
2023-04-20 11:25:58.254537 (MainThread): 11:25:58    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `companyid` cannot be resolved. Did you mean one of the following? [`hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`ceo`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`city`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`name`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`batchid`, `hive_metastore`.`dbt_shabbirkdb`.`DimCompany`.`enddate`].; line 69 pos 9
2023-04-20 11:25:58.254643 (MainThread): 11:25:58  
2023-04-20 11:25:58.254754 (MainThread): 11:25:58  Runtime Error in model DimAccount (models/incremental/DimAccount.sql)
2023-04-20 11:25:58.254859 (MainThread): 11:25:58    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `a`.`customerid`, `a`.`brokerid`, `a`.`accountid`, `c`.`batchid`].; line 30 pos 4
2023-04-20 11:25:58.254962 (MainThread): 11:25:58  
2023-04-20 11:25:58.255073 (MainThread): 11:25:58  Runtime Error in model DimCustomer (models/incremental/DimCustomer.sql)
2023-04-20 11:25:58.255177 (MainThread): 11:25:58    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`sk_customerid` cannot be resolved. Did you mean one of the following? [`c`.`customerid`, `p`.`iscustomer`, `c`.`batchid`, `c`.`country`, `c`.`firstname`].; line 19 pos 2
2023-04-20 11:25:58.255298 (MainThread): 11:25:58  
2023-04-20 11:25:58.255421 (MainThread): 11:25:58  Done. PASS=5 WARN=0 ERROR=4 SKIP=6 TOTAL=15
2023-04-20 11:25:59.597215 (Thread-363): handling poll request
2023-04-20 11:25:59.597716 (Thread-363): 11:25:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509b2c40>]}
2023-04-20 11:25:59.600011 (Thread-363): sending response (<Response 93853 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:25:59.983641 (Thread-364): handling ps request
2023-04-20 11:25:59.984128 (Thread-364): 11:25:59  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509b2d30>]}
2023-04-20 11:25:59.985141 (Thread-364): sending response (<Response 10152 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:26:00.452926 (Thread-365): handling status request
2023-04-20 11:26:00.453413 (Thread-365): 11:26:00  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509b2f70>]}
2023-04-20 11:26:00.453889 (Thread-365): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:26:20.467945 (Thread-366): handling status request
2023-04-20 11:26:20.468446 (Thread-366): 11:26:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509b2c70>]}
2023-04-20 11:26:20.468922 (Thread-366): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:26:20.475738 (Thread-367): handling list request
2023-04-20 11:26:20.476079 (Thread-367): 11:26:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509bb040>]}
2023-04-20 11:26:20.508389 (Thread-367): 11:26:20  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509a6460>]}
2023-04-20 11:26:20.508951 (Thread-367): 11:26:20  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:26:20.509322 (Thread-367): 11:26:20  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55aa2f460>]}
2023-04-20 11:26:20.511679 (Thread-367): sending response (<Response 5200 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:28:18.506029 (Thread-368): handling status request
2023-04-20 11:28:18.507932 (Thread-368): 11:28:18  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509bb0d0>]}
2023-04-20 11:28:18.508429 (Thread-368): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:28:21.945541 (Thread-369): handling status request
2023-04-20 11:28:21.946050 (Thread-369): 11:28:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509bb850>]}
2023-04-20 11:28:21.946529 (Thread-369): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:28:21.951587 (Thread-370): handling list request
2023-04-20 11:28:21.951952 (Thread-370): 11:28:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509bbe20>]}
2023-04-20 11:28:21.990373 (Thread-370): 11:28:21  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb553176490>]}
2023-04-20 11:28:21.991064 (Thread-370): 11:28:21  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:28:21.991420 (Thread-370): 11:28:21  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5649b4a90>]}
2023-04-20 11:28:21.993784 (Thread-370): sending response (<Response 5535 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:28:57.250909 (Thread-371): handling status request
2023-04-20 11:28:57.251399 (Thread-371): 11:28:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509fd610>]}
2023-04-20 11:28:57.251907 (Thread-371): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:14.729881 (Thread-372): handling status request
2023-04-20 11:29:14.730387 (Thread-372): 11:29:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509fdb20>]}
2023-04-20 11:29:14.730853 (Thread-372): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:14.868358 (Thread-373): handling ps request
2023-04-20 11:29:14.868806 (Thread-373): 11:29:14  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509fd5e0>]}
2023-04-20 11:29:14.869838 (Thread-373): sending response (<Response 10736 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:15.772652 (Thread-374): handling ps request
2023-04-20 11:29:15.773136 (Thread-374): 11:29:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509fdf40>]}
2023-04-20 11:29:15.774197 (Thread-374): sending response (<Response 10736 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:15.834253 (Thread-375): handling run_sql request
2023-04-20 11:29:15.834714 (Thread-375): 11:29:15  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509fdc40>]}
2023-04-20 11:29:18.573808 (Thread-375): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:18.609418 (MainThread): 11:29:18  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '668db86d-14d7-483c-a0c7-fac05e891036', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f273942bd30>]}
2023-04-20 11:29:18.610000 (MainThread): 11:29:18  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:29:18.611335 (Thread-1): 11:29:18  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:29:18.611597 (Thread-1): 11:29:18  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:29:18.615750 (Thread-1): 11:29:18  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:29:18.611648 => 2023-04-20 11:29:18.615577
2023-04-20 11:29:18.615965 (Thread-1): 11:29:18  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:29:18.616388 (Thread-1): 11:29:18  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:29:18.616754 (Thread-1): 11:29:18  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) sk_companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(replace(substring(value, 99, 8),'',NULL), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
      -- AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:29:18.616895 (Thread-1): 11:29:18  Opening a new connection, currently in state init
2023-04-20 11:29:19.259963 (Thread-376): handling ps request
2023-04-20 11:29:19.260620 (Thread-376): 11:29:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509d8dc0>]}
2023-04-20 11:29:19.262389 (Thread-376): sending response (<Response 11244 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:19.273978 (Thread-377): handling ps request
2023-04-20 11:29:19.274340 (Thread-377): 11:29:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509d8ee0>]}
2023-04-20 11:29:19.275192 (Thread-377): sending response (<Response 11244 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:19.803668 (Thread-378): handling status request
2023-04-20 11:29:19.804157 (Thread-378): 11:29:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc6a0>]}
2023-04-20 11:29:19.804666 (Thread-378): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:19.876212 (Thread-379): handling poll request
2023-04-20 11:29:19.876686 (Thread-379): 11:29:19  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc100>]}
2023-04-20 11:29:19.899397 (Thread-379): sending response (<Response 5779 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:20.051275 (Thread-1): 11:29:20  SQL status: OK in 1.43 seconds
2023-04-20 11:29:20.082037 (Thread-1): 11:29:20  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:29:18.616011 => 2023-04-20 11:29:20.081787
2023-04-20 11:29:20.082301 (Thread-1): 11:29:20  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:29:20.246259 (Thread-380): handling poll request
2023-04-20 11:29:20.246744 (Thread-380): 11:29:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc0d0>]}
2023-04-20 11:29:20.247347 (Thread-380): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:20.324639 (Thread-381): handling status request
2023-04-20 11:29:20.325128 (Thread-381): 11:29:20  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc8b0>]}
2023-04-20 11:29:20.325612 (Thread-381): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:21.778150 (Thread-382): handling poll request
2023-04-20 11:29:21.778642 (Thread-382): 11:29:21  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dcd00>]}
2023-04-20 11:29:21.783393 (Thread-382): sending response (<Response 171202 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:21.999738 (Thread-383): handling ps request
2023-04-20 11:29:22.000245 (Thread-383): 11:29:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc1c0>]}
2023-04-20 11:29:22.001321 (Thread-383): sending response (<Response 11269 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:22.441807 (Thread-384): handling status request
2023-04-20 11:29:22.442320 (Thread-384): 11:29:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc2b0>]}
2023-04-20 11:29:22.442857 (Thread-384): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:22.721170 (Thread-385): handling poll request
2023-04-20 11:29:22.721660 (Thread-385): 11:29:22  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dcd30>]}
2023-04-20 11:29:22.726412 (Thread-385): sending response (<Response 177597 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:23.661313 (Thread-386): handling status request
2023-04-20 11:29:23.661836 (Thread-386): 11:29:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dcfa0>]}
2023-04-20 11:29:23.662340 (Thread-386): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:29:23.958077 (Thread-387): handling ps request
2023-04-20 11:29:23.958575 (Thread-387): 11:29:23  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc130>]}
2023-04-20 11:29:23.959657 (Thread-387): sending response (<Response 11269 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:02.800258 (Thread-388): handling status request
2023-04-20 11:30:02.800774 (Thread-388): 11:30:02  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc8e0>]}
2023-04-20 11:30:02.801241 (Thread-388): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:28.897839 (Thread-389): handling run_sql request
2023-04-20 11:30:28.898363 (Thread-389): 11:30:28  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc760>]}
2023-04-20 11:30:28.935656 (Thread-390): handling ps request
2023-04-20 11:30:28.936562 (Thread-390): 11:30:28  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550927520>]}
2023-04-20 11:30:28.938425 (Thread-390): sending response (<Response 11782 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:29.455800 (Thread-391): handling poll request
2023-04-20 11:30:29.456279 (Thread-391): 11:30:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550927790>]}
2023-04-20 11:30:29.456805 (Thread-391): sending response (<Response 422 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:29.886633 (Thread-392): handling status request
2023-04-20 11:30:29.887113 (Thread-392): 11:30:29  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550927730>]}
2023-04-20 11:30:29.887661 (Thread-392): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:30.082616 (Thread-393): handling ps request
2023-04-20 11:30:30.083106 (Thread-393): 11:30:30  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550927850>]}
2023-04-20 11:30:30.084215 (Thread-393): sending response (<Response 11782 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:31.692840 (Thread-389): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:31.732487 (MainThread): 11:30:31  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '477a930a-62c6-4b88-bfd0-ae4ed9344772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f558e691e20>]}
2023-04-20 11:30:31.733105 (MainThread): 11:30:31  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:30:31.734494 (Thread-1): 11:30:31  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:30:31.734742 (Thread-1): 11:30:31  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:30:31.739426 (Thread-1): 11:30:31  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:30:31.734791 => 2023-04-20 11:30:31.739245
2023-04-20 11:30:31.739699 (Thread-1): 11:30:31  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:30:31.740064 (Thread-1): 11:30:31  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:30:31.740545 (Thread-1): 11:30:31  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) sk_companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
      -- AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:30:31.740700 (Thread-1): 11:30:31  Opening a new connection, currently in state init
2023-04-20 11:30:32.234945 (Thread-394): handling poll request
2023-04-20 11:30:32.235453 (Thread-394): 11:30:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509279d0>]}
2023-04-20 11:30:32.236126 (Thread-394): sending response (<Response 5806 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:32.250043 (Thread-395): handling ps request
2023-04-20 11:30:32.250558 (Thread-395): 11:30:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550927bb0>]}
2023-04-20 11:30:32.251737 (Thread-395): sending response (<Response 11777 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:32.292532 (Thread-396): handling ps request
2023-04-20 11:30:32.293005 (Thread-396): 11:30:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550927e20>]}
2023-04-20 11:30:32.294445 (Thread-396): sending response (<Response 11777 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:32.454760 (Thread-1): 11:30:32  Databricks adapter: Error while running:

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) sk_companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
      -- AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:30:32.454981 (Thread-1): 11:30:32  Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 41, pos 51)

== SQL ==

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) sk_companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
---------------------------------------------------^^^
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
      -- AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */

2023-04-20 11:30:32.455107 (Thread-1): 11:30:32  Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 41, pos 51)

== SQL ==

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) sk_companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
---------------------------------------------------^^^
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
      -- AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:601)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:500)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 41, pos 51)

== SQL ==

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) sk_companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
---------------------------------------------------^^^
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
      -- AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:354)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:340)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:354)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1024)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:351)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:229)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:309)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:305)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

2023-04-20 11:30:32.455221 (Thread-1): 11:30:32  Databricks adapter: operation-id: b'\x01\xed\xdfn\xc2\x9e\x19I\x9a\xe6\xfbZ\x0e\x1a\xe8\xfa'
2023-04-20 11:30:32.455418 (Thread-1): 11:30:32  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:30:31.739751 => 2023-04-20 11:30:32.455294
2023-04-20 11:30:32.455640 (Thread-1): 11:30:32  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:30:32.689674 (Thread-397): handling status request
2023-04-20 11:30:32.690198 (Thread-397): 11:30:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550927970>]}
2023-04-20 11:30:32.690686 (Thread-397): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:32.781302 (Thread-1): Got an exception: Runtime Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 41, pos 51)
  
  == SQL ==
  
  SELECT 
    * FROM (
    SELECT
      cast(cik as BIGINT) sk_companyid,
      st.st_name status,
      companyname name,
      ind.in_name industry,
      if(
        SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
        SPrating, 
        cast(null as string)) sprating, 
      CASE
        WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
        WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
        ELSE cast(null as boolean)
        END as islowgrade, 
      ceoname ceo,
      addrline1 addressline1,
      addrline2 addressline2,
      postalcode,
      city,
      stateprovince stateprov,
      country,
      description,
      foundingdate,
      nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
      1 batchid,
      date(pts) effectivedate,
      coalesce(
        lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
        cast('9999-12-31' as date)) enddate
    FROM (
      SELECT
        to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
        trim(substring(value, 19, 60)) AS CompanyName,
        trim(substring(value, 79, 10)) AS CIK,
        trim(substring(value, 89, 4)) AS Status,
        trim(substring(value, 93, 2)) AS IndustryID,
        trim(substring(value, 95, 4)) AS SPrating,
        to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
  ---------------------------------------------------^^^
        trim(substring(value, 107, 80)) AS AddrLine1,
        trim(substring(value, 187, 80)) AS AddrLine2,
        trim(substring(value, 267, 12)) AS PostalCode,
        trim(substring(value, 279, 25)) AS City,
        trim(substring(value, 304, 20)) AS StateProvince,
        trim(substring(value, 324, 24)) AS Country,
        trim(substring(value, 348, 46)) AS CEOname,
        trim(substring(value, 394, 150)) AS Description
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'CMP'
        -- AND trim(substring(value, 99, 8)) <> ''
         ) cmp
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
  )
  limit 500
  /* limit added automatically by dbt cloud */
  
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 423, in exception_handler
    yield
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 468, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 331, in execute
    self._cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 833, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 925, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 767, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/usr/local/lib/python3.8/dist-packages/databricks/sql/thrift_backend.py", line 484, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 41, pos 51)

== SQL ==

SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) sk_companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
---------------------------------------------------^^^
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
      -- AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 367, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 316, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 415, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 108, in execute
    _, execute_result = self.adapter.execute(compiled_sql, fetch=True)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/impl.py", line 121, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/base/impl.py", line 270, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 490, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 484, in add_query
    cursor.close()
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/dist-packages/dbt/adapters/databricks/connections.py", line 428, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 41, pos 51)
  
  == SQL ==
  
  SELECT 
    * FROM (
    SELECT
      cast(cik as BIGINT) sk_companyid,
      st.st_name status,
      companyname name,
      ind.in_name industry,
      if(
        SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
        SPrating, 
        cast(null as string)) sprating, 
      CASE
        WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
        WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
        ELSE cast(null as boolean)
        END as islowgrade, 
      ceoname ceo,
      addrline1 addressline1,
      addrline2 addressline2,
      postalcode,
      city,
      stateprovince stateprov,
      country,
      description,
      foundingdate,
      nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
      1 batchid,
      date(pts) effectivedate,
      coalesce(
        lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
        cast('9999-12-31' as date)) enddate
    FROM (
      SELECT
        to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
        trim(substring(value, 19, 60)) AS CompanyName,
        trim(substring(value, 79, 10)) AS CIK,
        trim(substring(value, 89, 4)) AS Status,
        trim(substring(value, 93, 2)) AS IndustryID,
        trim(substring(value, 95, 4)) AS SPrating,
        to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
  ---------------------------------------------------^^^
        trim(substring(value, 107, 80)) AS AddrLine1,
        trim(substring(value, 187, 80)) AS AddrLine2,
        trim(substring(value, 267, 12)) AS PostalCode,
        trim(substring(value, 279, 25)) AS City,
        trim(substring(value, 304, 20)) AS StateProvince,
        trim(substring(value, 324, 24)) AS Country,
        trim(substring(value, 348, 46)) AS CEOname,
        trim(substring(value, 394, 150)) AS Description
      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
      WHERE rectype = 'CMP'
        -- AND trim(substring(value, 99, 8)) <> ''
         ) cmp
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
  )
  limit 500
  /* limit added automatically by dbt cloud */
2023-04-20 11:30:32.783243 (Thread-1): Got exception RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': "Runtime Error in rpc request (from remote system.sql)\n  \n  [PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 41, pos 51)\n  \n  == SQL ==\n  \n  SELECT \n    * FROM (\n    SELECT\n      cast(cik as BIGINT) sk_companyid,\n      st.st_name status,\n      companyname name,\n      ind.in_name industry,\n      if(\n        SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n        SPrating, \n        cast(null as string)) sprating, \n      CASE\n        WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n        WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n        ELSE cast(null as boolean)\n        END as islowgrade, \n      ceoname ceo,\n      addrline1 addressline1,\n      addrline2 addressline2,\n      postalcode,\n      city,\n      stateprovince stateprov,\n      country,\n      description,\n      foundingdate,\n      nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n      1 batchid,\n      date(pts) effectivedate,\n      coalesce(\n        lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n        cast('9999-12-31' as date)) enddate\n    FROM (\n      SELECT\n        to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n        trim(substring(value, 19, 60)) AS CompanyName,\n        trim(substring(value, 79, 10)) AS CIK,\n        trim(substring(value, 89, 4)) AS Status,\n        trim(substring(value, 93, 2)) AS IndustryID,\n        trim(substring(value, 95, 4)) AS SPrating,\n        to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n  ---------------------------------------------------^^^\n        trim(substring(value, 107, 80)) AS AddrLine1,\n        trim(substring(value, 187, 80)) AS AddrLine2,\n        trim(substring(value, 267, 12)) AS PostalCode,\n        trim(substring(value, 279, 25)) AS City,\n        trim(substring(value, 304, 20)) AS StateProvince,\n        trim(substring(value, 324, 24)) AS Country,\n        trim(substring(value, 348, 46)) AS CEOname,\n        trim(substring(value, 394, 150)) AS Description\n      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n      WHERE rectype = 'CMP'\n        -- AND trim(substring(value, 99, 8)) <> ''\n         ) cmp\n    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n  )\n  limit 500\n  /* limit added automatically by dbt cloud */\n  ", 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) sk_companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP'\n      -- AND trim(substring(value, 99, 8)) <> ''\n       ) cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) sk_companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP'\n      -- AND trim(substring(value, 99, 8)) <> ''\n       ) cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/task/sql_commands.py", line 148, in _in_thread
    self.node_results.append(runner.safe_run(self.manifest))
  File "/usr/local/lib/python3.8/dist-packages/dbt/task/base.py", line 385, in safe_run
    result = self.error_result(ctx.node, error, started, [])
  File "/usr/local/lib/python3.8/dist-packages/dbt_rpc/rpc/node_runners.py", line 68, in error_result
    raise error
dbt_rpc.rpc.error.RPCException: RPCException(10001, Runtime error, {'type': 'DbtRuntimeError', 'message': "Runtime Error in rpc request (from remote system.sql)\n  \n  [PARSE_SYNTAX_ERROR] Syntax error at or near ',': extra input ','.(line 41, pos 51)\n  \n  == SQL ==\n  \n  SELECT \n    * FROM (\n    SELECT\n      cast(cik as BIGINT) sk_companyid,\n      st.st_name status,\n      companyname name,\n      ind.in_name industry,\n      if(\n        SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n        SPrating, \n        cast(null as string)) sprating, \n      CASE\n        WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n        WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n        ELSE cast(null as boolean)\n        END as islowgrade, \n      ceoname ceo,\n      addrline1 addressline1,\n      addrline2 addressline2,\n      postalcode,\n      city,\n      stateprovince stateprov,\n      country,\n      description,\n      foundingdate,\n      nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n      1 batchid,\n      date(pts) effectivedate,\n      coalesce(\n        lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n        cast('9999-12-31' as date)) enddate\n    FROM (\n      SELECT\n        to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n        trim(substring(value, 19, 60)) AS CompanyName,\n        trim(substring(value, 79, 10)) AS CIK,\n        trim(substring(value, 89, 4)) AS Status,\n        trim(substring(value, 93, 2)) AS IndustryID,\n        trim(substring(value, 95, 4)) AS SPrating,\n        to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n  ---------------------------------------------------^^^\n        trim(substring(value, 107, 80)) AS AddrLine1,\n        trim(substring(value, 187, 80)) AS AddrLine2,\n        trim(substring(value, 267, 12)) AS PostalCode,\n        trim(substring(value, 279, 25)) AS City,\n        trim(substring(value, 304, 20)) AS StateProvince,\n        trim(substring(value, 324, 24)) AS Country,\n        trim(substring(value, 348, 46)) AS CEOname,\n        trim(substring(value, 394, 150)) AS Description\n      FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n      WHERE rectype = 'CMP'\n        -- AND trim(substring(value, 99, 8)) <> ''\n         ) cmp\n    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n    JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n  )\n  limit 500\n  /* limit added automatically by dbt cloud */\n  ", 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) sk_companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP'\n      -- AND trim(substring(value, 99, 8)) <> ''\n       ) cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'compiled_code': "\nSELECT \n  * FROM (\n  SELECT\n    cast(cik as BIGINT) sk_companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(iff(trim(substring(value, 99, 8))='',,NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`\n    WHERE rectype = 'CMP'\n      -- AND trim(substring(value, 99, 8)) <> ''\n       ) cmp\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id\n  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id\n)\nlimit 500\n/* limit added automatically by dbt cloud */", 'tags': None}, None)
2023-04-20 11:30:33.018925 (Thread-398): handling poll request
2023-04-20 11:30:33.019448 (Thread-398): 11:30:33  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb550927400>]}
2023-04-20 11:30:33.020350 (Thread-398): sending response (<Response 50579 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:34.590274 (Thread-399): handling poll request
2023-04-20 11:30:34.590781 (Thread-399): 11:30:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509273d0>]}
2023-04-20 11:30:34.591642 (Thread-399): sending response (<Response 57684 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:34.739259 (Thread-400): handling poll request
2023-04-20 11:30:34.739786 (Thread-400): 11:30:34  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc760>]}
2023-04-20 11:30:34.740631 (Thread-400): sending response (<Response 57684 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:35.095295 (Thread-401): handling ps request
2023-04-20 11:30:35.095904 (Thread-401): 11:30:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509278e0>]}
2023-04-20 11:30:35.097190 (Thread-401): sending response (<Response 11800 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:35.718890 (Thread-402): handling status request
2023-04-20 11:30:35.719403 (Thread-402): 11:30:35  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dc280>]}
2023-04-20 11:30:35.719920 (Thread-402): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:51.904094 (Thread-403): handling run_sql request
2023-04-20 11:30:51.904671 (Thread-403): 11:30:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5509dcd90>]}
2023-04-20 11:30:51.906383 (Thread-404): handling ps request
2023-04-20 11:30:51.907013 (Thread-404): 11:30:51  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2ae8e0>]}
2023-04-20 11:30:51.908221 (Thread-404): sending response (<Response 12313 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:52.463254 (Thread-405): handling poll request
2023-04-20 11:30:52.464241 (Thread-405): 11:30:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2aeee0>]}
2023-04-20 11:30:52.465104 (Thread-405): sending response (<Response 421 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:52.870797 (Thread-406): handling status request
2023-04-20 11:30:52.871316 (Thread-406): 11:30:52  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2ae940>]}
2023-04-20 11:30:52.871866 (Thread-406): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:53.144420 (Thread-407): handling ps request
2023-04-20 11:30:53.144902 (Thread-407): 11:30:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2b5070>]}
2023-04-20 11:30:53.146477 (Thread-407): sending response (<Response 12313 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:54.671072 (Thread-403): sending response (<Response 133 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:54.708040 (MainThread): 11:30:54  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7a6925f4-d697-48be-ac1b-171a367fd662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd002133e20>]}
2023-04-20 11:30:54.708650 (MainThread): 11:30:54  Found 15 models, 1 test, 0 snapshots, 0 analyses, 595 macros, 0 operations, 0 seed files, 23 sources, 0 exposures, 0 metrics
2023-04-20 11:30:54.710031 (Thread-1): 11:30:54  Acquiring new databricks connection 'rpc.dbsql_dbt_tpch.request'
2023-04-20 11:30:54.710262 (Thread-1): 11:30:54  Began compiling node rpc.dbsql_dbt_tpch.request
2023-04-20 11:30:54.714934 (Thread-1): 11:30:54  Timing info for rpc.dbsql_dbt_tpch.request (compile): 2023-04-20 11:30:54.710310 => 2023-04-20 11:30:54.714765
2023-04-20 11:30:54.715155 (Thread-1): 11:30:54  Began executing node rpc.dbsql_dbt_tpch.request
2023-04-20 11:30:54.715551 (Thread-1): 11:30:54  Using databricks connection "rpc.dbsql_dbt_tpch.request"
2023-04-20 11:30:54.716017 (Thread-1): 11:30:54  On rpc.dbsql_dbt_tpch.request: 
SELECT 
  * FROM (
  SELECT
    cast(cik as BIGINT) sk_companyid,
    st.st_name status,
    companyname name,
    ind.in_name industry,
    if(
      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), 
      SPrating, 
      cast(null as string)) sprating, 
    CASE
      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false
      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true
      ELSE cast(null as boolean)
      END as islowgrade, 
    ceoname ceo,
    addrline1 addressline1,
    addrline2 addressline2,
    postalcode,
    city,
    stateprovince stateprov,
    country,
    description,
    foundingdate,
    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,
    1 batchid,
    date(pts) effectivedate,
    coalesce(
      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),
      cast('9999-12-31' as date)) enddate
  FROM (
    SELECT
      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,
      trim(substring(value, 19, 60)) AS CompanyName,
      trim(substring(value, 79, 10)) AS CIK,
      trim(substring(value, 89, 4)) AS Status,
      trim(substring(value, 93, 2)) AS IndustryID,
      trim(substring(value, 95, 4)) AS SPrating,
      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,
      trim(substring(value, 107, 80)) AS AddrLine1,
      trim(substring(value, 187, 80)) AS AddrLine2,
      trim(substring(value, 267, 12)) AS PostalCode,
      trim(substring(value, 279, 25)) AS City,
      trim(substring(value, 304, 20)) AS StateProvince,
      trim(substring(value, 324, 24)) AS Country,
      trim(substring(value, 348, 46)) AS CEOname,
      trim(substring(value, 394, 150)) AS Description
    FROM `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`FinWire`
    WHERE rectype = 'CMP'
      -- AND trim(substring(value, 99, 8)) <> ''
       ) cmp
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`StatusType` st ON cmp.status = st.st_id
  JOIN `roberto_salcido_tpcdi_dlt_advanced_10_wh`.`Industry` ind ON cmp.industryid = ind.in_id
)
limit 500
/* limit added automatically by dbt cloud */
2023-04-20 11:30:54.716165 (Thread-1): 11:30:54  Opening a new connection, currently in state init
2023-04-20 11:30:55.010505 (Thread-408): handling poll request
2023-04-20 11:30:55.010983 (Thread-408): 11:30:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2b5130>]}
2023-04-20 11:30:55.011603 (Thread-408): sending response (<Response 5805 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:55.241411 (Thread-409): handling ps request
2023-04-20 11:30:55.241900 (Thread-409): 11:30:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2b57c0>]}
2023-04-20 11:30:55.265885 (Thread-409): sending response (<Response 12308 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:55.300630 (Thread-410): handling ps request
2023-04-20 11:30:55.301053 (Thread-410): 11:30:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2b55b0>]}
2023-04-20 11:30:55.302101 (Thread-410): sending response (<Response 12308 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:55.428540 (Thread-411): handling status request
2023-04-20 11:30:55.429013 (Thread-411): 11:30:55  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2b5b50>]}
2023-04-20 11:30:55.429485 (Thread-411): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:56.033257 (Thread-412): handling poll request
2023-04-20 11:30:56.033751 (Thread-412): 11:30:56  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2b5310>]}
2023-04-20 11:30:56.034349 (Thread-412): sending response (<Response 5805 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:56.101486 (Thread-1): 11:30:56  SQL status: OK in 1.38 seconds
2023-04-20 11:30:56.132607 (Thread-1): 11:30:56  Timing info for rpc.dbsql_dbt_tpch.request (execute): 2023-04-20 11:30:54.715200 => 2023-04-20 11:30:56.132383
2023-04-20 11:30:56.132860 (Thread-1): 11:30:56  On rpc.dbsql_dbt_tpch.request: Close
2023-04-20 11:30:57.588262 (Thread-413): handling poll request
2023-04-20 11:30:57.588734 (Thread-413): 11:30:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2aef70>]}
2023-04-20 11:30:57.594356 (Thread-413): sending response (<Response 181479 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:57.644027 (Thread-414): handling poll request
2023-04-20 11:30:57.644439 (Thread-414): 11:30:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2aee20>]}
2023-04-20 11:30:57.649523 (Thread-414): sending response (<Response 176089 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:57.863189 (Thread-415): handling ps request
2023-04-20 11:30:57.863682 (Thread-415): 11:30:57  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2aefd0>]}
2023-04-20 11:30:57.864778 (Thread-415): sending response (<Response 12333 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:58.514944 (Thread-416): handling status request
2023-04-20 11:30:58.515457 (Thread-416): 11:30:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2ae340>]}
2023-04-20 11:30:58.515983 (Thread-416): sending response (<Response 1600 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:30:58.739606 (Thread-417): handling ps request
2023-04-20 11:30:58.740074 (Thread-417): 11:30:58  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2ae490>]}
2023-04-20 11:30:58.741468 (Thread-417): sending response (<Response 12333 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:31:09.027237 (Thread-418): 11:31:09  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:31:09.051973 (Thread-418): 11:31:09  Unable to do partial parsing because config vars, config profile, or config target have changed
2023-04-20 11:31:09.052382 (Thread-418): 11:31:09  previous checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, current checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886
2023-04-20 11:31:09.052593 (Thread-418): 11:31:09  Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54e2ae550>]}
2023-04-20 11:31:09.522040 (Thread-419): handling status request
2023-04-20 11:31:09.522556 (Thread-419): 11:31:09  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5559ad2e0>]}
2023-04-20 11:31:09.523011 (Thread-419): sending response (<Response 163 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:31:10.877985 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/DimTrade.sql
2023-04-20 11:31:10.881386 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/DimCustomerStg.sql
2023-04-20 11:31:10.884657 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/FactCashBalances.sql
2023-04-20 11:31:10.887586 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/FactHoldings.sql
2023-04-20 11:31:10.890552 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/FactMarketHistory.sql
2023-04-20 11:31:10.893269 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
2023-04-20 11:31:10.896396 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/DimCustomer.sql
2023-04-20 11:31:10.899544 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/Prospect.sql
2023-04-20 11:31:10.902585 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/DimAccount.sql
2023-04-20 11:31:10.905358 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
2023-04-20 11:31:10.908650 (Thread-418): 11:31:10  1699: static parser successfully parsed incremental/FactWatches.sql
2023-04-20 11:31:10.911550 (Thread-418): 11:31:10  1699: static parser successfully parsed silver/DimSecurity.sql
2023-04-20 11:31:10.914167 (Thread-418): 11:31:10  1699: static parser successfully parsed silver/DimBroker.sql
2023-04-20 11:31:10.916944 (Thread-418): 11:31:10  1699: static parser successfully parsed silver/Financial.sql
2023-04-20 11:31:10.920035 (Thread-418): 11:31:10  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:31:11.072956 (Thread-418): 11:31:11  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdad1c0>]}
2023-04-20 11:31:11.920850 (Thread-420): handling status request
2023-04-20 11:31:11.921382 (Thread-420): 11:31:11  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bdad8b0>]}
2023-04-20 11:31:11.922019 (Thread-420): sending response (<Response 6812 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:31:13.103293 (Thread-421): 11:31:13  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:31:13.383899 (Thread-421): 11:31:13  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
2023-04-20 11:31:13.384540 (Thread-421): 11:31:13  Partial parsing: updated file: dbsql_dbt_tpch://models/silver/DimCompany.sql
2023-04-20 11:31:13.394538 (Thread-421): 11:31:13  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:31:13.456603 (Thread-421): 11:31:13  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd2b8b0>]}
2023-04-20 11:31:13.613511 (Thread-422): handling status request
2023-04-20 11:31:13.614014 (Thread-422): 11:31:13  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd1dd60>]}
2023-04-20 11:31:13.614505 (Thread-422): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:32:53.740450 (Thread-423): handling status request
2023-04-20 11:32:53.742397 (Thread-423): 11:32:53  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bd1d700>]}
2023-04-20 11:32:53.742877 (Thread-423): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:33:00.512514 (Thread-424): 11:33:00  checksum: 0049c9ca814b191acba678e7b1a0d7a85eeac3e13700309cfc1ce1a2c78d3886, vars: {}, profile: user, target: None, version: 1.4.6
2023-04-20 11:33:00.941303 (Thread-424): 11:33:00  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
2023-04-20 11:33:00.941916 (Thread-424): 11:33:00  Partial parsing: updated file: dbsql_dbt_tpch://models/silver/DimCompany.sql
2023-04-20 11:33:00.949506 (Thread-424): 11:33:00  1699: static parser successfully parsed silver/DimCompany.sql
2023-04-20 11:33:01.010538 (Thread-424): 11:33:01  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb6a9d0>]}
2023-04-20 11:33:01.038727 (Thread-425): handling status request
2023-04-20 11:33:01.039124 (Thread-425): 11:33:01  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb64a60>]}
2023-04-20 11:33:01.039607 (Thread-425): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:33:37.926550 (Thread-426): handling status request
2023-04-20 11:33:37.927047 (Thread-426): 11:33:37  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb64b80>]}
2023-04-20 11:33:37.927544 (Thread-426): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:33:38.023009 (Thread-427): handling ps request
2023-04-20 11:33:38.023430 (Thread-427): 11:33:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb64a30>]}
2023-04-20 11:33:38.024567 (Thread-427): sending response (<Response 12333 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:37:32.897275 (Thread-428): handling status request
2023-04-20 11:37:32.899177 (Thread-428): 11:37:32  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb67370>]}
2023-04-20 11:37:32.899694 (Thread-428): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:38:36.571249 (Thread-429): handling status request
2023-04-20 11:38:36.573101 (Thread-429): 11:38:36  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb64f10>]}
2023-04-20 11:38:36.573581 (Thread-429): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
2023-04-20 11:39:38.301114 (Thread-430): handling status request
2023-04-20 11:39:38.301622 (Thread-430): 11:39:38  Sending event: {'category': 'dbt', 'action': 'rpc_request', 'label': '04a0933b-fb5e-4307-8a5b-81a7dd09e200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb54bb675e0>]}
2023-04-20 11:39:38.302124 (Thread-430): sending response (<Response 1913 bytes [200 OK]>) to 10.0.136.211
